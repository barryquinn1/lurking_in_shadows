@ARTICLE{Clausing2016-uc,
  title    = "The Effect of Profit Shifting on the Corporate Tax Base in the
              United States and Beyond",
  author   = "Clausing, Kimberly A",
  abstract = "This paper estimates the effect of profit shifting on corporate
              tax base erosion for the United States. Using Bureau of Economic
              Analysis survey data on U.S. mu",
  month    =  jun,
  year     =  2016,
  keywords = "international taxation, corporate taxation, transfer pricing,
              BEPS, corporate tax base erosion, profit shifting, income
              shifting"
}

@TECHREPORT{Heckemeyer2013-to,
  title       = "Multinationals' Profit Response to Tax Differentials: Effect
                 Size and Shifting Channels",
  author      = "Heckemeyer, Jost and Overesch, Michael",
  abstract    = "This paper provides a quantitative review of the empirical
                 literature on profit-shifting behavior of multinational firms.
                 We synthesize the evidence from 25 stu",
  number      = "Discussion Paper No. 13-045",
  institution = "Centre for European Economic Research",
  month       =  jul,
  year        =  2013,
  keywords    = "Profit Shifting, Multinational Firm, Corporate Income Tax,
                 Meta-Analysis"
}

@BOOK{Andreff2015-wj,
  title     = "Disequilibrium Sports Economics: Competitive Imbalance and
               Budget Constraints",
  author    = "Andreff, Wladimir",
  abstract  = "For decades, sports economics has been set within the framework
               of equilibrium economics, in particular when modelling team
               sport leagues. Based on a conviction that this does not reflect
               real life, this book addresses a gap in the literature and opens
               up a new research area by applying concepts drawn from
               disequilibrium economics. It is divided into two parts, the
               first of which focuses on economic disequilibrium in sports
               markets and competitive imbalance in sporting contests. The
               second part concentrates on soft budget constraints and their
               consequences for club governance and management.",
  publisher = "Edward Elgar Publishing",
  month     =  sep,
  year      =  2015,
  language  = "en"
}

@MISC{Andreff_undated-qx,
  title   = "A new research area: disequilibrium sports economics",
  author  = "Andreff, Wladimir and Wladimir, Andreff",
  journal = "Disequilibrium Sports Economics",
  pages   = "1--8"
}

@ARTICLE{Dobson1998-ng,
  title     = "Performance and revenue in professional league football:
               evidence from Granger causality tests",
  author    = "Dobson, S M and Goddard, J A",
  abstract  = "Using a dataset comprising annual performance (measured by final
               league position) and gate revenue for 77 Football League clubs
               which maintained unbroken league membership between 1946 and
               1994, the relationship between performance and revenue is
               investigated using cointegration and causality tests. A
               cointegrating relationship between performance and revenue is
               established in only 10 cases out of 77, although it is argued
               that some caution is required in interpreting these results, due
               to the low power of the relevant tests in relatively small
               samples. In Granger causality tests, more evidence is found of
               causality running from lagged revenue to current performance
               than of causality in the opposite direction, while the
               dependence of performance on revenue seems to be greater for the
               smaller clubs than for the larger. These results lend empirical
               support to the popular view that, unless checked by mechanisms
               for revenue redistribution within the league, the natural
               tendency is for success to become concentrated increasingly
               among a small group of elite, wealthy clubs.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  30,
  number    =  12,
  pages     = "1641--1651",
  month     =  dec,
  year      =  1998
}

@ARTICLE{noauthor_undated-ur,
  title = "[{PDF]Benchmarking} - Commission for Energy Regulation"
}

@ARTICLE{Kuosmanen2014-om,
  title     = "Applying insights from production theory and frontier estimation
               to sustainability assessment in agriculture",
  author    = "Kuosmanen, Natalia and {Others}",
  journal   = "MTT Science",
  publisher = "Helsingin yliopisto",
  year      =  2014
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Farrell1957-sl,
  title   = "The Measurement of Productive EfÔ¨Åciency",
  author  = "Farrell, M J",
  journal = "Journal of Royal Statistical Society: Series A",
  volume  =  20,
  number  =  3,
  pages   = "253--290",
  year    =  1957
}

@ARTICLE{Meeusen1977-aj,
  title     = "Technical efficiency and dimension of the firm: Some results on
               the use of frontier production functions",
  author    = "Meeusen, W and van den Broeck, J",
  abstract  = "Estimates of the average technical efficiency on the 2-digit
               industry-level for France (1962) are obtained by means of a
               maximum likelihood estimation of the Cobb-Douglas frontier
               production model with composed error. The theoretical and
               statistical implications of a possible relationship between
               technical efficiency and firm-size are discussed. The
               computations are carried out for each industry as a whole and
               the results are compared with the respective results for
               separate size-classes.",
  journal   = "Empir. Econ.",
  publisher = "Physica-Verlag",
  volume    =  2,
  number    =  2,
  pages     = "109--122",
  month     =  jun,
  year      =  1977,
  language  = "en"
}

@INCOLLECTION{Meucci2010-ao,
  title     = "{Black--Litterman} Approach",
  booktitle = "Encyclopedia of Quantitative Finance",
  author    = "Meucci, Attilio",
  abstract  = "We take the reader through the Black--Litterman approach,
               providing all the proofs. We show how minor modifications of the
               original model greatly improve its range of applications. We
               discuss full generalizations of this and related models",
  publisher = "John Wiley \& Sons, Ltd",
  year      =  2010,
  keywords  = "views; shrinkage; equilibrium; nonnormal markets; entropy
               pooling"
}

@ARTICLE{Satchell2000-wy,
  title     = "A demystification of the {Black--Litterman} model: Managing
               quantitative and traditional portfolio construction",
  author    = "Satchell, S and Scowcroft, A",
  abstract  = "The purpose of this paper is to present details of Bayesian
               portfolio construction procedures which have become known in the
               asset management industry as Black--Litterman models. We explain
               their construction, present some extensions and argue that these
               models are valuable tools for financial management.Journal of
               Asset Management (2000) 1, 138--150;
               doi:10.1057/palgrave.jam.2240011",
  journal   = "Journal of Asset Management",
  publisher = "ingentaconnect.com",
  volume    =  1,
  number    =  2,
  pages     = "138--150",
  month     =  sep,
  year      =  2000
}

@ARTICLE{Idzorek2002-st,
  title     = "A step-by-step guide to the {Black-Litterman} model",
  author    = "Idzorek, Thomas M",
  abstract  = "Abstract The Black -- Litterman model enables investors to
               combine their unique views regarding the performance of various
               assets with the market equilibrium in a manner that results in
               intuitive, diversified portfolios. This paper consolidates
               insights from the ...",
  journal   = "Forecasting expected returns in the financial markets",
  publisher = "books.google.com",
  pages     = "17",
  year      =  2002
}

@ARTICLE{He2002-mu,
  title     = "The Intuition Behind {Black-Litterman} Model Portfolios",
  author    = "He, Guangliang and Litterman, Robert",
  abstract  = "In this article we demonstrate that the optimal portfolios
               generated by the Black-Litterman asset allocation model have a
               very simple, intuitive property. The",
  journal   = "Available at SSRN 334304",
  publisher = "papers.ssrn.com",
  month     =  oct,
  year      =  2002,
  keywords  = "Baysian, Black-Litterman model, CAPM, mean-variance analysis,
               portfolio selection"
}

@ARTICLE{Black1992-fy,
  title     = "Global Portfolio Optimization",
  author    = "Black, Fischer and Litterman, Robert",
  abstract  = "Consideration of the global CAPM equilibrium can significantly
               improve the usefulness of these models. In particular,
               equilibrium returns for equities, bonds and currencies provide
               neutral starting points for estimating the set of expected
               excess returns needed to drive the ...",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  48,
  number    =  5,
  pages     = "28--43",
  month     =  sep,
  year      =  1992
}

@ARTICLE{Lindholm2016-jk,
  title     = "Can {I} please have a slice of Ronaldo? The legality of {FIFA's}
               ban on third-party ownership under European union law",
  author    = "Lindholm, Johan",
  abstract  = "Trading football players is a massive, global business where
               several single trades have brought in more than \$100 million in
               transfer fees. This has brought about a business practice where
               third-party investors provide or receive a share of the transfer
               fees, so-called third-party ownership. However, effective May 1,
               2015, F{\'e}d{\'e}ration Internationale de Football Association
               (FIFA) has imposed a worldwide ban on this practice. In the face
               of already-initiated legal challenges, this article considers
               whether the ban is compatible with European Union law. It finds
               that the ban violates both free movement rights and competition
               law and that FIFA would be well served to consider a
               less-restrictive measure.",
  journal   = "Int Sports Law J",
  publisher = "Springer International Publishing",
  volume    =  15,
  number    = "3-4",
  pages     = "137--148",
  month     =  jan,
  year      =  2016,
  language  = "en"
}

@ARTICLE{McKillop2011-fm,
  title     = "Credit Unions: A Theoretical and Empirical Overview",
  author    = "McKillop, Donal and Wilson, John O S",
  abstract  = "In 2009 there were over 49,330 credit unions across 98 countries
               with more than 184 million members and approximately \$1,354
               billion in assets. There is a great diversity within the credit
               union movement across these countries. This reflects the various
               economic, historic and cultural contexts within which credit
               unions operate. This paper traces the evolution of the credit
               union movement. It examines credit union objectives, and
               considers issues relating to efficiency, technology adoption,
               product diversification, merger, failure and demutualization.
               The regulatory environment within which credit unions operate is
               also explored under the themes of interest rate regulation,
               common bond requirements, taxation, deposit insurance and
               capital regulation. The overview also considers demutualization
               and the costs and benefits to credit unions of altering their
               organizational form.",
  journal   = "Financial Markets, Institutions \& Instruments",
  publisher = "Blackwell Publishing Inc",
  volume    =  20,
  number    =  3,
  pages     = "79--123",
  month     =  aug,
  year      =  2011,
  keywords  = "Credit unions; common bond; diversification; efficiency; merger
               and failure; regulation; technology; G21"
}

@ARTICLE{Bauer2009-oj,
  title    = "The effect of mergers on credit union performance",
  author   = "Bauer, Keldon J and Miles, Linda L and Nishikawa, Takeshi",
  abstract = "The motivation for mergers in the credit union industry differs
              from the commercial bank industry due to the lack of residual
              claimants to benefit from wealth gains. In the cooperative
              ownership environment of credit unions, the owners/members gain
              utility via the rates offered for loans and deposits. Credit
              union regulators also gain utility when mergers remove risky
              credit unions from the industry. We measure these utility gains
              using the event study method of Bauer [Bauer, K., 2008. Detecting
              abnormal credit union performance. Journal of Banking and Finance
              32, 573--586] employing quadrant tests based on a multivariate
              test of equality of centroids. We find gains to the
              owners/members of the target credit union and to the regulators
              but not to the acquiring firm. We posit that the acquiring credit
              unions may encounter regulatory pressure to merge. In addition,
              the owners/members of the acquiring firm may avoid potential
              disutility in the cooperative insurance environment were the
              target firm allowed to fail.",
  journal  = "Journal of Banking \& Finance",
  volume   =  33,
  number   =  12,
  pages    = "2267--2274",
  month    =  dec,
  year     =  2009,
  keywords = "Mergers; Credit unions"
}

@ARTICLE{Wheelock2013-pg,
  title    = "The evolution of cost-productivity and efficiency among {US}
              credit unions",
  author   = "Wheelock, David C and Wilson, Paul W",
  abstract = "Advances in information-processing technology have eroded the
              advantages of small scale and proximity to customers that
              traditionally enabled small lenders to thrive. Nonetheless, the
              membership and market share of US credit unions have increased,
              though their average size has also risen. We investigate changes
              in the efficiency and productivity of US credit unions during
              1989--2006 by benchmarking the performance of individual firms
              against an estimated order-$\alpha$ quantile lying ``near'' the
              efficient frontier. We construct a cost analog of the Malmquist
              productivity index, which we decompose to estimate changes in
              cost and scale efficiency, and changes in technology. We find
              that cost-productivity fell on average across all credit unions
              but especially among smaller credit unions. Smaller credit unions
              confronted a shift in technology that increased the minimum cost
              required to produce given amounts of output. All but the largest
              credit unions also became less scale efficient over time.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  1,
  pages    = "75--88",
  month    =  jan,
  year     =  2013,
  keywords = "Efficiency; Productivity; Quantile estimation; Credit unions"
}

@ARTICLE{Brown2009-tw,
  title    = "Capital management in mutual financial institutions",
  author   = "Brown, Christine and Davis, Kevin",
  abstract = "Capital management by mutual financial institutions (such as
              credit unions) provides a valuable testing ground for assessing
              the impact of capital regulation and theories of managerial
              behaviour in financial institutions. Limited access to external
              equity capital means that capital accumulation must be met
              primarily by reliance on retained earnings. To deal with shocks
              to the capital position and avoid breaching regulatory
              requirements, managers will aim to have a buffer of capital in
              excess of the regulatory minimum. Moreover, mutual governance
              arrangements and an absence of capital market discipline mean
              that managers have discretion to set target capital ratios which
              differ significantly from industry averages. This paper develops
              a formal model of capital management and risk management in
              mutual financial institutions such as credit unions which
              reflects these industry characteristics. The model is tested
              using data from larger credit unions in Australia, which have
              been subject to the Basel Accord Risk Weighted Capital
              Requirements since 1993. The data supports the hypothesis that
              credit unions manage their capital position by setting a short
              term target profit rate (return on assets) which is positively
              related to asset growth and which is aimed at gradually removing
              discrepancies between the actual and desired capital ratio.
              Desired capital ratios vary significantly across credit unions.
              There is little evidence of short run adjustments to the risk of
              the asset portfolio to achieve a desired capital position.",
  journal  = "Journal of Banking \& Finance",
  volume   =  33,
  number   =  3,
  pages    = "443--455",
  month    =  mar,
  year     =  2009,
  keywords = "Credit unions; Capital management; Capital requirements"
}

@ARTICLE{Goddard2008-io,
  title    = "The diversification and financial performance of {US} credit
              unions",
  author   = "Goddard, John and McKillop, Donal and Wilson, John O S",
  abstract = "For US credit unions, revenue from non-interest sources has
              increased significantly in recent years. We investigate the
              impact of revenue diversification on financial performance for
              the period 1993--2004. The impact of a change in strategy that
              alters the share of non-interest income is decomposed into a
              direct exposure effect, reflecting the difference between
              interest and non-interest bearing activities, and an indirect
              exposure effect which reflects the effect of the institution's
              own degree of diversification. On both risk-adjusted and
              unadjusted returns measures, a positive direct exposure effect is
              outweighed by a negative indirect exposure effect for all but the
              largest credit unions. This may imply that similar
              diversification strategies are not appropriate for large and
              small credit unions. Small credit unions should eschew
              diversification and continue to operate as simple savings and
              loan institutions, while large credit unions should be encouraged
              to exploit new product opportunities around their core expertise.",
  journal  = "Journal of Banking \& Finance",
  volume   =  32,
  number   =  9,
  pages    = "1836--1849",
  month    =  sep,
  year     =  2008,
  keywords = "Credit unions; Diversification; Risk; Performance"
}

@ARTICLE{Bauer2008-ib,
  title    = "Detecting abnormal credit union performance",
  author   = "Bauer, Keldon",
  abstract = "Credit unions are an important financial intermediary, but little
              credit union research is done. A primary reason for the lack of
              research is the cooperative nature of the industry, making
              traditional methods of detecting abnormal performance
              inappropriate. This paper proposes two methods of detecting
              abnormal performance, one parametric, the other non-parametric.
              Instead of testing the efficiency of the institution, this paper
              proposes testing the return vector, as indicated in the
              theoretical objective function of the member. Simulations
              demonstrate that both methods are correctly specified and
              powerful. ?? 2007 Elsevier B.V. All rights reserved.",
  journal  = "Journal of Banking \& Finance",
  volume   =  32,
  number   =  4,
  pages    = "573--586",
  month    =  apr,
  year     =  2008,
  keywords = "Credit union; Event study; Non-parametric"
}

@BOOK{Hirsch1977-tq,
  title     = "Social limits to growth",
  author    = "Hirsch, Fred",
  abstract  = "The promise of economic growth which has dominated society for
               so long has reached an impasse. In his classic analysis, Fred
               Hirsch argued that the causes of this were essentially social
               rather than physical. Affluence brings its own problems. As
               societies become richer, ...",
  publisher = "Routledge",
  year      =  1977
}

@MISC{Neill2016-et,
  title        = "Regulating {U.S}. Banks: Which Regulatory Characteristics
                  Engender Financial Stability in the Regulated Banking System?",
  author       = "Neill, Ashleigh",
  month        =  sep,
  year         =  2016,
  howpublished = "Queen's Unversity Management School MSc Dissertation"
}

@ARTICLE{Powell_undated-zr,
  title  = " Endogeniety in nonparmetric and semiparametric models",
  author = "{Powell} and {Blundell}"
}

@BOOK{Birchall2013-gr,
  title     = "Resilience in a downturn: The power of financial cooperatives",
  author    = "Birchall, Johnston and Office, International Labour",
  abstract  = "Jump to navigation. The Canadian CED Network. Strengthening
               Canada's Communities. Fran{\c c}ais; English. Search form.
               Search. Follow CCEDNet Online: Facebook; Twitter; YouTube.
               Resilience in a downturn: The power of financial cooperatives .
               You are here. ...",
  publisher = "ILO",
  year      =  2013
}

@UNPUBLISHED{Wilcox2016-nm,
  title  = "The Increasing Interactions Between Banks, Credit Unions, and Small
            Business",
  author = "Wilcox, James A",
  year   =  2016
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Foucault2016-go,
  title    = "News Trading and Speed",
  author   = "Foucault, Thierry and Hombert, Johan and Ro{\c S}u, Ioanid",
  abstract = "We compare the optimal trading strategy of an informed speculator
              when he can trade ahead of incoming news (is ``fast''), versus
              when he cannot (is ``slow''). We find that speed matters: the
              fast speculator's trades account for a larger fraction of trading
              volume, and are more correlated with short‚Äêrun price changes.
              Nevertheless, he realizes a large fraction of his profits from
              trading on long‚Äêterm price changes. The fast speculator's
              behavior matches evidence about high‚Äêfrequency traders. We
              predict that stocks with more informative news are more liquid
              even though they attract more activity from informed
              high‚Äêfrequency traders.",
  journal  = "J. Finance",
  volume   =  71,
  number   =  1,
  pages    = "335--382",
  month    =  feb,
  year     =  2016
}

@MISC{Cleliac2016-pa,
  title        = "Bank business models in Europe: why does it matter for the
                  future of regulation and resolution? - International Research
                  Centre on Cooperative Finance",
  booktitle    = "International Research Centre on Cooperative Finance",
  author       = "{cleliac}",
  abstract     = "In this changing context of evolving market structures and
                  regulations, the bank business models analysis can provide
                  market participants, depositors, creditors, regulators and
                  supervisors with a useful tool to better understand the
                  nature of risk attached to each bank business model and its
                  contribution to systemic risk throughout the economic cycle.",
  month        =  jul,
  year         =  2016,
  howpublished = "\url{http://financecoop.hec.ca/en/publications/bank-business-models-europe-future-regulation-resolution/}",
  note         = "Accessed: 2016-7-21"
}

@MISC{Cleliac2016-um,
  title        = "Regulatory Arbitrage in {EU} Banking: Do Business Models
                  Matter? - International Research Centre on Cooperative
                  Finance",
  booktitle    = "International Research Centre on Cooperative Finance",
  author       = "{cleliac}",
  abstract     = "This paper has three main aims: differentiate levels of bank
                  risk, explain those differences via the adoption of IRB and
                  RWA dispersion and explore the degree of regulatory arbitrage
                  varies across bank business models.",
  month        =  jul,
  year         =  2016,
  howpublished = "\url{http://financecoop.hec.ca/en/publications/regulatory-arbitrage-eu-banking-business-models/}",
  note         = "Accessed: 2016-7-21"
}

@ARTICLE{Ayadi2015-pg,
  title   = "A study on international regulatory compliance and bank
             prerformance",
  author  = "Ayadi, Rym and Ben Naceur, Sami and Casu, Barbara and Quinn, Barry",
  journal = "IMF Working Paper Series",
  year    =  2015
}

@ARTICLE{Taylor1971-fq,
  title     = "The Credit Union as A Cooperative Institution",
  author    = "Taylor, Ryland A",
  abstract  = "A Credit Union is a rather unique social institution. It is a
               financial intermediary, gathering. savings from its members by
               selling deposit type savings accounts called shares and lending
               these. funds to its members in the form of consumer loans. It is
               also a cooperative, ...",
  journal   = "Rev. Soc. Econ.",
  publisher = "Routledge",
  volume    =  29,
  number    =  2,
  pages     = "207--217",
  month     =  sep,
  year      =  1971
}

@MISC{Pesaran2006-td,
  title   = "Forecasting Time Series Subject to Multiple Structural Breaks",
  author  = "Pesaran, M Hashem and Hashem Pesaran, M and Pettenuzzo, Davide and
             Timmermann, Allan",
  journal = "Review of Economic Studies",
  volume  =  73,
  number  =  4,
  pages   = "1057--1084",
  year    =  2006
}

@ARTICLE{Conefrey2011-ay,
  title     = "The macro-economic impact of changing the rate of corporation
               tax",
  author    = "Conefrey, Thomas and Fitz Gerald, John D",
  abstract  = "This paper considers the impact of changes in the rate of
               corporation tax in Ireland affecting the business and financial
               services sector. A model is estimated that relates services
               exports and output to world activity, competitiveness and the
               rate of corporation tax. This model indicates that a reduction
               in the rate of corporation tax in the 1990s stimulated exports
               and, even allowing for profit repatriations by foreign firms and
               replacement of lost tax revenue, it resulted in an increase in
               domestic output. The increase in profitability suggests that
               some of the increased output involved relocation of profits to
               Ireland by multinational firms.",
  journal   = "Econ. Model.",
  publisher = "Elsevier",
  volume    =  28,
  number    =  3,
  pages     = "991--999",
  month     =  may,
  year      =  2011,
  keywords  = "Corporation tax; Ireland; Services sector"
}

@ARTICLE{Hoff2007-jx,
  title    = "Second stage {DEA}: Comparison of approaches for modelling the
              {DEA} score",
  author   = "Hoff, Ayoe",
  abstract = "Tobit regression is often encountered in second stage data
              envelopment analysis (DEA), i.e. when the relationship between
              exogenous factors (non-physical inputs) and DEA efficiency scores
              is assessed. It is however not obvious that tobit is the only, or
              optimal, approach to modelling DEA scores. This paper presents
              two alternative approaches to second stage DEA, the results of
              which are compared with the tobit approach through a case study
              for the Danish fishery. Furthermore the three models are compared
              to OLS regression, representing a linear approximation to the
              models. It is firstly concluded that the tobit approach will in
              most cases be sufficient in representing second stage DEA models.
              Secondly it is shown that OLS may actually in many cases replace
              tobit as a sufficient second stage DEA model.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  181,
  number   =  1,
  pages    = "425--435",
  month    =  aug,
  year     =  2007,
  keywords = "Data envelopment analysis; Non-linear regression; Second stage
              DEA; Tobit regression"
}

@ARTICLE{Arellano1991-og,
  title    = "Some Tests of Specification for Panel Data: Monte Carlo Evidence
              and an Application to Employment Equations",
  author   = "Arellano, Manuel and Bond, Stephen",
  abstract = "This paper presents specification tests that are applicable after
              estimating a dynamic model from panel data by the generalized
              method of moments (GMM), and studies the practical performance of
              these procedures using both generated and real data. Our GMM
              estimator optimally exploits all the linear moment restrictions
              that follow from the assumption of no serial correlation in the
              errors, in an equation which contains individual effects, lagged
              dependent variables and no strictly exogenous variables. We
              propose a test of serial correlation based on the GMM residuals
              and compare this with Sargan tests of over-identifying
              restrictions and Hausman specification tests. [ABSTRACT FROM
              AUTHOR]",
  journal  = "Rev. Econ. Stud.",
  volume   =  58,
  number   =  2,
  pages    = "277",
  month    =  apr,
  year     =  1991,
  keywords = "ESTIMATION theory; GAMES of chance (Mathematics); MATHEMATICAL
              models; MOMENTS method (Statistics); MONTE Carlo method;
              NUMERICAL analysis; NUMERICAL calculations; SPECIFICATIONS;
              STOCHASTIC processes"
}

@ARTICLE{Phillips2007-qu,
  title    = "Bias in dynamic panel estimation with fixed effects, incidental
              trends and cross section dependence",
  author   = "Phillips, Peter C B and Sul, Donggyu",
  abstract = "Explicit asymptotic bias formulae are given for dynamic panel
              regression estimators as the cross section sample size N ??? ???.
              The results extend earlier work by Nickell [1981. Biases in
              dynamic models with fixed effects. Econometrica 49, 1417-1426]
              and later authors in several directions that are relevant for
              practical work, including models with unit roots, deterministic
              trends, predetermined and exogenous regressors, and errors that
              may be cross sectionally dependent. The asymptotic bias is found
              to be so large when incidental linear trends are fitted and the
              time series sample size is small that it changes the sign of the
              autoregressive coefficient. Another finding of interest is that,
              when there is cross section error dependence, the probability
              limit of the dynamic panel regression estimator is a random
              variable rather than a constant, which helps to explain the
              substantial variability observed in dynamic panel estimates when
              there is cross section dependence even in situations where N is
              very large. Some proposals for bias correction are suggested and
              finite sample performance is analyzed in simulations. ?? 2006
              Elsevier B.V. All rights reserved.",
  journal  = "J. Econom.",
  volume   =  137,
  number   =  1,
  pages    = "162--188",
  month    =  mar,
  year     =  2007,
  keywords = "Autoregression; Bias; Bias correction; Cross section dependence;
              Dynamic factors; Dynamic panel estimation; Incidental trends;
              Panel unit root"
}

@ARTICLE{Blundell1998-pv,
  title    = "Initial conditions and moment restrictions in dynamic panel data
              models",
  author   = "Blundell, Richard and Bond, Stephen",
  abstract = "Estimation of the dynamic error components model is considered
              using two alternative linear estimators that are designed to
              improve the properties of the standard first-differenced GMM
              estimator. Both estimators require restrictions on the initial
              conditions process. Asymptotic efficiency comparisons and Monte
              Carlo simulations for the simple AR(1) model demonstrate the
              dramatic improvement in performance of the proposed estimators
              compared to the usual first-differenced GMM estimator, and
              compared to non-linear GMM. The importance of these results is
              illustrated in an application to the estimation of a labour
              demand model using company panel data.",
  journal  = "J. Econom.",
  volume   =  87,
  number   =  1,
  pages    = "115--143",
  month    =  nov,
  year     =  1998,
  keywords = "C23; Dynamic panel data; Error components; GMM; Initial
              conditions; Weak instruments"
}

@INCOLLECTION{Hughes2014-ul,
  title     = "Measuring the Performance of Banks, theory, Practice, Evidence,
               and some Policy implications",
  booktitle = "The Oxford Handbook of Banking",
  author    = "Hughes, Joseph P and Mester, Loretta J",
  editor    = "Wilson, John O S and Molyneux, Philip and Berger, Allen N",
  publisher = "Oxford University Press",
  pages     = "247--271",
  chapter   =  10,
  edition   = "Second",
  year      =  2014
}

@TECHREPORT{Haldane2013-ea,
  title       = "Constraining discretion in bank regulation",
  author      = "Haldane, Andrew",
  abstract    = "Conclusion Over the course of the past 20 years, banking
                 regulation has edged in a self- regulatory direction for
                 understandable, but self-defeating, reasons. The regulatory
                 regime has tilted from constrained discretion to unconstrained
                 indiscretion. It will be a long ...",
  pages       = "1--19",
  institution = "Bank of England",
  year        =  2013,
  address     = "London",
  keywords    = "Bank of England; banking regulation"
}

@TECHREPORT{Imf2014-za,
  title       = "Review Of The Financial Sector Assessment Program: Further
                 Adaptation To The Post Crisis Era Review Of The Financial
                 Sector Assessment Program --- Further Adaptation To The
                 {Post-Crisis} Era",
  author      = "{Imf}",
  institution = "International Monetary Fund",
  year        =  2014
}

@ARTICLE{Acharya2014-ue,
  title    = "A Crisis of Banks as Liquidity Providers",
  author   = "Acharya, Viral V and Mora, Nada",
  abstract = "Can banks maintain their advantage as liquidity providers when
              exposed to a financial\textbackslashncrisis? While banks honored
              credit lines drawn by firms during the 2007 to 2009
              crisis,\textbackslashnthis liquidity provision was only possible
              because of explicit, large support from
              the\textbackslashngovernment and government-sponsored agencies.
              At the onset of the crisis, aggregate\textbackslashndeposit
              inflows into banks weakened and their loan-to-deposit shortfalls
              widened. These\textbackslashnpatterns were pronounced at banks
              with greater undrawn commitments. Such banks\textbackslashnsought
              to attract deposits by offering higher rates, but the resulting
              private funding was\textbackslashninsufficient to cover
              shortfalls and they reduced new credit.",
  journal  = "J. Finance",
  volume   = "forthcomin",
  number   =  1,
  year     =  2014,
  keywords = "acharya is with new; and ecgi; and kristen regehr; cepr; during
              the 2007-2009 crisis; federal reserve bank of; financial crisis;
              flight to safety; help with the bank; jacob schak; jon
              christensson; liquidity risk; mora is with the; nber; paul
              rotilie; provided valuable research assistance; rate monitor
              data; ruth judson for her; solvency risk; thad sieracki; we are
              grateful to; york university"
}

@ARTICLE{Alexander2002-fb,
  title    = "Cointegration and asset allocation: A new active hedge fund
              strategy",
  author   = "Alexander, Carol and Giblin, Ian and Weddington, Wayne",
  abstract = "Models that are based on mean-variance analysis seek portfolio
              weights to minimise the variance of the portfolio for a given
              level of return. The portfolio variance is measured using a
              covariance matrix that represents the volatility and correlation
              of asset returns. However these matrices are notoriously
              difficult to estimate and ad hoc methods often need to be applied
              to limit or smooth the mean-variance efficient allocations that
              are recommended by the model. Moreover the mean- variance
              criterion has nothing to ensure that tracking errors are
              stationary. Although the portfolios will be efficient, the
              tracking errors will in all probability be random walks.
              Therefore the replicating portfolio can drift very far from the
              benchmark unless it is frequently re-balanced. A significant
              difference between traditional hedge fund strategies and the
              model presented in this paper is that portfolio optimization is
              based upon the cointegration of asset prices rather than the
              correlation of asset returns. We show that it is possible to
              devise allocations that always have stationary tracking errors.
              Moreover, efficient market neutral long short hedge strategies
              may be achieved with relatively few stocks and with much lower
              turnover rates compared to traditional market neutral strategies.",
  journal  = "ISMA Centre Discussion Papers in Finance Series",
  volume   =  16,
  number   =  0,
  pages    = "40",
  year     =  2002,
  keywords = "HG Finance"
}

@ARTICLE{Demirguc-Kunt2011-lc,
  title     = "Basel Core Principles and bank soundness: Does compliance
               matter?",
  author    = "Demirg{\"u}{\c c}-Kunt, Asli and Detragiache, Enrica",
  abstract  = "This paper studies whether compliance with the Basel Core
               Principles for effective banking supervision (BCPs) is
               associated with bank soundness. Using data for over 3000 banks
               in 86 countries, we find that neither the overall index of BCP
               compliance nor its individual components are robustly associated
               with bank risk measured by individual bank Z-scores. We also
               fail to find a relationship between BCP compliance and systemic
               risk measured by a system-wide Z-score.",
  journal   = "Journal of Financial Stability",
  publisher = "Elsevier",
  volume    =  7,
  number    =  4,
  pages     = "179--190",
  year      =  2011,
  keywords  = "Bank soundness; Bank soundness Regulation and supervision Basel
               co; Basel core principles; Regulation and supervision; Bank
               soundness Regulation and supervision Basel core principles"
}

@ARTICLE{Podpiera2004-uz,
  title    = "Does Compliance with Basel Core Principles Bring Measurable
              Benefits ?",
  author   = "Podpiera, R",
  abstract = "We explore the relationship between banking sector performance
              and the quality of regulation and supervision as measured by
              compliance with the Basel Core Principles for Effective Banking
              Supervision (BCP). Using BCP assessment results for 65 countries
              and 1998--2002 panel data for other variables, we find a
              significant positive impact of higher compliance with BCP on
              banking sector performance, as measured by nonperforming loans
              and net interest margin, after controlling for the level of
              development of the economy and the financial system and
              macroeconomic and structural factors.",
  journal  = "IMF working paper 04/024",
  volume   =  53,
  number   =  2,
  pages    = "306--326",
  series   = "IMF Working Papers",
  year     =  2004,
  address  = "Washiington"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Lepetit2015-zy,
  title     = "Bank insolvency risk and time-varying Z-score measures:a
               refinement",
  author    = "Lepetit, Laetitia and Strobel, Frank",
  abstract  = "We re-examine the probabilistic foundation of the link between
               Z- score measures and banks' probability of insolvency, offering
               an improved measure of that probability without imposing further
               distributional assumptions. While the traditional measure of the
               probability of insolvency thus provides a less effective upper
               bound of the probability of insolvency, it can be meaningfully
               rein- terpreted as a measure capturing the odds of insolvency
               instead. We similarly obtain reÔ¨Åned probabilistic
               interpretations of the commonly used simple and log-transformed
               Z-score measures; in particular, the log of the Z-score is shown
               to be negatively propor- tional to the log odds of insolvency.",
  journal   = "Finance Research Letters",
  publisher = "Elsevier Inc.",
  year      =  2015,
  keywords  = "Insolvency risk; Odds; Z-score; probability"
}

@ARTICLE{Baele2015-bl,
  title    = "Model uncertainty and systematic risk in {US} banking",
  author   = "Baele, Lieven and De Bruyckere, Valerie and De Jonghe, Olivier
              and Vander Vennet, Rudi",
  abstract = "This paper uses Bayesian Model Averaging to examine the driving
              factors of equity returns of US Bank Holding Companies. BMA has
              as an advantage over OLS that it accounts for the considerable
              uncertainty about the correct set (model) of bank risk factors.
              We find that out of a broad set of 12risk factors only the
              market, real estate, and high-minus-low Fama--French factors are
              reliably related to US bank stock returns over the period
              1986--2010. Other factors are either only relevant over specific
              subperiods or for subsets of bank holding companies. We discuss
              the implications of our findings for empirical banking research.",
  journal  = "Journal of Banking \& Finance",
  volume   =  53,
  pages    = "49--66",
  month    =  apr,
  year     =  2015,
  keywords = "Bank stock returns; Banking risk; Bayesian Model Average; G01;
              G20; G21; G28; L25"
}

@ARTICLE{Leung2015-ne,
  title    = "The determinants of bank risks: Evidence from the recent
              financial crisis",
  author   = "Leung, W S and Taylor, N and Evans, K P",
  abstract = "We investigate whether US bank holding company fundamental
              characteristics are related to bank risk over a period that
              covers the recent 2007--09 financial crisis. We extend prior
              studies to consider bank equity risk exposure to market-wide
              default risk, the structured finance market, and the asset-backed
              money market in a variance decomposition. Four important results
              emerge: (1) the risk in bank opaque assets is not accurately
              priced; (2) banks with lower earnings have higher risk; (3) a
              positive relationship between non-performing loans and bank risk
              increased threefold during the crisis and (4) banks with a larger
              buffer of Tier 1 capital have lower risk and lower exposure to
              shocks in market-wide default risk and the structured finance
              market in particular. These results highlight the importance to
              investors of studying fundamentals, while from a bank regulatory
              perspective, effective management of regulatory capital may
              manage risks arising from contagion stemming from structured
              finance markets and funding illiquidity.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  34,
  pages    = "277--293",
  month    =  jan,
  year     =  2015,
  keywords = "ABX index; Bank equity risk; Bank holding companies; Funding
              illiquidity risk"
}

@ARTICLE{Klomp2013-hm,
  title    = "Bank regulation , the quality of institutions and banking risk in
              emerg- ing and developing countries : An empirical analysis",
  author   = "Klomp, Jeroen and Haan, Jakob De",
  pages    = "1--57",
  year     =  2013,
  keywords = "bank regulation and supervision; banking risk; institutional
              quality"
}

@ARTICLE{Kneip2012-ah,
  title  = "When Bias Kills the Variance : Central Limit Theorems for {DEA} and
            {FDH} Efficiency Scores",
  author = "Kneip, Alois and Wilson, Paul W",
  year   =  2012
}

@ARTICLE{Gaganis2013-ah,
  title    = "Financial supervision regimes and bank efficiency: International
              evidence",
  author   = "Gaganis, Chrysovalantis and Pasiouras, Fotios",
  abstract = "There exists a lively debate as for the appropriate architecture
              of the financial supervision regime, with a long list of
              theoretical advantages and disadvantages associated with each one
              of its key dimensions. The present study investigates whether and
              how bank profit efficiency is influenced by the central bank's
              involvement in financial supervision, the unification of
              financial authorities, and the independence of the central bank.
              The results show that efficiency decreases as the number of the
              financial sectors that are supervised by the central bank
              increases. Additionally, banks operating in countries with
              greater unification of supervisory authorities are less profit
              efficient. Finally, central bank independence has a negative
              impact on bank profit efficiency. ?? 2013 Elsevier B.V.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  12,
  pages    = "5463--5475",
  month    =  dec,
  year     =  2013,
  keywords = "Central bank; Efficiency; Independence; Supervision; Unification"
}

@ARTICLE{Park2006-xi,
  title    = "A note on efficiency and productivity growth in the Korean
              Banking Industry, 1992--2002",
  author   = "Park, Kang H and Weber, William L",
  abstract = "In this paper we present estimates of Korean bank inefficiency
              and productivity change for the period 1992--2002 that are
              derived from the directional technology distance function. Our
              method controls for loan losses that are an undesirable
              by-product arising from the production of loans and allows the
              aggregation of individual bank inefficiency and productivity
              growth to the industry level. Our findings indicate that
              technical progress during the period was more than enough to
              offset efficiency declines so that the banking industry
              experienced productivity growth.",
  journal  = "Journal of Banking \& Finance",
  volume   =  30,
  number   =  8,
  pages    = "2371--2386",
  month    =  aug,
  year     =  2006,
  keywords = "Bank efficiency; Bank productivity; G21; Korean banks"
}

@ARTICLE{Koop2001-jz,
  title    = "The valuation of {IPO} and {SEO} firms",
  author   = "Koop, Gary and Li, Kai",
  abstract = "We examine the pricing of initial public offering (IPO) and
              seasoned equity offering (SEO) firms using a stochastic frontier
              methodology. The stochastic frontier framework models the
              difference between the maximum possible value of the firm and its
              actual market capitalization at the time of the offering as a
              function of observable firm characteristics. Using a new data
              set, we find that commonly used pricing factors do indeed
              influence valuation. Ceteris paribus, firms in industries with
              great earnings potential are more highly valued, and IPO firms
              are underpriced. Theories regarding underwriter reputation or
              windows of opportunity for equity issuance are not supported in
              our empirical results. ?? 2001 Elsevier Science B.V.",
  journal  = "Journal of Empirical Finance",
  volume   =  8,
  number   =  4,
  pages    = "375--401",
  month    =  sep,
  year     =  2001,
  keywords = "Bayesian analysis; Bayesian inference; C11; C15; Corporate
              finance - general; Financing policy; G14; G30; G32; Gibbs
              sampling; Information and market efficiency; Misvaluation;
              Statistical simulation methods; Stochastic frontier; Underpricing"
}

@ARTICLE{Elliott2014-me,
  title    = "A Double {HMM} approach to Altman Z-scores and credit ratings",
  author   = "Elliott, Robert J and Siu, Tak Kuen and Fung, Eric S",
  abstract = "Credit ratings and accounting-based Altman Z-scores are two
              important sources of information for assessing the
              creditworthiness of firms. In this paper we build a model based
              on a double hidden Markov model, (DHMM), to extract information
              about the ``true'' credit qualities of firms from both the
              Z-scores evaluated from the accounting ratios of the firms and
              their posted credit ratings. The evolution of the ``true'' credit
              quality over time is estimated from observed data using filtering
              methods and the EM algorithm. Recursive updates of optimal
              estimates are provided via filtering so that the model is
              ``self-tuning'', or ``self-calibrating''. We illustrate the
              practical implementation of the proposed model using actual
              accounting ratios data of firms from different regions and their
              posted credit ratings data.",
  journal  = "Expert Syst. Appl.",
  volume   =  41,
  number   =  4,
  pages    = "1553--1560",
  month    =  mar,
  year     =  2014,
  keywords = "Altman Z-scores; Credit ratings; Double hidden Markov models; EM
              algorithm; Filters; Altman -scores"
}

@ARTICLE{Lepetit2013-jl,
  title    = "Bank insolvency risk and time-varying Z-score measures",
  author   = "Lepetit, Laetitia and Strobel, Frank",
  abstract = "We compare the different existing approaches to the construction
              of time-varying Z-score measures, plus an additional alternative
              one, using a panel of banks for the G20 group of countries
              covering the period 1992--2009. We examine which ways of
              estimating the moments used in these different approaches best
              fit the data, using a simple root mean squared error criterion.
              Our results are supportive of our alternative time-varying
              Z-score measure: it uses mean and standard deviation estimates of
              the return on assets calculated over full samples combined with
              current values of the capital-asset ratio, and is thus
              straightforward to implement.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  25,
  pages    = "73--87",
  month    =  jul,
  year     =  2013,
  keywords = "G21; Insolvency risk; Mean squared error; Time-varying; Z-score;
              -score"
}

@ARTICLE{Bowen2016-ub,
  title   = "Pairs trading in the {UK} equity market: risk and return",
  author  = "Bowen, David A and Hutchinson, Mark C",
  journal = "The European Journal of Finance",
  volume  =  22,
  number  =  14,
  pages   = "1363--1387",
  month   =  nov,
  year    =  2016
}

@ARTICLE{Glosten1985-zr,
  title    = "Bid, ask and transaction prices in a specialist market with
              heterogeneously informed traders",
  author   = "Glosten, Lawrence R and Milgrom, Paul R",
  abstract = "The presence of traders with superior information leads to a
              positive bid-ask spread even when the specialist is risk-neutral
              and makes zero expected profits. The resulting transaction prices
              convey information, and the expectation of the average spread
              squared times volume is bounded by a number that is independent
              of insider activity. The serial correlation of transaction price
              differences is a function of the proportion of the spread due to
              adverse selection. A bid-ask spread implies a divergence between
              observed returns and realizable returns. Observed returns are
              approximately realizable returns plus what the uninformed
              anticipate losing to the insiders.",
  journal  = "J. financ. econ.",
  volume   =  14,
  number   =  1,
  pages    = "71--100",
  month    =  mar,
  year     =  1985
}

@ARTICLE{Karp1991-jp,
  title    = "Artificial intelligence methods for theory representation and
              hypothesis formation",
  author   = "Karp, P D",
  abstract = "This article describes artificial intelligence methods for
              representing theories in molecular biology, and for improving the
              predictive power of these theories using experimental data. A
              program called GENSIM provides a framework for representing
              theories that includes descriptions of classes of biological
              objects (genes, enzymes, etc.), and processes that specify
              potential interactions among these objects (such as enzymatic
              reactions). GENSIM can employ a theory specified within this
              framework to predict the outcomes of biological experiments. A
              program called HYPGENE comes into play when the observed outcome
              of an experiment does not match the outcome predicted by GENSIM.
              HYPGENE works backward from the error in GENSIMs prediction to
              postulate changes to both the theory embodied by GENSIM, and the
              presumed initial conditions of the experiment. I view HYPGENEs
              hypothesis generation task as a design problem, and I have
              adapted AI methods developed for design and planning to this
              task. These techniques were developed in conjunction with an
              in-depth study of the discovery of the gene regulation mechanism
              of attenuation in the E. coli tryptophan operon. Both GENSIM and
              HYPGENE have been tested on sample problems from the history of
              attenuation, and produced many of the same solutions as
              biologists did.",
  journal  = "Comput. Appl. Biosci.",
  volume   =  7,
  number   =  3,
  pages    = "301--308",
  month    =  jul,
  year     =  1991,
  keywords = "liquidity; theory",
  language = "en"
}

@ARTICLE{Timmermann2008-tj,
  title    = "Elusive return predictability",
  author   = "Timmermann, Allan",
  abstract = "Investors' searches for successful forecasting models cause the
              data generating process for financial returns to change over
              time, which means that individual return forecasting models can,
              at best, hope to uncover evidence of 'local' predictability. We
              illustrate this point on a suite of forecasting models used to
              predict US stock returns, and propose an adaptive forecast
              combination approach. Most of the time the forecasting models
              perform rather poorly, but there is evidence of relatively
              short-lived periods with modest return predictability. The short
              duration of the episodes where return predictability appears to
              be present and the relatively weak degree of predictability even
              during such periods makes predicting returns an extraordinarily
              challenging task. ?? 2007 International Institute of Forecasters.",
  journal  = "Int. J. Forecast.",
  volume   =  24,
  number   =  1,
  pages    = "1--18",
  month    =  jan,
  year     =  2008,
  keywords = "Adaptive forecast combination; Creative self-destruction;
              Out-of-sample forecasting performance; Predictability of stock
              returns"
}

@ARTICLE{Imbierowicz2014-jl,
  title    = "The relationship between liquidity risk and credit risk in banks",
  author   = "Imbierowicz, Bj{\"o}rn and Rauch, Christian",
  abstract = "This paper investigates the relationship between the two major
              sources of bank default risk: liquidity risk and credit risk. We
              use a sample of virtually all US commercial banks during the
              period 1998--2010 to analyze the relationship between these two
              risk sources on the bank institutional-level and how this
              relationship influences banks' probabilities of default (PD). Our
              results show that both risk categories do not have an
              economically meaningful reciprocal contemporaneous or time-lagged
              relationship. However, they do influence banks' probability of
              default. This effect is twofold: whereas both risks separately
              increase the PD, the influence of their interaction depends on
              the overall level of bank risk and can either aggravate or
              mitigate default risk. These results provide new insights into
              the understanding of bank risk and serve as an underpinning for
              recent regulatory efforts aimed at strengthening banks (joint)
              risk management of liquidity and credit risks.",
  journal  = "Journal of Banking \& Finance",
  volume   =  40,
  pages    = "242--256",
  month    =  mar,
  year     =  2014,
  keywords = "Bank default probability; Bank risk; Credit risk; G21; G28; G32;
              G33; Liquidity risk"
}

@ARTICLE{Liu2010-vy,
  title     = "{DEA} models with undesirable inputs and outputs",
  author    = "Liu, W B and Meng, W and Li, X X and Zhang, D Q",
  abstract  = "Data Envelopment Analysis (DEA) models with undesirable inputs
               and outputs have been frequently discussed in DEA literature,
               e.g., via data transformation. These studies were scatted in the
               literature, and often confined to some particular applications.
               In this paper we present a systematic investigation on model
               building of DEA without transferring undesirable data. We first
               describe the disposability assumptions and a number of different
               performance measures in the presence of undesirable inputs and
               outputs, and then discuss different combinations of the
               disposability assumptions and the metrics. This approach leads
               to a unified presentation of several classes of DEA models with
               undesirable inputs and/or outputs.",
  journal   = "Ann. Oper. Res.",
  publisher = "Springer US",
  volume    =  173,
  number    =  1,
  pages     = "177--194",
  month     =  jan,
  year      =  2010,
  keywords  = "Data Envelopment Analysis; Extended Strong Disposability;
               Undesirable inputs and outputs",
  language  = "en"
}

@MISC{Simper2014-ig,
  title       = "How Relevant is the Choice of Risk Management Control Variable
                 to Non-parametric Bank Profit Efficiency Analysis ? The Case
                 of South Korean Banks",
  author      = "Simper, Richard and Hall, Maximilian J B and Liu, Wenbin B and
                 Zelenyuk, Valentin and Zhou, Zhongbao and Simper, Richard and
                 Hall, Maximilian J B and Liu, Wenbin B and Zelenyuk, Valentin",
  abstract    = "Adopting a profit-based approach to the estimation of the
                 technical efficiency of South Korean banks over the 2007Q3 to
                 2011Q2 period, we systematically analyse, within a non-
                 parametric DEA analysis, how the choice of risk management
                 control variable impacts upon such estimates. Using the model
                 of Liu et al. (2010), we examine the dependency of the
                 estimated technical efficiency scores on the chosen risk
                 control variables embracing loan loss provisions and equity as
                 good inputs and non-performing loans as a bad output. We duly
                 find that, both for individual banks and banking groups, the
                 mean estimates are indeed model dependent although, for the
                 former, rank correlations do not change much at the extremes.
                 Based on the application of the Simar and Zelenyuk (2006)
                 adapted Li (1996) test, we then find that, if only one of the
                 three risk control variables is to be included in such an
                 analysis, then it should be loan loss provisions. We also
                 show, however, that the inclusion of all three risk control
                 variable is to be preferred to just including one, but that
                 the inclusion of two such variables is about as good as
                 including all three. We therefore conclude that the optimal
                 approach is to include (any) two of the three risk control
                 variables identified. The wider implication for research into
                 bank efficiency is that the optimal choice of risk management
                 control variable is likely to be crucial to both the delivery
                 of un-biased estimates of bank efficiency and the
                 specification of the model to be estimated.",
  number      =  1932,
  series      = "Centr for Efficiency and Productivity Analysis",
  institution = "School of Economics, University of Queensland",
  year        =  2014
}

@ARTICLE{Fukuyama2009-ze,
  title    = "A directional slacks-based measure of technical inefficiency",
  author   = "Fukuyama, Hirofumi and Weber, William L",
  abstract = "Radial measures of efficiency estimated using linear programming
              (LP) methods can be biased since slack in the constraints
              defining the technology suggests that at least one input can be
              reduced, or one output can be expanded, even though a firm is
              deemed to be ``technically efficient.'' In this paper, we propose
              a directional slacks-based measure of technical inefficiency to
              account for the potential of slack in technological constraints.
              When no such slacks exist, directional slacks-based inefficiency
              collapses to the directional technology distance function. Our
              proposed measure helps to generalize some of the existing
              slacks-based measures of inefficiency. We examine the financial
              services provided by Japanese cooperative Shinkin banks, and
              estimate their inefficiency during the period 2002--2005. This
              inefficiency declined slightly during the period. We thus propose
              that slack is an important source of inefficiency which is often
              not captured by the directional technology distance function.",
  journal  = "Socioecon. Plann. Sci.",
  volume   =  43,
  number   =  4,
  pages    = "274--287",
  month    =  dec,
  year     =  2009,
  keywords = "Data envelopment analysis (DEA); Directional technology distance
              function; Japanese Shinkin banks (credit associations); RAM;
              Slacks-based measures"
}

@ARTICLE{Diamond2014-pz,
  title    = "Learning and Trusting Cointegration in Statistical Arbitrage",
  author   = "Diamond, Richard V",
  abstract = "The paper offers adaptation of cointegration analysis for
              statistical arbitrage. Cointegration is a structural relationship
              model that relies on dynamic correct",
  journal  = "SSRN Electronic Journal",
  month    =  sep,
  year     =  2014,
  keywords = "Ornstein-Uhlenbeck; cointegration; equilibrium correction;
              forecasting; mean-reversion; spread trading; statistical
              arbitrage; time series decomposition"
}

@ARTICLE{Wilson2009-rz,
  title    = "Theorizing about theorizing: an examination of the contributions
              of William I. Grossman to psychoanalysis",
  author   = "Wilson, Arnold",
  abstract = "William Grossman's contributions to psychoanalysis are studied in
              the light of an interest that suffuses his papers: the remarkably
              complex ways an analyst develops his or her mind in order to
              become an effective analyst. Grossman sought to detail the many
              subtle factors infusing Freudian theory, from its initial sources
              to its consolidation as a system, to its embrace in the mind of
              an analyst who will use it, and on to the many iterations of its
              progress on the way to being applied in the clinical situation.
              This progression assumes a recognition of the analyst's need to
              be at once both experiencer and observer. Implicit in the attempt
              to understand another is a self-reflective taking of oneself as
              an object of analysis. How an essential tension is worked out
              between the subjective and objective points of view is an issue
              that pervades Grossman's writings. This is but one instance of a
              larger tendency characterizing his ideas-thinking
              psychoanalytically about psychoanalysis itself. Factors he
              implicated in being a contemporary Freudian analyst are then
              taken up.",
  journal  = "J. Am. Psychoanal. Assoc.",
  volume   =  57,
  number   =  1,
  pages    = "9--36",
  month    =  feb,
  year     =  2009,
  keywords = "20th Century; 21st Century; Freudian Theory; History;
              Psychoanalysis; Psychoanalysis: history; United States",
  language = "en"
}

@MISC{Lamont2003-hz,
  title    = "Anomalies: The Law of One Price in Financial Markets",
  author   = "Lamont, Owen A and Thaler, Richard H",
  abstract = "Economics can be distinguished from other social sciences by the
              belief that most (all?) behavior can be explained by assuming
              that rational agents with stable, well-defined preferences
              interact in markets that (eventually) clear. An empirical result
              qualifies as an anomaly if it is difficult to rationalize or if
              implausible assumptions are necessary to explain it within the
              paradigm.",
  journal  = "J. Econ. Perspect.",
  volume   =  17,
  pages    = "191--202",
  year     =  2003
}

@ARTICLE{Coates2008-ty,
  title    = "Endogenous steroids and financial risk taking on a London trading
              floor",
  author   = "Coates, J M and Herbert, J",
  abstract = "Little is known about the role of the endocrine system in
              financial risk taking. Here, we report the findings of a study in
              which we sampled, under real working conditions, endogenous
              steroids from a group of male traders in the City of London. We
              found that a trader's morning testosterone level predicts his
              day's profitability. We also found that a trader's cortisol rises
              with both the variance of his trading results and the volatility
              of the market. Our results suggest that higher testosterone may
              contribute to economic return, whereas cortisol is increased by
              risk. Our results point to a further possibility: testosterone
              and cortisol are known to have cognitive and behavioral effects,
              so if the acutely elevated steroids we observed were to persist
              or increase as volatility rises, they may shift risk preferences
              and even affect a trader's ability to engage in rational choice.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  105,
  number   =  16,
  pages    = "6167--6172",
  month    =  apr,
  year     =  2008,
  language = "en"
}

@ARTICLE{Coates2010-dq,
  title    = "From molecule to market: steroid hormones and financial
              risk-taking",
  author   = "Coates, John M and Gurnell, Mark and Sarnyai, Zoltan",
  abstract = "Little is known about the role of the endocrine system in
              financial decision-making. Here, we survey research on steroid
              hormones and their cognitive effects, and examine potential links
              to trader performance in the financial markets. Preliminary
              findings suggest that cortisol codes for risk and testosterone
              for reward. A key finding of this endocrine research is the
              different cognitive effects of acute versus chronic exposure to
              hormones: acutely elevated steroids may optimize performance on a
              range of tasks; but chronically elevated steroids may promote
              irrational risk-reward choices. We present a hypothesis
              suggesting that the irrational exuberance and pessimism observed
              during market bubbles and crashes may be mediated by steroid
              hormones. If hormones can exaggerate market moves, then perhaps
              the age and sex composition among traders and asset managers may
              affect the level of instability witnessed in the financial
              markets.",
  journal  = "Philos. Trans. R. Soc. Lond. B Biol. Sci.",
  volume   =  365,
  number   =  1538,
  pages    = "331--343",
  month    =  jan,
  year     =  2010,
  language = "en"
}

@ARTICLE{Tsolas2015-gv,
  title    = "Incorporating risk into bank efficiency: A satisficing {DEA}
              approach to assess the Greek banking crisis",
  author   = "Tsolas, Ioannis E and Charles, Vincent",
  abstract = "This paper is motivated by recent concerns, prompted by the
              recent financial crisis, that regulatory capital guidelines on
              loan loss reserves can generate dysfunctional outcomes and,
              moreover, by the fact that the Greek bonds held by the banks have
              an important impact on the risk level of the bank portfolio. The
              purpose of this paper is to incorporate risk into bank efficiency
              and to provide a snapshot of the efficiency profile of the Greek
              banking industry. Efficiency is measured by means of a
              satisficing data envelopment analysis (DEA) model in which the
              financial risk is proxied by credit risk provisions and by the
              participation of banks in the Private Sector Involvement (PSI), a
              controllable and an uncontrollable factor by the bank management,
              respectively. The results of the proposed probabilistic DEA model
              derived through the Monte-Carlo simulation are compared with the
              results of the respective deterministic model. As the constructed
              stochastic frontier screens further some of the `best-in-class'
              banks, the merit of the proposed metric is evident.",
  journal  = "Expert Syst. Appl.",
  volume   =  42,
  number   =  7,
  pages    = "3491--3500",
  month    =  may,
  year     =  2015,
  keywords = "Data envelopment analysis; Finance and banking; Mathematical
              programming; Satisficing DEA"
}

@ARTICLE{Jia2013-vx,
  title    = "Contest functions: Theoretical foundations and issues in
              estimation",
  author   = "Jia, Hao and Skaperdas, Stergios and Vaidya, Samarth",
  abstract = "Contest functions (alternatively, contest success functions)
              determine probabilities of winning and losing as a function of
              contestants' effort. They are used widely in many areas of
              economics that employ contest games, from tournaments and
              rent-seeking to conflict and sports. We first examine the
              theoretical foundations of contest functions and classify them
              into four types of derivation: stochastic, axiomatic,
              optimally-derived, and microfounded. The additive form (which
              includes the ratio or ``Tullock'' functional form) can be derived
              in all four different ways. We also explore issues in the
              econometric estimation of contest functions, including concerns
              with data, endogeneity, and model comparison.",
  journal  = "Int. J. Ind Organiz",
  volume   =  31,
  number   =  3,
  pages    = "211--222",
  month    =  may,
  year     =  2013,
  keywords = "Advertising; C72; Conflict; D72; D74; L23; L41; Litigation; M37;
              Rent-seeking; Tournaments"
}

@ARTICLE{Kang2010-tr,
  title    = "Probability of information-based trading and the January effect",
  author   = "Kang, Moonsoo",
  abstract = "We investigate the seasonality in the probability of
              information-based trading (PIN)--return relationship, the
              `January PIN effect'. We find that on average stock returns
              decrease with PIN in January, in contrast to other calendar
              months. This pattern is more apparent for small stocks. We argue
              that this seasonality is related to the January effect. According
              to the analysis, the December selling pressure associated with
              the January effect decreases in PIN, especially for small stocks.
              This suggests that when the price bounces back in January,
              low-PIN stocks will exhibit a larger return within a small stock
              group, leading to the negative PIN--return pattern. Furthermore,
              this seasonality is not the same as other January anomalies
              associated with momentum and idiosyncratic risk.",
  journal  = "Journal of Banking \& Finance",
  volume   =  34,
  number   =  12,
  pages    = "2985--2994",
  month    =  dec,
  year     =  2010,
  keywords = "G10; G12; G14; Institutional investor; January effect;
              Probability of information-based trading (PIN)"
}

@ARTICLE{Sun2010-if,
  title    = "Risk and the January effect",
  author   = "Sun, Qian and Tong, Wilson H S",
  abstract = "We use a time-series GARCH framework with the conditional
              variance/covariance as proxies for systematic risk to reexamine
              the proposition by Rozeff and Kinney (1976) and Rogalski and
              Tinic (1986) that the January effect may be a phenomenon of risk
              compensation in the month. We find no clear evidence that either
              conditional volatility or unconditional volatility in January is
              predominantly higher across the sampling years. Hence, against
              the proposition, the January effect is not due to risk per se.
              Rather, we find strong evidence that the January effect is due to
              higher compensation for risk in the month. This may be possible
              if investors have an increasing RRA utility function. Although
              many studies find that volatility tends to be higher in January,
              we find it to be period-specific and mostly in value-weighted
              return series, but not in equal-weighted return series. This is
              true both for the unconditional and conditional return
              volatility.",
  journal  = "Journal of Banking \& Finance",
  volume   =  34,
  number   =  5,
  pages    = "965--974",
  month    =  may,
  year     =  2010,
  keywords = "G12; GARCH; January effect; Risk premium; Seasonality; Volatility"
}

@ARTICLE{Easterday2009-fy,
  title    = "The persistence of the small firm/January effect: Is it
              consistent with investors' learning and arbitrage efforts?",
  author   = "Easterday, Kathryn E and Sen, Pradyot K and Stephan, Jens A",
  abstract = "Using improved methodology and an expanded research design, we
              examine whether the small firm/January effect (Keim, D. B.
              (1983). Size-related anomalies and stock return seasonality:
              further empirical evidence. Journal of Financial Economics
              12:13--32), is declining over time due to market efficiency.
              First, we find that January returns are smaller after 1963--1979,
              but have simply reverted to levels that existed before that time.
              Second, we show that the January effect is not limited to mature
              markets but also appears in firms trading on the relatively new
              NASDAQ exchange in the 1970s. Third, trading volume for small
              firms in December and January is not different from other months,
              implying that traders are not actively arbitraging the anomaly.
              Together, our results suggest that this anomaly continues to defy
              rational explanation in an efficient market.",
  journal  = "Q. Rev. Econ. Finance",
  volume   =  49,
  number   =  3,
  pages    = "1172--1193",
  month    =  aug,
  year     =  2009,
  keywords = "Arbitrage; G12; January effect; Market efficiency; Trading volume"
}

@ARTICLE{Gu2003-bq,
  title    = "The declining January effect: evidences from the {U.S}. equity
              markets",
  author   = "Gu, Anthony Yanxiang",
  abstract = "The January effect exhibits a pronounced declining trend for both
              large and small firm stock indices since 1988 and the effect is
              disappearing for the Russell indices. The declining trend is also
              evident in the Dow 30, since 1930. While the trend is upward for
              the Dow 30 and the S\&P 500 for the post-war period through the
              1970s, because of the extremely high January returns in 1975 and
              1976, the trend lines are flat when these outliers are excluded.
              The downward trend is more apparent for indices containing small
              stocks than for indices of large stocks. The January effect is
              negatively connected to actual and expected real GDP growth,
              inflation, and return of the year, and it is positively related
              to volatility. The power ratio method provides a consistent way
              to reveal the relative contribution of January return in the
              year. Finding the pattern of changes in the anomaly has
              implications for investment strategies.",
  journal  = "Q. Rev. Econ. Finance",
  volume   =  43,
  number   =  2,
  pages    = "395--404",
  month    =  jun,
  year     =  2003,
  keywords = "Declining January effect; Market efficiency"
}

@ARTICLE{Heckman2012-xt,
  title    = "Hard evidence on soft skills",
  author   = "Heckman, James J and Kautz, Tim",
  abstract = "This paper summarizes recent evidence on what achievement tests
              measure; how achievement tests relate to other measures of
              ``cognitive ability'' like IQ and grades; the important skills
              that achievement tests miss or mismeasure, and how much these
              skills matter in life. Achievement tests miss, or perhaps more
              accurately, do not adequately capture, soft skills-personality
              traits, goals, motivations, and preferences that are valued in
              the labor market, in school, and in many other domains. The
              larger message of this paper is that soft skills predict success
              in life, that they causally produce that success, and that
              programs that enhance soft skills have an important place in an
              effective portfolio of public policies.",
  journal  = "Labour Econ.",
  volume   =  19,
  number   =  4,
  pages    = "451--464",
  month    =  aug,
  year     =  2012,
  keywords = "Achievement tests; Cognition; D01; I20; IQ; Personality",
  language = "en"
}

@ARTICLE{Mason2014-pc,
  title    = "Jump processes in natural gas markets",
  author   = "Mason, Charles F and A. Wilmot, Neil",
  abstract = "Many analysts believe that natural gas will have an increasingly
              important role in the next few decades. Accordingly,
              understanding the underpinnings of natural gas prices is likely
              to be critical, both to policy analysts and to market
              participants. At present, it is common to assume that these
              prices follow a geometric Brownian motion, i.e., that log returns
              -- the inter-temporal differences in the natural log of prices --
              are normally distributed (possibly allowing for some form of
              mean-reversion). Increasingly, however, it has been recognized
              that the arrival of new information can lead to unexpectedly
              rapid changes -- or jumps -- in spot prices. The implication is
              that the presumption of normally distributed log-returns may be
              suspect. In particular, the prospect for abnormally fat tails
              becomes important. This article investigates the potential
              presence of jumps in two key natural gas prices: the spot price
              at the Henry Hub in the U. S., and the spot price for natural gas
              at the National Balancing Point in the U. K. We found compelling
              empirical evidence for the importance of jumps in both markets,
              though jumps appear to be more important in the U. K.",
  journal  = "Energy Econ.",
  volume   =  46,
  pages    = "S69--S79",
  month    =  dec,
  year     =  2014,
  keywords = "C58; GARCH; Jump diffusion; Natural gas prices; Q30; Q40"
}

@ARTICLE{Ray1982-ew,
  title     = "A Translog Cost Function Analysis of {U.S}. Agriculture,
               1939--77",
  author    = "Ray, Subhash C",
  abstract  = "The translog cost function provides a convenient framework for
               analyzing U.S. agricultural production in a multioutput context.
               Treating crops and livestock as two distinct outputs, this study
               utilizes standard results of neoclassical duality theory to
               obtain measures of pairwise elasticities of substitution between
               inputs, price elasticities of factor demands, and the rate of
               Hicks-neutral technical change. Results obtained from joint GLS
               estimation of parameters of cost and share equations indicate a
               declining trend in the degree of substitutability between
               capital and labor. Price elasticity of demand for all inputs
               increased over time. The measured rate of technical change was
               1.8\%per year.",
  journal   = "Am. J. Agric. Econ.",
  publisher = "Oxford University Press",
  volume    =  64,
  number    =  3,
  pages     = "490--498",
  month     =  aug,
  year      =  1982,
  keywords  = "elasticities of substitution; hicks-neutral technical change;
               neoclassical"
}

@INCOLLECTION{Greene2008-qh,
  title     = "Econometric Approach to Efficiency Analysis",
  booktitle = "The Measure of Productive Efficiency and Productivity Change",
  author    = "Greene, William",
  editor    = "Fried, Harold O and Lovell, C A Knox and Schmidt, Shelton S",
  publisher = "Oxford University Press",
  edition   = "Second",
  month     =  feb,
  year      =  2008
}

@MISC{Jondrow1982-uj,
  title    = "On the estimation of technical inefficiency in the stochastic
              frontier production function model",
  author   = "Jondrow, James and Knox Lovell, C A and Materov, Ivan S and
              Schmidt, Peter",
  abstract = "The error term in the stochastic frontier model is of the form
              (v--u), where v is a normal error term representing pure
              randomness, and u is a non-negative error term representing
              technical inefficiency. The entire (v--u) is easily estimated for
              each observation, but a previously unsolved problem is how to
              separate it into its two components, v and u. This paper suggests
              a solution to this problem, by considering the expected value of
              u, conditional on (v--u). An explicit formula is given for the
              half-normal and exponential cases.",
  journal  = "J. Econom.",
  volume   =  19,
  pages    = "233--238",
  year     =  1982
}

@ARTICLE{Glass1989-sr,
  title     = "A {MULTI-PRODUCT} {MULTI-INPUT} {COST} {FUNCTION} {ANALYSIS}
               {OF} {NORTHERN} {IRELAND} {AGRICULTURE}, 1955--85",
  author    = "Glass, J C and McKillop, D G",
  abstract  = "A multi-product, multi-input translog cost function is used to
               investigate the structure of Northern Ireland agriculture. The
               two-output four-input cost model utilizes duality theory to
               facilitate econometric measurement of partial elasticities of
               substitution between inputs for each year, the annual own- and
               cross-price elasticities of input demand, the annual overall
               scale economy parameter, and the annual rate of Hicks-neutral
               technical change. It also permits an assessment of cost
               complementarity between the two outputs. The results obtained
               are compared with a similar, earlier study (Ray, 1982) on United
               States agriculture, and with descriptive analyses of Northern
               Ireland agriculture.",
  journal   = "Journal of Agricultural Economics",
  publisher = "Blackwell Publishing Ltd",
  volume    =  40,
  number    =  1,
  pages     = "57--70",
  month     =  jan,
  year      =  1989
}

@ARTICLE{Kuosmanen2006-op,
  title    = "Combining Virtues of {SFA} and {DEA} in a Unified Framework:
              Stochastic Nonparametric Envelopment of Data :",
  author   = "Kuosmanen, Timo",
  journal  = "MTT Discussion Paper",
  volume   = "No. 3/2006",
  year     =  2006,
  keywords = "(SFA); data envelopment analysis (DEA); frontier estimation;
              nonparametric regression; productive efficiency analysis;
              stochastic frontier analysis"
}

@ARTICLE{Aigner1977-ow,
  title    = "Formulation and estimation of stochastic frontier production
              function models",
  author   = "Aigner, Dennis and Lovell, C A Knox and Schmidt, Peter",
  abstract = "Previous studies of the so-called frontier production function
              have not utilized an adequate characterization of the disturbance
              term for such a model. In this paper we provide an appropriate
              specification, by defining the disturbance term as the sum of
              symmetric normal and (negative) half-normal random variables.
              Various aspects of maximum-likelihood estimation for the
              coefficients of a production function with an additive
              disturbance term of this sort are then considered.",
  journal  = "J. Econom.",
  volume   =  6,
  number   =  1,
  pages    = "21--37",
  month    =  jul,
  year     =  1977
}

@ARTICLE{Pettenuzzo2014-we,
  title    = "Forecasting stock returns under economic constraints",
  author   = "Pettenuzzo, Davide and Timmermann, Allan and Valkanov, Rossen",
  abstract = "We propose a new approach to imposing economic constraints on
              time series forecasts of the equity premium. Economic constraints
              are used to modify the posterior distribution of the parameters
              of the predictive return regression in a way that better allows
              the model to learn from the data. We consider two types of
              constraints: non-negative equity premia and bounds on the
              conditional Sharpe ratio, the latter of which incorporates
              time-varying volatility in the predictive regression framework.
              Empirically, we find that economic constraints systematically
              reduce uncertainty about model parameters, reduce the risk of
              selecting a poor forecasting model, and improve both statistical
              and economic measures of out-of-sample forecast performance.",
  journal  = "J. financ. econ.",
  volume   =  114,
  number   =  3,
  pages    = "517--553",
  month    =  dec,
  year     =  2014,
  keywords = "C11; C22; Economic constraints; Equity premium predictions; G11;
              G12; Sharpe ratio"
}

@MISC{Chen_undated-te,
  title    = "Chapter 76 Large Sample Sieve Estimation of {Semi-Nonparametric}
              Models",
  author   = "Chen, Xiaohong",
  abstract = "Often researchers find parametric models restrictive and
              sensitive to deviations from the parametric specifications;
              semi-nonparametric models are more flexible and robust, but lead
              to other complications such as introducing infinite-dimensional
              parameter spaces that may not be compact and the optimization
              problem may no longer be well-posed. The method of sieves
              provides one way to tackle such difficulties by optimizing an
              empirical criterion over a sequence of approximating parameter
              spaces (i.e., sieves); the sieves are less complex but are dense
              in the original space and the resulting optimization problem
              becomes well-posed. With different choices of criteria and
              sieves, the method of sieves is very flexible in estimating
              complicated semi-nonparametric models with (or without)
              endogeneity and latent heterogeneity. It can easily incorporate
              prior information and constraints, often derived from economic
              theory, such as monotonicity, convexity, additivity,
              multiplicity, exclusion and nonnegativity. It can simultaneously
              estimate the parametric and nonparametric parts in
              semi-nonparametric models, typically with optimal convergence
              rates for both parts. This chapter describes estimation of
              semi-nonparametric econometric models via the method of sieves.
              We present some general results on the large sample properties of
              the sieve estimates, including consistency of the sieve extremum
              estimates, convergence rates of the sieve M-estimates, pointwise
              normality of series estimates of regression functions, root-n
              asymptotic normality and efficiency of sieve estimates of smooth
              functionals of infinite-dimensional parameters. Examples are used
              to illustrate the general results. ?? 2007 Elsevier B.V. All
              rights reserved.",
  journal  = "Handbook of Econometrics",
  volume   =  6,
  pages    = "5549--5632",
  keywords = "endogeneity in semi-nonparametric models; semiparametric two-step
              estimation; series; sieve extremum estimation; sieve minimum
              distance"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kymn2001-zq,
  title     = "The {CES‚Äê‚ÄêTranslog} Production Function, Returns to Scale and
               {AES}",
  author    = "Kymn, K O and Hisnanick, J J",
  abstract  = "ABSTRACT The translog functional form imposes no a priori
               restrictions on the substitution possibilities between the
               factor inputs, by relaxing the assumption of strong
               separability, and the CES$\pm$translog cost function
               specification allows for testing homothetic technology with ...",
  journal   = "Bull. Econ. Res.",
  publisher = "Wiley Online Library",
  volume    =  53,
  pages     = "207--214",
  year      =  2001
}

@ARTICLE{Ray1998-vg,
  title    = "Measuring Scale Efficiency from a Translog Production Function",
  author   = "Ray, Subhash C",
  abstract = "In parametric analysis based on a frontier production function,
              usually the scale elasticity rather than scale efficiency level
              is reported. In this paper we show how one can use an estimated
              translog production function to obtain output- and input-oriented
              measures of scale efficiency at an observed input bundle. We also
              show how the estimated model can be used to determine the optimal
              quantity of labor input for an exogenously fixed quantity of
              capital.",
  journal  = "Journal of Productivity Analysis",
  volume   =  11,
  pages    = "183--194",
  year     =  1998,
  keywords = "at any specific point; but different; characteristics of a
              technology; efficiency are two important; function; is the ratio
              of; measures relating to; most productive scale size; mpss; on
              the production; ray average productivity; scale elasticity; scale
              elasticity and scale; the first; the output to a; the
              proportionate change in; the returns to scale"
}

@ARTICLE{Solow1957-jw,
  title    = "Technical Change and the Aggregate Production Function",
  author   = "Solow, Robert M",
  abstract = "In this day of rationally designed econometric studies and
              super-input-output tables, it takes something more than the usual
              ``willing suspension of disbelief'' to talk seriously of the
              aggregate production function. But the aggregate production
              function is only a little less legitimate a concept than, say,
              the aggregate consumption function, and for some kinds of
              long-run macro-models it is almost as indispensable as the latter
              is for the short-run. As long as we insist on practicing
              macro-economics we shall need aggregate relationships. Even so,
              there would hardly be any justification for returning to this
              old-fashioned topic if I had no novelty to suggest. The new
              wrinkle I want to describe is an elementary way of segregating
              variations in output per head due to technical change from those
              due to changes in the availability of capital per head.
              Naturally, every additional bit of information has its price. In
              this case the price consists of one new required time series, the
              share of labor or property in total income, and one new
              assumption, that factors are paid their marginal products. Since
              the former is probably more respectable than the other data I
              shall use, and since the latter is an assumption often made, the
              price may not be unreasonably high. Before going on, let me be
              explicit that I would not try to justify what follows by calling
              on fancy theorems on aggregation and index numbers.' Either this
              kind of aggregate economics appeals or it doesn't. Personally I
              belong to both schools. If it does, I think one can draw some
              crude but useful conclusions from the results.",
  journal  = "Rev. Econ. Stat.",
  volume   =  39,
  number   =  3,
  pages    = "312",
  month    =  aug,
  year     =  1957
}

@ARTICLE{Bos2010-mm,
  title    = "Do all countries grow alike?",
  author   = "Bos, J W B and Economidou, C and Koetter, M and Kolari, J W",
  abstract = "This paper investigates the driving forces of output change in 77
              countries during the period 1970-2000. A flexible modeling
              strategy is adopted that accounts for (i) the inefficient use of
              resources, and (ii) different production technologies across
              countries. The proposed model can identify technical, efficiency,
              and input change for each of three endogenously determined
              regimes. Membership in these regimes is estimated, rather than
              determined ex ante. This framework enables explorations into the
              determinants of output growth and convergence issues in each
              regime. ?? 2009 Elsevier B.V. All rights reserved.",
  journal  = "J. Dev. Econ.",
  volume   =  91,
  number   =  1,
  pages    = "113--127",
  month    =  jan,
  year     =  2010,
  keywords = "Efficiency; Frontier analysis; Growth; Latent class; Regimes;
              Stochastic"
}

@ARTICLE{Bos2010-pr,
  title    = "Technology clubs, {R\&D} and growth patterns: Evidence from {EU}
              manufacturing",
  author   = "Bos, J W B and Economidou, C and Koetter, M",
  abstract = "This paper investigates the forces driving output change in a
              panel of EU manufacturing industries. A flexible modeling
              strategy is adopted that accounts for: (i) inefficient use of
              resources and (ii) differences in the production technology
              across industries. With our model we are able to identify
              technical, efficiency, and input growth for endogenously
              determined technology clubs. Technology club membership is
              modeled as a function of R\&D intensity. This framework allows us
              to explore the components of output growth in each club,
              technology spillovers and catch-up issues across industries and
              countries. ?? 2009.",
  journal  = "Eur. Econ. Rev.",
  volume   =  54,
  number   =  1,
  pages    = "60--79",
  month    =  jan,
  year     =  2010,
  keywords = "Efficiency; Growth; Latent class; R\&D; Stochastic frontier
              analysis; Technology clubs"
}

@ARTICLE{Kumbhakar2013-uz,
  title     = "A zero inefficiency stochastic frontier model",
  author    = "Kumbhakar, Subal C and Parmeter, Christopher F and Tsionas,
               Efthymios G",
  abstract  = "Traditional stochastic frontier models impose inefficient
               behavior on all firms in the sample of interest. If the data
               under investigation represent a mixture of both fully efficient
               and inefficient firms then off-the-shelf frontier models are
               statistically inadequate. We introduce the zero inefficiency
               stochastic frontier model which can accommodate the presence of
               both efficient and inefficient firms in the sample. We derive
               the corresponding log-likelihood function, conditional mean of
               inefficiency, to estimate observation-specific inefficiency and
               discuss testing for the presence of fully efficient firms. We
               provide both simulated evidence as well as an empirical example
               which demonstrates the applicability of the proposed method.",
  journal   = "J. Econom.",
  publisher = "Elsevier B.V.",
  volume    =  172,
  number    =  1,
  pages     = "66--76",
  month     =  jan,
  year      =  2013
}

@ARTICLE{Becchetti2011-lk,
  title     = "Corporate social responsibility and firm efficiency: a latent
               class stochastic frontier analysis",
  author    = "Becchetti, Leonardo and Trovato, Giovanni",
  abstract  = "The nexus between corporate social responsibility and corporate
               performance is of fundamental importance to understand if the
               former can be a sustainable strategy in the competitive race. In
               this paper we test this relationship on a sample of firms
               observed in a 13-year interval by focusing on a performance
               indicator (productive efficiency) seldom explored in this
               literature with a novel approach (latent class stochastic
               frontiers). Our empirical findings show that firms included in
               the Domini 400 index (a CSR stock market index) do not appear to
               be more distant from the production frontier than firms in the
               control sample after controlling for the heterogeneity of
               production structure.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  36,
  number    =  3,
  pages     = "231--246",
  month     =  dec,
  year      =  2011,
  keywords  = "Corporate social responsibility; Mixture models; Stochastic
               frontier",
  language  = "en"
}

@MISC{Pestana_Barros2009-fa,
  title    = "The Measurement of Efficiency of {UK} Airports, Using a
              Stochastic Latent Class Frontier Model",
  author   = "Pestana Barros, Carlos",
  abstract = "In this paper, the latent stochastic frontier model is used to
              estimate the technical efficiency of UK airports. These airports
              are ranked according to their technical efficiency for the period
              2000-06 and the airports are disentangled and segmented by the
              cost frontier model, which leads us to advise the implementation
              of policies by segments. Economic implications arising from the
              study are also considered.",
  journal  = "Transp. Rev.",
  volume   =  29,
  pages    = "479--498",
  year     =  2009
}

@ARTICLE{Besstremyannaya2011-py,
  title    = "Managerial performance and cost efficiency of Japanese local
              public hospitals: a latent class stochastic frontier model",
  author   = "Besstremyannaya, Galina",
  abstract = "The paper explores the link between managerial performance and
              cost efficiency of 617 Japanese general local public hospitals in
              1999-2007. Treating managerial performance as unobservable
              heterogeneity, the paper employs a panel data stochastic cost
              frontier model with latent classes. Financial parameters
              associated with better managerial performance are found to be
              positively significant in explaining the probability of belonging
              to the more efficient latent class. The analysis of latent class
              membership was consistent with the conjecture that unobservable
              technological heterogeneity reflected in the existence of the
              latent classes is related to managerial performance. The findings
              may support the cause for raising efficiency of Japanese local
              public hospitals by enhancing the quality of management.",
  journal  = "Health Econ.",
  volume   = "20 Suppl 1",
  pages    = "19--34",
  month    =  sep,
  year     =  2011,
  keywords = "hospital efficiency; latent classes; management; stochastic
              frontier analysis",
  language = "en"
}

@ARTICLE{Greene2005-mg,
  title    = "Reconsidering heterogeneity in panel data estimators of the
              stochastic frontier model",
  author   = "Greene, William",
  abstract = "This paper examines several extensions of the stochastic frontier
              that account for unmeasured heterogeneity as well as firm
              inefficiency. The fixed effects model is extended to the
              stochastic frontier model using results that specifically employ
              its nonlinear specification. Based on Monte Carlo results, we
              find that the incidental parameters problem operates on the
              coefficient estimates in the fixed effects stochastic frontier
              model in ways that are somewhat at odds with other familiar
              results. We consider a special case of the random parameters
              model that produces a random effects model that preserves the
              central feature of the stochastic frontier model and accommodates
              heterogeneity. We then examine random parameters and latent class
              models. In these cases, explicit models for firm heterogeneity
              are built into the stochastic frontier. Comparisons with received
              results for these models are presented in an application to the
              U.S. banking industry. ?? 2004 Elsevier B.V. All rights reserved.",
  journal  = "J. Econom.",
  volume   =  126,
  number   =  2,
  pages    = "269--303",
  month    =  jun,
  year     =  2005,
  keywords = "Computation; Fixed effects; Latent class; Monte Carlo; Panel
              data; Random effects; Random parameters; Stochastic frontier;
              Technical efficiency"
}

@ARTICLE{Orea2004-gb,
  title     = "Efficiency measurement using a latent class stochastic frontier
               model",
  author    = "Orea, Luis and Kumbhakar, Subal C",
  abstract  = ".Efficiency estimation in stochastic frontier models typically
               assumes that the underlying production technology is the same
               for all firms. There might, however, be unobserved differences
               in technologies that might be inappropriately labeled as
               inefficiency if such variations in technology are not taken into
               account. We address this issue by estimating a latent class
               stochastic frontier model in a panel data framework. An
               application of the model is presented using Spanish banking
               data. Our results show that bank-heterogeneity can be fully
               controlled when a model with four classes is estimated.",
  journal   = "Empir. Econ.",
  publisher = "Springer-Verlag",
  volume    =  29,
  number    =  1,
  pages     = "169--183",
  month     =  jan,
  year      =  2004,
  keywords  = "Banks; Latent class model; Panel data; Stochastic cost frontier",
  language  = "en"
}

@ARTICLE{Scott_Frame2003-ui,
  title    = "Do credit unions use their tax advantage to benefit members?
              Evidence from a cost function",
  author   = "Scott Frame, W and Karels, Gordon V and McClatchey, Christine A",
  abstract = "This paper examines whether the credit union income tax subsidy
              is passed along to members or consumed by managers. To that end,
              we estimate a translog cost function for credit unions and mutual
              thrifts that is tailored to the unique objectives of mutually
              owned depository institutions. We find that credit unions with
              residential common bonds have higher costs than mutual thrifts,
              but single common bond occupational and associational credit
              unions are more cost efficient. Thus, it appears that residential
              credit unions engage in expense preference behavior and hence
              redirect some portion of their tax benefit away from members.
              \copyright{} 2003 Elsevier Science Inc. All rights reserved.",
  journal  = "Rev. Financ. Econ.",
  volume   =  12,
  number   =  1,
  pages    = "35--47",
  month    =  jan,
  year     =  2003,
  keywords = "Cost function; Credit unions; Differential tax treatment; Expense
              preference behavior"
}

@ARTICLE{Frick2011-uk,
  title     = "Temporal variations in technical efficiency: evidence from
               German soccer",
  author    = "Frick, Bernd and Lee, Young Hoon",
  abstract  = "This paper applies stochastic production frontier models with
               time-varying technical efficiency to a panel data set including
               hitherto unavailable information on team wage bills from the
               first division in German professional soccer (the
               ``Bundesliga'') covering the years 1981--2003. We demonstrate
               that individual teams experience significant variation in
               technical efficiency over an extended period of 22 seasons while
               the league's average level of efficiency remains constant over
               time. More detailed analyses reveal that, first, the decision to
               fire or to retain the head coach is mainly influenced by changes
               in managerial efficiency between two adjacent seasons and,
               second, relegated teams on average experience considerable
               reductions in technical efficiency compared to the previous
               season.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  35,
  number    =  1,
  pages     = "15--24",
  month     =  feb,
  year      =  2011,
  keywords  = "efficiency {\'a}; head coaches {\'a} player; salaries {\'a}
               soccer; stochastic frontier {\'a} technical",
  language  = "en"
}

@INCOLLECTION{Kuosmanen2014-uj,
  title     = "Stochastic nonparametric approach to efficiency analysis : A
               Unified Framework",
  booktitle = "Handbook on Data Envelopment Analysis",
  author    = "Kuosmanen, Timo and Johnson, Andrew and Saastamoinen, Antti",
  editor    = "Zhu, Joe",
  publisher = "Springer",
  pages     = "22--70",
  edition   = "II",
  year      =  2014,
  address   = "New York"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Jardin2009-bi,
  title    = "Efficiency of French football clubs and its dynamics",
  author   = "Jardin, Mathieu",
  abstract = "In the paper we evaluate the efficiency of French football clubs
              (Ligue 1) from 2004 to 2007 using Data Envelopment Analysis (DEA)
              with ¬´ Assurance Region ¬ª. Then, we study the dynamics of clubs'
              performances. Contrary to previous works on other championships,
              best teams in competition or most profitable clubs are not the
              most efficient units in our sample. High average scores show that
              French First League is efficient. The first source of
              inefficiency in the Ligue 1 is linked to size problems and
              over-investments. Despite an average club performance stable over
              the period, we exhibit a deterioration of conditions in which
              clubs operate.",
  month    =  jun,
  year     =  2009,
  keywords = "Gambling; L21 - Business Objectives of the Firm; L83 - Sports;
              Recreation; Tourism"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bosca2009-zr,
  title    = "Increasing offensive or defensive efficiency? An analysis of
              Italian and Spanish football‚òÜ",
  author   = "Bosca, J and Liern, V and Martinez, A and Sala, R",
  journal  = "Omega",
  volume   =  37,
  number   =  1,
  pages    = "63--78",
  month    =  feb,
  year     =  2009,
  keywords = "dea; efficiency; sports"
}

@BOOK{Szymanski2010-ap,
  title     = "Football Economics and Policy",
  author    = "Szymanski, Stefan",
  publisher = "palgrave macmillian",
  year      =  2010
}

@ARTICLE{Fry2014-fr,
  title    = "Let's Get Messi? {Top-Scorer} Productivity in the European
              Champions League",
  author   = "Fry, Tim R L and Galanos, Guillaume and Posso, Alberto",
  abstract = "Getting a player like Lionel Messi in the squad would seem like a
              dream come true for a professional football manager, but is it
              always best to have top-quality players? We study the
              determinants of top goal-scorers' productivity in the UEFA
              Champions League. We find evidence of a concave relationship
              between age and productivity and uncover an inverted-U
              relationship between performance and minutes played. Finally, we
              find a positive effect on height, being left footed and being a
              striker on the probability of scoring a goal. The results have
              important implications for managers both in looking to sign on
              new players and to maximise their potential during a competitive
              match.",
  journal  = "Scott. J. Polit. Econ.",
  volume   =  61,
  number   =  3,
  pages    = "261--279",
  month    =  jul,
  year     =  2014,
  keywords = "champions league (soccer tournament); probability theory;
              professional soccer; soccer players; sports statistics"
}

@ARTICLE{Barajas2014-jy,
  title    = "Spanish Football in Need of Financial Therapy: Cut Expenses and
              Inject Capital",
  author   = "Barajas, {\'A}ngel and Rodriguez, Pl{\'a}cido",
  abstract = "European football in general and Spanish football in particular
              is experiencing huge financial difficulties. This paper analyzes
              Spanish clubs during the period",
  journal  = "Int. J. Sports Financ.",
  pages    = "73--90",
  month    =  feb,
  year     =  2014,
  keywords = "administration; financial crisis; insolvencies; professional
              soccer; z-score; Altman Z-score"
}

@ARTICLE{Barros2014-bu,
  title     = "Cost efficiency of French soccer league teams",
  author    = "Barros, Carlos Pestana and Peypoch, Nicolas and Tainsky, Scott",
  abstract  = "This article evaluates the operational activities of French
               soccer clubs from 2003 to 2011 by using a finite mixture model
               that allows controlling for unobserved heterogeneity. In doing
               so, a stochastic frontier latent class model, which allows the
               existence of different technologies, is adopted to estimate cost
               frontiers. This procedure not only enables us to identify
               different groups of French soccer clubs but also permits to
               analyse their cost efficiency. The main result is that there are
               two groups among the French soccer clubs, both following
               completely different ?technologies? to obtain league points,
               suggesting that business strategies need to be adapted to the
               characteristics of the clubs. Some managerial implications are
               developed.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  46,
  number    =  8,
  pages     = "781--789",
  month     =  mar,
  year      =  2014
}

@ARTICLE{Haas2004-lr,
  title   = "Measuring Efficiency of German Football Teams by Data Envelopment
             Analysis",
  author  = "Haas, D and Kocher, M G and Sutter, Matthias",
  journal = "CEJOR Cent. Eur. J. Oper. Res.",
  volume  =  12,
  pages   = "251--268",
  year    =  2004
}

@MISC{Carmichael2000-mi,
  title    = "Team performance: the case of English Premiership football",
  author   = "Carmichael, Fiona and Thomas, Dennis and Ward, Robert",
  abstract = "Sporting production function studies have been almost entirely US
              based concentrating largely, although not exclusively, on
              baseball. Mainly due to a dearth of match play statistics, there
              have been few studies of other sports, with that of association
              football being a significant omission given the sport's
              international appeal and global coverage. This study attempts to
              redress the balance by utilizing a new data source, containing
              information on a range of specific play variables, to estimate a
              production function for English Premiership football. Our results
              emphasize the key attacking and defensive skills, and provide
              support for the notion that teams may intentionally employ
              dubious or illegal tactics to succeed. The inclusion of team
              effects provides evidence consistent with the view of the
              emergence of an elite group of clubs dominating the league.
              Copyright \copyright{} 2000 John Wiley \& Sons, Ltd.",
  journal  = "Manage. Decis. Econ.",
  volume   =  21,
  pages    = "31--45",
  year     =  2000
}

@ARTICLE{Kern2005-rw,
  title     = "Managerial Efficiency in German Top League Soccer: An
               Econometric Analysis of Club Performances On and Off the Pitch",
  author    = "Kern, Markus and S{\"u}ssmuth, Bernd",
  abstract  = "Abstract. This study applies stochastic frontier analytic
               techniques in the estimation of sporting production functions.
               As ex-ante input factors, we use pre-seasonal estimates of wage
               bills of players and coaches that are transformed during the
               production process of a season into ex-post pecuniary revenues
               and sporting success. In the case of athletic output we find a
               robust pattern of technical efficiency over subsequent seasons.
               Estimates based on economic output, however, do not support an
               efficiency model. A significant inter-seasonal change in overall
               technical productivity rather highlights the economic
               instability of the German soccer industry.",
  journal   = "Ger. Econ. Rev.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  6,
  number    =  4,
  pages     = "485--506",
  month     =  nov,
  year      =  2005,
  keywords  = "Human capital; Sporting production function; Stochastic
               frontier; L83; C21; D24"
}

@ARTICLE{Garcia-Sanchez2007-aa,
  title     = "Efficiency and effectiveness of Spanish football teams: a
               {three-stage-DEA} approach",
  author    = "Garc{\'\i}a-S{\'a}nchez, I M",
  abstract  = "In this paper, we apply a three-stage-DEA model to the Spanish
               Professional Football League, which means separating the teams'
               economic behaviour into three components: operating
               efficiency---of the offence and defence---athletic or operating
               effectiveness, and social effectiveness. The results showed that
               the technical inefficiency of the defence is greater than that
               of the offence, both being caused by aspects linked to the poor
               management of players' abilities and by the football team's
               size. Teams showed a favourable evolution of their offensive and
               defensive efficiency during the 2004/2005 season and to a lesser
               extent in the season before. The point system assigned by the
               professional football league regulations evaluates the teams'
               athletic effectiveness, but we detected that the teams with the
               most experience perform athletically in a more effective manner.
               Their social effectiveness is strongly related to the level of
               play in itself and to factors linked to their PFL ranking:
               participation in international competitions for important
               football teams; or the struggle of minor football teams to stay
               in the first division.",
  journal   = "cent.eur.j.oper.res.",
  publisher = "Springer-Verlag",
  volume    =  15,
  number    =  1,
  pages     = "21--45",
  month     =  mar,
  year      =  2007,
  keywords  = "Data envelopment analysis; Effectiveness; Efficiency; Football;
               Sports",
  language  = "en"
}

@ARTICLE{Espitia-Escuer2010-fs,
  title     = "Measurement of the efficiency of football teams in the Champions
               League",
  author    = "Espitia-Escuer, Manuel and Garc{\'\i}a-Cebri{\'a}n, Luc{\'\i}a
               Isabel",
  abstract  = "In the case of football it could be argued that the purpose of
               clubs is to win the competitions in which they participate.
               However, the assessment of football clubs from the efficiency
               would be relevant in judging whether the results have been
               obtained without waste. The chosen sample is football teams who
               played in the Champions League from 2003 to 2007 and the method
               of calculating the efficiency will be both the traditional
               version of the DEA as well as the version proposed by Andersen
               and Petersen (1993), which allows discrimination among efficient
               units. Copyright \copyright{} 2010 John Wiley \& Sons, Ltd.",
  journal   = "Manage. Decis. Econ.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  31,
  number    =  6,
  pages     = "373--386",
  month     =  sep,
  year      =  2010
}

@ARTICLE{Goddard2005-di,
  title    = "Regression models for forecasting goals and match results in
              association football",
  author   = "Goddard, John",
  abstract = "In the previous literature, two approaches have been used to
              model match outcomes in association football (soccer): first,
              modelling the goals scored and conceded by each team; and second,
              modelling win-draw-lose match results directly. There have been
              no previous attempts to compare the forecasting performance of
              these two types of model. This paper aims to fill this gap.
              Bivariate Poisson regression is used to estimate forecasting
              models for goals scored and conceded. Ordered probit regression
              is used to estimate forecasting models for match results. Both
              types of models are estimated using the same 25-year data set on
              English league football match outcomes. The best forecasting
              performance is achieved using a 'hybrid' specification, in which
              goals-based team performance covariates are used to forecast
              win-draw-lose match results. However, the ifferences between the
              forecasting performance of models based on goals data and models
              based on results data appear to be relatively small. ?? 2004
              International Institute of Forecasters. Published by Elsevier
              B.V. All rights reserved.",
  journal  = "Int. J. Forecast.",
  volume   =  21,
  number   =  2,
  pages    = "331--340",
  month    =  apr,
  year     =  2005,
  keywords = "Bivariate Poisson; Football match results; Ordered probit"
}

@ARTICLE{Guzman2007-lt,
  title     = "Measuring efficiency and productivity in professional football
               teams: evidence from the English Premier League",
  author    = "Guzm{\'a}n, Isidoro and Morrow, Stephen",
  abstract  = "Professional football clubs are unusual businesses, their
               performance judged on and off the field of play. This study is
               concerned with measuring the efficiency of clubs in the English
               Premier League. Information from clubs' financial statements is
               used as a measure of corporate performance. To measure changes
               in efficiency and productivity the Malmquist non-parametric
               technique has been used. This is derived from the Data
               Envelopment Analysis (DEA) linear programming approach, with
               Canonical Correlation Analysis (CCA) being used to ensure the
               cohesion of the input--output variables. The study concludes
               that while clubs operate close to efficient levels for the
               assessed models, there is limited technological advance in their
               performance in terms of the displacement of the technological
               frontier.",
  journal   = "cent.eur.j.oper.res.",
  publisher = "Springer-Verlag",
  volume    =  15,
  number    =  4,
  pages     = "309--328",
  month     =  nov,
  year      =  2007,
  keywords  = "Canonical correlation analysis; Data envelopment analysis;
               Economics; Efficiency; Football; Malmquist productivity index",
  language  = "en"
}

@ARTICLE{Dawson2000-lm,
  title    = "Stochastic Frontiers and the Temporal Structure of Managerial
              Efficiency in English Soccer",
  author   = "Dawson, Peter and Dobson, Stephen and Gerrard, Bill",
  abstract = "This article provides estimates of technical efficiency for a
              panel of managers in English soccer's Premier League for the
              period 1992 to 1998. In contrast to other studies of sporting
              team production, efficiency is estimated at the level of the
              individual manager rather than the club. Fixed and random effects
              models are used to generate managerial efficiency scores assuming
              that efficiency is both time invariant and time varying. The
              efficiency rankings of the different time invariant models are
              very similar. In contrast, the temporal structure and the
              estimation procedures of the time-varying models produce very
              different results. There is evidence that managerial efficiency
              has fallen over the sample period.",
  journal  = "J. Sports Econom.",
  volume   =  1,
  number   =  4,
  pages    = "341--362",
  month    =  nov,
  year     =  2000
}

@ARTICLE{De_Dios_Tena2007-au,
  title    = "Within-season dismissal of football coaches: Statistical analysis
              of causes and consequences",
  author   = "de Dios Tena, Juan and Forrest, David",
  abstract = "The paper examines the triggers for, and, consequences of,
              within-season dismissals of managers (head coaches) in the top
              division of the Spanish Football League during seasons 2002-2003
              to 2004-2005. A major reason for directors deciding on dismissal
              is shown to have been concern that the club in question was in
              danger of demotion out of the division. This suggests that the
              clubs hoped to bring about short-term improvement in performance
              by changing manager. Employing an ordered probit model of match
              results, we demonstrate that an improvement in results tended to
              be achieved but only in home matches. The finding vindicates the
              decisions taken by club directors who dismissed their managers
              and implies that appeasing fans can have on-the-field benefits.
              It is consistent with the importance attributed to crowd support
              in the literature on home advantage in sports. ?? 2006 Elsevier
              B.V. All rights reserved.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  181,
  number   =  1,
  pages    = "362--373",
  month    =  aug,
  year     =  2007,
  keywords = "Football; Home advantage; Managerial change; Ordered probit;
              Scapegoat hypothesis"
}

@ARTICLE{Lee2012-db,
  title    = "Competitive Balance: Time Series Lessons from the English Premier
              League",
  author   = "Lee, Young Hoon and Fort, Rodney",
  abstract = "Structural break points in the First Division/English Premier
              League time series of competitive balance identify an Early
              Period, a Pre-World War II Period, a Post-War Period, and a
              Modern Period. The Early Period corresponds to technology
              diffusion (defense and tactics) along with important economic
              structural imposition by leagues. The war periods are common to
              many time series. The Modern Period's sharp decline in balance
              corresponds to the newest version of the Champions League in
              1994/1995 and the Bosman Ruling of 1995. Rottenberg's invariance
              principle suggests that it would be the former, rather than the
              latter, responsible for the historical rate of decline that
              follows this structural break.",
  journal  = "Scott. J. Polit. Econ.",
  volume   =  59,
  number   =  3,
  pages    = "266--282",
  month    =  jul,
  year     =  2012,
  keywords = "athletic leagues; champions league (soccer tournament); economic
              aspects; structural break (economics); technology transfer; time
              series analysis"
}

@ARTICLE{Leggett2002-ga,
  title    = "Membership growth, multiple membership groups and agency control
              at credit unions",
  author   = "Leggett, Keith J and Strand, Robert W",
  abstract = "Starting in 1982, the federal credit union regulator allowed
              credit unions to add multiple membership groups. The policy was
              disallowed by the U.S. Supreme Court in 1998, but revalidated by
              the U.S. Congress later that year. Allowing credit unions to
              attach multiple membership groups has contributed significantly
              to rapid growth in the industry. As credit unions add unrelated
              groups and expand, the prospects for separation between ownership
              and control increases, creating potential agency control
              problems. This potential is compounded by the one member, one
              vote governance structure of credit unions. This research finds
              empirical evidence that agency problems grow as credit unions add
              membership groups and members. If a credit union takes on more
              than one membership group, and as membership increases,
              management is apparently able to channel residual earnings away
              from members (in the form of higher net interest margins) toward
              itself (higher salaries and operating expenses).",
  journal  = "Rev. Financ. Econ.",
  volume   =  11,
  number   =  1,
  pages    = "37--46",
  month    =  jan,
  year     =  2002,
  keywords = "Corporate governance; Credit unions; G21; G28; G34"
}

@ARTICLE{Barros2011-wa,
  title     = "Productivity drivers and market dynamics in the Spanish first
               division football league",
  author    = "Barros, Carlos Pestana and Garcia-del-Barrio, Pedro",
  abstract  = "This paper analyses efficiency drivers of a representative
               sample of Spanish football clubs by means of the two-stage data
               envelopment analysis (DEA) procedure proposed by Simar and
               Wilson (J Econ, 136:31--64, 2007). In the first stage, the
               technical efficiency of football clubs is estimated using a
               bootstrapped DEA model in order to establish which of them are
               the most efficient; the ranking is based on total productivity
               in the period 1996--2004. In the second stage, the Simar and
               Wilson (J Econ, 136:31--64, 2007) procedure is used to bootstrap
               the DEA scores with a truncated bootstrapped regression. Policy
               implications of the main findings are also considered.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  35,
  number    =  1,
  pages     = "5--13",
  month     =  feb,
  year      =  2011,
  keywords  = "analysis {\'a} truncated regression; data envelopment; spanish
               football clubs {\'a}; {\'a} bootstrapping",
  language  = "en"
}

@ARTICLE{Madden2012-jb,
  title     = "Welfare Economics of ``Financial Fair Play'' in a Sports League
               With Benefactor Owners",
  author    = "Madden, Paul",
  abstract  = "With European soccer leagues in mind, a novel model of club
               owner objectives nests standard profit (and win) maximization,
               but adds benefactor behavior where owners inject personal funds
               to increase their team?s quality. A ?generosity? parameter
               differentiates owners; parameter value zero equates to profit
               maximizers, with benefactors emerging at sufficiently positive
               values. The model is used to investigate consequences of Union
               of European Football Associations? (UEFA) ?Financial Fair Play?
               regulations (FFP) for the league, aimed to preclude benefactor
               injections. Assuming (post-Bosman) a relatively large elasticity
               of talent supply to the league, FFP is a poor regulatory device,
               creating welfare losses for fans, owners, and players.",
  journal   = "J. Sports Econom.",
  publisher = "SAGE Publications",
  volume    =  16,
  number    =  2,
  pages     = "159--184",
  month     =  nov,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Sass2014-dl,
  title     = "Glory Hunters, Sugar Daddies, and {Long-Term} Competitive
               Balance Under {UEFA} Financial Fair Play",
  author    = "Sass, Markus",
  abstract  = "This article analyzes the long-term effects of Union of European
               Football Associations (UEFA) Financial Fair Play on competitive
               balance using a multiperiod adaption of a professional team
               sports model. This study accounts for the empirical fact that a
               club?s market size is positively affected by historic success.
               An increasingly successful club can attract more and more
               supporters and thus yield higher revenues that lead to even more
               success and an ever-growing market size. It is argued that this
               development will result in an utmost uneven contest if so-called
               sugar daddies are prevented from overspending by Financial Fair
               Play and thus cannot longer outweigh a club?s smaller market
               size.",
  journal   = "J. Sports Econom.",
  publisher = "SAGE Publications",
  volume    =  17,
  number    =  2,
  pages     = "148--158",
  month     =  mar,
  year      =  2014
}

@ARTICLE{Toole2014-ke,
  title   = "{EUROPEAN} {FOOTBALL} ' {S} {FINANCIAL} {FAIR} {PLAY}",
  author  = "Toole, O",
  journal = "Acountancy Ireland",
  year    =  2014
}

@ARTICLE{Kuosmanen2013-dx,
  title    = "What is the best practice for benchmark regulation of electricity
              distribution? Comparison of {DEA}, {SFA} and {StoNED} methods",
  author   = "Kuosmanen, Timo and Saastamoinen, Antti and Sipil{\"a}inen, Timo",
  abstract = "Electricity distribution is a natural local monopoly. In many
              countries, the regulators of this sector apply frontier methods
              such as data envelopment analysis (DEA) or stochastic frontier
              analysis (SFA) to estimate the efficient cost of operation. In
              Finland, a new StoNED method was adopted in 2012. This paper
              compares DEA, SFA and StoNED in the context of regulating
              electricity distribution. Using data from Finland, we compare the
              impacts of methodological choices on cost efficiency estimates
              and acceptable cost. While the efficiency estimates are highly
              correlated, the cost targets reveal major differences. In
              addition, we examine performance of the methods by Monte Carlo
              simulations. We calibrate the data generation process (DGP) to
              closely match the empirical data and the model specification of
              the regulator. We find that the StoNED estimator yields a root
              mean squared error (RMSE) of 4\% with the sample size 100.
              Precision improves as the sample size increases. The DEA
              estimator yields an RMSE of approximately 10\%, but performance
              deteriorates as the sample size increases. The SFA estimator has
              an RMSE of 144\%. The poor performance of SFA is due to the wrong
              functional form and multicollinearity. \copyright{} 2013.",
  journal  = "Energy Policy",
  volume   =  61,
  pages    = "740--750",
  month    =  oct,
  year     =  2013,
  keywords = "Frontier estimation; Nonparametric production analysis;
              Productive efficiency"
}

@ARTICLE{Kuosmanen2012-ut,
  title    = "Stochastic semi-nonparametric frontier estimation of electricity
              distribution networks: Application of the {StoNED} method in the
              Finnish regulatory model",
  author   = "Kuosmanen, Timo",
  abstract = "Electricity distribution network is a prime example of a natural
              local monopoly. In many countries, electricity distribution is
              regulated by the government. Many regulators apply frontier
              estimation techniques such as data envelopment analysis (DEA) or
              stochastic frontier analysis (SFA) as an integral part of their
              regulatory framework. While more advanced methods that combine
              nonparametric frontier with stochastic error term are known in
              the literature, in practice, regulators continue to apply
              simplistic methods. This paper reports the main results of the
              project commissioned by the Finnish regulator for further
              development of the cost frontier estimation in their regulatory
              framework. The key objectives of the project were to integrate a
              stochastic SFA-style noise term to the nonparametric, axiomatic
              DEA-style cost frontier, and to take the heterogeneity of firms
              and their operating environments better into account. To achieve
              these objectives, a new method called stochastic nonparametric
              envelopment of data (StoNED) was examined. Based on the insights
              and experiences gained in the empirical analysis using the real
              data of the regulated networks, the Finnish regulator adopted the
              StoNED method in use from 2012 onwards. ?? 2012 Elsevier B.V.",
  journal  = "Energy Econ.",
  volume   =  34,
  number   =  6,
  pages    = "2189--2199",
  month    =  nov,
  year     =  2012,
  keywords = "Energy markets; Heterogeneity; Nonparametric production analysis;
              Productive efficiency"
}

@MISC{Hamil2010-qk,
  title    = "Financial performance in English professional football: `an
              inconvenient truth'",
  author   = "Hamil, Sean and Walters, Geoff",
  abstract = "This article presents an analysis of the financial performance of
              English football since the creation of the Premier League in
              1992. It demonstrates that despite large increases in revenue, in
              particular from broadcasting, football clubs in the Premier
              League and Football League have year-on-year collectively failed
              to post a pre-tax profit; there have been many examples of clubs
              in the Football League having to enter into administration; and
              debt levels have risen. The article argues that, in their denial
              that the financial crisis in the wider economy will seriously
              impact the economic health of English Football, many of the
              industry's leading figures share many of the same hugely
              optimistic assumptions of those who seek to play down the
              significance of climate change. In doing so they overlook the
              'inconvenient truth' coined by former US Vice-President Al Gore
              in the context of the climate change debate, that English
              football is not immune from the impact of wider financial
              instability. The article questions whether, given the current
              economic climate, a failure to implement pro-active regulatory
              action to address the problem of chronic unprofitability and
              unsustainable debt will lead to a major financial crisis in
              English football. [ABSTRACT FROM AUTHOR]",
  journal  = "Soccer Soc.",
  volume   =  11,
  pages    = "354--372",
  year     =  2010
}

@MISC{Szymanski2010-jq,
  title    = "The financial crisis and English football: The dog that will not
              bark",
  author   = "Szymanski, Stefan",
  abstract = "This paper considers the financial crisis of 2008 and its likely
              impact on English football, notably the English Premier League.
              It mostly examines the history of financial instability and
              sporting stability, in the sense of club survival, that is
              characteristic of English football and possibly much of football
              in the rest of the world. The paper suggests that while
              shareholders often lose money, clubs seldom disappear. It also
              suggests that while clubs are not immune to economic cycles, the
              impact is likely to be limited. The reasons for the financial
              instability of particular clubs and stability and success of the
              English leagues are discussed. ABSTRACT FROM AUTHOR",
  journal  = "Int. J. Sports Financ.",
  volume   =  5,
  pages    = "28--40",
  year     =  2010,
  keywords = "Financial crisis; Football clubs"
}

@ARTICLE{Muller2012-oa,
  title    = "The financial fair play regulations of {UEFA}: An adequate
              concept to ensure the long-term viability and sustainability of
              European club football?",
  author   = "M{\"u}ller, J Christian and Lammert, Joachim and Hovemann, Gregor",
  abstract = "In response to the severe financial plight of many clubs tbat
              regularly take part in European competitions, UEFA developed the
              concept of Financial Fair Play as an extension of its licensing
              regulations. The aim of the concept is to curtail financial foul
              play in European football (nonpayment of liabilities owing to
              rival clubs or employ- ees) and financial doping (excessive
              funding provided to cover losses arising from expenses for
              playing talent not balanced by revenues). The paper addresses the
              question if the Financial Fair Play is an adequate concept to
              ensure the long-term viability and sustainability of European
              club football as intended by UEFA. To answer this question, we
              ilustrate the empirical background and search for a theoretical
              justification within the field of sport economics. Based on
              structuring UEFA's objectives, we analyze and evaluate the major
              amendments of the Financial Fair Play Regulations.",
  journal  = "Int. J. Sports Financ.",
  volume   =  7,
  pages    = "117--140",
  year     =  2012,
  keywords = "Financial fair play; Integrity; Licensing; Professional football;
              Rat race; Regulation; Sports leagues"
}

@ARTICLE{Vrooman2007-yj,
  title     = "{THEORY} {OF} {THE} {BEAUTIFUL} {GAME}: {THE} {UNIFICATION} {OF}
               {EUROPEAN} {FOOTBALL}",
  author    = "Vrooman, John",
  abstract  = "European football is in a spiral of intra-league and
               inter-league polarization of talent and wealth. The invariance
               proposition is revisited with adaptations for win-maximizing
               sportsman owners facing an uncertain Champions League prize.
               Sportsman and champion effects have driven European football
               clubs to the edge of insolvency and polarized competition
               throughout Europe. Revenue revolutions and financial crises of
               the Big Five leagues are examined and estimates of competitive
               balance are compared. The European Super League completes the
               open-market solution after Bosman. A 30-team Super League is
               proposed based on the National Football League.",
  journal   = "Scott. J. Polit. Econ.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  54,
  number    =  3,
  pages     = "314--354",
  month     =  jul,
  year      =  2007
}

@ARTICLE{Franck2014-hy,
  title     = "Financial Fair Play in European Club Football: What Is It All
               About?",
  author    = "Franck, Egon",
  abstract  = "The new UEFA Club Licensing and Financial Fair Play Regulations
               have encountered stiff criticism. The concerns are that the new
               regulations may harm football in three different ways: By
               forgoing the potential benefits from substantial injections of
               ``external'' money into payrolls, by restricting competition in
               the player market without at the same time achieving benefits
               from more balanced competition, and by creating some sort of
               barrier to entry which could ``freeze'' the current hierarchy of
               clubs. It is the purpose of this paper to take these concerns as
               a starting point for discussing the likely effects of the new
               regulations. As a by-product it will become obvious why and in
               which points the concerns are unfounded.",
  journal   = "Int. J. Sports Financ.",
  publisher = "Fitness Information Technology",
  volume    =  9,
  number    =  3,
  pages     = "193--217",
  year      =  2014,
  keywords  = "competitive balance; financial fair play; hard budget
               constraints; moral hazard; ossification; rent-seeking; soft
               budget constraints; sugar daddies"
}

@ARTICLE{Heckman1998-kk,
  title     = "Matching As An Econometric Evaluation Estimator",
  author    = "Heckman, James J and Ichimura, Hidehiko and Todd, Petra",
  abstract  = "This paper develops the method of matching as an econometric
               evaluation estimator. A rigorous distribution theory for
               kernel-based matching is presented. The method of matching is
               extended to more general conditions than the ones assumed in the
               statistical literature on the topic. We focus on the method of
               propensity score matching and show that it is not necessarily
               better, in the sense of reducing the variance of the resulting
               estimator, to use the propensity score method even if propensity
               score is known. We extend the statistical literature on the
               propensity score by considering the case when it is estimated
               both parametrically and nonparametrically. We examine the
               benefits of separability and exclusion restrictions in improving
               the efficiency of the estimator. Our methods also apply to the
               econometric selection bias estimator.",
  journal   = "Rev. Econ. Stud.",
  publisher = "Oxford University Press",
  volume    =  65,
  number    =  2,
  pages     = "261--294",
  month     =  apr,
  year      =  1998
}

@ARTICLE{Malekmohammadi2011-lk,
  title   = "Data envelopment scenario analysis with imprecise data",
  author  = "Malekmohammadi, Najmeh and Lotfi, Farhad Hosseinzadeh and Jaafar,
             Azmi B",
  journal = "CEJOR Cent. Eur. J. Oper. Res.",
  volume  =  19,
  number  =  1,
  pages   = "65--79",
  month   =  mar,
  year    =  2011
}

@BOOK{Box1978-km,
  title     = "Statistics for experimenters: an introduction to design, data
               analysis, and model building",
  author    = "Box, George E P and Hunter, William Gordon and Stuart Hunter, J",
  abstract  = "Introduces the philosophy of experimentation and the part that
               statistics play in experimentation. Emphasizes the need to
               develop a capability for ``statistical thinking'' by using
               examples drawn from actual case studies.",
  publisher = "Wiley",
  volume    = "1st",
  pages     = "653",
  month     =  jul,
  year      =  1978,
  language  = "en"
}

@BOOK{Gelman2007-yw,
  title     = "Data Analysis Using Regression and {Multilevel/Hierarchical}
               Models",
  author    = "Gelman, Andrew and Hill, Jennifer",
  abstract  = "Data Analysis Using Regression and Multilevel/Hierarchical
               Models is a comprehensive manual for the applied researcher who
               wants to perform data analysis using linear and nonlinear
               regression and multilevel models. The book introduces a wide
               variety of models, whilst at the same time instructing the
               reader in how to fit these models using available software
               packages. The book illustrates the concepts by working through
               scores of real data examples that have arisen from the authors'
               own applied research, with programming codes provided for each
               one. Topics covered include causal inference, including
               regression, poststratification, matching, regression
               discontinuity, and instrumental variables, as well as multilevel
               logistic regression and missing-data imputation. Practical tips
               regarding building, fitting, and understanding are provided
               throughout. Author resource page:
               http://www.stat.columbia.edu/~gelman/arm/",
  publisher = "Cambridge University Press",
  pages     = "1--651",
  year      =  2007,
  language  = "en"
}

@INCOLLECTION{Rubin2008-kw,
  title     = "Best Practices in {Quasi--Experimental} Designs: Matching
               Methods for Causal Inference : {SAGE} Research Methods",
  booktitle = "Best practices in quantitative methods",
  author    = "Rubin, Donald B and Stuart, Elizabeth A",
  year      =  2008
}

@ARTICLE{Morgan2006-nu,
  title    = "Matching Estimators of Causal Effects: Prospects and Pitfalls in
              Theory and Practice",
  author   = "Morgan, Stephen L and Harding, David J",
  abstract = "As the counterfactual model of causality has increased in
              popularity, sociologists have returned to matching as a research
              methodology. In this article, advances over the past two decades
              in matching estimators are explained, and the practical
              limitations of matching techniques are emphasized. The authors
              introduce matching methods by focusing first on ideal scenarios
              in which stratification and weighting procedures warrant causal
              inference. Then, they discuss how matching is often undertaken in
              practice, offering an overview of the most prominent data
              analysis routines. With four hypothetical examples, they
              demonstrate how the assumptions behind matching estimators often
              break down in practice. Even so, the authors argue that matching
              techniques can be used effectively to strengthen the prosecution
              of causal questions in sociology.",
  journal  = "Sociol. Methods Res.",
  volume   =  35,
  number   =  1,
  pages    = "3--60",
  month    =  aug,
  year     =  2006
}

@ARTICLE{Barros2010-sn,
  title     = "Brazilian Football League Technical Efficiency: A Simar and
               Wilson Approach",
  author    = "Barros, Carlos Pestana and Assaf, Albert and S{\'a}-Earp, Fabio",
  abstract  = "This article introduces the Data Envelopment Analysis (DEA)
               bootstrap procedure to analyze the technical efficiency of
               Brazilian first league football clubs. For comparison purpose
               the study also estimates the efficiency scores with the
               traditional DEA model. From the results, it is clear that there
               is a significant difference between the efficiency scores
               derived from the traditional DEA and the bootstrap DEA models.
               In terms of the average performance of the Brazilian league,
               both methods indicate that the Brazilian clubs operate at a high
               level of inefficiency. Factors that contributed to these results
               as well as other policy implications are provided.",
  journal   = "J. Sports Econom.",
  publisher = "SAGE PublicationsSage CA: Los Angeles, CA",
  volume    =  11,
  number    =  6,
  pages     = "641--651",
  month     =  jan,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Pana2013-jf,
  title    = "The Impact of {Internet-Based} Services on Credit Unions: A
              Propensity Score Matching Approach",
  author   = "Pana, Elisabeta and Vitzthum, Sascha and Willis, David Mitchell",
  abstract = "Credit unions focus their profit management on providing benefits
              to their members and augmenting their institutional well-being
              through capital accumulation. I",
  journal  = "Review of Quantitative Finance and Accounting",
  volume   = "Forthcomin",
  month    =  jan,
  year     =  2013,
  keywords = "Credit unions; Information and internet services; Technology;
              Value"
}

@ARTICLE{Wintoki2012-bl,
  title    = "Endogeneity and the dynamics of internal corporate governance",
  author   = "Wintoki, M Babajide and Linck, James S and Netter, Jeffry M",
  abstract = "We use a well-developed dynamic panel generalized method of
              moments (GMM) estimator to alleviate endogeneity concerns in two
              aspects of corporate governance research: the effect of board
              structure on firm performance and the determinants of board
              structure. The estimator incorporates the dynamic nature of
              internal governance choices to provide valid and powerful
              instruments that address unobserved heterogeneity and
              simultaneity. We re-examine the relation between board structure
              and performance using the GMM estimator in a panel of 6,000 firms
              over a period from 1991 to 2003, and find no causal relation
              between board structure and current firm performance. We
              illustrate why other commonly used estimators that ignore the
              dynamic relationship between current governance and past firm
              performance may be biased. We discuss where it may be appropriate
              to consider the dynamic panel GMM estimator in corporate
              governance research, as well as caveats to its use.",
  journal  = "J. financ. econ.",
  volume   =  105,
  number   =  3,
  pages    = "581--606",
  month    =  sep,
  year     =  2012,
  keywords = "Board independence; Board size; Board structure; Corporate
              governance; Dynamic panel GMM estimator; Endogeneity; G32; G34;
              K22"
}

@ARTICLE{Flannery2013-ma,
  title    = "Estimating dynamic panel models in corporate finance",
  author   = "Flannery, Mark J and Hankins, Kristine Watson",
  abstract = "Dynamic panel models play a natural role in several important
              areas of corporate finance, but the combination of fixed effects
              and lagged dependent variables introduces serious econometric
              bias. Several methods of counteracting these biases are available
              and these methodologies have been tested on small datasets with
              independent, normally-distributed explanatory variables. However,
              no one has evaluated the methods' performance with corporate
              finance data, in which the dependent variable may be clustered or
              censored and independent variables may be missing, correlated
              with one another, or endogenous. We find that the data's
              properties substantially affect the estimators' performances. We
              provide evidence about the impact of various data set
              characteristics on the estimators, so that researchers can
              determine the best approach for their datasets.",
  journal  = "Journal of Corporate Finance",
  volume   =  19,
  pages    = "1--19",
  month    =  feb,
  year     =  2013,
  keywords = "C23; Corporate finance; Dynamic panels; Econometrics; G30"
}

@ARTICLE{Pope2011-ad,
  title    = "Scope Properties: Nonparametric Assessment, Policy Insights and
              Functional Estimation",
  author   = "Pope, Brandon and Johnson, Andrew",
  abstract = "Knowledge of the production function's scope and scale properties
              can provide insights for firms choosing their operating strategy,
              policy makers considering industry structure, and analysts
              determining which tools are appropriate. Although scale
              properties and their assessment have been popular topics in the
              productivity literature, scope properties have received less
              attention. We introduce a new property, returns to scope, which
              is disentangled from scale properties and does not rely on price
              information. We consider existing frontier estimation procedures
              under various scope and scale properties, and identify methods
              which impose restrictions on the frontier consistent with these
              properties. Based on desirable characteristics of an estimator of
              returns to scope, we propose two methods for its assessment.
              Finally, we present examples and insights using simulated and
              real data.",
  journal  = "SSRN Journal",
  volume   =  40,
  number   =  2,
  pages    = "239--250",
  year     =  2011,
  keywords = "Nonparametric Estimation; Production Functions; Scope"
}

@ARTICLE{Panzar1981-xv,
  title     = "Economies of Scope",
  author    = "Panzar, John C and Willig, Robert D",
  journal   = "Am. Econ. Rev.",
  publisher = "American Economic Association",
  volume    =  71,
  number    =  2,
  pages     = "268--272",
  year      =  1981
}

@ARTICLE{Syverson2011-vu,
  title    = "What Determines Productivity?",
  author   = "Syverson, Chad",
  abstract = "Economists have shown that large and persistent differences in
              productivity levels across businesses are ubiquitous. This
              finding has shaped research agendas in a number of fields,
              including (but not limited to) macroeconomics, industrial
              organization, labor, and trade. This paper surveys and evaluates
              recent empirical work addressing the question of why businesses
              differ in their measured productivity levels. The causes are
              manifold, and differ depending on the particular setting. They
              include elements sourced in production practices---and therefore
              over which producers have some direct control, at least in
              theory---as well as from producers' external operating
              environments. After evaluating the current state of knowledge, I
              lay out what I see are the major questions that research in the
              area should address going forward.",
  journal  = "J. Econ. Lit.",
  volume   =  49,
  number   =  2,
  pages    = "326--365",
  month    =  jun,
  year     =  2011
}

@ARTICLE{Kuosmanen2014-gx,
  title    = "Orthogonality conditions for identification of joint production
              technologies : Axiomatic nonparametric approach to the estimation
              of stochastic distance functions",
  author   = "Kuosmanen, Timo and Johnson, Andrew and Parmeter, Christopher",
  journal  = "J. Econom.",
  number   = "Forthcoming",
  pages    = "1--37",
  year     =  2014,
  keywords = "c14; c21; c51; d24; economies of scope; efficiency analysis;
              endogeneity; frontier estimation; jel classification;
              productivity measurement; simultaneity bias"
}

@ARTICLE{Montgomery2014-vb,
  title    = "Too big to succeed? Banking sector consolidation and efficiency",
  author   = "Montgomery, Heather and Harimaya, Kozo and Takahashi, Yuki",
  abstract = "This study examines the effect of banking sector consolidation on
              bank profit and cost efficiency using data from Japan. Our
              analysis shows that bank merger events have little impact on
              profit efficiency, but significantly lower cost efficiency. This
              suggests that government-coordinated consolidation of banks,
              especially in a post-crisis environment, results in less cost
              efficient entities, although the bottom line of profit efficiency
              is maintained. Our analysis of changes in banking sector
              competitiveness over the same period suggests that these merged
              banks are able to maintain their ``bottom line'' due to increased
              market power.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  32,
  pages    = "86--106",
  month    =  sep,
  year     =  2014,
  keywords = "Bank; Crisis; Efficiency; G01; G21; G28; Merger"
}

@ARTICLE{Picazo-Tadeo2010-pw,
  title     = "Does playing several competitions influence a team's league
               performance? Evidence from Spanish professional football",
  author    = "Picazo-Tadeo, Andr{\'e}s J and Gonz{\'a}lez-G{\'o}mez, Francisco",
  abstract  = "The sporting performance of professional football teams has
               often been assessed considering their results in the major
               regular competition, namely the national league. Here, we show
               that evaluating league performance without controlling for extra
               games played in other competitions might produce misleading
               results. Using Data Envelopment Analysis, we assess the
               performance of Spanish professional football teams in the League
               controlling for the extra games played in the King's Cup,
               Champions League and UEFA Cup. Results show that assessing
               performance omitting extra games underestimates teams' true
               performance in the League, the more extra games played leading
               to greater bias. Consequently, the multioutput nature of
               football must be considered when assessing team performance.",
  journal   = "CEJOR Cent. Eur. J. Oper. Res.",
  publisher = "Springer-Verlag",
  volume    =  18,
  number    =  3,
  pages     = "413--432",
  month     =  sep,
  year      =  2010,
  keywords  = "data envelopment analysis; football -- competitions; performance
               evaluation; professional sports; soccer teams; statistical
               analysis in sports",
  language  = "en"
}

@ARTICLE{Peeters2014-mp,
  title     = "Financial fair play in European football",
  author    = "Peeters, Thomas and Szymanski, Stefan",
  abstract  = "In 2010 UEFA, the governing body of European football, announced
               a set of financial restraints, which clubs must observe when
               seeking to enter its competitions, notably the UEFA Champions
               League. We analyse the financial and sporting impact of these
               `Financial Fair Play' (FFP) regulations in four major European
               football leagues. We first discuss the details of FFP and frame
               these regulations in the institutional set-up of the European
               football industry. We then show how the break-even constraint
               embedded in FFP could substantially reduce average payrolls and
               wage-to-turnover ratios, while strengthening the position of the
               traditional top teams. Since the benefits of the break-even rule
               to consumers remain unclear, we argue that these rent-shifting
               regulations might fall foul of European competition law.---
               Thomas Peeters and Stefan Szymanski",
  journal   = "Econ. Policy",
  publisher = "Oxford University Press",
  volume    =  29,
  number    =  78,
  pages     = "343--390",
  month     =  apr,
  year      =  2014,
  keywords  = "champions league (soccer tournament); finance; football; soccer"
}

@ARTICLE{Tunaru2005-gd,
  title    = "An option pricing framework for valuation of football players",
  author   = "Tunaru, Radu and Clark, Ephraim and Viney, Howard",
  abstract = "In this paper we develop a contingent claims framework for
              determining the financial value of professional football players.
              Contingent claims style modelling is used to develop two models.
              The pricing of football players is based on a performance index
              such as the Carling Opta Index. Unexpected events such as
              injuries are included into the models as Poisson jump processes.
              The value of a player varies from club to club, depending on club
              turnover and the total number of performance points generated by
              the entire team. ?? 2005 Elsevier Inc. All rights reserved.",
  journal  = "Rev. Financ. Econ.",
  volume   =  14,
  number   = "3-4",
  pages    = "281--295",
  month    =  jan,
  year     =  2005,
  keywords = "Geometric Brownian motion; Investment analysis; Ito's lemma; Jump
              processes; Real options"
}

@ARTICLE{Rottenberg1956-kv,
  title     = "The Baseball Players' Labor Market",
  author    = "Rottenberg, Simon",
  abstract  = "No abstract is available for this item.",
  journal   = "J. Polit. Econ.",
  publisher = "University of Chicago Press",
  volume    =  64,
  pages     = "242--242",
  year      =  1956
}

@ARTICLE{Chambers1996-zp,
  title    = "Benefit and Distance Functions",
  author   = "Chambers, Robert G and Chung, Yangho and F{\"a}re, Rolf",
  abstract = "We explore the relationship between R. W. Shephard's input
              distance function (``Cost and Production Functions,'' Princeton
              Univ. Press, Princeton, 1953) and D. G. Luenberger's benefit
              function (J. Math. Econ.21(1992a), 461-481). We point out that
              the latter can be recognized in a production context as a
              directional input distance function which can exhaustively
              characterize technologies in both price and input space. D.
              McFadden's (Cost, revenue, and profit functions,in``Production
              Economics: A Dual Approach to Theory and Applications,
              ''North-Holland/Elsevier, New York, 1978) composition rules for
              input sets and input distance functions are then extended to the
              directional input distance function.Journal of Economic
              LiteratureClassification Numbers : D21, D24, D29.",
  journal  = "J. Econ. Theory",
  volume   =  70,
  number   =  2,
  pages    = "407--419",
  month    =  aug,
  year     =  1996
}

@ARTICLE{Fare2000-qe,
  title     = "Theory and Application of Directional Distance Functions",
  author    = "F{\"a}re, Rolf and Grosskopf, Shawna",
  abstract  = "In 1957 Farrell demonstrated how cost inefficiency could be
               decomposed into two mutually exclusive and exhaustive
               components: technical and allocative inefficiency. This result
               is consequence of the fact that---as shown by Shephard---the
               cost function and the input distance function (the reciprocal of
               Farrell's technical efficiency measure) are `dual' to each
               other. Similarly, the revenue function and the output distance
               function are dual providing the basis for the decomposition of
               revenue inefficiency into technical and allocative components
               (see for example, F{\"a}re, Grosskopf and Lovell (1994)). Here
               we extend those results to include the directional distance
               function and its dual, the profit function. This provides the
               basis for defining and decomposing profit efficiency. As we
               show, the output and input distance functions (reciprocals of
               Farrell efficiency measures) are special cases of the
               directional distance function. We also show how to use the
               directional distance function as a tool for measuring capacity
               utilization using DEA type techniques.",
  journal   = "Journal of Productivity Analysis",
  publisher = "Kluwer Academic Publishers",
  volume    =  13,
  number    =  2,
  pages     = "93--103",
  month     =  mar,
  year      =  2000,
  keywords  = "directional distance function; farrell efficiency; profit
               efficiency",
  language  = "en"
}

@ARTICLE{Chung1997-aa,
  title    = "Productivity and Undesirable Outputs: A Directional Distance
              Function Approach",
  author   = "Chung, Y H and F{\"a}re, R and Grosskopf, S",
  abstract = "Undesirable outputs are often produced together with desirable
              outputs. This joint production of good and bad outputs is
              typically ignored in traditional measures of productivity since
              ``prices'' are typically unavailable for bad outputs. Here we
              introduce a directional distance function and use it as a
              component in a new productivity index that readily models joint
              production of goods and bads, credits firms for reductions in
              bads and increases in goods, and does not require shadow prices
              of bad outputs. This index, as an empirical example shows, solves
              the problem caused by the joint production of good and bad
              outputs, and provides a practical managerial tool.",
  journal  = "J. Environ. Manage.",
  volume   =  51,
  number   =  3,
  pages    = "229--240",
  month    =  nov,
  year     =  1997,
  keywords = "Malmquist--Luenberger productivity index; directional distance
              function; productivity"
}

@ARTICLE{Johnson2008-ph,
  title    = "Outlier detection in two-stage semiparametric {DEA} models",
  author   = "Johnson, Andrew L and McGinnis, Leon F",
  abstract = "In the use of peer group data to assess individual, typical or
              best practice performance, the effective detection of outliers is
              critical for achieving useful results, particularly for two-stage
              analyses. In the DEA-related literature, prior work on this issue
              has focused on the efficient frontier as a basis for detecting
              outliers. An iterative approach for dealing with the potential
              for one outlier to mask the presence of another has been proposed
              but not demonstrated. This paper proposes using both the
              efficient frontier and the inefficient frontier to identify
              outliers and thereby improve the accuracy of second stage results
              in two-stage nonparametric analysis. The iterative outlier
              detection approach is implemented in a leave-one-out method using
              both the efficient frontier and the inefficient frontier and
              demonstrated in a two-stage semi-parametric bootstrapping
              analysis of a classic data set. The results show that the
              conclusions drawn can be different when outlier identification
              includes consideration of the inefficient frontier.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  187,
  number   =  2,
  pages    = "629--635",
  month    =  jun,
  year     =  2008,
  keywords = "Data envelopment analysis; Inefficient frontier; Outlier
              detection; Productivity"
}

@MISC{Banker2008-od,
  title    = "Evaluating Contextual Variables Affecting Productivity Using Data
              Envelopment Analysis",
  author   = "Banker, R D and Natarajan, R",
  abstract = "A DEA-based stochastic frontier estimation framework is presented
              to evaluate contextual variables affecting productivity that
              allows for both one-sided inefficiency deviations as well as
              two-sided random noise. Conditions are identified under which a
              two-stage procedure consisting of DEA followed by ordinary least
              squares (OLS) regression analysis yields consistent estimators of
              the impact of contextual variables. Conditions are also
              identified under which DEA in the first stage followed by maximum
              likelihood estimation (MLE) in the second stage yields consistent
              estimators of the impact of contextual variables. This requires
              the contextual variables to be independent of the input
              variables, but the contextual variables may be correlated with
              each other. Monte Carlo simulations are carried out to compare
              the performance of our two-stage approach with one-stage and
              two-stage parametric approaches. Simulation results indicate that
              DEA-based procedures with OLS, maximum likelihood, or even Tobit
              estimation in the second stage perform as well as the best of the
              parametric methods in the estimation of the impact of contextual
              variables on productivity. Simulation results also indicate that
              DEA-based procedures perform better than parametric methods in
              the estimation of individual decision-making unit (DMU)
              productivity. Overall, the results establish DEA as a
              nonparametric stochastic frontier estimation (SFE) methodology.
              [ABSTRACT FROM AUTHOR]",
  journal  = "Oper. Res.",
  volume   =  56,
  pages    = "48--58",
  year     =  2008
}

@ARTICLE{Johnson2012-xy,
  title    = "One-stage and two-stage {DEA} estimation of the effects of
              contextual variables",
  author   = "Johnson, Andrew L and Kuosmanen, Timo",
  abstract = "Two-stage data envelopment analysis (2-DEA) is commonly used in
              productive efficiency analysis to estimate the effects of
              operational conditions and practices on performance. In this
              method the DEA efficiency estimates are regressed on contextual
              variables representing the operational conditions. We reexamine
              the statistical properties of the 2-DEA estimator, and find that
              it is statistically consistent under more general conditions than
              earlier studies assume. We further show that the finite sample
              bias of DEA in the first stage carries over to the second stage
              regression, causing bias in the estimated coefficients of the
              contextual variables. This bias is particularly severe when the
              contextual variables are correlated with inputs. To address this
              shortcoming, we apply the result that DEA can be formulated as a
              constrained special case of the convex nonparametric least
              squares (CNLS) regression. Applying the CNLS formulation, we
              develop a new semi-nonparametric one-stage estimator for the
              coefficients of the contextual variables that directly
              incorporates contextual variables to the standard DEA problem.
              The proposed method is hence referred to as one-stage DEA
              (1-DEA). Evidence from Monte Carlo simulations suggests that the
              new 1-DEA estimator performs systematically better than the
              conventional 2-DEA estimator both in deterministic and noisy
              scenarios. \copyright{} 2012 Elsevier B.V. All rights reserved.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  220,
  number   =  2,
  pages    = "559--570",
  month    =  jul,
  year     =  2012,
  keywords = "Data envelopment analysis; Partial linear model;
              Semi-nonparametric regression"
}

@ARTICLE{Wheelock2010-bp,
  title     = "Are Credit Unions Too Small?",
  author    = "Wheelock, David C and Wilson, Paul W",
  abstract  = "Abstract U.S. credit unions serve 93 million members, hold 10\%
               of U.S. savings deposits, and make 13.2\% of all nonrevolving
               consumer loans. Since 1985, the share of U.S. depository
               institution assets held by credit unions has nearly doubled, and
               the average (inflation-adjusted) size of credit unions has
               increased over 600\%. We use a local-linear estimator,
               dimesion-reduction techniques, and bootstrap methods to estimate
               and make inference about ray scale and expansion-path scale
               economies. We find substantial evidence of increasing returns to
               scale among credit unions of all sizes, suggesting that further
               consolidation and growth among credit unions are likely.",
  journal   = "Rev. Econ. Stat.",
  publisher = "MIT Press",
  volume    =  93,
  number    =  4,
  pages     = "1343--1359",
  month     =  sep,
  year      =  2010
}

@ARTICLE{Banker1986-iv,
  title     = "Efficiency Analysis for Exogenously Fixed Inputs and Outputs",
  author    = "Banker, Rajiv D and Morey, Richard C",
  abstract  = "We evaluate, by means of mathematical programming formulations,
               the relative technical and scale efficiencies of decision making
               units (DMUs) when some of the inputs or outputs are exogenously
               fixed and beyond the discretionary control of DMU managers. This
               approach further develops the work on efficiency evaluation and
               on estimation of efficient production frontiers known as data
               envelopment analysis (DEA). We also employ the model to provide
               efficient input and output targets for DMU managers in a way
               that specifically accounts for the fixed nature of some of the
               inputs or outputs. We illustrate the approach, using real data,
               for a network of fast food restaurants.",
  journal   = "Oper. Res.",
  publisher = "INFORMS",
  volume    =  34,
  number    =  4,
  pages     = "513--521",
  month     =  aug,
  year      =  1986
}

@ARTICLE{Ruggiero1996-eu,
  title    = "On the measurement of technical efficiency in the public sector",
  author   = "Ruggiero, John",
  abstract = "Existing measures of technical inefficiency obtained through
              linear programming models in the public sector do not properly
              control for environmental variables that affect production. It
              will be shown that the consequences of not controlling for these
              fixed factors are biased estimates of technical efficiency. This
              paper extends the mathematical programming approach to frontier
              estimation known as Data Envelopment Analysis to allow for
              environmental variables. This modified model will be then
              contrasted with the existing model that purportedly controls for
              exogeneous factors to measure public sector efficiency with
              simulated data. The results provide evidence that the existing
              Data Envelopment Analysis model will overestimate the level of
              technical inefficiency and that the modified model developed in
              this paper does a better job controlling for exogenous factors.
              The modified model is also applied to analyze the technical
              efficiency of school districts.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  90,
  number   =  3,
  pages    = "553--565",
  month    =  may,
  year     =  1996,
  keywords = "Data Envelopment Analysis; Efficiency; Environmental variables;
              Public sector production"
}

@ARTICLE{Fama2012-eh,
  title    = "Size, value, and momentum in international stock returns",
  author   = "Fama, Eugene F and French, Kenneth R",
  abstract = "In the four regions (North America, Europe, Japan, and Asia
              Pacific) we examine, there are value premiums in average stock
              returns that, except for Japan, decrease with size. Except for
              Japan, there is return momentum everywhere, and spreads in
              average momentum returns also decrease from smaller to bigger
              stocks. We test whether empirical asset pricing models capture
              the value and momentum patterns in international average returns
              and whether asset pricing seems to be integrated across the four
              regions. Integrated pricing across regions does not get strong
              support in our tests. For three regions (North America, Europe,
              and Japan), local models that use local explanatory returns
              provide passable descriptions of local average returns for
              portfolios formed on size and value versus growth. Even local
              models are less successful in tests on portfolios formed on size
              and momentum.",
  journal  = "J. financ. econ.",
  volume   =  105,
  number   =  3,
  pages    = "457--472",
  month    =  sep,
  year     =  2012,
  keywords = "Four-factor model; G12; Momentum; Three-factor model; Value
              premium"
}

@ARTICLE{Papadimitriou2014-am,
  title    = "Forecasting energy markets using support vector machines",
  author   = "Papadimitriou, Theophilos and Gogas, Periklis and Stathakis,
              Efthimios",
  abstract = "In this paper we investigate the efficiency of a support vector
              machine (SVM)-based forecasting model for the next-day
              directional change of electricity prices. We first adjust the
              best autoregressive SVM model and then we enhance it with various
              related variables. The system is tested on the daily Phelix index
              of the German and Austrian control area of the European Energy
              Exchange ($E$$E$$X$) wholesale electricity market. The forecast
              accuracy we achieved is 76.12\% over a 200day period.",
  journal  = "Energy Econ.",
  volume   =  44,
  pages    = "135--142",
  month    =  jul,
  year     =  2014,
  keywords = "Autoregressive model; C45; C51; Day-ahead market; E44; European
              Energy Exchange; G10; G17; Q43; Support vector machines"
}

@ARTICLE{Sanchez-Ubeda2007-tu,
  title    = "Modeling and forecasting industrial end-use natural gas
              consumption",
  author   = "S{\'a}nchez-{\'U}beda, Eugenio Fco and Berzosa, Ana",
  abstract = "Forecasting industrial end-use natural gas consumption is an
              important prerequisite for efficient system operation and a basis
              for planning decisions. This paper presents a novel prediction
              model that provides forecasting in a medium-term horizon (1--3
              years) with a very high resolution (days) based on a
              decomposition approach. The forecast is obtained by the
              combination of three different components: one that captures the
              trend of the time series, a seasonal component based on the
              Linear Hinges Model, and a transitory component to estimate daily
              variations using explanatory variables. The flexibility of the
              model allows describing demand patterns in a very wide range of
              historical profiles. Furthermore, the proposed method combines a
              very simple representation of the forecasting model, which allows
              the expert to integrate judgmental analysis and adjustment of the
              statistical forecast, with accuracy and high computational
              efficiency. Realistic case studies are provided.",
  journal  = "Energy Econ.",
  volume   =  29,
  number   =  4,
  pages    = "710--742",
  month    =  jul,
  year     =  2007,
  keywords = "Decomposition models; Judgmental forecasting; Medium-term
              forecasting; Natural gas demand; Q41, C"
}

@ARTICLE{Kankal2011-yh,
  title    = "Modeling and forecasting of Turkey's energy consumption using
              socio-economic and demographic variables",
  author   = "Kankal, Murat and Akp{\i}nar, Adem and K{\"o}m{\"u}rc{\"u}, Murat
              {\.I}hsan and {\"O}z{\c s}ahin, Talat {\c S}{\"u}kr{\"u}",
  abstract = "This study deals with the modeling of the energy consumption in
              Turkey in order to forecast future projections based on
              socio-economic and demographic variables (gross domestic
              product-GDP, population, import and export amounts, and
              employment) using artificial neural network (ANN) and regression
              analyses. For this purpose, four diverse models including
              different indicators were used in the analyses. As the result of
              the analyses, this research proposes Model 2 as a suitable ANN
              model (having four independent variables being GDP, population,
              the amount of import and export) to efficiently estimate the
              energy consumption for Turkey. The proposed model predicted the
              energy consumption better than the regression models and the
              other three ANN models. Thus, the future energy consumption of
              Turkey is calculated by means of this model under different
              scenarios. The predicted forecast results by ANN were compared
              with the official forecasts. Finally, it was concluded that all
              the scenarios that were analyzed gave lower estimates of the
              energy consumption than the MENR projections and these scenarios
              also showed that the future energy consumption of Turkey would
              vary between 117.0 and 175.4Mtoe in 2014.",
  journal  = "Appl. Energy",
  volume   =  88,
  number   =  5,
  pages    = "1927--1939",
  month    =  may,
  year     =  2011,
  keywords = "Artificial neural network; Energy consumption; Regression models;
              Turkey"
}

@ARTICLE{Suganthi2012-tb,
  title    = "Energy models for demand forecasting---A review",
  author   = "Suganthi, L and Samuel, Anand A",
  abstract = "Energy is vital for sustainable development of any nation -- be
              it social, economic or environment. In the past decade energy
              consumption has increased exponentially globally. Energy
              management is crucial for the future economic prosperity and
              environmental security. Energy is linked to industrial
              production, agricultural output, health, access to water,
              population, education, quality of life, etc. Energy demand
              management is required for proper allocation of the available
              resources. During the last decade several new techniques are
              being used for energy demand management to accurately predict the
              future energy needs. In this paper an attempt is made to review
              the various energy demand forecasting models. Traditional methods
              such as time series, regression, econometric, ARIMA as well as
              soft computing techniques such as fuzzy logic, genetic algorithm,
              and neural networks are being extensively used for demand side
              management. Support vector regression, ant colony and particle
              swarm optimization are new techniques being adopted for energy
              demand forecasting. Bottom up models such as MARKAL and LEAP are
              also being used at the national and regional level for energy
              demand management.",
  journal  = "Renewable Sustainable Energy Rev.",
  volume   =  16,
  number   =  2,
  pages    = "1223--1240",
  month    =  feb,
  year     =  2012,
  keywords = "Demand side management; Econometric models; Energy demand
              management; Energy models; Forecasting model"
}

@ARTICLE{Leroy2014-kd,
  title    = "Competition and the bank lending channel in Eurozone",
  author   = "Leroy, Aur{\'e}lien",
  abstract = "This paper examines how banks respond to the monetary policy of
              the European Central Bank (ECB) according to their
              characteristics and, in particular, to their market power, using
              banking micro-data from Eurozone countries over the period from
              1999 to 2011. Our results suggest that banks with market power,
              which is proxied by the Lerner index, have a credit supply that
              is less sensitive to monetary policy shock. The market structures
              (aggregated measures) in which the banks operate have a similar
              effect. Therefore, increased competition enhances the
              effectiveness of monetary policy transmission through the bank
              lending channel. We find also that over the period from 2008 to
              2011, this channel has been strengthened, nevertheless the
              negative effect of market power on monetary effectiveness has
              remained.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  31,
  pages    = "296--314",
  month    =  jul,
  year     =  2014,
  keywords = "Bank competition; Bank lending channel; E52; European Monetary
              Union; G21; Lerner index; Monetary policy transmission"
}

@ARTICLE{Egilmez2013-ki,
  title    = "Benchmarking road safety of {U.S}. states: a {DEA-based}
              Malmquist productivity index approach",
  author   = "Egilmez, Gokhan and McAvoy, Deborah",
  abstract = "In this study, a DEA based Malmquist index model was developed to
              assess the relative efficiency and productivity of U.S. states in
              decreasing the number of road fatalities. Even though the
              national trend in fatal crashes has reached to the lowest level
              since 1949 (Traffic Safety Annual Assessment Highlights, 2010), a
              state-by-state analysis and comparison has not been studied
              considering other characteristics of the holistic national road
              safety assessment problem in any work in the literature or
              organizational reports. In this study, a DEA based Malmquist
              index model was developed to assess the relative efficiency and
              productivity of 50 U.S. states in reducing the number of fatal
              crashes. The single output, fatal crashes, and five inputs were
              aggregated into single road safety score and utilized in the
              DEA-based Malmquist index mathematical model. The period of
              2002-2008 was considered due to data availability for the inputs
              and the output considered. According to the results, there is a
              slight negative productivity (an average of -0.2 percent
              productivity) observed in the U.S. on minimizing the number of
              fatal crashes along with an average of 2.1 percent efficiency
              decline and 1.8 percent technological improvement. The
              productivity in reducing the fatal crashes can only be attributed
              to the technological growth since there is a negative efficiency
              growth is occurred. It can be concluded that even though there is
              a declining trend observed in the fatality rates, the efficiency
              of states in utilizing societal and economical resources towards
              the goal of zero fatality is not still efficient. More effective
              policy making towards increasing safety belt usage and better
              utilization of safety expenditures to improve road condition are
              derived as the key areas to focus on for state highway safety
              agencies from the scope of current research.",
  journal  = "Accid. Anal. Prev.",
  volume   =  53,
  pages    = "55--64",
  month    =  apr,
  year     =  2013,
  keywords = "Accidents, Traffic; Accidents, Traffic: mortality; Accidents,
              Traffic: prevention \& control; Benchmarking; Benchmarking:
              methods; Databases, Factual; Humans; Models, Theoretical; Safety;
              Safety: standards; Safety: statistics \& numerical data; United
              States",
  language = "en"
}

@ARTICLE{Wang2011-qc,
  title    = "Measuring Malmquist productivity index: A new approach based on
              double frontiers data envelopment analysis",
  author   = "Wang, Ying-Ming and Lan, Yi-Xin",
  abstract = "This paper proposes a new approach to measuring Malmquist
              productivity index (MPI) by using both optimistic and pessimistic
              data envelopment analyses (DEA) simultaneously, which we refer to
              as the double frontiers data envelopment analysis (DFDEA). The
              MPIs measured from the two different DEA points of view are
              geometrically averaged to generate an integrated MPI, which we
              refer to as the DFDEA-based MPI. This DFDEA-based MPI reflects
              the productivity changes of decision making units (DMUs) over
              time more truly and more comprehensively than the traditional
              optimistic DEA-based MPI. The proposed new approach is tested
              with a numerical example and applied to the productivity analysis
              of the industrial economy of China.",
  journal  = "Math. Comput. Model.",
  volume   =  54,
  number   =  11,
  pages    = "2760--2771",
  month    =  dec,
  year     =  2011,
  keywords = "Chinese industrial economy; Data envelopment analysis; Double
              frontiers data envelopment analysis; Malmquist productivity
              index; Productivity analysis"
}

@ARTICLE{Portela2010-ik,
  title    = "Malmquist-type indices in the presence of negative data: An
              application to bank branches",
  author   = "Portela, Maria C A S and Thanassoulis, Emmanuel",
  abstract = "In this paper we develop an index and an indicator of
              productivity change that can be used with negative data. For that
              purpose the range directional model (RDM), a particular case of
              the directional distance function, is used for computing
              efficiency in the presence of negative data. We use RDM
              efficiency measures to arrive at a Malmquist-type index, which
              can reflect productivity change, and we use RDM inefficiency
              measures to arrive at a Luenberger productivity indicator, and
              relate the two. The productivity index and indicator are
              developed relative to a fixed meta-technology and so they are
              referred to as a meta-Malmquist index and meta-Luenberger
              indicator. We also address the fact that VRS technologies are
              used for computing the productivity index and indicator (a
              requirement under negative data), which raises issues relating to
              the interpretability of the index. We illustrate how the
              meta-Malmquist index can be used, not only for comparing the
              performance of a unit in two time periods, but also for comparing
              the performance of two different units at the same or different
              time periods. The proposed approach is then applied to a sample
              of bank branches where negative data were involved. The paper
              shows how the approach yields information from a variety of
              perspectives on performance which management can use.",
  journal  = "Journal of Banking \& Finance",
  volume   =  34,
  number   =  7,
  pages    = "1472--1483",
  month    =  jul,
  year     =  2010,
  keywords = "Bank branches; C67; D24; DEA; Directional distance functions;
              G21; Luenberger indicators; Malmquist indices; Meta-frontier;
              Negative data in DEA"
}

@ARTICLE{Epure2011-kh,
  title    = "Bank productivity and performance groups: A decomposition
              approach based upon the Luenberger productivity indicator",
  author   = "Epure, Mircea and Kerstens, Kristiaan and Prior, Diego",
  abstract = "The purpose of this paper is twofold. First, in the framework of
              the strategic groups' literature, it analyZes changes in
              productivity and efficiency of Spanish private and savings banks
              over an eight-year period (1998--2006). Second, by adapting the
              decomposition of the Malmquist productivity indices suggested by
              F{\"a}re et al. (1994), it proposes similar components
              decomposing the Luenberger productivity indicator. Initially,
              productivity is decomposed into technological and efficiency
              changes. Thereafter, this efficiency change is decomposed into
              pure efficiency, scale and congestion changes. Empirical results
              demonstrate that productivity improvements are partially due to
              technological innovation. Furthermore, it is shown how the
              competition between private and savings banks develops in terms
              of the analyzed productivity and efficiency components. While
              private banks enjoy better efficiency change, savings banks
              contribute more to technological progress. Consequently, the
              Luenberger components are used as cluster analysis inputs. Thus,
              economic interpretations of the resulting performance groups are
              made via key differences in productivity components. Finally,
              following the strategic groups' literature, supplementary
              insights are gained by linking these performance groups with
              banking ratios.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  211,
  number   =  3,
  pages    = "630--641",
  month    =  jun,
  year     =  2011,
  keywords = "Banking ratios; Efficiency; Luenberger decomposition; Performance
              groups; Productivity; Spanish banking sector"
}

@ARTICLE{Tsekouras2010-pt,
  title    = "Does the adoption of new technology boost productive efficiency
              in the public sector? The case of {ICUs} system",
  author   = "Tsekouras, Kostas and Papathanassopoulos, Fotis and Kounetas,
              Kostas and Pappous, Giorgos",
  abstract = "In the last decade, a significant amount of financial resources
              has been devoted by the Greek Government and the European Union
              to provide the intensive care units (ICU) of the Greek Public
              Health Care System with high-tech medical equipment in order to
              improve their productive efficiency. Using a unique data set, we
              employ the DEA bootstrap of Simar and Wilson (2007) approach to
              estimate the efficiency of each ICU and to explore the impact of
              these investments on their efficiency. Our results indicate that,
              although the technical efficiency is benefited from the
              embodiment of new medical technology, the scale efficiency
              remains unaffected. The role of the asymmetric information, of
              the ICUs' proximity to pools of knowledge and of the composition
              of the medical personnel, seems to be the crucial factors for the
              improvement of their productive efficiency.",
  journal  = "Int. J. Prod. Econ.",
  volume   =  128,
  number   =  1,
  pages    = "427--433",
  month    =  nov,
  year     =  2010,
  keywords = "Bootstrapped DEA; Intensive care units; Medical technology;
              Productive efficiency"
}

@ARTICLE{Simon2011-gf,
  title    = "Changes in productivity of Spanish university libraries",
  author   = "Simon, Jose and Simon, Clara and Arias, Alicia",
  abstract = "This paper analyzes productivity growth, technical progress, and
              efficiency change in a sample of 34 Spanish university libraries
              between 2003 and 2007. Data envelopment analysis and a Malmquist
              index are combined with a bootstrap method to provide statistical
              inference estimators of individual productivity, technical
              progress, pure efficiency, and scale efficiency scores. To
              calculate productivity, a three-stage service model has been
              developed, examining productivity changes in the relationships
              between the libraries' basic inputs, intermediate outputs, and
              final outputs. The results indicate a growth in the productivity
              of the libraries (relationship between basic inputs and
              intermediate outputs) and in the productivity of the service
              (relationship between basic inputs and final outputs). The growth
              in productivity in both relationships is due to technical
              progress. If the variable representing the use of electronic
              information resources is removed from the final output, the
              result is a significant reduction in productivity.",
  journal  = "Omega",
  volume   =  39,
  number   =  5,
  pages    = "578--588",
  month    =  oct,
  year     =  2011,
  keywords = "Bootstrap technique; Case study; Library productivity; Malmquist
              productivity index"
}

@ARTICLE{Von_Hirschhausen2010-kr,
  title    = "A nonparametric efficiency analysis of German public transport
              companies",
  author   = "von Hirschhausen, Christian and Cullmann, Astrid",
  abstract = "In this paper, we present a nonparametric comparative efficiency
              analysis of 179 communal public transport bus companies in
              Germany (1990--2004). We apply both deterministic data
              envelopment analysis (DEA) and bootstrapping to test the
              robustness of our estimates and to test the hypothesis of global
              and individual constant returns to scale. We find that the
              average technical efficiency of German bus companies is
              relatively low. We observe that the industry appears to be
              characterized by increasing returns to scale for smaller
              companies. These results would imply increasing pressure on bus
              companies to restructure.",
  journal  = "Transp. Res. Part E: Logist. Trans. Rev.",
  volume   =  46,
  number   =  3,
  pages    = "436--445",
  month    =  may,
  year     =  2010,
  keywords = "Bootstrapping; Buses; C14; DEA; Efficiency analysis; L11; L51;
              L92; Nonparametric methods; Public transport"
}

@ARTICLE{Pestana_Barros2010-my,
  title    = "Productivity changes in Portuguese bus companies",
  author   = "Pestana Barros, Carlos and Peypoch, Nicolas",
  abstract = "This paper proposes a framework for benchmarking Portuguese bus
              companies and the rationalisation of their operational
              activities, using the Luenberger productivity indicator. A key
              advantage of this method is that it allows for both input
              contraction and output expansion in determining relative
              efficiencies and productivity changes. For comparative purposes,
              a Malmquist index is also estimated. The Malmquist index
              overvalues the Luenberger productivity indicator. Results
              indicate that public bus companies have similar efficiency to
              private bus companies. Several interesting and useful managerial
              insights and policy implications arise from the study.",
  journal  = "Transp. Policy",
  volume   =  17,
  number   =  5,
  pages    = "295--302",
  month    =  sep,
  year     =  2010,
  keywords = "Bus companies; Luenberger productivity indicator; Portugal"
}

@ARTICLE{Assaf2011-ui,
  title    = "Performance analysis of the Gulf hotel industry: A Malmquist
              index with bias correction",
  author   = "Assaf, A George and Barros, Carlos",
  abstract = "This study uses the Malmquist index with bias correction to
              analyze the performance of hotel chains from the UAE, Saudi
              Arabia and Oman. We show that Saudi Arabia hotel chains have the
              highest productivity growth, followed by the UAE and Omani hotel
              chains. A further decomposition of productivity indicates that a
              small number of hotel chains experienced an increase in revenues
              for lower occupancy rate, while most other hotel chains
              experienced an increase in occupancy rate for lower revenues.
              Related market discussions of the results are provided.",
  journal  = "Int. J. Hosp. Manage.",
  volume   =  30,
  number   =  4,
  pages    = "819--826",
  month    =  dec,
  year     =  2011,
  keywords = "Biased technical change; Gulf hotel chains; Market trends;
              Productivity"
}

@ARTICLE{Essid2014-ih,
  title    = "Productivity, efficiency, and technical change of Tunisian
              schools: a bootstrapped Malmquist approach with quasi-fixed
              inputs",
  author   = "Essid, H{\'e}di and Ouellette, Pierre and Vigeant, St{\'e}phane",
  abstract = "In this study, we measure the productivity of high schools in
              Tunisia over the period 2000/2001--2003/2004 using a Malmquist
              productivity index that we decompose into technical efficiency,
              scale efficiency and technological change. This decomposition
              allows us to identify the source of productivity changes. We also
              adapt the definition of the Malmquist index to take into account
              quasi-fixed factors. The distance functions are then estimated
              using data envelopment analysis. To assess the statistical
              precision of the estimators, we implement a smooth homogenous
              bootstrap procedure that allows us to approximate the sampling
              distribution of the estimators, to correct their bias, and to
              construct confidence intervals for the various components of the
              Malmquist index. The application of our methodology to Tunisian
              high schools shows the high sensitivity to sample variations of
              the observed index changes and of its components. However, we are
              able to show that there is virtually no significant productivity
              variation over the period studied. This absence of productivity
              can be almost all attributed to technical regression and, to a
              lesser extent, to technical inefficiency.",
  journal  = "Omega",
  volume   =  42,
  number   =  1,
  pages    = "88--97",
  month    =  jan,
  year     =  2014,
  keywords = "Bootstrap; Data envelopment analysis; Education economics;
              Malmquist index; Productivity; Quasi-fixed inputs; School
              efficiency"
}

@ARTICLE{Kerstens2014-cy,
  title    = "Comparing Malmquist and {Hicks--Moorsteen} productivity indices:
              Exploring the impact of unbalanced vs. balanced panel data",
  author   = "Kerstens, Kristiaan and Van de Woestyne, Ignace",
  abstract = "We explore the effect of balancing unbalanced panel data when
              estimating primal productivity indices using non-parametric
              frontier estimators. First, we list a series of pseudo-solutions
              aimed at making an unbalanced panel balanced. Then, we discuss
              some intermediate solutions (e.g., balancing 2-years by 2years).
              Furthermore, we link this problem with a variety of literatures
              on infeasibilities, statistical inference of non-parametric
              frontier estimators, and the index theory literature focusing on
              the dynamics of entry and exit in industries. We then empirically
              illustrate these issues comparing both Malmquist and
              Hicks--Moorsteen productivity indices on two data sets. In
              particular, we test for the differences in distribution when
              comparing balanced and unbalanced results for a given index and
              when comparing Malmquist and Hicks--Moorsteen productivity
              indices for a given type of data set. The latter tests are
              crucial in answering the question to which extent the Malmquist
              index can approximate the Hicks--Moorsteen index that has a Total
              Factor Productivity (TFP) interpretation. Finally, we draw up a
              list of remaining issues that could benefit from further
              exploration.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  233,
  number   =  3,
  pages    = "749--758",
  month    =  mar,
  year     =  2014,
  keywords = "Balanced panel; Hicks--Moorsteen productivity index; Malmquist
              productivity index; Unbalanced panel"
}

@ARTICLE{Tsolas2011-be,
  title    = "Performance assessment of mining operations using nonparametric
              production analysis: A bootstrapping approach in {DEA}",
  author   = "Tsolas, Ioannis E",
  abstract = "This paper presents a Data Envelopment Analysis (DEA) model
              combined with bootstrapping to assess performance in mining
              operations. Since DEA-type indicators based on nonparametric
              production analysis are simply point estimates without any
              standard error, we provide a methodology to assess the
              performance of strip mining operations by means of a DEA
              bootstrapping approach. This methodology is applied to a sample
              of fifteen Illinois strip coal mines using publicly available
              data (Thompson et al., 1995). The applied approach uses a mixed
              mine environmental performance indicator (MMEPI) that is derived
              by means of a VRS DEA environmental technology treating
              overburden as an undesirable output under the weak disposability
              assumption, and we compare this measure with a traditional
              output-oriented mine performance indicator (MPI) omitting
              overburden. Although omitting undesirable output results in
              biased performance estimates, these findings are based on sample
              specific results and indicate this bias is not statistically
              significant. The confidence intervals derived by the
              bootstrapping of the proposed MMEPI point estimates indicate that
              significant inefficiency has taken place in the analyzed sample
              of Illinois strip mines.",
  journal  = "Resour. Policy",
  volume   =  36,
  number   =  2,
  pages    = "159--167",
  month    =  jun,
  year     =  2011,
  keywords = "Bootstrapping; C14; C15; Coal mining; Data Envelopment Analysis
              (DEA); Environmental effects; Illinois"
}

@ARTICLE{Oh2010-yh,
  title    = "A sequential {Malmquist--Luenberger} productivity index:
              Environmentally sensitive productivity growth considering the
              progressive nature of technology",
  author   = "Oh, Dong-Hyun and Heshmati, Almas",
  abstract = "This study proposes an index for measuring environmentally
              sensitive productivity growth which appropriately considers the
              nature of technical change. The rationale of this methodology is
              to exclude a spurious technical regress from the macroeconomic
              perspective. In order to incorporate this in developing the
              index, a directional distance function and the concept of the
              successive sequential production possibility set are combined.
              With this combination, the conventional Malmquist--Luenberger
              productivity index is modified to give the sequential
              Malmquist--Luenberger productivity index. This index is employed
              in measuring environmentally sensitive productivity growth and
              its decomposed components of 26 OECD countries for the period
              1970--2003.We distinguish two main empirical findings. First,
              even though the components of the conventional
              Malmquist--Luenberger productivity index and the proposed index
              are different, the trends of rates of average productivity growth
              are similar. Second, unlike in previous studies, the efficiency
              change is the main contributor to the earlier study period,
              whereas the effect of technical change has prevailed over time.",
  journal  = "Energy Econ.",
  volume   =  32,
  number   =  6,
  pages    = "1345--1355",
  month    =  nov,
  year     =  2010,
  keywords = "C61; Directional distance function; E22; Environmentally
              sensitive productivity growth inde; O57; Q43; Q56; productivity
              index; Environmentally sensitive productivity growth index"
}

@ARTICLE{Kao2010-pf,
  title    = "Malmquist productivity index based on common-weights {DEA}: The
              case of Taiwan forests after reorganization",
  author   = "Kao, Chiang",
  abstract = "The performance of a decision making unit (DMU) can be evaluated
              in either a cross-sectional or a time-series manner, and data
              envelopment analysis (DEA) is a useful method for both types of
              evaluation. In order to eliminate the inconsistency caused by
              using different frontier facets to calculate efficiency,
              common-weights DEA models have been developed, under which a
              group of DMUs can be ranked for a specific period. This study
              proposes a common-weights DEA model for time-series evaluations
              to calculate the global Malmquist productivity index (MPI) so
              that the productivity changes of all DMUs have a common basis for
              comparison. The common-weights global MPI not only has sound
              properties, but also produces reliable results. The case of
              Taiwan forests after reorganization shows that the MPIs
              calculated from the conventional DEA model produce misleading
              results. The common-weights global MPI approach, on the other
              hand, correctly identifies districts with unsatisfactory
              performance before the reorganization and those with
              unsatisfactory productivity improvement after the reorganization.",
  journal  = "Omega",
  volume   =  38,
  number   =  6,
  pages    = "484--491",
  month    =  dec,
  year     =  2010,
  keywords = "Common weight; Data envelopment analysis; Efficiency; Forestry;
              Malmquist productivity index"
}

@ARTICLE{George_Assaf2011-aq,
  title    = "Productivity and efficiency analysis of Shinkin banks: Evidence
              from bootstrap and Bayesian approaches",
  author   = "George Assaf, A and Barros, Carlos P and Matousek, Roman",
  abstract = "This paper analyzes the productivity and efficiency of Shinkin
              banks and the various prefectures in Japan, over the period from
              2000 to 2006. We obtain estimates of efficiency growth and
              productivity growth, using the bootstrapped Malmquist index, and
              estimates of efficiency using the Bayesian distance frontier
              approach. We confirm that the efficiency growth and productivity
              growth of Shinkin banks did not improve significantly over the
              period of this study. In addition, we show that the efficiency of
              Shinkin banks is homogenous, with little variation across the
              banks analyzed. Methodologically, we also prove that a failure to
              impose theoretical regularity on the distance function could lead
              to false conclusions about the average efficiency or efficiency
              ranking of Shinkin banks. The study also includes an analysis of
              the correlates of productivity and efficiency growth, and
              provides efficiency and productivity estimates of the prefectures
              in which the banks are located.",
  journal  = "Journal of Banking \& Finance",
  volume   =  35,
  number   =  2,
  pages    = "331--342",
  month    =  feb,
  year     =  2011,
  keywords = "Bayesian distance function; Bootstrapped Malmquist index; C11;
              D21; G21; Japan; Shinkin banks; banks"
}

@ARTICLE{Simar2012-rz,
  title    = "Statistical inference for {DEA} estimators of directional
              distances",
  author   = "Simar, L{\'e}opold and Vanhems, Anne and Wilson, Paul W",
  abstract = "In productivity and efficiency analysis, the technical efficiency
              of a production unit is measured through its distance to the
              efficient frontier of the production set. The most familiar
              non-parametric methods use Farrell--Debreu, Shephard, or
              hyperbolic radial measures. These approaches require that inputs
              and outputs be non-negative, which can be problematic when using
              financial data. Recently, Chambers et al. (1998) have introduced
              directional distance functions which can be viewed as additive
              (rather than multiplicative) measures efficiency. Directional
              distance functions are not restricted to non-negative input and
              output quantities; in addition, the traditional input and
              output-oriented measures are nested as special cases of
              directional distance functions. Consequently, directional
              distances provide greater flexibility. However, until now, only
              free disposal hull (FDH) estimators of directional distances (and
              their conditional and robust extensions) have known statistical
              properties (Simar and Vanhems, 2012). This paper develops the
              statistical properties of directional d estimators, which are
              especially useful when the production set is assumed convex. We
              first establish that the directional Data Envelopment Analysis
              (DEA) estimators share the known properties of the traditional
              radial DEA estimators. We then use these properties to develop
              consistent bootstrap procedures for statistical inference about
              directional distance, estimation of confidence intervals, and
              bias correction. The methods are illustrated in some empirical
              examples.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  220,
  number   =  3,
  pages    = "853--864",
  month    =  aug,
  year     =  2012,
  keywords = "Bootstrap; Data envelopment analysis; Directional distances;
              Efficiency; Non-parametric frontier estimation; Productivity"
}

@ARTICLE{Kao2014-qb,
  title    = "Multi-period efficiency and Malmquist productivity index in
              two-stage production systems",
  author   = "Kao, Chiang and Hwang, Shiuh-Nan",
  abstract = "Conventional two-stage data envelopment analysis (DEA) models
              measure the overall performance of a production system composed
              of two stages (processes) in a specified period of time, where
              variations in different periods are ignored. This paper takes the
              operations of individual periods into account to develop a
              multi-period two-stage DEA model, which is able to measure the
              overall and period efficiencies at the same time, with the former
              expressed as a weighted average of the latter. Since the
              efficiency of a two-stage system in a period is the product of
              the two process efficiencies, the overall efficiency of a
              decision making unit (DMU) in the specified period of time can be
              decomposed into the process efficiency of each period. Based on
              this decomposition, the sources of inefficiency in a DMU can be
              identified. The efficiencies measured from the model can also be
              used to calculate a common-weight global Malmquist productivity
              index (MPI) between two periods, in that the overall MPI is the
              product of the two process MPIs. The non-life insurance industry
              in Taiwan is used to verify the proposed model, and to explain
              why some companies performed unsatisfactorily in the specified
              period of time.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  232,
  number   =  3,
  pages    = "512--521",
  month    =  feb,
  year     =  2014,
  keywords = "Data envelopment analysis; Efficiency; Malmquist productivity
              index; Parallel system; Two-stage system"
}

@ARTICLE{Gitto2012-sb,
  title    = "Bootstrapping the Malmquist indexes for Italian airports",
  author   = "Gitto, Simone and Mancuso, Paolo",
  abstract = "This paper uses data envelopment analysis to assess the
              operational performance of 28 Italian airports during the period
              of 2000 through 2006. Recent developments in bootstrapping
              techniques are used to correct total factor productivity
              estimates for bias and to assess the uncertainty surrounding such
              estimates. This study found that the Italian airport industry
              experienced a significant technological regress, with few
              airports achieving an increase in productivity led by
              improvements in efficiency. Moreover, the paper shows that the
              form of ownership (public majority vs. private majority) of an
              airport management company does not significantly affect
              performance. In contrast, this type of the concession agreement
              has positive and significant effects on airport productivity.
              Finally, the paper highlights the existence of a productivity gap
              between airports located in the North-Central part of the country
              and those located in the south.",
  journal  = "Int. J. Prod. Econ.",
  volume   =  135,
  number   =  1,
  pages    = "403--411",
  month    =  jan,
  year     =  2012,
  keywords = "Bootstrap; DEA; Italian airports; Malmquist"
}

@ARTICLE{Pires2012-zu,
  title    = "Malmquist financial efficiency analysis for airlines",
  author   = "Pires, Heloisa M{\'a}rcia and Fernandes, Elton",
  abstract = "This article discusses the financial efficiency of 42 airlines
              from 25 countries, in 2001 (the year of the September 11
              terrorist attack in the United States), and their profitability
              in the following year. The Malmquist index was used to indicate
              the airlines' capital structure changes from 2001 to 2002. The
              results show airline capital structure management and
              profitability dynamics following the unexpected event of 2001.
              The main conclusion is that airlines which moved more intensively
              to reduce their indebtedness showed improved profitability, given
              their size, fleet and intangible assets.",
  journal  = "Transp. Res. Part E: Logist. Trans. Rev.",
  volume   =  48,
  number   =  5,
  pages    = "1049--1055",
  month    =  sep,
  year     =  2012,
  keywords = "Airlines; Capital structure; DEA; Malmquist index; Unexpected
              events"
}

@ARTICLE{Simar1999-zf,
  title    = "Estimating and bootstrapping Malmquist indices",
  author   = "Simar, L{\'e}opold and Wilson, Paul W",
  abstract = "This paper develops a consistent bootstrap estimation procedure
              for obtaining confidence intervals for Malmquist indices of
              productivity and their decompositions. Although the exposition is
              in terms of input-oriented indices, the techniques can be
              trivially extended to the output orientation. The bootstrap
              methodology is an extension of earlier work described in Simar
              and Wilson (Simar, L., Wilson, P.W., 1998, Management Science).
              Some empirical examples are also given, using data on Swedish
              pharmacies.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  115,
  number   =  3,
  pages    = "459--471",
  month    =  jun,
  year     =  1999,
  keywords = "Bootstrap; DEA; Malmquist indices; Productivity; Resampling"
}

@ARTICLE{Odeck2009-nv,
  title    = "Statistical precision of {DEA} and Malmquist indices: A bootstrap
              application to Norwegian grain producers",
  author   = "Odeck, James",
  abstract = "Previous applications of data envelopment analysis (DEA) and its
              subsequent Malmquist indices to efficiency and productivity
              measurements have been criticised for not providing statistical
              inferences regarding the significance of observed results. In
              this paper, DEA and a Malmquist index are combined with a
              bootstrap method in order to provide succinct statistical
              inferences that determine the performance of grain producers in
              Eastern Norway. The data cover the period between 1987 and 1997.
              Results reveal: (i) a significant degree of inefficiency
              (approximately 11\%) and an average productivity progress of 38\%
              over the period considered; (ii) the formidable productivity
              progress observed is primarily explained by technical efficiency
              changes that enabled producers to catch up with front runners;
              and (iii) environmental factors, such as weather conditions,
              impact both efficiency and productivity. Finally, the analysis
              reveals that using bootstrapping to make statistical inferences
              suggests that researchers should be careful in making performance
              comparisons based on conventional DEA methods, as any discovered
              differences may not be significant.",
  journal  = "Omega",
  volume   =  37,
  number   =  5,
  pages    = "1007--1017",
  month    =  oct,
  year     =  2009,
  keywords = "Bootstrapping; Efficiency measurement; Grain production; L11;
              O33; Productivity growth; Q12; Significance testing of DEA"
}

@ARTICLE{Holod2011-dh,
  title    = "Resolving the deposit dilemma: A new {DEA} bank efficiency model",
  author   = "Holod, Dmytro and Lewis, Herbert F",
  abstract = "One of the weaknesses of current bank efficiency models is a
              disagreement as to the role of deposits in the bank production
              process. Some models view deposits as an input, while others view
              them as an output. Such disparity of approaches results in
              inconsistent efficiency estimates. In this study we propose an
              alternative Data Envelopment Analysis (DEA) bank efficiency model
              that treats deposits as an intermediate product, thus emphasizing
              the dual role of deposits in the bank production process.
              Consequently, the effect of the amount of deposits on bank
              efficiency depends on the efficiency at both stages of the bank
              production process. The main advantage of our model is that it
              does not require a researcher to make a judgment call as to
              whether having more (production approach) or less (intermediation
              approach) deposits is ``better'' for bank efficiency. Our unified
              framework has the potential to produce more consistent efficiency
              estimates.",
  journal  = "Journal of Banking \& Finance",
  volume   =  35,
  number   =  11,
  pages    = "2801--2810",
  month    =  nov,
  year     =  2011,
  keywords = "Bank efficiency; C67; DEA; Financial intermediation; G2;
              Input--output models"
}

@ARTICLE{Casu2013-ve,
  title     = "Regulatory Reform and Productivity Change in Indian Banking",
  author    = "Casu, Barbara and Ferrari, Alessandra and Zhao, Tianshu",
  abstract  = "This paper examines the impact of regulatory reform on
               productivity growth and its components for Indian banks from
               1992 to 2009. We estimate parametric and nonparametric
               efficiency frontiers, followed by Divisia and Malmquist indexes
               of total factor productivity, respectively. To account for
               technology heterogeneity among ownership types, we use a
               metafrontier approach. Results are consistent across
               methodologies and show sustained productivity growth, driven
               mainly by technological progress. Furthermore, results indicate
               that different ownership types react differently to changes in
               the operating environment. The position of foreign banks becomes
               increasingly dominant, and their production technology becomes
               the best practice in the industry. \copyright{} 2013 The
               President and Fellows of Harvard College and the Massachusetts
               Institute of Technology.",
  journal   = "Rev. Econ. Stat.",
  publisher = "MIT Press",
  volume    =  95,
  number    =  3,
  pages     = "1066--1077",
  year      =  2013,
  keywords  = "DEA; SFA; deregulation; metafrontier; ownership; productivity
               change"
}

@ARTICLE{Barth2013-ia,
  title    = "Do bank regulation, supervision and monitoring enhance or impede
              bank efficiency?",
  author   = "Barth, James R and Lin, Chen and Ma, Yue and Seade, Jes{\'u}s and
              Song, Frank M",
  abstract = "The recent global financial crisis has spurred renewed interest
              in identifying those reforms in bank regulation that would work
              best to promote bank development, performance and stability.
              Building upon three recent world-wide surveys on bank regulation
              (Barth et al., 2004, 2006, 2008), we contribute to this
              assessment by examining whether bank regulation, supervision and
              monitoring enhance or impede bank operating efficiency. Based on
              an un-balanced panel analysis of 4050 banks observations in 72
              countries over the period 1999-2007, we find that tighter
              restrictions on bank activities are negatively associated with
              bank efficiency, while greater capital regulation stringency is
              marginally and positively associated with bank efficiency. We
              also find that a strengthening of official supervisory power is
              positively associated with bank efficiency only in countries with
              independent supervisory authorities. Moreover, independence
              coupled with a more experienced supervisory authority tends to
              enhance bank efficiency. Finally, market-based monitoring of
              banks in terms of more financial transparency is positively
              associated with bank efficiency. \copyright{} 2013 Elsevier B.V.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  8,
  pages    = "2879--2892",
  month    =  aug,
  year     =  2013,
  keywords = "Bank regulation; Operating efficiency; Supervision; g21; g28"
}

@ARTICLE{Berger1998-hf,
  title     = "The Efficiency Cost Of Market Power In The Banking Industry: A
               Test Of The ``Quiet Life'' And Related Hypotheses",
  author    = "Berger, Allen N and Hannan, Timothy H",
  abstract  = "Traditional concerns about concentration in product markets have
               centered on the social loss associated with the mispricing that
               occurs when market power is exercised. This paper focuses on a
               potentially greater loss from market power - a reduction in cost
               efficiency brought about by the lack of market discipline in
               concentrated markets. We employ data from the commercial banking
               industry, which produces very homogeneous products in multiple
               markets with differing degrees of market concentration. We find
               the estimated efficiency cost of concentration to be several
               times larger than the social loss from mispricing as
               traditionally measured by the welfare triangle. \copyright{}
               1998 by the President and Fellows of Harvard College and the
               Massachusetts Institute of Technology",
  journal   = "Rev. Econ. Stat.",
  publisher = "MIT Press",
  volume    =  80,
  number    =  3,
  pages     = "454--465",
  year      =  1998
}

@ARTICLE{Casu2004-os,
  title    = "Productivity change in European banking: A comparison of
              parametric and non-parametric approaches",
  author   = "Casu, Barbara and Girardone, Claudia and Molyneux, Philip",
  abstract = "This paper compares parametric and non-parametric estimates of
              productivity change in European banking between 1994 and 2000.
              Productivity change has also been further decomposed into
              technological change, or change in best practice, and efficiency
              change. Both the parametric and non-parametric approaches
              consistently identify those systems that have benefited most (and
              least) from productivity change during the 1990s. The results
              also suggest that (where found) productivity growth has mainly
              been brought about by improvements in technological change and
              there does not appear to have been 'catch-up' by
              non-best-practice institutions. Competing methodologies sometimes
              identify conflicting findings for the sources of productivity for
              individual years. However, the two approaches generally do not
              yield markedly different results in terms of identifying the
              components of productivity growth in European banking during the
              1990s. ?? 2003 Elsevier B.V. All rights reserved.",
  journal  = "Journal of Banking \& Finance",
  volume   =  28,
  number   =  10,
  pages    = "2521--2540",
  month    =  oct,
  year     =  2004,
  keywords = "DEA; Finance and banking; Malmquist index; Productivity change;
              Stochastic frontier"
}

@INPROCEEDINGS{Battese2004-ai,
  title     = "A metafrontier production function for estimation of technical
               efficiencies and technology gaps for firms operating under
               different technologies",
  booktitle = "Journal of Productivity Analysis",
  author    = "Battese, George E and Prasada Rao, D S and O'Donnell,
               Christopher J",
  abstract  = "This paper presents a metafrontier production function model for
               firms in different groups having different technologies. The
               metafrontier model enables the calculation of comparable
               technical efficiencies for firms operating under different
               technologies. The model also enables the technology gaps to be
               estimated for firms under different technologies relative to the
               potential technology available to the industry as a whole. The
               metafrontier model is applied in the analysis of panel data on
               garment firms in five different regions of Indonesia, assuming
               that the regional stochastic frontier production function models
               have technical inefficiency effects with the time-varying
               structure proposed by Battese and Coelli (1992). [ABSTRACT FROM
               AUTHOR] Copyright of Journal of Productivity Analysis is the
               property of Springer Science \& Business Media B.V. and its
               content may not be copied or emailed to multiple sites or posted
               to a listserv without the copyright holder's express written
               permission. However, users may print, download, or email
               articles for individual use. This abstract may be abridged. No
               warranty is given about the accuracy of the copy. Users should
               refer to the original published version of the material for the
               full abstract. (Copyright applies to all Abstracts.)",
  volume    =  21,
  pages     = "91--103",
  year      =  2004,
  keywords  = "Indonesia garment industry; Metafrontier; Technical efficiency;
               Technology gap"
}

@ARTICLE{Kane1977-df,
  title     = "Good Intentions and Unintended Evil: The Case Against Selective
               Credit Allocation",
  author    = "Kane, Edward J",
  abstract  = "Relying on Adam Smith's concept of the ``invisible hand,'' every
               economist\textbackslashncan explain how markets react to
               individual economic power. Markets\textbackslashnshape and
               reshape tliemselves to neutralize self-interested
               economic\textbackslashnpower by coaxing it into channels that
               serve the public good. But\textbackslashneconomists have devoted
               less attention to analyzing how markets
               systematically\textbackslashncounteract coalitions of
               individuals' political power.
               Regulatory\textbackslashnrestraints imposed by the ``visible
               hand'' of political power shape\textbackslashnmarkets just as
               surely as economic power does, but in ways
               designed\textbackslashnto create or perpetuate economic power
               [15]. This paper endeavors\textbackslashnto show that
               introducing political power into economic affairs
               initiates\textbackslashna dialectical process of adjustments and
               counteradjustments",
  journal   = "J. Money Credit Bank.",
  publisher = "[Wiley, Ohio State University Press]",
  volume    =  9,
  number    =  1,
  pages     = "55",
  month     =  feb,
  year      =  1977,
  keywords  = "federal credit programs; financial innovation; financial
               stability; financialisation; form follows function; housing
               policy"
}

@ARTICLE{Keshvari2013-vb,
  title    = "Stochastic non-convex envelopment of data: Applying isotonic
              regression to frontier estimation",
  author   = "Keshvari, Abolfazl and Kuosmanen, Timo",
  abstract = "Isotonic nonparametric least squares (INLS) is a regression
              method for estimating a monotonic function by fitting a step
              function to data. In the literature of frontier estimation, the
              free disposal hull (FDH) method is similarly based on the minimal
              assumption of monotonicity. In this paper, we link these two
              separately developed nonparametric methods by showing that FDH is
              a sign-constrained variant of INLS. We also discuss the
              connections to related methods such as data envelopment analysis
              (DEA) and convex nonparametric least squares (CNLS). Further, we
              examine alternative ways of applying isotonic regression to
              frontier estimation, analogous to corrected and modified ordinary
              least squares (COLS/MOLS) methods known in the parametric stream
              of frontier literature. We find that INLS is a useful extension
              to the toolbox of frontier estimation both in the deterministic
              and stochastic settings. In the absence of noise, the corrected
              INLS (CINLS) has a higher discriminating power than FDH. In the
              case of noisy data, we propose to apply the method of non-convex
              stochastic envelopment of data (non-convex StoNED), which
              disentangles inefficiency from noise based on the skewness of the
              INLS residuals. The proposed methods are illustrated by means of
              simulated examples. ?? 2013 Elsevier B.V. All rights reserved.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  231,
  number   =  2,
  pages    = "481--491",
  month    =  dec,
  year     =  2013,
  keywords = "Data envelopment analysis; Free disposal hull; Nonparametric
              regression; Productive efficiency analysis; Stochastic frontier
              analysis"
}

@ARTICLE{Olesen2014-fi,
  title    = "Maintaining the Regular Ultra Passum Law in data envelopment
              analysis",
  author   = "Olesen, Ole B and Ruggiero, John",
  abstract = "The variable returns to scale data envelopment analysis (DEA)
              model is developed with a maintained hypothesis of convexity in
              input--output space. This hypothesis is not consistent with
              standard microeconomic production theory that posits an S-shape
              for the production frontier, i.e. for production technologies
              that obey the Regular Ultra Passum Law. Consequently, measures of
              technical efficiency assuming convexity are biased downward. In
              this paper, we provide a more general DEA model that allows the
              S-shape.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  235,
  number   =  3,
  pages    = "798--809",
  month    =  jun,
  year     =  2014,
  keywords = "Convex hull estimation; Data envelopment analysis (DEA); Isoquant
              estimation; S-shaped production function"
}

@ARTICLE{Ondrich2001-rx,
  title    = "Efficiency measurement in the stochastic frontier model",
  author   = "Ondrich, Jan and Ruggiero, John",
  abstract = "Deterministic models of technical efficiency assume that all
              deviations from the production frontier are due to inefficiency.
              Critics argue that no allowance is made for measurement error and
              other statistical noise so that the resulting efficiency measure
              will be contaminated. The stochastic frontier model is an
              alternative that allows both inefficiency and measurement error.
              Advocates argue that the stochastic frontier models should be
              used despite other potential limitations because of the superior
              conceptual treatment of noise. As will be demonstrated in this
              paper, however, the assumed shape of the error distributions is
              used to identify a key production function parameter. Therefore,
              the stochastic frontier models, like the deterministic models,
              cannot produce absolute measures of efficiency. Moreover, we show
              that rankings for firm-specific inefficiency estimates produced
              by traditional stochastic frontier models do not change from the
              rankings of the composed errors. As a result, the performance of
              the deterministic models is qualitatively similar to that of the
              stochastic frontier models.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  129,
  number   =  2,
  pages    = "434--442",
  month    =  mar,
  year     =  2001,
  keywords = "Deterministic frontier; Efficiency measurement; Stochastic
              frontier"
}

@MISC{Mizrach2008-hi,
  title    = "The next tick on Nasdaq",
  author   = "Mizrach, Bruce",
  abstract = "The Nasdaq stock market provides information about buying and
              selling interest in its limit order book. Using a vector
              autoregressive model of trades and returns, I assess the effect
              of the entire order book on the next tick. I also determine the
              influence of individual market makers and electronic networks and
              find evidence that the identity of market participants can be
              useful information. Finally, I produce a set of dynamic market
              price responses to buy and sell orders, and I find that these
              estimates vary with standard measures of liquidity.",
  journal  = "Quant. Finance",
  volume   =  8,
  pages    = "19--40",
  year     =  2008
}

@ARTICLE{Marrouch2014-ch,
  title    = "Joint market power in banking: Evidence from developing countries",
  author   = "Marrouch, Walid and Turk-Ariss, Rima",
  abstract = "We propose an oligopsony-oligopoly model to study bank behavior
              under uncertainty in developing countries and derive a pricing
              structure that acknowledges joint market power in both the
              deposit and loan markets. The model identifies two main
              components to pricing: rent extraction and input costs. We
              measure the ability of the banking industry to extract rents from
              the exercise of joint market power using a sample of 103
              developing countries. We find that market power rents are
              economically significant. Also, the role played by the rent
              extraction share in loan pricing dominates the share of input
              costs on average.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  31,
  pages    = "253--267",
  month    =  jul,
  year     =  2014,
  keywords = "Market power; Bank pricing; Uncertainty; Developing countries"
}

@ARTICLE{Deng2013-zk,
  title    = "Corporate social responsibility and stakeholder value
              maximization: Evidence from mergers",
  author   = "Deng, Xin and Kang, Jun-Koo and Low, Buen Sin",
  abstract = "Using a large sample of mergers in the US, we examine whether
              corporate social responsibility (CSR) creates value for acquiring
              firms' shareholders.We find that compared with low CSR acquirers,
              high CSR acquirers realize higher merger announcement returns,
              higher announcement returns on the value-weighted portfolio of
              the acquirer and the target, and larger increases in post-merger
              long-term operating performance. They also realize positive
              long-term stock returns, suggesting that the market does not
              fully value the benefits of CSR immediately. In addition, we find
              that mergers by high CSR acquirers take less time to complete and
              are less likely to fail than mergers by low CSR acquirers. These
              results suggest that acquirers' social performance is an
              important determinant of merger performance and the probability
              of its completion, and they support the stakeholder value
              maximization view of stakeholder theory.",
  journal  = "J. financ. econ.",
  volume   =  110,
  number   =  1,
  pages    = "87--109",
  month    =  oct,
  year     =  2013,
  keywords = "Announcement return; Corporate social responsibility (CSR);
              Merger; Nexus of contract; Stakeholder theory; corporate social
              responsibility; csr"
}

@ARTICLE{Garcia-Appendini2013-oc,
  title    = "Firms as liquidity providers: Evidence from the 2007--2008
              financial crisis",
  author   = "Garcia-Appendini, Emilia and Montoriol-Garriga, Judit",
  abstract = "Using a supplier--client matched sample, we study the effect of
              the 2007--2008 financial crisis on between-firm liquidity
              provision. Consistent with a causal effect of a negative shock to
              bank credit, we find that firms with high precrisis liquidity
              levels increased the trade credit extended to other corporations
              and subsequently experienced better performance as compared with
              ex ante cash-poor firms. Trade credit taken by constrained firms
              increased during this period. These findings are consistent with
              firms providing liquidity insurance to their clients when bank
              credit is scarce and offer an important precautionary savings
              motive for accumulating cash reserves.",
  journal  = "J. financ. econ.",
  volume   =  109,
  number   =  1,
  pages    = "272--291",
  month    =  jul,
  year     =  2013,
  keywords = "Corporate liquidity; Crisis; Financial constraints; Liquidity;
              Trade credit"
}

@ARTICLE{Ang2007-wq,
  title     = "Stock Return Predictability: Is it There?",
  author    = "Ang, Andrew and Bekaert, Geert",
  abstract  = "We examine the predictive power of the dividend yields for
               forecasting excess returns, cash flows, and interest rates.
               Dividend yields predict excess returns only at short horizons
               together with the short rate and do not have any long-horizon
               predictive power. At short horizons, the short rate strongly
               negatively predicts returns. These results are robust in
               international data and are not due to lack of power. A present
               value model that matches the data shows that discount rate and
               short rate movements play a large role in explaining the
               variation in dividend yields. Finally, we find that earnings
               yields significantly predict future cash flows. (JEL C12, C51,
               C52, E49, F30, G12)",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  20,
  number    =  3,
  pages     = "651--707",
  month     =  may,
  year      =  2007
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Campbell2006-et,
  title    = "Efficient tests of stock return predictability‚òÜ",
  author   = "Campbell, J and Yogo, M",
  abstract = "Conventional tests of the predictability of stock returns could
              be invalid, that is reject the null too frequently, when the
              predictor variable is persistent and its innovations are highly
              correlated with returns. We develop a pretest to determine
              whether the conventional t-test leads to invalid inference and an
              efficient test of predictability that corrects this problem.
              Although the conventional t-test is invalid for the
              dividend-price and smoothed earnings-price ratios, our test finds
              evidence for predictability. We also find evidence for
              predictability with the short rate and the long-short yield
              spread, for which the conventional t-test leads to valid
              inference.",
  journal  = "J. financ. econ.",
  volume   =  81,
  pages    = "27--60",
  year     =  2006
}

@ARTICLE{Hasbrouck1993-lb,
  title    = "The Trades of Market Makers : An Empirical Analysis of {NYSE}
              Specialists",
  author   = "Hasbrouck, Joel and Sofianos, George",
  abstract = "This paper presents a transaction-level empirical analysis of the
              trading activities of New York Stock Exchange specialists. The
              main findings of the analysis are the following. Adjustment lags
              in inventories vary across stocks, and are in some cases as long
              as one or two months. Decomposition of specialist trading profits
              by trading horizon shows that the principal source of these
              profits is short term. An analysis of the dynamic relations among
              inventories, signed order flow, and quote changes suggests that
              trades in which the specialist participates have a higher
              immediate impact on the quotes than trades with no specialist
              participation.",
  journal  = "J. Finance",
  volume   =  48,
  pages    = "1565--1593",
  year     =  1993
}

@MISC{Shiller1980-wm,
  title    = "Do Stock Prices Move Too Much to be Justified by Subsequent
              Change in Dividends?",
  author   = "Shiller, Robert J",
  abstract = "An ex-post rational real common stock price series, formed as the
              present value of subsequent detrended real dividends, is found to
              be a very stable and smooth series when compared with the
              actual\textbackslashndetrended real stock price series. An
              efficient markets model which makes price the optimal forecast of
              the ex-post rational price is inconsistent with this data if the
              long-run trend of real dividends is assumed given. To reconcile
              the data with the efficient markets model, one must assume that
              the market expected real dividends deviate from their long-run
              trend much more than they did historically.\textbackslashn",
  journal  = "NBER Work. Pap. Ser.",
  year     =  1980
}

@ARTICLE{Sonnhof1975-my,
  title    = "Inhibitory postsynaptic actions of taurine, {GABA} and other
              amino acids on motoneurons of the isolated frog spinal cord",
  author   = "Sonnhof, U and Grafe, P and Krumnikl, J and Linder, M and
              Schindler, L",
  abstract = "The actions of glycine, GABA, alpha-alanine, beta-alanine and
              taurine were studied by intracellular recordings from lumbar
              motoneurons of the isolated spinal cord of the frog. All amino
              acids tested produced a reduction in the amplitude of
              postsynaptic potentials, a blockade of the antidromic action
              potential and an increase of membrane conductance. Furthermore,
              membrane polarizations occurred, which were always in the same
              direction as the IPSP. All these effects indicate a postsynaptic
              inhibitory action of these amino acids. When the relative
              strength of different amino acids was compared, taurine had the
              strongest inhibitory potency, followed by beta-alanine,
              alpha-alanine, GABA and glycine. Topically applied strychnine and
              picrotoxin induced different changes of post-synaptic potentials,
              indicating that distinct inhibitory systems might be influenced
              by these two convulsants. Interactions with amino acids showed
              that picrotoxin seletively diminished the postsymaptic actions of
              GABA, while strychnine reduced the effects of taurine, glycine,
              alpha- and beta-alanine. But differences in the susceptibility of
              these amino acid actions to strychnine could be detected: the
              action of taurine was more sensitively blocked by strychnine
              compared with glycine, alpha- and beta-alanine. With regard to
              these results the importance of taurine and GABA as transmitters
              of postsynaptic inhibition on motoneurons in the spinal cord of
              the frog is discussed.",
  journal  = "Brain Res.",
  volume   =  100,
  number   =  2,
  pages    = "327--341",
  month    =  dec,
  year     =  1975,
  language = "en"
}

@MISC{Fama1988-zu,
  title   = "Dividend yields and expected stock returns",
  author  = "Fama, Eugene F and French, Kenneth R",
  journal = "J. financ. econ.",
  volume  =  22,
  pages   = "3--25",
  year    =  1988
}

@ARTICLE{Fama1990-pf,
  title    = "Stock returns, expected returns, and real activity",
  author   = "Fama, Eugene F",
  abstract = "Measuring the total return variation explained by shocks to
              expected cash flows, time-varying expected returns, and shocks to
              expected returns is one way to judge the rationality of stock
              prices. Variables that proxy for expected returns and
              expected-return shocks capture 30 percent of the variance of
              annual NYSE value-weighted returns. Growth rates of production,
              used to proxy for shocks to expected cash flows, explain 43
              percent of the return variance. Whether the combined explanatory
              power of the variables--about 58 percent of the variance of
              annual returns--is good or bad news about market efficiency is
              left for the reader to judge. Copyright 1990 by American Finance
              Association.",
  journal  = "J. Finance",
  volume   =  45,
  pages    = "1089--1108",
  year     =  1990
}

@MISC{Fama1993-qp,
  title    = "{Fama-French} {APT} three-factor model",
  author   = "Fama, Eugene F and French, Kenneth R",
  abstract = "This paper identifies five common risk factors in the returns on
              stocks and bonds. There are three stock-market factors: an
              overall market factor and factors related to firm size and
              book-to-market equity. There are two bond-market factors, related
              to maturity and default risks. Stock returns have shared
              variation due to the stock-market factors, and they are linked to
              bond returns through shared variation in the bond-market factors.
              Except for low-grade corporates, the bond-market factors capture
              the common variation in bond returns. Most important, the five
              factors seem to explain average returns on stocks and bonds.",
  journal  = "J. financ. econ.",
  volume   =  33,
  pages    = "3--56",
  year     =  1993
}

@MISC{Fama1965-yb,
  title    = "The Behavior of {Stock-Market} Prices",
  author   = "Fama, Eugene F",
  abstract = "This paper discusses the theory of random-walk model and then
              tests the model's empirical validity. The main conclusion will be
              that the data seem to present consistent and strong support for
              the model. This implies, of course, that chart reading, though
              perhaps an interesting pastime, is of no real value to the stock
              market investor. This is an extreme statement and the chart
              reader is certainly free to take exception. Since the empirical
              evidence produced by this and other studies in support of the
              random-walk model is now so voluminous, the counterarguments of
              the chart reader will be completely lacking in force if they are
              not equally well supported by empirical work. By contrast the
              stock market trader has a much more practical criterion for
              judging what constitutes important dependence in successive price
              changes. For his purposes the random walk model is valid as long,
              as knowledge of the past behavior of the series of price changes
              cannot be used to increase expected gains. More specifically, the
              independence assumption is an adequate description of reality as
              long as the actual degree of dependence in the series of price
              changes is not sufficient to allow the past history of the series
              to be used to predict the future in a way, which makes expected
              profits greater than they would be under a naive buy-and-hold
              model.",
  journal  = "The Journal of Business",
  volume   =  38,
  pages    = "34",
  year     =  1965
}

@MISC{Schiereck1999-cj,
  title    = "Contrarian and Momentum Strategies in Germany",
  author   = "Schiereck, Dirk and De Bondt, Werner and Weber, Martin",
  abstract = "Two traditional methods of managing equity portfolios are
              investing based on price momentum and value-based contrarian
              investing. These strategies may be motivated by a behavioral
              theori/ of under- and overreaction to news or by empirical
              research, mostly for the NYSE, that has found persistence in
              price movements over short horizons and reversion to the mean
              over longer horizons. Hoxuever, the apparent success of these
              strategies may be due to institutional factors and the
              mismeasurement of risk, or it may result from data mining. For
              these reasons, we studied all major German companies listed on
              the Frankfurt Stock Exchange for the three decades between 1961
              and 1991. The dynamics of stock prices in Frankfurt are
              remarkably similar to New York. The data suggest that equity
              priees reflect investor forecasts of company profits that are
              predictably wrong",
  journal  = "Financial Analysts Journal",
  volume   =  55,
  pages    = "104--116",
  year     =  1999
}

@ARTICLE{Chan1996-lb,
  title    = "Momentum Strategies",
  author   = "Chan, Louis K C and Jegadeesh, Narasimhan and Lakonishok, Josef",
  abstract = "We examine whether the predictability of future returns from past
              returns is due to the market's underreaction to information, in
              particular to past earnings news. Past return and past earnings
              surprise each predict large drifts in future returns after
              controlling for the other. Market risk, size, and book-to-market
              effects do not explain the drifts. There is little evidence of
              subsequent reversals in the returns of stocks with high price and
              earnings momentum. Security analysts' earnings forecasts also
              respond sluggishly to past news, especially in the case of stocks
              with the worst past performance. The results suggest a market
              that responds only gradually to new information.",
  journal  = "J. Finance",
  volume   =  51,
  pages    = "1681--1713",
  year     =  1996
}

@MISC{Asness1997-hf,
  title    = "The Interaction of Value and Momentum Strategies",
  author   = "Asness, Clifford S",
  abstract = "Value and momentum strategies both have demonstrated power to
              predict the cross-section of stock returns, but are these
              strategies related? Measures of momentum and value are negatively
              correlated across stocks, yet each is positively related to the
              cross-section of average stock returns. We examine whether the
              marginal power of value or momentum differs depending upon the
              level of the other variable. Value strategies work, in general,
              but are strongest among low-momentum (loser) stocks and weakest
              among high-momentum (winner) stocks. The momentum strategy works,
              in general, but is particularly strong among low-value
              (expensive) stocks. These results hold despite finding comparable
              spreads in value measures among stocks with different levels of
              momentum and comparable spreads in the momentum measure among
              stocks with different levels of value. Any explanation for why
              value and momentum work must explain this interaction.",
  journal  = "Financial Analysts Journal",
  volume   =  53,
  pages    = "29--36",
  year     =  1997
}

@MISC{Fedenia1992-rh,
  title    = "Options Trading and the {Bid-Ask} Spread of the Underlying Stocks",
  author   = "Fedenia, Mark and Grammatikos, Theoharry",
  abstract = "This study shows that options listing significantly affects the
              spreads on the underlying stock. We identify a trade-off between
              the benefits of increased liquidity and the cost of informational
              externalities. Highly liquid stocks tend to have spread
              increases, while illiquid stocks experience spread decreases. The
              effect occurs in concert with nonlisting volatility changes. The
              spread changes do not appear to be caused by shifts in liquidity
              between the stock and options market. Often, spread changes are
              large enough to affect significantly the cost of equity capital.",
  journal  = "The Journal of Business",
  volume   =  65,
  pages    = "335",
  year     =  1992
}

@MISC{Amihud1986-hp,
  title    = "Asset pricing and the bid-ask spread",
  author   = "Amihud, Yakov and Mendelson, Haim",
  abstract = "This paper studies the effect of the bid-ask spread on asset
              pricing. We analyze a model in which investors with different
              expected holding periods trade assets with different relative
              spreads. The resulting testable hypothesis is that
              market-observed expexted return is an increasing and concave
              function of the spread. We test this hypothesis, and the
              empirical results are consistent with the predictions of the
              model.",
  journal  = "J. financ. econ.",
  volume   =  17,
  pages    = "223--249",
  year     =  1986
}

@MISC{Dupont2007-aq,
  title    = "Effects of Securities Transaction Taxes on Depth and {Bid-Ask}
              Spread",
  author   = "Dupont, Dominique Y and Lee, Gabriel S",
  abstract = "This note investigates the effects of introducing a transaction
              tax on depth and bid-ask spread using a static model where a
              competitive market maker faces informed and liquidity traders.
              When the degree of information asymmetry is low, an increase in
              the transaction tax can result into a smaller rise in the selling
              price and the depth may even increase in some cases. When
              information asymmetry is high, the dealer could increase the
              selling price more than the tax and sometimes lowers the depth.
              This can result in a market shutdown if the liquidity trader is
              driven out of the market.",
  journal  = "Econom. Theory",
  volume   =  31,
  pages    = "393--400",
  year     =  2007
}

@ARTICLE{Chung1998-uj,
  title     = "Insider Trading and the {Bid-Ask} Spread",
  author    = "Chung, Kee H and Charoenwong, Charlie",
  abstract  = "This study examines the intertemporal and cross-sectional
               association between the bid-ask spread and insider trading.
               Empirical results from the cross-sectional regression analysis
               reveal that market makers establish larger spreads for stocks
               with a greater extent of insider trading. The time-series
               regression analysis, however, finds no evidence of spread
               changes on insider trading days. These results suggest that
               although market makers may not be able to detect insider trading
               when it occurs, they protect themselves by maintaining larger
               spreads for stocks with a greater tendency of insider trading.
               The results also reveal that market makers establish larger
               spreads when there are unusually large transactions. In
               addition, this study finds that spreads are positively
               associated with risk and negatively with trading volume, the
               number of exchange listings, share price, and firm size.",
  journal   = "Financial Review",
  publisher = "Blackwell Publishing Ltd",
  volume    =  33,
  number    =  3,
  pages     = "1--20",
  month     =  aug,
  year      =  1998,
  keywords  = "bid-ask spread, insider trading; G14"
}

@MISC{Bollen2001-sa,
  title    = "Modeling the bid/ask spread: On the effects of hedging costs and
              competition",
  author   = "Bollen, Nicolas P B and Smith, Tom and Whaley, Robert E",
  abstract = "The need to understand and measure market maker bid/ask spreads
              is crucial in evaluating the merits of competing market
              structures and security designs. Prior studies of bid/ask spreads
              suffer from several forms of misspecification, including
              inadvertent and erroneous use of weighted least squares
              regression. This study develops a simple, parsimonious model of
              the determinants of spread, and then tests it empirically on a
              sample of NASDAQ stocks. The model performs well and avoids the
              distortions of prior work. The study demonstrates the importance
              of proper model specification in providing meaningful inference
              regarding the determinants of spread. August",
  journal  = "Proc. Univ. Otago Med. Sch.",
  pages    = "30",
  year     =  2001
}

@ARTICLE{Copeland1983-yx,
  title    = "Information Effects on the {Bid-Ask} Spread",
  author   = "Copeland, Thomas E and Galai, D a N",
  abstract = "An individual who chooses to serve as a market-maker is assumed
              to optimize his position by setting a bid-ask spread which
              maximizes the difference between expected revenues received from
              liquidity-motivated traders and expected losses to
              information-motivated traders. By characterizing the cost of
              supplying quotes, as writing a put and a call option to an
              information-motivated trader, it is shown that the bid-ask spread
              is a positive function of the price level and return variance, a
              negative function of measures of market activity, depth, and
              continuity, and negatively correlated with the degree of
              competition. Thus, the theory of information effects on the
              bid-ask spread proposed in this paper is consistent with the
              empirical literature. ABSTRACT FROM AUTHOR Copyright of Journal
              of Finance is the property of Blackwell Publishing Limited and
              its content may not be copied or emailed to multiple sites or
              posted to a listserv without the copyright holder's express
              written permission. However, users may print, download, or email
              articles for individual use. This abstract may be abridged. No
              warranty is given about the accuracy of the copy. Users should
              refer to the original published version of the material for the
              full abstract. (Copyright applies to all Abstracts)",
  journal  = "J. Finance",
  volume   =  38,
  number   =  5,
  pages    = "1457--1469",
  year     =  1983,
  keywords = "ASKED price; EXPECTED returns; INFORMATION theory in economics;
              OPTIONS (Finance); PRICE levels; TENDER offers (Securities)"
}

@ARTICLE{Hedge1994-ao,
  title    = "Trading Mechanisms and the Components of the {Bid-Ask} Spread",
  author   = "Hedge, Shantaram P and Miller, Robert E and Affleck-Graves, John",
  abstract = "We compare the relative magnitudes of the components of the
              bid-ask spread for New York Stock Exchange (NYSE)/American Stock
              Exchange (AMEX) stocks to those of National Association of
              Securities Dealers Automated Quotations (NASDAQ)/National Market
              System (NMS) stocks. We find that the order- processing cost
              component is smaller, and the adverse selection component is.
              greater on the NYSE/AMEX trading systems than on the NASDAQ/NMS
              system. The inventory holding component is also greater for
              exchange-traded stocks than for NASDAQ/NMS stocks, but this may
              be attributable to differences in the character- istics of the
              firms whose stocks trade on the respective systems.",
  journal  = "J. Finance",
  volume   =  49,
  pages    = "1471--1488",
  year     =  1994
}

@ARTICLE{Chung2009-ur,
  title     = "Volatility, Market Structure, and the {Bid-Ask} Spread*",
  author    = "Chung, Kee H and Kim, Youngsoo",
  abstract  = "We test the conjecture that the specialist system on the New
               York Stock Exchange (NYSE) provides better liquidity services
               than the NASDAQ dealer market in times of high return volatility
               when adverse selection and inventory risks are high. We motivate
               our conjecture from the observation that there is a designated
               specialist for each stock on the NYSE who is directly
               responsible for maintaining a reasonable level of liquidity
               (i.e., the bid-ask spread) as the `liquidity provider of last
               resort' whereas there is no such designated dealer on NASDAQ.
               Empirical evidence is consistent with our conjecture. In a
               similar vein, we show that the specialist system provides better
               liquidity than the dealer market in thin markets.",
  journal   = "Asia-Pacific Journal of Financial Studies",
  publisher = "Blackwell Publishing Ltd",
  volume    =  38,
  number    =  1,
  pages     = "67--107",
  month     =  feb,
  year      =  2009,
  keywords  = "bid-ask spreads; dealer; fair and orderly; market structure;
               specialist; Fair and Orderly markets"
}

@ARTICLE{Mitchell2001-be,
  title     = "Characteristics of Risk and Return in Risk Arbitrage",
  author    = "Mitchell, Mark and Pulvino, Todd",
  abstract  = "This paper analyzes 4,750 mergers from 1963 to 1998 to
               characterize the risk and return in risk arbitrage. Results
               indicate that risk arbitrage returns are positively correlated
               with market returns in severely depreciating markets but
               uncorrelated with market returns in flat and appreciating
               markets. This suggests that returns to risk arbitrage are
               similar to those obtained from selling uncovered index put
               options. Using a contingent claims analysis that controls for
               the nonlinear relationship with market returns, and after
               controlling for transaction costs, we find that risk arbitrage
               generates excess returns of four percent per year.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishers, Inc.",
  volume    =  56,
  number    =  6,
  pages     = "2135--2175",
  month     =  dec,
  year      =  2001
}

@ARTICLE{Mitchell2002-mx,
  title    = "Limited Arbitrage in Equity Markets",
  author   = "Mitchell, Mark and Pulvino, Todd and Stafford, Erik",
  abstract = "We examine 82 situations where the market value of a company is
              less than its subsidiary. These situations imply arbitrage
              opportunities, providing an ideal set- ting to study the risks
              and market frictions that prevent arbitrageurs from imme- diately
              forcing prices to fundamental values. For 30 percent of the
              sample, the link between the parent and its subsidiary is severed
              before the relative value discrep- ancy is corrected.
              Furthermore, returns to a specialized arbitrageur would be 50
              per- cent larger if the path to convergence was smooth rather
              than as observed. Uncertainty about the distribution of returns
              and characteristics of the risks limits arbitrage.",
  journal  = "J. Finance",
  volume   =  57,
  number   =  2,
  pages    = "551--584",
  month    =  apr,
  year     =  2002
}

@MISC{Noll2002-jy,
  title    = "The Economics of Promotion and Relegation in Sports Leagues: The
              Case of English Football",
  author   = "Noll, R G",
  abstract = "In most of the world's professsional sports leagues, the worst
              teams in better leagues are demoted while the best teams in
              weaker leagues are promoted. This article examines the economics
              of promotion and relegation, using data from English football.
              The crucial findings are as follows: players earn higher wages
              under promotion and relegations, promotion and relegation has a
              net positive effect on attendance, adn the effect of promotion
              and relegation on competitive balance is ambiguous. The
              unbalancing effect arises because the system places some teams in
              leagues in which they have no realistic chance to afford a
              winning teams, thereby causing teams to spend less on players
              during their (brief) stay in a higher league than they spent
              while trying to be promoted from a lesser league. The articles
              concludes with an analysis of how promotion and relegation might
              be implemented in North America.",
  journal  = "J. Sports Econom.",
  volume   =  3,
  pages    = "169--203",
  year     =  2002
}

@ARTICLE{Becker2002-ka,
  title    = "Estimation of average treatment effects based on propensity
              scores",
  author   = "Becker, Sascha O and Ichino, Andrea and {Others}",
  abstract = "In this paper, we give a short overview of some propensity score
              matching estimators suggested in the evaluation literature, and
              we provide a set of Stata programs, which we illustrate using the
              National SupportedWork (NSW) demonstration widely known in labor
              economics.",
  journal  = "Stata J.",
  volume   =  2,
  number   =  4,
  pages    = "358--377",
  year     =  2002
}

@MISC{Zhao2004-ak,
  title    = "Using Matching to Estimate Treatment Effects: Data Requirements,
              Matching Metrics, and Monte Carlo Evidence",
  author   = "Zhao, Zhong",
  abstract = "We compare propensity-score matching methods with covari-
              atematchingestimators.Wefirstdiscussthedatarequirementsofpropensity-
              score matching estimators and covariate matching estimators. Then
              we propose two new matching metrics incorporating the treatment
              outcome information and participation indicator information, and
              discuss the mo- tivations of different metrics. Next we study the
              small-sample properties of propensity-score matching versus
              covariate matching estimators, and of different matching metrics,
              through Monte Carlo experiments. Through a series of simulations,
              we provide some guidance to practitioners on how to choose among
              different matching estimators and matching metrics.",
  journal  = "Rev. Econ. Stat.",
  volume   =  86,
  pages    = "91--107",
  year     =  2004
}

@ARTICLE{Abadie2005-ct,
  title    = "Semiparametric {Difference-in-Differences} Estimators",
  author   = "Abadie, Alberto",
  abstract = "The difference-in-differences (DID) estimator is one of the most
              popular tools for applied research in economics to evaluate the
              effects of public interventions and other treatments of interest
              on some relevant outcome variables. However, it is well known
              that the DID estimator is based on strong identifying
              assumptions. In particular, the conventional DID estimator
              requires that, in the absence of the treatment, the average
              outcomes for the treated and control groups would have followed
              parallel paths over time. This assumption may be implausible if
              pre-treatment characteristics that are thought to be associated
              with the dynamics of the outcome variable are unbalanced between
              the treated and the untreated. That would be the case, for
              example, if selection for treatment is influenced by
              individual-transitory shocks on past outcomes (Ashenfelter's
              dip). This article considers the case in which differences in
              observed characteristics create non-parallel outcome dynamics
              between treated and controls. It is shown that, in such a case, a
              simple two-step strategy can be used to estimate the average
              effect of the treatment for the treated. In addition, the
              estimation framework proposed in this article allows the use of
              covariates to describe how the average effect of the treatment
              varies with changes in observed characteristics.",
  journal  = "Rev. Econ. Stud.",
  volume   =  72,
  pages    = "1--19",
  year     =  2005
}

@ARTICLE{Hahn1998-xm,
  title     = "On the Role of the Propensity Score in Efficient Semiparametric
               Estimation of Average Treatment Effects",
  author    = "Hahn, Jinyong",
  abstract  = "In this paper, the role of the propensity score in the efficient
               estimation of average treatment effects is examined. Under the
               assumption that the treatment is ignorable given some observed
               characteristics, it is shown that the propensity score is
               ancillary for estimation of the average treatment effects. The
               propensity score is not ancillary for estimation of average
               treatment effects on the treated. It is suggested that the
               marginal value of the propensity score lies entirely in the
               ``dimension reduction.'' Efficient semiparametric estimators of
               average treatment effects and average treatment effects on the
               treated are shown to take the form of relevant sample averages
               of the data completed by the nonparametric imputation method. It
               is shown that the projection on the propensity score is not
               necessary for efficient semiparametric estimation of average
               treatment effects on the treated even if the propensity score is
               known. An application to the experimental data reveals that
               conditioning on the propensity score may even result in a loss
               of efficiency.",
  journal   = "Econometrica",
  publisher = "[Wiley, Econometric Society]",
  volume    =  66,
  number    =  2,
  pages     = "315--331",
  year      =  1998
}

@ARTICLE{Heckman1998-na,
  title     = "Characterizing Selection Bias Using Experimental Data",
  author    = "Heckman, James and Ichimura, Hidehiko and Smith, Jeffrey and
               Todd, Petra",
  abstract  = "Semiparametric methods are developed to estimate the bias that
               arises from using nonexperimental comparison groups to evaluate
               social programs and to test the identifying assumptions that
               justify matching, selection models, and the method of
               difference-in-differences. Using data from an experiment on a
               prototypical social program and data from nonexperimental
               comparison groups, we reject the assumptions justifying matching
               and our extensions of it. The evidence supports the selection
               bias model and the assumptions that justify a semiparametric
               version of the method of difference-in-differences. We extend
               our analysis to consider applications of the methods to ordinary
               observational data.",
  journal   = "Econometrica",
  publisher = "[Wiley, Econometric Society]",
  volume    =  66,
  number    =  5,
  pages     = "1017--1098",
  year      =  1998,
  keywords  = "program evaluation; selection bias; semiparametric; training
               programs"
}

@ARTICLE{Quade1982-xz,
  title    = "Nonparametric analysis of covariance by matching",
  author   = "Quade, D",
  abstract = "The basic problem under consideration is the comparison of
              treatments with respect to a response Y when a covariable X is
              taken into account. Various methods involving matching may be
              regarded as compromises between the standard analysis of
              covariance and the standard analysis of independent matched
              pairs. First, there is no need to restrict attention to
              independent matched pairs, but rather all matched pairs may be
              incorporated. Then, if X is concomitant, that is, if its
              distribution is the same regardless of treatment, methods may be
              used which are based ultimately on randomization although in
              practice they are based on analysis of variance. When X is not
              concomitant, methods related to partial correlation (between Y
              and 'treatment', given X) are applicable. All methods considered
              may use either the actual magnitudes of Y or analogues of their
              ranks.",
  journal  = "Biometrics",
  volume   =  38,
  number   =  3,
  pages    = "597--611",
  month    =  sep,
  year     =  1982,
  language = "en"
}

@ARTICLE{Rubin1979-lv,
  title     = "Using Multivariate Matched Sampling and Regression Adjustment to
               Control Bias in Observational Studies",
  author    = "Rubin, Donald B",
  abstract  = "Monte Carlo methods are used to study the efficacy of
               multivariate matched sampling and regression adjustment for
               controlling bias due to specific matching variables X when
               dependent variables are moderately nonlinear in X. The general
               conclusion is that nearest available Mahalanobis metric matching
               in combination with regression adjustment on matched pair
               differences is a highly effective plan for controlling bias due
               to X.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  74,
  number    =  366,
  pages     = "318--328",
  year      =  1979,
  keywords  = "covariance adjustment"
}

@MISC{Abadie2011-bs,
  title    = "{Bias-Corrected} Matching Estimators for Average Treatment
              Effects",
  author   = "Abadie, Alberto and Imbens, Guido W",
  abstract = "Matching estimators for average treatment effects are widely used
              in evaluation research despite the fact that their large sample
              properties have not been established in many cases. The absence
              of formal results in this area may be partly due to the fact that
              standard asymptotic expansions do not apply to matching
              estimators with a fixed number of matches because such estimators
              are highly nonsmooth functionals of the data. In this article we
              develop new methods for analyzing the large sample properties of
              matching estimators and establish a number of new results. We
              focus on matching with replacement with a fixed number of
              matches. First, we show that matching estimators are not
              N1/2-consistent in general and describe conditions under which
              matching estimators do attain N1/2-consistency. Second, we show
              that even in settings where matching estimators are
              N1/2-consistent, simple matching estimators with a fixed number
              of matches do not attain the semiparametric efficiency bound.
              Third, we provide a consistent estimator for the large sample
              variance that does not require consistent nonparametric
              estimation of unknown functions. Software for implementing these
              methods is available in Matlab, Stata, and R.",
  journal  = "J. Bus. Econ. Stat.",
  volume   =  29,
  pages    = "1--11",
  year     =  2011
}

@MISC{Imbens2008-xi,
  title   = "Regression discontinuity designs: A guide to practice",
  author  = "Imbens, Guido W and Lemieux, Thomas",
  journal = "J. Econom.",
  volume  =  142,
  pages   = "615--635",
  year    =  2008
}

@MISC{Imbens2004-yu,
  title    = "Nonparametric Estimation of Average Treatment Effects Under
              Exogeneity: A Review",
  author   = "Imbens, Guido W",
  abstract = "Recently there has been a surge in econometric work focusing on
              estimating average treatment effects under various sets of
              assumptions. One strand of this literature has developed methods
              for estimating average treatment effects for a binary treatment
              under assumptions variously described as exogeneity,
              unconfoundedness, or selection on observables. The implication of
              these assumptions is that systematic (for example, average or
              distributional) differences in outcomes between treated and
              control units with the same values for the covariates are
              attributable to the treatment. Recent analysis has considered
              estimation and inference for average treatment effects under
              weaker assumptions than typical of the earlier literature by
              avoiding distributional and functional-form assumptions. Various
              methods of semiparametric estimation have been proposed,
              including estimating the unknown regression functions, matching,
              methods using the propensity score such as weighting and
              blocking, and combinations of these appro...",
  journal  = "Rev. Econ. Stat.",
  volume   =  86,
  pages    = "4--29",
  year     =  2004
}

@ARTICLE{Angrist1995-gv,
  title    = "{Two-Stage} Least Squares Estimation of Average Causal Effects in
              Models with Variable Treatment Intensity",
  author   = "Angrist, J D and Imbens, G W",
  abstract = "Two-stage least squares (TSLS) is widely used in econometrics to
              estimate parameters in systems of linear simultaneous equations
              and to solve problems of omitted-variables bias in
              single-equation estimation. We show here that TSLS can also be
              used to estimate the average causal effect of variable treatments
              such as drug dosage, hours of exam preparation, cigarette
              smoking, and years of schooling. The average causal effect in
              which we are interested is a conditional expectation of the
              difference between the outcomes of the treated and what these
              outcomes would have been in the absence of treatment. Given mild
              regularity assumptions, the probability limit of TSLS is a
              weighted average of per-unit average causal effects along the
              length of an appropriately defined causal response function. The
              weighting function is illustrated in an empirical example based
              on the relationship between schooling and earnings.",
  journal  = "J. Am. Stat. Assoc.",
  volume   =  90,
  pages    = "431--442",
  year     =  1995,
  keywords = "Schooling"
}

@MISC{Abadie2009-pa,
  title    = "Matching on the estimated propensity score",
  author   = "Abadie, A and Imbens, G W",
  abstract = "Propensity score matching estimators (Rosenbaum and Rubin, 1983)
              are widely used in evaluation research to estimate average
              treatment effects. In this article, we derive the large sample
              distribution of propensity score matching estimators. Our
              derivations take into account that the propensity score is itself
              estimated in a first step, prior to matching. We prove that first
              step estimation of the propensity score affects the large sample
              distribution of propensity score matching estimators. Moreover,
              we derive an adjustment to the large sample variance of
              propensity score matching estimators that corrects for first step
              estimation of the propensity score. In spite of the great
              popularity of propensity score matching estimators, these results
              were previously unavailable in the literature.",
  journal  = "NBER Work. Pap. Ser.",
  volume   =  15301,
  year     =  2009
}

@ARTICLE{Abadie2004-dp,
  title     = "Implementing matching estimators for average treatment effects
               in Stata",
  author    = "Abadie, Alberto and Drukker, David and Herr, Jane Leber and
               Imbens, Guido W and {Others}",
  abstract  = "This paper presents an implementation of matching estimators for
               average treatment effects in Stata. The nnmatch command allows
               you to estimate the average effect for all units or only for the
               treated or control units; to choose the number of matches; to
               specify the distance metric; to select a bias adjustment; and to
               use heteroskedastic-robust variance estimators.",
  journal   = "Stata J.",
  publisher = "STATA PRESS",
  volume    =  4,
  pages     = "290--311",
  year      =  2004,
  keywords  = "average treatment effects; exogeneity; ignorability; matching;
               nnmatch; st0072; unconfoundedness"
}

@MISC{Chamberlain2003-sf,
  title    = "Nonparametric Applications of Bayesian Inference",
  author   = "Chamberlain, Gary and Imbens, Guido W",
  abstract = "This article evaluates the usefulness of a nonparametric approach
              to Bayesian inference by presenting two applications. Our first
              application considers an educational choice problem. We focus on
              obtaining a predictive distribution for earnings corresponding to
              various levels of schooling. This predictive distribution
              incorporates the parameter uncertainty, so that it is relevant
              for decision making under uncertainty in the expected utility
              framework of microeconomics. The second application is to
              quantile regression. Our point here is to examine the potential
              of the nonparametric framework to provide inferences without
              relying on asymptotic approximations. Unlike in the first
              application, the standard asymptotic normal approximation turns
              out not to be a good guide.",
  journal  = "J. Bus. Econ. Stat.",
  volume   =  21,
  pages    = "12--18",
  year     =  2003
}

@ARTICLE{Athey2007-in,
  title     = "{DISCRETE} {CHOICE} {MODELS} {WITH} {MULTIPLE} {UNOBSERVED}
               {CHOICE} {CHARACTERISTICS*}",
  author    = "Athey, Susan and Imbens, Guido W",
  abstract  = "Since the pioneering work by Daniel McFadden,
               utility-maximization-based multinomial response models have
               become important tools of empirical researchers. Various
               generalizations of these models have been developed to allow for
               unobserved heterogeneity in taste parameters and choice
               characteristics. Here we investigate how rich a specification of
               the unobserved components is needed to rationalize arbitrary
               choice patterns in settings with many individual decision
               makers, multiple markets, and large choice sets. We find that if
               one restricts the utility function to be monotone in the
               unobserved choice characteristics, then up to two unobserved
               choice characteristics may be needed to rationalize the choices.",
  journal   = "Int. Econ. Rev.",
  publisher = "Blackwell Publishing Inc",
  volume    =  48,
  number    =  4,
  pages     = "1159--1192",
  month     =  nov,
  year      =  2007,
  keywords  = "discrete response; mixed logit"
}

@ARTICLE{Imbens1997-jl,
  title     = "{One-Step} Estimators for {Over-Identified} Generalized Method
               of Moments Models",
  author    = "Imbens, Guido W",
  abstract  = "In this paper I discuss alternatives to the GMM estimators
               proposed by Hansen (1982) and others. These estimators are shown
               to have a number of advantages. First of all, there is no need
               to estimate in an initial step a weight matrix as required in
               the conventional estimation procedure. Second, it is
               straightforward to derive the distribution of the estimator
               under general misspecification. Third, some of the alternative
               estimators have appealing information-theoretic interpretations.
               In particular, one of the estimators is an empirical likelihood
               estimator with an interpretation as a discrete support maximum
               likelihood estimator. Fourth, in an empirical example one of the
               new estimators is shown to perform better than the conventional
               estimators. Finally, the new estimators make it easier for the
               researcher to get better approximations to their distributions
               using saddlepoint approximations. The main cost is
               computational: the system of equations that has to be solved is
               of greater dimension than the number of parameters of interest.
               In practice this may or may not be a problem in particular
               applications.",
  journal   = "Rev. Econ. Stud.",
  publisher = "Oxford University Press",
  volume    =  64,
  number    =  3,
  pages     = "359--383",
  month     =  jul,
  year      =  1997
}

@MISC{Abadie2002-rk,
  title    = "Instrumental Variables Estimates of the Effect of Subsidized
              Training on the Quantiles of Trainee Earnings",
  author   = "Abadie, Alberto and Angrist, Joshua and Imbens, Guido",
  abstract = "This paper reports estimates of the effects of JTPA training
              programs on the distribution of earnings. The estimation uses a
              new instrumental variable (IV) method that measures program
              impacts on quantiles. The quantile treatment effects (QTE)
              estimator reduces to quantile regression when selection for
              treatment is exogenously determined. QTE can be computed as the
              solution to a convex linear programming problem, although this
              requires first-step estimation of a nuisance function. We develop
              distribution theory for the case where the first step is
              estimated nonparametrically. For women, the empirical results
              show that the JTPA program had the largest proportional impact at
              low quantiles. Perhaps surprisingly, however, JTPA training
              raised the quantiles of earnings for men only in the upper half
              of the trainee earnings distribution.",
  journal  = "Econometrica",
  volume   =  70,
  pages    = "91--117",
  year     =  2002
}

@ARTICLE{Imbens2010-jh,
  title    = "An economist's perspective on Shadish (2010) and West and
              Thoemmes (2010)",
  author   = "Imbens, Guido W",
  abstract = "In Shadish (2010) and West and Thoemmes (2010), the authors
              contrasted 2 approaches to causality. The first originated in the
              psychology literature and is associated with work by Campbell
              (e.g., Shadish, Cook, \& Campbell, 2002), and the second has its
              roots in the statistics literature and is associated with work by
              Rubin (e.g., Rubin, 2006). In this article, I discuss some of the
              issues raised by Shadish and by West and Thoemmes. I focus mostly
              on the impact the 2 approaches have had on research in a 3rd
              field, economics. In economics, the ideas of both Campbell and
              Rubin have been very influential, with some of the methods they
              developed now routinely taught in graduate programs and routinely
              used in empirical work and other methods receiving much less
              attention. At the same time, economists have added to the
              understanding of these methods and through these extensions have
              further improved researchers' ability to draw causal inferences
              in observational studies.",
  journal  = "Psychol. Methods",
  volume   =  15,
  number   =  1,
  pages    = "47--55",
  month    =  mar,
  year     =  2010,
  language = "en"
}

@MISC{Hirano2003-rz,
  title    = "Efficient Estimation of Average Treatment Effects Using the
              Estimated Propensity Score",
  author   = "Hirano, Keisuke and Imbens, Guido W and Ridder, Geert",
  abstract = "We are interested in estimating the average effect of a binary
              treatment on a scalar outcome. If assignment to the treatment is
              exogenous or unconfounded, that is, independent of the potential
              outcomes given covariates, biases associated with simple
              treatment-control average comparisons can be removed by adjusting
              for differences in the covariates. Rosenbaum and Rubin (1983)
              show that adjusting solely for differences between treated and
              control units in the propensity score removes all biases
              associated with differences in covariates. Although adjusting for
              differences in the propensity score removes all the bias, this
              can come at the expense of efficiency, as shown by Hahn (1998),
              Heckman, Ichimura, and Todd (1998), and Robins, Mark, and Newey
              (1992). We show that weighting by the inverse of a nonparametric
              estimate of the propensity score, rather than the true propensity
              score, leads to an efficient estimate of the average treatment
              effect. We provide intuition for this result by showing that this
              estimator can be interpreted as an empirical likelihood estimator
              that efficiently incorporates the information about the
              propensity score.",
  journal  = "Econometrica",
  volume   =  71,
  pages    = "1161--1189",
  year     =  2003
}

@ARTICLE{Imbens2007-gr,
  title    = "Cluster and Stratified Sampling",
  author   = "Imbens, G W and Wooldridge, J M",
  abstract = "These notes consider estimation and inference with cluster
              samples and samples obtained by stratifying the population. The
              main focus is on true cluster samples, although the case of
              applying cluster-sample methods to panel data is treated,
              including recent work where the sizes of the cross section and
              time series are similar.Wooldridge (2003, extended version 2006)
              contains a survey, but more recent work is discussed here.",
  journal  = "What's New in Econometrics Lecture Note",
  volume   =  8,
  pages    = "1--53",
  year     =  2007
}

@ARTICLE{Imbens2012-mt,
  title    = "Optimal Bandwidth Choice for the Regression Discontinuity
              Estimator",
  author   = "Imbens, Guido W and Kalyanaraman, Karthik",
  abstract = "We investigate the choice of the bandwidth for the regression
              discontinuity estimator. We focus on estimation by local linear
              regression, which was shown to have attractive properties
              (Porter, J. 2003, ``Estimation in the Regression Discontinuity
              Model'' (unpublished, Department of Economics, University of
              Wisconsin, Madison)). We derive the asymptotically optimal
              bandwidth under squared error loss. This optimal bandwidth
              depends on unknown functionals of the distribution of the data
              and we propose simple and consistent estimators for these
              functionals to obtain a fully data-driven bandwidth algorithm. We
              show that this bandwidth estimator is optimal according to the
              criterion of Li (1987, ``Asymptotic Optimality for Cp, CL,
              Cross-validation and Generalized Cross-validation: Discrete Index
              Set'', Annals of Statistics, 15, 958--975), although it is not
              unique in the sense that alternative consistent estimators for
              the unknown functionals would lead to bandwidth estimators with
              the same optimality properties. We illustrate the proposed
              bandwidth, and the sensitivity to the choices made in our
              algorithm, by applying the methods to a data set previously
              analysed by Lee (2008, ``Randomized Experiments from Non-random
              Selection in U.S. House Elections'', Journal of Econometrics,
              142, 675--697) as well as by conducting a small simulation study.",
  journal  = "Rev. Econ. Stud.",
  volume   =  79,
  pages    = "933--959",
  year     =  2012,
  keywords = "local linear regression; optimal bandwidth selection; regression
              discontinuity designs"
}

@ARTICLE{Wooldridge2007-dn,
  title    = "Difference in Differences Estimation",
  author   = "Wooldridge, Jeff and Imbens, Guido",
  abstract = "These notes provide an overview of standard
              difference-in-differences methods that have been used to study
              numerous policy questions. We consider some recent advances in
              Hansen (2007a,b) on issues of inference, focusing on what can be
              learned with various group/time period dimensions and serial
              independence in group-level shocks. Both the repeated cross
              sections and panel data cases are considered. We discuss recent
              work by Athey and Imbens (2006) on nonparametric approaches to
              difference-in-differences, and Abadie, Diamond, and Hainmueller
              (2007) on constructing synthetic control groups.",
  journal  = "What ' s New in Econometrics",
  pages    = "1--31",
  year     =  2007
}

@ARTICLE{Imbens2007-co,
  title    = "Linear panel data models",
  author   = "Imbens, G W and Wooldridge, J M",
  abstract = "These notes cover some recent topics in linear panel data models.
              They begin with a ``modern'' treatment of the basic linear model,
              and then consider some embellishments, such as random slopes and
              time-varying factor loads. In addition, fully robust tests for
              correlated random effects, lack of strict exogeneity, and
              contemporaneous endogeneity are presented. Section 4 considers
              estimation of models without strictly exogenous regressors, and
              Section 5 presents a unified framework for analyzing pseudo
              panels (constructed from repeated cross sections).",
  journal  = "Lecture Notes",
  pages    = "1--31",
  year     =  2007
}

@MISC{Athey2006-hp,
  title    = "Identification and Inference in Nonlinear
              {Difference-in-Differences} Models",
  author   = "Athey, Susan and Imbens, Guido W",
  abstract = "This paper develops a generalization of the widely used
              Difference-In-Difference (DID) method for evaluating the effects
              of policy changes. We propose a model that allows the control
              group and treatment groups to have different average benefits
              from the treatment. The assumptions of the proposed model are
              invariant to the scaling of the outcome. We provide conditions
              under which the model is nonparametrically identified and propose
              an estimator that can be applied using either repeated
              cross-section or panel data. Our ap- proach provides an estimate
              of the entire counterfactual distribution of outcomes that would
              have been experienced by the treatment group in the absence of
              the treatment, and likewise for the untreated group in the
              presence of the treatment. Thus, it enables the evaluation of
              policy interventions according to criteria such as a
              mean-variance tradeoff. We also propose methods for inference,
              showing that our estimator for the average treatment effect is
              root- N consistent and asymptotically normal. We consider
              extensions to allow for covariates, discrete dependent variables,
              and multiple groups and time periods.",
  journal  = "Econometrica",
  volume   =  74,
  pages    = "431--497",
  year     =  2006
}

@MISC{Imbens2003-km,
  title    = "Sensitivity to Exogeneity Assumptions in Program Evaluation",
  author   = "Imbens, Guido W",
  abstract = "In assessing whether observed covariates (e.g. matching) are
              suffcient to obtain exogeneity, if we suspect not, can at least
              obtain bounds on magnitude of problem and how much it affects
              outcomes.",
  journal  = "Am. Econ. Rev.",
  volume   =  93,
  pages    = "126--132",
  year     =  2003
}

@ARTICLE{Abadie2006-lu,
  title     = "Large Sample Properties of Matching Estimators for Average
               Treatment Effects",
  author    = "Abadie, Alberto and Imbens, Guido W",
  abstract  = "Matching estimators for average treatment effects are widely
               used in evaluation research despite the fact that their large
               sample properties have not been established in many cases. The
               absence of formal results in this area may be partly due to the
               fact that standard asymptotic expansions do not apply to
               matching estimators with a fixed number of matches because such
               estimators are highly nonsmooth functionals of the data. In this
               article we develop new methods for analyzing the large sample
               properties of matching estimators and establish a number of new
               results. We focus on matching with replacement with a fixed
               number of matches. First, we show that matching estimators are
               not N1/2-consistent in general and describe conditions under
               which matching estimators do attain N1/2-consistency. Second, we
               show that even in settings where matching estimators are
               N1/2-consistent, simple matching estimators with a fixed number
               of matches do not attain the semiparametric efficiency bound.
               Third, we provide a consistent estimator for the large sample
               variance that does not require consistent nonparametric
               estimation of unknown functions. Software for implementing these
               methods is available in Matlab, Stata, and R.",
  journal   = "Econometrica",
  publisher = "[Wiley, Econometric Society]",
  volume    =  74,
  number    =  1,
  pages     = "235--267",
  year      =  2006
}

@PHDTHESIS{Jain2012-hi,
  title    = "Essays on the Effect of Tax Exemption on Competitiveness,
              Performance and Portfolio Risk of Credit Unions and Subchapter
              {S} Banks",
  author   = "Jain, Ajeet",
  abstract = "The tax exemptions granted to financial institutions like
              Subchapter S banks and credit unions cost billions of dollars to
              the government. The dissertation investigates the effect of tax
              exemption on competitiveness, performance and portfolio risk of
              credit unions and Subchapter S banks. The methodologies include
              difference in differences estimation, univariate and multivariate
              analysis. The first essay entitled ``The tax exemption to
              Subchapter S banks: who gets the benefit?'' investigates the
              effect of tax exemption to Subchapter S banks on stakeholders and
              on job creation. Specifically, we investigate the effect of
              adoption of Subchapter S status on the four stakeholders of the
              banks: the customers of the bank, the employees of the bank, the
              owners of the bank and the government. The results indicate that
              the tax exemptions to Subchapter S banks do not create new jobs,
              and that the owners of the bank are the sole beneficiary of the
              tax exemptions since there is a significant increase in bank's
              return on equity after it adopts the Subchapter S status. vThe
              second essay entitled ``A comparison of credit unions and
              Subchapter S banks: who shares higher tax benefits with
              customers?'' examines whether credit unions are doing a better
              job of sharing the tax benefit with its customers. The results
              indicate that the credit union members do not receive the benefit
              in terms of lower loan rates, higher deposit rates or lower
              service charges. The findings also indicate that tax exemptions
              have been directed to support inefficient operations. The third
              essay entitled ``Asset quality comparison between credit unions
              and Subchapter S banks'' compares the asset quality of for-profit
              Subchapter S banks with not-for-profit credit unions. The results
              indicate that credit unions have better asset quality, but
              Subchapter S banks are superior in utilizing assets and
              generating higher net interest margin.",
  year     =  2012,
  school   = "FIU Electronic Theses and Dissertations."
}

@ARTICLE{Isik2003-he,
  title     = "Efficiency, Ownership and Market Structure, Corporate Control
               and Governance in the Turkish Banking Industry",
  author    = "Isik, Ihsan and Hassan, M Kabir",
  abstract  = "Turkish banks are quite heterogeneous in terms of organizational
               form, ownership structure, size, age, portfolio concentration,
               growth prospects and attitude toward risk. They also exhibit
               strong variations in performance as measured by several
               efficiency indices. In the light of theoretical advances in
               corporate finance and financial institutions, this paper is an
               in-depth cross-sectional analysis of the Turkish banking sector,
               which explores the various bank, market and regulatory
               characteristics that may explain the efficiency variations
               across banks. Consistent with the related hypotheses
               investigated, the results indicate that a number of independent
               bank characteristics are significantly correlated with various
               efficiency measures.",
  journal   = "J. Bus. Finance Account.",
  publisher = "Blackwell Science Ltd",
  volume    =  30,
  number    = "9-10",
  pages     = "1363--1421",
  month     =  dec,
  year      =  2003,
  keywords  = "efficiency; ownership; governance; Turkish banks; DEA"
}

@ARTICLE{Jiang2009-jf,
  title    = "The effects of governance changes on bank efficiency in China: A
              stochastic distance function approach",
  author   = "Jiang, Chunxia and Yao, Shujie and Zhang, Zongyi",
  abstract = "China has accelerated banking reform since joining the Word Trade
              Organisation (WTO) in 2001. Employing a stochastic distance
              function approach, this paper examines bank technical efficiency
              and differentiates the static, selection and dynamic governance
              effects on bank efficiency for the 11-year period 1995--2005. The
              results show that bank efficiency has improved. Joint-stock
              ownership is associated with better performance in terms of
              profitability than state ownership (static effect). Strong
              selection effects are found for both foreign acquisition and
              going public reform strategies. Foreign acquisition may benefit
              domestic banks by efficiency gains in the long run, but
              privatization via initial public offerings (IPOs) appears to have
              only some short-term effects.",
  journal  = "China Econ. Rev.",
  volume   =  20,
  number   =  4,
  pages    = "717--731",
  month    =  dec,
  year     =  2009,
  keywords = "Distance function; Efficiency; Banking; China"
}

@ARTICLE{Fries2005-lr,
  title    = "Cost efficiency of banks in transition: Evidence from 289 banks
              in 15 post-communist countries",
  author   = "Fries, Steven and Taci, Anita",
  abstract = "To understand the transformation of banking in the post-communist
              transition, we examine the cost efficiency of 289 banks in 15
              East European countries. We find that banking systems in which
              foreign-owned banks have a larger share of total assets have
              lower costs and that the association between a country's progress
              in banking reform and cost efficiency is non-linear. Early stages
              of reform are associated with cost reductions, while costs tend
              to rise at more advanced stages. Private banks are more efficient
              than state-owned banks, but there are differences among private
              banks. Privatised banks with majority foreign ownership are the
              most efficient and those with domestic ownership are the least.",
  journal  = "Journal of Banking \& Finance",
  volume   =  29,
  number   =  1,
  pages    = "55--81",
  month    =  jan,
  year     =  2005,
  keywords = "Banking; C30; Cost efficiency; G21; P20; Transition economies"
}

@ARTICLE{Zelenyuk2006-zg,
  title     = "Corporate Governance and Firm's Efficiency: The Case of a
               Transitional Country, Ukraine",
  author    = "Zelenyuk, Valentin and Zheka, Vitaliy",
  abstract  = "No abstract is available for this item.",
  journal   = "Journal of Productivity Analysis",
  publisher = "Springer",
  volume    =  25,
  number    =  1,
  pages     = "143--157",
  year      =  2006,
  keywords  = "C24; Corporate governance; D24; DEA; Efficiency; G30; Ownership;
               P27; Ukraine"
}

@ARTICLE{Demirguc-Kunt2004-xx,
  title   = "Regulations, market structure, institutions, and the cost of
             financial intermediation",
  author  = "Demirg{\"u}{\c c}-Kunt, Asli and Laeven, Luc and Levine, Ross",
  journal = "J. Money Credit Bank.",
  volume  =  36,
  pages   = "593--622",
  year    =  2004
}

@ARTICLE{Beck2006-xz,
  title     = "Bank supervision and corruption in lending",
  author    = "Beck, Thorsten and Demirguc-Kunt, Asli and Levine, Ross",
  abstract  = "Which commercial bank supervisory policies ease - or intensify -
               the degree to which bank corruption is an obstacle to firms
               raising external finance? Based on new data from more than 2,500
               firms across 37 countries, this paper provides the first
               empirical assessment of the impact of different bank supervisory
               policies on firms\%u2019 financing obstacles. We find that the
               traditional approach to bank supervision, which involves
               empowering official supervisory agencies to directly monitor,
               discipline, and influence banks, does not improve the integrity
               of bank lending. Rather, we find that a supervisory strategy
               that focuses on empowering private monitoring of banks by
               forcing banks to disclose accurate information to the private
               sector tends to lower the degree to which corruption of bank
               officials is an obstacle to firms raising external finance. In
               extensions, we find that regulations that empower private
               monitoring exert a particularly beneficial effect on the
               integrity of bank lending in countries with sound legal
               institutions.(This abstract was borrowed from another version of
               this item.)",
  journal   = "J. Monet. Econ.",
  publisher = "Elsevier",
  volume    =  53,
  number    =  8,
  pages     = "2131--2163",
  year      =  2006
}

@ARTICLE{Koetter2008-iy,
  title     = "The stability of bank efficiency rankings when risk preferences
               and objectives are different",
  author    = "Koetter, Michael",
  abstract  = "We analyze the stability of efficiency rankings of German
               universal banks between 1993 and 2004. First, we estimate
               traditional efficiency scores with stochastic cost and
               alternative profit frontier analysis. Then, we explicitly allow
               for different risk preferences and measure efficiency with a
               structural model based on utility maximization. Using the almost
               ideal demand system, we estimate input- and profit-demand
               functions to obtain proxies for expected return and risk.
               Efficiency is then measured in this risk-return space. Mean
               risk-return efficiency is somewhat higher than cost and
               considerably higher than profit efficiency (PE). More
               importantly, rank-order correlation between these measures are
               low or even negative. This suggests that best-practice
               institutes should not be identified on the basis of traditional
               efficiency measures alone. Apparently, low cost and/or PE may
               merely result from alternative yet efficiently chosen
               risk-return trade-offs.",
  journal   = "The European Journal of Finance",
  publisher = "Taylor \& Francis Journals",
  volume    =  14,
  number    =  2,
  pages     = "115--135",
  year      =  2008,
  keywords  = "Germany; banks; efficiency; risk"
}

@TECHREPORT{Moon1997-pf,
  title     = "Measuring Bank Efficiency When Managers Trade Return for Reduced
               Risk",
  author    = "Moon, Choon-Goel and Hughes, Joseph P",
  abstract  = "No abstract is available for this item.",
  publisher = "Rutgers University, Department of Economics",
  number    =  199520,
  month     =  feb,
  year      =  1997
}

@ARTICLE{Fare2004-zs,
  title     = "The effect of risk-based capital requirements on profit
               efficiency in banking",
  author    = "F{\"a}re, Rolf and Grosskopf, Shawna and Weber *, William L",
  abstract  = "The purpose of this paper is twofold: to show how to measure
               profit efficiency in banking using a newly developed technique,
               and to use that technique to determine the effect of risk-based
               capital requirements on the profit performance of US banks. The
               measure of profit efficiency used captures deviations from
               profit maximization arising from technical inefficiency, caused
               by a lack of managerial oversight and allocative inefficiency,
               which is caused by managers choosing a nonoptimal mix of inputs
               and outputs. A leverage ratio constraint and a risk-weighted
               capital ratio constraint are explicitly included in the model,
               which allows identification of the effect on profits of those
               constraints. The techniques are applied to random samples of US
               banks for 1990, 1992, and 1994. The results indicate that
               allocative inefficiency is a larger source of profit loss than
               technical inefficiency and that the risk-based capital standards
               have a significant effect on bank allocative efficiency.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  36,
  number    =  15,
  pages     = "1731--1743",
  month     =  sep,
  year      =  2004
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Avkiran2009-yn,
  title    = "Removing the impact of environment with units-invariant efficient
              frontier analysis: An illustrative case study with intertemporal
              panel data‚òÜ",
  author   = "Avkiran, N",
  abstract = "The four-stage methodology consists of a units-invariant
              efficient frontier analysis followed by Tobit regression,
              adjustment of data, and a repeat of the efficient frontier
              analysis. The outlined methodology is an improvement over
              existing similar approaches because the playing field can be
              levelled by adjusting data based on input as well as output
              slacks for managers who may have been advantaged or disadvantaged
              by their environments. The accompanying case study investigates
              the influence of the general level of interest rates on bank
              efficiency using intertemporal panel data spanning 8 years and
              two countries. Key findings support the assertion that changes in
              interest rates can distort measurement of bank efficiency.",
  journal  = "Omega",
  volume   =  37,
  number   =  3,
  pages    = "535--544",
  month    =  jun,
  year     =  2009,
  keywords = "banking; case study; dea; efficiency"
}

@ARTICLE{Liu2008-bm,
  title     = "A multistage method to measure efficiency and its application to
               Japanese banking industry",
  author    = "Liu, Junming and Tone, Kaoru",
  abstract  = "When measuring technical efficiency with existing data
               envelopment analysis (DEA) techniques, mean efficiency scores
               generally exhibit volatile patterns over time. This appears to
               be at odds with the general perception of learning-by-doing
               management, due to Arrow [The economic implications of learning
               by doing. Review of Economic Studies 1964; 154-73]. Further,
               this phenomenon is largely attributable to the fundamental
               assumption of deterministic data maintained in DEA models, and
               to the difficulty such models have in incorporating
               environmental influences. This paper proposes a three-stage
               method to measure DEA efficiency while controlling for the
               impacts of both statistical noise and environmental factors.
               Using panel data on Japanese banking over the period 1997-2001,
               we demonstrate that the proposed approach greatly mitigates
               these weaknesses of DEA models. We find a stable upward trend in
               mean measured efficiency, indicating that, on average, the
               bankers were learning over the sample period. Therefore, we
               conclude that this new method is a significant improvement
               relative to those DEA models currently used by researchers,
               corporate management, and industrial regulatory bodies to
               evaluate performance of their respective interests.",
  journal   = "Socioecon. Plann. Sci.",
  publisher = "Elsevier",
  volume    =  42,
  number    =  2,
  pages     = "75--91",
  year      =  2008
}

@ARTICLE{Fiordelisi2007-ng,
  title     = "Shareholder value efficiency in European banking",
  author    = "Fiordelisi, Franco",
  abstract  = "No abstract is available for this item.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  31,
  number    =  7,
  pages     = "2151--2171",
  year      =  2007
}

@ARTICLE{Lozano-Vivas2002-up,
  title     = "An Efficiency Comparison of European Banking Systems Operating
               under Different Environmental Conditions",
  author    = "Lozano-Vivas, Ana and Pastor, Jes{\'u}s and Pastor, Jos{\'e}",
  abstract  = "The paper investigates the operating efficiency differences of a
               sample of commercial banks across 10 European countries. First,
               the paper analyzes the technical efficiency of each country
               sample following the ``basic'' Data Envelopment Analysis (DEA)
               model incorporating only banking variables. Then, a ``complete''
               DEA model is introduced, incorporating environmental factors
               together with the banking variables of the basic model. The
               comparison between the two models shows that country-specific
               environmental conditions exercise a strong influence over the
               behavior of each country's banking industry. Copyright Kluwer
               Academic Publishers 2002",
  journal   = "Journal of Productivity Analysis",
  publisher = "Springer",
  volume    =  18,
  number    =  1,
  pages     = "59--77",
  year      =  2002,
  keywords  = "DEA; European banking; efficiency; environmental conditions"
}

@INCOLLECTION{Paradi2011-da,
  title     = "Assessing Bank and Bank Branch Performance",
  booktitle = "Handbook on Data Envelopment Analysis",
  author    = "Paradi, Joseph C and Yang, Zijiang and Zhu, Haiyan",
  editor    = "Cooper, William W and Seiford, Larry M and Zhu, Joe",
  abstract  = "The banking industry has been the object of DEA analyses by a
               significant number of researchers and probably is the most
               heavily studied of all business sectors. Various DEA models have
               been applied in performance assessing problems, and the banks'
               complex production processes have further motivated the
               development and improvement of DEA techniques. The main
               application areas for DEA in bank and branch performance
               analysis include the following: efficiency ranking; resource
               allocation, efficiency trends investigation; environmental
               impacts compensation; examining the impacts of new technology,
               ownership, deregulation, corporate, economic, and political
               events, etc.",
  publisher = "Springer, Boston, MA",
  pages     = "315--361",
  chapter   =  13,
  series    = "International Series in Operations Research \& Management
               Science",
  edition   = "Second",
  year      =  2011,
  language  = "en"
}

@MISC{Hughes2013-nm,
  title       = "Who Said Large Banks Don't Experience Scale Economies?
                 Evidence from a {Risk-Return-Driven} Cost Function",
  author      = "Hughes, Joseph P and Mester, Loretta J",
  abstract    = "The Great Recession focused attention on large financial
                 institutions and systemic risk. We investigate whether large
                 size provides any cost advantages to the economy and, if so,
                 whether these cost advantages are due to technological scale
                 economies or too-big-to-fail subsidies. Estimating scale
                 economies is made more complex by risk-taking. Better
                 diversification resulting from larger scale generates scale
                 economies but also incentives to take more risk. When this
                 additional risk-taking adds to cost, it can obscure the
                 underlying scale economies and engender misleading econometric
                 estimates of them. Using data pre- and post-crisis, we
                 estimate scale economies using two production models. The
                 standard model ignores endogenous risk-taking and finds little
                 evidence of scale economies. The model accounting for
                 managerial risk preferences and endogenous risk-taking finds
                 large scale economies, which are not driven by too-big-to-fail
                 considerations. We evaluate the costs and competitive
                 implications of breaking up the largest banks into smaller
                 banks.This paper supersedes Federal Reserve Bank of
                 Philadelphia Working Paper No. 11-27.",
  journal     = "FRB of Philadelphia Working Paper No. 13-13",
  series      = "Working Paper",
  institution = "FRB of Philadelphia",
  month       =  apr,
  year        =  2013,
  keywords    = "Banking; Production; Risk; Scale economies; Too big to fail"
}

@ARTICLE{Halkos2004-uj,
  title    = "Efficiency measurement of the Greek commercial banks with the use
              of financial ratios: a data envelopment analysis approach",
  author   = "Halkos, George E and Salamouris, Dimitrios S",
  abstract = "This study offers an application of a non-parametric analytic
              technique (data envelopment analysis, DEA) in measuring the
              performance of the Greek banking sector. It explores the
              efficiency of Greek banks with the use of a number of suggested
              financial efficiency ratios for the time period 1997--1999. In
              this way the proposed model offers an empirical reference set for
              comparing the inefficient banks with the efficient ones. It
              departs from most frontier studies of bank performance, by using
              these suggested ratios as output measures and with no use of
              input measures. The proposed model is compared to the
              conventionally used input--output analysis as well as to the
              simple ratio analysis. It is shown that data envelopment analysis
              can be used as either an alternative or complement to ratio
              analysis for the evaluation of an organization's performance. We
              find that the higher the size of total assets the higher the
              efficiency. We also find a wide variation in performance and we
              show that the increase in efficiency is accompanied with a
              reduction in the number of small banks due to mergers and
              acquisitions. Finally, from the efficiency results it seems that
              there is a non-systematic relationship between transfer of
              ownership and last period's performance.",
  journal  = "Management Accounting Research",
  volume   =  15,
  number   =  2,
  pages    = "201--224",
  month    =  jun,
  year     =  2004,
  keywords = "banking; data envelopment analysis; financial ratios"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{McKillop2009-xp,
  title     = "Cost Performance of Irish Credit Unions",
  author    = "McKillop, Donal G and Quinn, Barry",
  abstract  = "There are 424 credit unions in Ireland with assets under their
               control of ‚Ç¨14.3bn and a membership of 2.5m which equates to
               about 66\% of the economically active population, the highest
               penetration level of any country. That said, the Irish movement
               sits at a critical development stage, well behind mature markets
               such as Canada and the US in terms of product provision,
               technological sophistication, fragmentation of trade bodies and
               regulatory environment. This study analyses relative cost
               efficiency or performance of Irish credit unions using the
               popular frontier approach which measures an entity's efficiency
               relative to a frontier of best practice. Parametric techniques
               are utilised, with variation in inefficiency being attributed to
               credit union-specific factors. The stochastic cost frontier
               parameters and the credit-union specific parameters are
               simultaneously estimated to produce valid statistical
               inferences. The study finds that the majority of Irish credit
               unions are not operating at optimal levels. It further
               highlights the factors which drive efficiency variation across
               credit unions and they include technological sophistication,
               'sponsor donated' resources, interest rate differentials and the
               levels of bad debt written off.",
  journal   = "Journal of Co-operative Studies",
  publisher = "UK Society for Co-operative Studies",
  volume    =  42,
  number    =  1,
  pages     = "23--36",
  year      =  2009
}

@ARTICLE{Wilson2003-em,
  title     = "Testing Independence in Models of Productive Efficiency",
  author    = "Wilson, Paul",
  abstract  = "Bootstrap methods for inference in nonparametric models of
               productive efficiency can be simplified, reducing computational
               burden, when the independence condition implicitly assumed in
               Simar and Wilson (1998) holds. This paper surveys nonparametric
               tests of independence that might be useful in this context.
               Copyright Kluwer Academic Publishers 2003",
  journal   = "Journal of Productivity Analysis",
  publisher = "Springer",
  volume    =  20,
  number    =  3,
  pages     = "361--390",
  year      =  2003,
  keywords  = "data envelopment analysis; dependence; productive efficiency"
}

@ARTICLE{Darrat2001-wk,
  title    = "On Testing the Random Walk Hypothesis: A {Model-Comparison}
              Approach",
  author   = "Darrat, Ali F and Zhong, Maosen",
  abstract = "The main intention of this paper is to investigate, with new
              daily data, whether prices in the two Chinese stock exchanges
              (Shanghai and Shenzhen) follow a rand",
  month    =  oct,
  year     =  2001,
  keywords = "Chinese stock market; artificial neural network; random walk
              hypothesis"
}

@ARTICLE{Moench2008-ri,
  title    = "Forecasting the yield curve in a data-rich environment: A
              no-arbitrage factor-augmented {VAR} approach",
  author   = "Moench, Emanuel",
  abstract = "This paper suggests a term structure model which parsimoniously
              exploits a broad macroeconomic information set. The model uses
              the short rate and the common components of a large number of
              macroeconomic variables as factors. Precisely, the dynamics of
              the short rate are modeled with a Factor-Augmented Vector
              Autoregression and the term structure is derived using parameter
              restrictions implied by no-arbitrage. The model has economic
              appeal and provides better out-of-sample yield forecasts at
              intermediate and long horizons than a number of previously
              suggested approaches. The forecast improvement is highly
              significant and particularly pronounced for short and medium-term
              maturities.",
  journal  = "J. Econom.",
  volume   =  146,
  number   =  1,
  pages    = "26--43",
  month    =  sep,
  year     =  2008,
  keywords = "c13; c32; e43; e44; e52"
}

@ARTICLE{Fang2011-mk,
  title    = "Market Reforms, Legal Changes and Bank {Risk-Taking} -- Evidence
              from Transition Economies",
  author   = "Fang, Yiwei and Hasan, Iftekhar and Marton, Katherin",
  abstract = "The policy changes and structural reforms in transition economies
              over the past two decades have created exogenous variations in
              institutional development, whic",
  journal  = "Bank of Finland Discussion Paper",
  number   =  7,
  series   = "Discussion Paper",
  month    =  mar,
  year     =  2011,
  address  = "Helenski",
  keywords = "bank risk; foreign ownership; institutional development;
              transition banking"
}

@ARTICLE{Marshall2013-he,
  title    = "{ETF} arbitrage: Intraday evidence",
  author   = "Marshall, Ben R and Nguyen, Nhut H and Visaltanachoti, Nuttawat",
  abstract = "We use two extremely liquid S\&P 500 ETFs to analyze the
              prevailing trading conditions when mispricing allowing arbitrage
              opportunities is created. While these ETFs are not perfect
              substitutes, our correlation and error correction results suggest
              investors view them as close substitutes. Spreads increase just
              before arbitrage opportunities, consistent with a decrease in
              liquidity. Order imbalance increases as markets become more
              one-sided and spread changes become more volatile which suggests
              an increase in liquidity risk. The price deviations are followed
              by a tendency to quickly correct back towards parity.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  9,
  pages    = "3486--3498",
  month    =  sep,
  year     =  2013,
  keywords = "g1; g14"
}

@INCOLLECTION{Hughes2010-ro,
  title     = "Efficiency in banking: theory, practice, and evidence",
  booktitle = "Handbook of Banking",
  author    = "Hughes, Joseph P and Mester, Loretta J",
  editor    = "Berger, Allen N and Wilson, John O S and Molyneux, Philip",
  year      =  2010
}

@ARTICLE{Casu2003-az,
  title     = "A comparative study of efficiency in European banking",
  author    = "Casu, Barbara and Molyneux, Philip",
  abstract  = "This paper investigates whether there has been an improvement in
               and convergence of productive efficiency across European banking
               markets since the creation of the Single Internal Market. Using
               efficiency measures derived from DEA estimation, the
               determinants of European bank efficiency are evaluated using the
               Tobit regression model approach. The established literature on
               modelling the determinants of bank efficiency is then extended
               by recognizing the problem of the inherent dependency of DEA
               efficiency scores when used in regression analysis. To overcome
               the dependency problem, a bootstrapping technique is applied.
               Overall, the results suggest that since the EU's Single Market
               Programme there has been a small improvement in bank efficiency
               levels, although there is little evidence to suggest that these
               have converged. The results also suggest that inference on the
               determinants of bank efficiency drawn from non-bootstrapped
               regression analysis may be biased and misleading.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  35,
  number    =  17,
  pages     = "1865--1876",
  month     =  nov,
  year      =  2003
}

@ARTICLE{Avkiran1999-wh,
  title    = "The evidence on efficiency gains: The role of mergers and the
              benefits to the public",
  author   = "Avkiran, Necmi Kemal",
  abstract = "Operating efficiencies, employee productivity, profit performance
              and average relative efficiency (using Data Envelopment Analysis)
              were measured for Australian trading banks from 1986 to 1995.
              Changes in a bank's market share of deposits is explored as a
              means of determining the extent to which efficiency gains are
              passed on to the public. In general, efficiencies rose in the
              post-deregulation period. Evidence from the merger cases studied
              supports the reports of others that acquiring banks are more
              efficient than target banks. However, the acquiring bank does not
              always maintain its pre-merger efficiency. Decision-makers ought
              to be more cautious in promoting mergers as a means to enjoying
              efficiency gains. There is mixed evidence on the extent to which
              the benefits of efficiency gains are passed on to the public.",
  journal  = "Journal of Banking \& Finance",
  volume   =  23,
  number   =  7,
  pages    = "991--1013",
  month    =  jul,
  year     =  1999,
  keywords = "d61; g21; g34"
}

@ARTICLE{Soteriou1999-pp,
  title    = "Using data envelopment analysis for costing bank products",
  author   = "Soteriou, Andreas C and Zenios, Stavros A",
  abstract = "An important problem that many banks face is to provide
              satisfactory cost estimates for the variety of products and
              services they offer. Accurate product cost estimates can be used
              to support better product mix and pricing decisions. In this
              paper we present a method for providing efficient and reliable
              cost estimates of bank products at the branch level, based on the
              non-parametric benchmarking technique of Data Envelopment
              Analysis (DEA). Results from an empirical study undertaken in a
              banking environment to demonstrate the applicability of the
              method are also presented.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  114,
  number   =  2,
  pages    = "234--248",
  month    =  apr,
  year     =  1999,
  keywords = "bank branch efficiency; bank product cost allocation; dea"
}

@ARTICLE{Brissimis2008-en,
  title    = "Exploring the nexus between banking sector reform and
              performance: Evidence from newly acceded {EU} countries",
  author   = "Brissimis, Sophocles N and Delis, Manthos D and Papanikolaou,
              Nikolaos I",
  abstract = "The aim of this study is to examine the relationship between
              banking sector reform and bank performance -- measured in terms
              of efficiency, total factor productivity growth and net interest
              margin -- accounting for the effects through competition and bank
              risk-taking. To this end, we develop an empirical model of bank
              performance, which is consistently estimated using recent
              econometric techniques. The model is applied to bank panel data
              from ten newly acceded EU countries. The results indicate that
              both banking sector reform and competition exert a positive
              impact on bank efficiency, while the effect of reform on total
              factor productivity growth is significant only toward the end of
              the reform process. Finally, the effect of capital and credit
              risk on bank performance is in most cases negative, while it
              seems that higher liquid assets reduce the efficiency and
              productivity of banks.",
  journal  = "Journal of Banking \& Finance",
  volume   =  32,
  number   =  12,
  pages    = "2674--2683",
  month    =  dec,
  year     =  2008,
  keywords = "c14; g21; l1"
}

@ARTICLE{Charnes1978-cp,
  title    = "Measuring the efficiency of decision making units",
  author   = "Charnes, A and Cooper, W W and Rhodes, E",
  abstract = "A nonlinear (nonconvex) programming model provides a new
              definition of efficiency for use in evaluating activities of
              not-for-profit entities participating in public programs. A
              scalar measure of the efficiency of each participating unit is
              thereby provided, along with methods for objectively determining
              weights by reference to the observational data for the multiple
              outputs and multiple inputs that characterize such programs.
              Equivalences are established to ordinary linear programming
              models for effecting computations. The duals to these linear
              programming models provide a new way for estimating extremal
              relations from observational data. Connections between
              engineering and economic approaches to efficiency are delineated
              along with new interpretations and ways of using them in
              evaluating and controlling managerial behavior in public
              programs.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  2,
  number   =  6,
  pages    = "429--444",
  month    =  nov,
  year     =  1978
}

@MISC{Cihak2008-ji,
  title       = "Quality of Financial Sector Regulation and Supervision Around
                 the World",
  author      = "Cihak, Martin and Tieman, Alexander",
  series      = "Working paper series No. 08/190",
  institution = "International Monetary Fund",
  year        =  2008
}

@ARTICLE{Barth2004-mb,
  title    = "Bank regulation and supervision: what works best?",
  author   = "Barth, James R and Caprio, Jr., Gerard and Levine, Ross",
  abstract = "This paper uses our new database on bank regulation and
              supervision in 107 countries to assess the relationship between
              specific regulatory and supervisory practices and banking-sector
              development, efficiency, and fragility. The paper examines: (i)
              regulatory restrictions on bank activities and the mixing of
              banking and commerce; (ii) regulations on domestic and foreign
              bank entry; (iii) regulations on capital adequacy; (iv) deposit
              insurance system design features; (v) supervisory power,
              independence, and resources; (vi) loan classification stringency,
              provisioning standards, and diversification guidelines; (vii)
              regulations fostering information disclosure and private-sector
              monitoring of banks; and (viii) government ownership. The
              results, albeit tentative, raise a cautionary flag regarding
              government policies that rely excessively on direct government
              supervision and regulation of bank activities. The findings
              instead suggest that policies that rely on guidelines that (1)
              force accurate information disclosure, (2) empower private-sector
              corporate control of banks, and (3) foster incentives for private
              agents to exert corporate control work best to promote bank
              development, performance and stability.",
  journal  = "Journal of Financial Intermediation",
  volume   =  13,
  number   =  2,
  pages    = "205--248",
  month    =  apr,
  year     =  2004,
  keywords = "g21; g38; l51"
}

@ARTICLE{Simar2000-at,
  title     = "A general methodology for bootstrapping in non-parametric
               frontier models",
  author    = "Simar, L{\'e}opold and Wilson, Paul W",
  abstract  = "The Data Envelopment Analysis method has been extensively used
               in the literature to provide measures of firms' technical
               efficiency. These measures allow rankings of firms by their
               apparent performance. The underlying frontier model is
               non-parametric since no particular functional form is assumed
               for the frontier model. Since the observations result from some
               data-generating process, the statistical properties of the
               estimated efficiency measures are essential for their
               interpretations. In the general multi-output multi-input
               framework, the bootstrap seems to offer the only means of
               inferring these properties (i.e. to estimate the bias and
               variance, and to construct confidence intervals). This paper
               proposes a general methodology for bootstrapping in frontier
               models, extending the more restrictive method proposed in Simar
               \& Wilson (1998) by allowing for heterogeneity in the structure
               of efficiency. A numerical illustration with real data is
               provided to illustrate the methodology.",
  journal   = "J. Appl. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  27,
  number    =  6,
  pages     = "779--802",
  month     =  aug,
  year      =  2000
}

@ARTICLE{Goddard2007-aj,
  title    = "European banking: An overview",
  author   = "Goddard, John and Molyneux, Philip and Wilson, John O S and
              Tavakoli, Manouche",
  abstract = "Against a background of far-reaching structural change in the
              banking sector, this article reviews the recent academic
              literature on developments in European banking. European banking
              markets have become increasingly integrated in recent years, but
              barriers to full integration, especially in retail banking, still
              remain. European integration has possible implications for
              systemic risk, and poses various challenges for the current
              supervisory framework. The banks' responses to the changing
              competitive environment include the pursuit of strategies of
              diversification, vertical product differentiation and
              consolidation. European integration has implications for
              competition in banking markets, for the nature of long-term
              borrower-lender relationships, and for the relationships between
              ownership structure, technological change and bank efficiency.
              The article concludes by reviewing recent literature on the
              credit channel in the monetary transmission mechanism, and
              interest rate pass-through.",
  journal  = "Journal of Banking \& Finance",
  volume   =  31,
  number   =  7,
  pages    = "1911--1935",
  month    =  jul,
  year     =  2007,
  keywords = "g21; l11"
}

@ARTICLE{Esho2001-et,
  title    = "The determinants of cost efficiency in cooperative financial
              institutions: Australian evidence",
  author   = "Esho, Neil",
  abstract = "Previous studies of efficiency in cooperative financial
              institutions assume subsidies to be a normal part of business
              operations. Using a sample of Australian credit unions (CUs) this
              paper shows that ignoring subsidies biases estimates of cost
              efficiency and efficiency rankings. Time varying cost efficiency
              is estimated using stochastic frontier and generalised WITHIN
              approaches and compared with the distribution free (DF) approach
              assuming constant efficiency. Despite significant industry
              rationalisation, average efficiency shows little improvement over
              the period 1985--1993, however efficiency rankings change
              considerably. Second stage regressions suggest bond type, size,
              age, average deposit size and interest rate spreads are
              significant determinants of relative cost efficiency.",
  journal  = "Journal of Banking \& Finance",
  volume   =  25,
  number   =  5,
  pages    = "941--964",
  month    =  may,
  year     =  2001,
  keywords = "g21"
}

@ARTICLE{Esho96-lh,
  title    = "X-efficiency of Australian Permanent Building Societies,
              1974-1990",
  author   = "Esho, Neilsharpe",
  abstract = "Examines the X-efficiency of Australian Permanent Building
              Societies (PBS) using the stochastic econometric frontier
              approach. Comparison of the X-efficiency estimates with similar
              estimates for banks and savings and loans (S\&Ls) in the United
              States; Difference between Australian PBSs from S\&Ls in the US;
              Effect of the neglect of cost function dynamics on the estimates
              of cost X-efficiency.",
  journal  = "Econ. Rec.",
  volume   =  72,
  number   =  218,
  pages    = "96",
  year     =  96,
  keywords = "financial institutions; savings \& loan associations"
}

@ARTICLE{Garden1999-ic,
  title    = "The x-efficiency and allocative efficiency effects of credit
              union mergers",
  author   = "Garden, Kaylee A and Ralston, Deborah E",
  abstract = "The relative efficiency effects of Australian credit union
              mergers are examined. The period of investigation is June
              1992--June 1997, which allows the examination of 16 credit union
              mergers in the 1993--1994 financial year. Multiple regression is
              applied to examine the impact of credit union mergers on
              x-efficiency and allocative efficiency. The data envelopment
              analysis (DEA) frontier approach is used to provide measures of
              x-efficiency and allocative efficiency. Results provide
              statistical evidence that, on average, credit union mergers do
              not result in an increase in x-efficiency or allocative
              efficiency postmerger relative to other credit unions.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  9,
  number   =  3,
  pages    = "285--301",
  month    =  aug,
  year     =  1999,
  keywords = "credit unions; efficiency; mergers"
}

@ARTICLE{McKillop2002-gp,
  title    = "Investigating the cost performance of {UK} credit unions using
              radial and non-radial efficiency measures",
  author   = "McKillop, D G and Glass, J C and Ferguson, C",
  abstract = "This paper examines the relative efficiency of UK credit unions.
              Radial and non-radial measures of input cost efficiency plus
              associated scale efficiency measures are computed for a selection
              of input output specifications. Both measures highlighted that UK
              credit unions have considerable scope for efficiency gains. It
              was mooted that the documented high levels of inefficiency may be
              indicative of the fact that credit unions, based on clearly
              defined and non-overlapping common bonds, are not in competition
              with each other for market share. Credit unions were also
              highlighted as suffering from a considerable degree of scale
              inefficiency with the majority of scale inefficient credit unions
              subject to decreasing returns to scale. The latter aspect
              highlights that the UK Government's goal of larger credit unions
              must be accompanied by greater regulatory freedom if inefficiency
              is to be avoided. One of the advantages of computing non-radial
              measures is that an insight into potential over- or
              under-expenditure on specific inputs can be obtained through a
              comparison of the non-radial measure of efficiency with the
              associated radial measure. Two interesting findings emerged, the
              first that UK credit unions over-spend on dividend payments and
              the second that they under-spend on labour costs.",
  journal  = "Journal of Banking \& Finance",
  volume   =  26,
  number   =  8,
  pages    = "1563--1591",
  month    =  aug,
  year     =  2002,
  keywords = "g21"
}

@ARTICLE{Ralston2001-ji,
  title    = "Can mergers ensure the survival of credit unions in the third
              millennium?",
  author   = "Ralston, Deborah and Wright, April and Garden, Kaylee",
  abstract = "The survival of small financial institutions in the third
              millennium depends on their competitiveness against large bank
              rivals. Accordingly, credit unions in Australia and the United
              States have attempted to increase efficiency through mergers. Our
              paper uses the data envelopment analysis methodology to evaluate
              the post-merger gains in technical and scale efficiency achieved
              by 31 Australian credit union mergers in 1993/1994 and 1994/1995,
              relative to non-merging credit unions. When compared with the
              only US study of credit union mergers [Journal of Banking \&
              Finance 23 (1999) 367--386], our findings suggests that mergers
              are not associated with improvements in efficiency superior to
              those achieved by internal growth.",
  journal  = "Journal of Banking \& Finance",
  volume   =  25,
  number   =  12,
  pages    = "2277--2304",
  month    =  dec,
  year     =  2001,
  keywords = "g210; g340"
}

@MISC{Worthington2001-ud,
  title     = "Efficiency in pre-merger and post-merger non-bank financial
               institutions",
  author    = "Worthington, Andrew C",
  abstract  = "A two-part process is employed to analyse the role of efficiency
               in merger and acquisition (M\&A) activity in Australian credit
               unions during the period 1993 to 1997. The measures of
               efficiency are derived using the nonparametric technique of data
               envelopment analysis. The first part uses panel data in the
               probit model to relate pure technical efficiency, along with
               other managerial, regulatory and financial factors, to the
               probability of merger activity, either as an acquiring or
               acquired entity. The results indicate that loan portfolio
               diversification, management ability, earnings and asset size are
               a significant influence on the probability of acquisition,
               though the primary determinant of being acquired is smaller
               asset size. The second part uses a tobit model adapted to a
               panel framework to analyse post-merger efficiency. Mergers
               appear to have improved both pure technical efficiency and scale
               efficiency in the credit union industry.",
  journal   = "Manage. Decis. Econ.",
  publisher = "John Wiley \& Sons",
  month     =  dec,
  year      =  2001,
  keywords  = "140302 Econometric and Statistical Methods; 150203 Financial
               Institutions (incl. Banking); Data envelopment analysis; credit
               unions; mergers; technical and scale efficiency"
}

@MISC{Worthington2000-zk,
  title     = "Cost efficiency in Australian non-bank financial institutions: A
               non-parametric approach",
  author    = "Worthington, Andrew C",
  abstract  = "A two-stage procedure is employed to evaluate non-bank financial
               institution cost efficiency. In the first stage, data
               envelopment analysis is used to calculate technical, allocative
               and cost efficiency indices using a sample of two hundred
               Australian credit unions. The results indicate that a typical
               credit union's costs in 1997 were thirty percent above what
               could be considered efficient on the basis of observed best
               practice. The major source of overall cost inefficiency would
               appear to be allocative inefficiency, rather than technical
               inefficiency. The second stage uses limited dependent variable
               regression techniques to relate credit union efficiency scores
               to financial statement information. The results indicate that
               commercial lending activities, expenditures on information
               technology and marketing and promotion, the proportion of
               non-interest income, and association membership are a
               significant influence on the level of cost efficiency. The
               results are found to be invariant to alternative model
               specifications where input prices are first assumed to be
               different for each credit union and then assumed to be identical
               across the sample.",
  journal   = "Accounting Finance",
  publisher = "Blackwell Publishing",
  month     =  mar,
  year      =  2000,
  keywords  = "140207 Financial Economics; 150203 Financial Institutions (incl.
               Banking); Credit unions; Data envelopment analysis; Technical;
               allocative and cost efficiency"
}

@MISC{Worthington1999-rh,
  title     = "Malmquist Indices of Productivity Change in Australian Financial
               Services",
  author    = "Worthington, Andrew C",
  abstract  = "In this study the nature and extent of efficiency and
               productivity growth in deposit-taking institutions is
               investigated using nonparametric frontier techniques. Employing
               Malmquist indices, productivity growth is decomposed into
               technical efficiency change and technological change for two
               hundred and sixty-nine Australian credit unions. The results
               indicate that most credit unions experienced technological
               progress after deregulation, and that any efficiency gain found
               was largely the result of improvements in technical efficiency
               rather than scale efficiency. That productivity growth which did
               occur due to an increase in efficiency over the period tended to
               be in credit unions with a small number of members and a large
               asset base, whilst technical progress was most pronounced in
               institutions with a relatively high proportion of residential
               and commercial loans.",
  journal   = "Journal of International Financial Markets, Institutions and
               Money",
  publisher = "Elsevier",
  month     =  aug,
  year      =  1999,
  keywords  = "140399 Econometrics not elsewhere classified; 150203 Financial
               Institutions (incl. Banking); Non; Technical and scale
               efficiency; bank financial institutions; nonparametric
               techniques"
}

@ARTICLE{Worthington1998-mz,
  title     = "The determinants of non-bank financial institution efficiency: a
               stochastic cost frontier approach",
  author    = "Worthington, Andrew C",
  abstract  = "A two-stage estimation procedure is employed to evaluate
               non-bank financial institution efficiency. In the first stage,
               maximum-likelihood estimates of an econometric cost function are
               obtained for a cross-section of 150 Australian credit unions.
               The results indicate that a typical credit union's costs in 1995
               were only some 7\% above what could be considered efficient. The
               second stage uses limited dependent variable regression
               techniques to relate credit union efficiency scores to
               structural and institutional considerations. The results
               indicate that non-core commercial activities are not a
               significant influence on the level of cost inefficiency,
               although asset size, capital adequacy regulation, and branch and
               agency networks are significant. A primary influence on credit
               union efficiency would appear to be the industrial or community
               associational bond under which they were created, and to a
               lesser extent the state-based regulatory framework.",
  journal   = "Applied Financial Economics",
  publisher = "Routledge",
  volume    =  8,
  number    =  3,
  pages     = "279--287",
  month     =  jun,
  year      =  1998
}

@ARTICLE{Drake96-uq,
  title    = "Productive and allocative inefficiencies in {U.K}. building.",
  author   = "Drake, Leighweyman-Jones",
  abstract = "Looks at the non-parametric and stochastic frontier techniques
              used by financial institutions in the United Kingdom (U.K.).
              Specification of costs, outputs and inputs for the U.K. building
              societies; Statistics derived from the use of non-parametric
              frontier methodology; Results of the stochastic cost frontier
              analysis.",
  journal  = "Manchester Sch. Econ. Soc. Stud.",
  volume   =  64,
  number   =  1,
  pages    = "22",
  year     =  96,
  keywords = "financial institutions; stochastic analysis"
}

@ARTICLE{Piesse1995-ja,
  title     = "The measurement of productive efficiency in {UK} building
               societies",
  author    = "Piesse, Jenifer and Townsend, Robert",
  abstract  = "This study applies data envelopment analysis (DEA) to 1992
               accounting data for 57 UK building societies. DEA allows
               productive efficiency to be measured without specifying a
               functional form and without imposing any behavioural
               assumptions, which is particularly useful in a sector
               distinguished by mutual ownership. The paper sets out a number
               of possible objectives for the organizations and measures their
               relative efficiency under each specific definition. Considerable
               variation results from the alternate objective functions tested.
               Decomposing total efficiency into scale and technical components
               suggests that there is no optimal size of operation for building
               societies, but rather that specialization allows a variety of
               activities and organizational structures to exist within the
               sector",
  journal   = "Applied Financial Economics",
  publisher = "Routledge",
  volume    =  5,
  number    =  6,
  pages     = "397--407",
  month     =  dec,
  year      =  1995
}

@ARTICLE{Garbaccio_undated-zb,
  title    = "A Comparison of Nonparametric Methods to Measure Efficiency in
              the Savings and Loan Industry",
  author   = "Garbaccio, Richard F Hermalin",
  abstract = "Using data on 1.360 savings and loan (SAL) institutions, we
              compare two nonparametric methods for measuring efficiency: data
              envelopment analysis (DEA) and algebraic methods based on Varian
              (1984). We show that both methods are vulnerable to measurement
              error, although both theoretically and empirically we find the
              Varian-style measures to be less vulnerable. Because we have data
              on the future insolvency of our S\&Ls. we can directly compare
              the two methods by seeing which does a better job of predicting
              insolvency (working under the hypothesis that efficiency and
              insolvency should be negatively correlated). We find that various
              measures perform very similarly, except for the technical
              efficiency measure in DEA. Importantly, this last measure
              frequently yields the implausible result that efficiency and
              insolvency are positively correlated, possibly because it does
              not account for factor prices. [ABSTRACT FROM AUTHOR]",
  journal  = "Journal of the American Real Estate \& Urban Economics
              Association",
  volume   =  22,
  number   =  1,
  pages    = "169--193",
  keywords = "banking industry; capital; mathematical models; saving \&
              investment; savings \& loan associations"
}

@ARTICLE{Eskelinen2013-gb,
  title    = "Intertemporal efficiency analysis of sales teams of a bank:
              Stochastic semi-nonparametric approach",
  author   = "Eskelinen, Juha and Kuosmanen, Timo",
  abstract = "The primary role of a bank branch is evolving from a service
              provider towards a sales channel. Previous branch-level studies
              of sales efficiency consider a static setting of a single time
              period, ignoring the stochastic nature of sales outcomes. In this
              paper, we examine efficiency and performance of sales teams in a
              bank branch network over time, taking into account the changing
              demand and operational conditions, as well as random
              disturbances. The intertemporal sales frontier is estimated from
              the panel of monthly data over the years 2007 -- 2010 using the
              stochastic semi-nonparametric envelopment of data (StoNED)
              method. The efficiency scores of sales teams and the trajectories
              of performance over time allow managers and the sales force to
              learn from past events and to develop the managerial and work
              practices across the network. While this study focuses on the
              case of a specific bank, some of the innovative features of our
              approach are applicable to sales efficiency assessment in other
              banks and financial institutions, as well as other network-based
              sales organizations.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  12,
  pages    = "5163--5175",
  month    =  dec,
  year     =  2013,
  keywords = "c14; d24; g21"
}

@ARTICLE{Thoraneenitiyan2009-qy,
  title    = "Measuring the impact of restructuring and country-specific
              factors on the efficiency of post-crisis East Asian banking
              systems: Integrating {DEA} with {SFA}",
  author   = "Thoraneenitiyan, Nakhun and Avkiran, Necmi K",
  abstract = "This paper investigates the relationship between post-crisis bank
              restructuring, country-specific conditions and bank efficiency in
              Asian countries from 1997 to 2001 using an approach that
              integrates data envelopment analysis and stochastic frontier
              analysis. We focus on restructuring measures related to bank
              ownership. The results indicate that although domestic mergers
              produce more efficient banks, overall, restructuring does not
              lead to more efficient banking systems. Banking system
              inefficiencies are mostly attributed to country-specific
              conditions, particularly, high interest rates, concentrated
              markets and economic development.",
  journal  = "Socioecon. Plann. Sci.",
  volume   =  43,
  number   =  4,
  pages    = "240--252",
  month    =  dec,
  year     =  2009,
  keywords = "g21; g28; g32"
}

@ARTICLE{Simar2007-at,
  title    = "Estimation and inference in two-stage, semi-parametric models of
              production processes",
  author   = "Simar, L{\'e}opold and Wilson, Paul W",
  journal  = "J. Econom.",
  volume   =  136,
  number   =  1,
  pages    = "31--64",
  month    =  jan,
  year     =  2007,
  keywords = "bootstrap; data envelopment analysis; dea; nonparametric;
              technical efficiency; two-stage"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{KoutsomanoliFilippaki2009-ob,
  title    = "Profit efficiency under a directional technology distance
              function approach",
  author   = "Koutsomanoli‚ÄêFilippaki, Anastasia and Margaritis, Dimitris and
              Staikouras, Christos",
  editor   = "Pasiouras, Fotios",
  abstract = "The directional technology distance function is introduced, given
              an interpretation as a min-max, and compared with other
              functional representations of the technology including the
              Shephard input and output distance functions and the McFadden
              gauge function. A dual correspondence is developed between the
              directional technology distance function and the profit function,
              and it is shown that all previous dual correspondences are
              special cases of this correspondence. We then show how Nerlovian
              (profit-based) efficiency measures can be computed using the
              directional technology distance function.",
  journal  = "Managerial Finance",
  volume   =  35,
  number   =  3,
  pages    = "276--296",
  month    =  feb,
  year     =  2009
}

@ARTICLE{Papke1996-cn,
  title   = "Econometric methods for fractional response variables with an
             application to 401 ( k ) plan participation rates",
  author  = "Papke, Leslie E and Wooldridge, Jeffrey M",
  journal = "J. Appl. Econometrics",
  volume  =  11,
  number  = "February",
  pages   = "619--632",
  year    =  1996
}

@BOOK{Pastor1997-bg,
  title  = "Efficiency of European banking systems: A correction by environment
            variables",
  author = "Pastor, J T and Pastor, J M and Lozano, Ana",
  year   =  1997
}

@ARTICLE{Kasman2006-fo,
  title     = "Cost and profit efficiencies in transition banking: the case of
               new {EU} members",
  author    = "Kasman, Adnan and Yildirim, Canan",
  abstract  = "This paper analyses cost and profit efficiencies in commercial
               banking in the eight Central and Eastern European countries that
               became new members to the European Union. Common stochastic cost
               and profit frontiers with country-specific variables are
               employed in order to take into account macro-economic and
               financial sector conditions that vary over time and across
               countries. The impact of foreign ownership on performance is
               also examined. The results indicate a wide range of cost and
               profit inefficiency scores across countries and across different
               size groups. All banking systems in the sample display
               significant levels of cost and profit inefficiency and there
               does not seem to be any continuous improvement in performance
               over time. There is also some evidence that foreign banks
               perform, on average, better than domestic banks.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  38,
  number    =  9,
  pages     = "1079--1090",
  month     =  may,
  year      =  2006
}

@ARTICLE{Europe2011-ht,
  title  = "Product Review of {BvD} Bankscope In-depth, independent review of
            the products, plus links to related resources",
  author = "Europe, Eastern and America, North and America, Central and East,
            Far",
  number = "October",
  year   =  2011
}

@ARTICLE{Chortareas2013-tr,
  title    = "Financial freedom and bank efficiency: Evidence from the European
              Union",
  author   = "Chortareas, Georgios E and Girardone, Claudia and Ventouri,
              Alexia",
  abstract = "This paper investigates the dynamics between the financial
              freedom counterparts of the economic freedom index drawn from the
              Heritage Foundation database and bank efficiency levels. We rely
              on a large sample of commercial banks operating in the 27
              European Union member states over the 2000s. After estimating
              bank-specific efficiency scores using Data Envelopment Analysis
              (DEA), we develop a truncated regression model combined with
              bootstrapped confidence intervals to test our main hypotheses.
              Results suggest that the higher the degree of an economy's
              financial freedom, the higher the benefits for banks in terms of
              cost advantages and overall efficiency. Our results also show
              that the effects of financial freedom on bank efficiency tend to
              be more pronounced in countries with freer political systems in
              which governments formulate and implement sound policies and
              higher quality governance.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  4,
  pages    = "1223--1231",
  month    =  apr,
  year     =  2013,
  keywords = "c1; g21; g28; p50"
}

@ARTICLE{Duygun-fethi2009-cx,
  title  = "Assessing Bank Performance with Operational Artificial Intelligence
            Techniques : A Survey University of Bath School of Management ,
            Working Paper Series",
  author = "Duygun-fethi, Meryem and Pasiouras, Fotios",
  year   =  2009
}

@ARTICLE{Barros2008-wl,
  title    = "Measuring the economic efficiency of airports: A {Simar--Wilson}
              methodology analysis",
  author   = "Barros, Carlos Pestana and Dieke, Peter U C",
  journal  = "Transp. Res. Part E: Logist. Trans. Rev.",
  volume   =  44,
  number   =  6,
  pages    = "1039--1051",
  month    =  nov,
  year     =  2008,
  keywords = "airports; bootstrap; data envelopment analysis; italy; truncated
              regression"
}

@ARTICLE{McDonald2009-bj,
  title    = "Using least squares and tobit in second stage {DEA} efficiency
              analyses",
  author   = "McDonald, John",
  abstract = "The paper examines second stage DEA efficiency analyses, within
              the context of a censoring data generating process (DGP) and a
              fractional data DGP, when efficiency scores are treated as
              descriptive measures of the relative performance of units in the
              sample. It is argued that the efficiency scores are not generated
              by a censoring process but are fractional data. Tobit estimation
              in this situation is inappropriate. In contrast, ordinary least
              squares is a consistent estimator, and, if White's [White, H.,
              1980. A heteroskedastic-consistent covariance matrix and a direct
              test for heteroskedasticity. Econometrica 48, 817--838]
              heteroskedastic-consistent standard errors are calculated, large
              sample tests can be performed which are robust to
              heteroskedasticity and the distribution of the disturbances. For
              a more refined analysis Papke and Wooldridge's [Papke, L.E.,
              Wooldridge, J.M., 1996. Econometric methods for fractional
              response variables with an application to 401(k) plan
              participation rates. Journal of Applied Econometrics 11 (6),
              619--632] method has some advantages, but is more complex and
              requires special programming.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  197,
  number   =  2,
  pages    = "792--798",
  month    =  sep,
  year     =  2009,
  keywords = "data envelopment analysis; fractional data; least squares;
              measuring efficiency; tobit"
}

@MISC{The_Mendeley_Support_Team2011-fi,
  title     = "Getting Started with Mendeley",
  author    = "{The Mendeley Support Team}",
  abstract  = "A quick introduction to Mendeley. Learn how Mendeley creates
               your personal digital library, how to organize and annotate
               documents, how to collaborate and share with colleagues, and how
               to generate citations and bibliographies.",
  journal   = "Mendeley Desktop",
  publisher = "Mendeley Ltd.",
  pages     = "1--16",
  year      =  2011,
  address   = "London",
  keywords  = "Mendeley; how-to; user manual"
}

@BOOK{Colin_Cameron2005-fg,
  title     = "Microeconometrics: Methods and Applications",
  author    = "Colin Cameron, A and Trivedi, Pravin K",
  abstract  = "This book provides the most comprehensive treatment to date of
               microeconometrics, the analysis of individual-level data on the
               economic behavior of individuals or firms using regression
               methods for cross section and panel data. The book is oriented
               to the practitioner. A basic understanding of the linear
               regression model with matrix algebra is assumed. The text can be
               used for a microeconometrics course, typically a second-year
               economics PhD course; for data-oriented applied
               microeconometrics field courses; and as a reference work for
               graduate students and applied researchers who wish to fill in
               gaps in their toolkit. Distinguishing features of the book
               include emphasis on nonlinear models and robust inference,
               simulation-based estimation, and problems of complex survey
               data. The book makes frequent use of numerical examples based on
               generated data to illustrate the key models and methods. More
               substantially, it systematically integrates into the text
               empirical illustrations based on seven large and exceptionally
               rich data sets.",
  publisher = "Cambridge University Press",
  volume    =  100,
  pages     = "1056",
  series    = "Cambridge Books",
  month     =  may,
  year      =  2005,
  language  = "en"
}

@BOOK{Greene2003-fb,
  title     = "Econometric Analysis",
  author    = "Greene, William H",
  editor    = "Education, Pearson",
  abstract  = "For a one-year graduate course in Econometrics. This text has
               two objectives. The first is to introduce students to applied
               econometrics, including basic techniques in regression analysis
               and some of the rich variety of models that are used when the
               linear model proves inadequate or inappropriate. The second is
               to present students with sufficient theoretical background that
               they will recognize new variants of the models learned about
               here as merely natural extensions that fit within a common body
               of principles. The Fifth Edition features a complete update of
               techniques and developments, a reorganization of material for
               improved presentation, and new material and applications.",
  publisher = "Prentice Hall",
  volume    =  97,
  pages     = "1026",
  year      =  2003,
  language  = "en"
}

@ARTICLE{Cutler2013-rp,
  title    = "But is it `Fair'? The {UK} Coalition Government, `Fairness' and
              the `Reform' of Public Sector Pensions",
  author   = "Cutler, Tony and Waine, Barbara",
  abstract = "The article analyses arguments for reform of public sector
              pension schemes by the UK Coalition government on the grounds
              that existing provision is `unfair'. Three dimensions of
              `fairness' are discussed. That between public and private sector
              provision; between the costs to public sector employees and other
              taxpayers; and between members of public sector schemes. The
              article argues that there are serious weaknesses in the Coalition
              position on each of these dimensions of `fairness'. It suggests
              that these weaknesses are rooted in the discussion of public
              sector pensions in isolation from the overall pattern of
              occupational pension provision in the UK and that a more
              satisfactory analysis requires reference to principles of
              distributive justice.",
  journal  = "Social Policy \& Administration",
  volume   =  47,
  number   =  3,
  pages    = "327--345",
  month    =  jun,
  year     =  2013,
  keywords = "Coalition; Distributive justice; Public pensions; Reform; UK"
}

@ARTICLE{Hanel2012-dl,
  title    = "The timing of retirement --- New evidence from Swiss female
              workers",
  author   = "Hanel, Barbara and Riphahn, Regina T",
  abstract = "We investigate the responsiveness of individual retirement
              decisions to changes in financial incentives. A reform increased
              women's normal retirement age (NRA) in two steps from age 62 to
              age 63 first and then to age 64. At the same time retirement at
              the previous NRA became possible at a benefit discount. Since the
              reform affected specific birth cohorts we can identify causal
              effects. We find strong and robust behavioral effects of changes
              in financial retirement incentives. A permanent reduction of
              retirement benefits by 3.4\% induces a decline in the
              age-specific annual retirement probability by over 50\%. The
              response to changes in financial retirement benefits varies with
              educational background: those with low education respond most
              strongly to an increase in the price of leisure.",
  journal  = "Labour Econ.",
  volume   =  19,
  number   =  5,
  pages    = "718--728",
  month    =  oct,
  year     =  2012,
  keywords = "H55; Incentives; J14; J26; Labor force exit; Natural experiment;
              Retirement insurance; Social security"
}

@BOOK{Jollans1997-yq,
  title     = "Pensions and the ageing population",
  author    = "Jollans, Alastair",
  abstract  = "The ageing population represents a major change for most of the
               Western world over the next fifty years or so. The change is not
               only demographic, but social and financial as well. We have the
               enormous advantage however that the change can be seen coming
               from a long way off, and will not burst on us suddenly and
               unexpectedly. There is therefore little excuse for not preparing
               for it. The problem is certainly well known, although it is
               arguably often not well understood. In Britain, at least as far
               as pensions are concerned, it is often seen as much more of a
               problem for our continental European neighbours than for us,
               both because our population is ageing less fast and because we
               have a pension system that is largely funded. There is also a
               perception that Britain has already taken action to deal with
               the problem. Measures such as indexation of the state pension to
               prices rather than wages, cutting back on SERPS, encouraging
               funded personal pensions, and raising the state retirement age
               for women, all seem to have helped to reduce the problem, or at
               least the apparent problem. This paper challenges some of these
               perceptions. It concentrates however on the implications of
               population ageing for pensions. It is not concerned, other than
               marginally, with other possible effects, for instance on the
               cost of healthcare, although these too could potentially be very
               significant.",
  publisher = "Faculty of Actuaries",
  year      =  1997
}

@ARTICLE{Novy-Marx2011-sv,
  title     = "Public Pension Promises: How Big Are They and What Are They
               Worth?",
  author    = "Novy-Marx, Robert and Rauh, Joshua",
  abstract  = "We calculate the present value of state employee pension
               liabilities using discount rates that reflect the risk of the
               payments from a taxpayer perspective. If benefits have the same
               default and recovery characteristics as state general obligation
               debt, the national total of promised liabilities based on
               current salary and service is $3.20 trillion. If pensions have
               higher priority than state debt, the value of liabilities is
               much larger. Using zero-coupon Treasury yields, which are
               default-free but contain other priced risks, promised
               liabilities are $4.43 trillion. Liabilities are even larger
               under broader concepts that account for salary growth and future
               service.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  66,
  number    =  4,
  pages     = "1211--1249",
  month     =  aug,
  year      =  2011
}

@ARTICLE{Hanel2010-km,
  title    = "Financial incentives to postpone retirement and further effects
              on employment --- Evidence from a natural experiment",
  author   = "Hanel, Barbara",
  abstract = "This paper examines the effect of the introduction of permanent
              benefit reductions for early retirees (i) on the duration until
              benefit claiming and (ii) on the duration until exit from gainful
              employment. I estimate discrete time duration models using
              different error term specifications. Administrative data
              containing the full earnings history of the individuals are used.
              Since the reform implementing the benefit reductions was a
              natural experiment, under some assumptions a causal effect can be
              identified. The permanent reduction of retirement benefit amounts
              causes a postponement of claiming benefits by about 14months and
              a delay of employment exit by about 10months on average.",
  journal  = "Labour Econ.",
  volume   =  17,
  number   =  3,
  pages    = "474--486",
  month    =  jun,
  year     =  2010,
  keywords = "Labor force participation; Natural experiment; Retirement
              insurance"
}

@ARTICLE{Bongaarts2004-kq,
  title     = "Population Aging and the Rising Cost of Public Pensions",
  author    = "Bongaarts, John",
  abstract  = "Rapid population aging is raising concerns about the
               sustainability of public pension systems in high-income
               countries. The first part of this study identifies the four
               factors that determine trends in public pension expenditures:
               population aging, pension benefit levels, the mean age at
               retirement, and the labor force participation rate. The second
               part presents projections to 2050 of the impact of demographic
               trends on public pension expenditures in the absence of changes
               in pension benefits, labor force participation, and age at
               retirement. These projections demonstrate that current trends
               are unsustainable, because without reforms population aging will
               produce an unprecedented and harmful accumulation of public
               debt. A number of projection variants assess the potential
               impact of policy options aimed at improving the sustainability
               of public pension systems. Although the conventional responses
               are considered, particular attention is given to the demographic
               options of encouraging higher fertility and permitting more
               immigration. This analysis is illustrated with data from the
               seven largest OECD countries.",
  journal   = "Popul. Dev. Rev.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  30,
  number    =  1,
  pages     = "1--23",
  month     =  mar,
  year      =  2004,
  keywords  = "aging; demographic changes; demography; pension system; public
               pensions; social security"
}

@MISC{Jefferson2009-mn,
  title    = "Women and Retirement Pensions: A Research Review",
  author   = "Jefferson, Therese",
  abstract = "The links between womens caring work and access to economic
              resources are particularly critical in the context of widespread
              public policy debates about retirement and pensions, many of
              which neglect care as a key issue for analysis. However, among
              feminist economists it is widely recognized that womens patterns
              of care provision have adverse implications for their access to
              economic resources in later life. The feminist economics
              literature examines many of the interactions between womens
              caring roles and their access to resources, particularly womens
              capacity to access economic resources through publicly mandated
              or regulated pension schemes. This article reviews research that
              places womens patterns of work and care at the center of analyses
              of retirement pension policy in an effort to provide a summary of
              research on gender and pensions policy and to contrast the extent
              to which differing institutional and policy frameworks
              accommodate womens caring roles.",
  journal  = "Fem. Econ.",
  volume   =  15,
  pages    = "115--145",
  year     =  2009
}

@ARTICLE{Tausch2013-ni,
  title     = "Preferences for redistribution and pensions. What can we learn
               from experiments?",
  author    = "Tausch, Franziska and Potters, Jan and Riedl, Arno",
  journal   = "J. Pension Econ. Financ.",
  publisher = "Cambridge University Press",
  volume    =  12,
  number    =  03,
  pages     = "298--325",
  month     =  feb,
  year      =  2013,
  keywords  = "Experimental economics; pension systems; redistribution
               preferences; solidarity"
}

@ARTICLE{Schoeplein1970-pl,
  title     = "{THE} {EFFECT} {OF} {PENSION} {PLANS} {ON} {OTHER} {RETIREMENT}
               {SAVING}",
  author    = "Schoeplein, Robert N",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  25,
  number    =  3,
  pages     = "633--637",
  month     =  jun,
  year      =  1970
}

@ARTICLE{Danzer2012-tl,
  title    = "Total Reward and pensions in the {UK} in the public and private
              sectors",
  author   = "Danzer, Alexander M and Dolton, Peter J",
  abstract = "Recent controversy has surrounded the relative value of public
              and private sector remuneration. We propose a comprehensive
              measure of Total Reward (TR) which includes not just pay, but
              pensions and other 'benefits in kind', evaluate it as the present
              value of the sum of all these payments over the lifetime and
              compare it for the highly educated in the UK public and private
              sectors. Our results suggest that TR is broadly equalised over
              the lifecycle for highly educated men while highly educated women
              have a clear TR advantage in the public sector by the end of
              their career. We suggest that the current controversy over
              public-private sector pension differentials and the perennial
              issues of public/private sector pay gaps requires a lifetime
              perspective and that the concept of TR is appropriate.
              \copyright{} 2012 Elsevier B.V.",
  journal  = "Labour Econ.",
  volume   =  19,
  number   =  4,
  pages    = "584--594",
  month    =  aug,
  year     =  2012,
  keywords = "Benefits in kind; Lifetime earnings; Pension; Public sector;
              Sector switching; Total reward"
}

@BOOK{Nalebuff1985-tm,
  title    = "Pensions and the Retirement Decision",
  author   = "Nalebuff, Barry and Zeckhauser, Richard J",
  abstract = "Pensions influence retirement decisions. The analysis provides a
              framework for assessing the phenomenon. The qualitative features
              of most defined benefit pension plans in the United States, as
              the first section demonstrates, can be used to induce optimal
              retirement choices. Pensions are viewed as a form of forced
              savings; their purposeis to enable the worker to ``commit
              himself'' by making it in his own self-interest to retire at an
              appropriate age. The remaining sections examine the use of
              pensions in populations that are heterogeneous with respect to
              such features as disutility of work or expected lifespan.Given
              heterogeneity, a major policy concern is whether pensions are
              actuarially fair to different groups, retirement cohorts,etc. It
              is proven that optimal pension plans cannot be actuarially more
              than fair, in the sense that someone who retires later must
              impose a smaller cost on the pension pool than he would were he
              to retire earlier. However, there are differences in life
              expectancy among cohorts defined by retirement age: late retirees
              generallyl ive longer. Late retirees may thus impose a greater
              expected cost on the pension fund under an optimal plan;
              interestingly, they do impose a higher cost than those retiring
              earlier under most common pension funds.In a first-best world, a
              separate pension plan would be designed for each group of
              workers. But, government-mandated retirement programs and
              legislation regulating private pensions require common treatment
              of different workers. Such homogenization is shown to work to the
              possible detriment of workers as a whole. Pensions are a
              workhorse compensation mechanism. They provide an additional
              instrument beyond wages for attracting, motivating, sorting, and
              retaining workers, while facilitating appropriate retirement
              decisions.",
  volume   = "I",
  pages    = "283--316",
  year     =  1985
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Black2012-iy,
  title    = "Kilts, governance, and information technology: change is coming
              to {SOGC}",
  author   = "Black, Douglas",
  abstract = "This article aims to offer an ex ante evaluation of the impact of
              a parametric reform of the Spanish pension system that would
              involve increasing the reference period used to calculate
              benefits, an approach proposed many times by various actors in
              the socio‚Äêeconomic field. Such gradual change may be categorized
              as a non‚Äêstructural reform of the pension system. This contrasts
              with reforms of a structural nature that have been very popular
              in Latin America and elsewhere, involving the creation of defined
              contribution individual account schemes. As regards the
              parametric reform proposed in this article, the main findings
              indicate that it would have a small but negative impact on
              pension income for pensioners and would reduce income
              distribution.",
  journal  = "J. Obstet. Gynaecol. Can.",
  volume   =  34,
  number   =  9,
  pages    = "803--806",
  month    =  sep,
  year     =  2012,
  keywords = "Pension scheme; Social security reform; Social solidarity; Spain",
  language = "en"
}

@ARTICLE{Heijdra2009-ho,
  title    = "Retirement, pensions, and ageing",
  author   = "Heijdra, Ben J and Romp, Ward E",
  abstract = "We study the effects of demographic shocks and changes in the
              pension system on the macroeconomic performance of an advanced
              small open economy facing a given world interest rate. We
              construct an overlapping-generations model which includes a
              realistic description of the mortality process. Individual agents
              choose their optimal retirement age, taking into account the
              time- and age profiles of wages, taxes, and the public pension
              system. The early retirement provision in most pension systems
              acts as a trap, inducing most workers to retire well before the
              normal retirement age. Simulations show that pension reform must
              be drastic for it to have any effects on the retirement behaviour
              of workers. ?? 2008 Elsevier B.V. All rights reserved.",
  journal  = "J. Public Econ.",
  volume   =  93,
  number   = "3-4",
  pages    = "586--604",
  month    =  apr,
  year     =  2009,
  keywords = "Ageing; Demography; Gompertz-Makeham Law of mortality;
              Overlapping generations; Pensions; Retirement; Small open economy"
}

@BOOK{Bogeoft2011-wh,
  title     = "Benchmarking with {DEA}, {SFA}, and {R}",
  author    = "Bogeoft, P and Otto, L",
  publisher = "Springer",
  year      =  2011,
  address   = "New York",
  keywords  = "Benchmarking with DEA; Econometrics; Operation Research /
               Decision Theory; SFA; and R"
}

@ARTICLE{De_Haas2006-ro,
  title    = "Foreign banks and credit stability in Central and Eastern Europe.
              A panel data analysis",
  author   = "de Haas, Ralph and van Lelyveld, Iman",
  abstract = "We examine whether foreign and domestic banks in Central and
              Eastern Europe react differently to business cycles and banking
              crises. Our panel dataset comprises data of more than 250 banks
              for the period 1993--2000, with information on bank ownership and
              mode of entry. During crisis periods domestic banks contracted
              their credit base, whereas greenfield foreign banks did not.
              Also, home country conditions matter for foreign bank growth, as
              there is a significant negative relationship between home country
              economic growth and host country credit by greenfields. Finally,
              greenfield foreign banks' credit growth is influenced by the
              health of the parent bank.",
  journal  = "Journal of Banking \& Finance",
  volume   =  30,
  number   =  7,
  pages    = "1927--1952",
  month    =  jul,
  year     =  2006,
  keywords = "Foreign banks; Transition economies; Credit growth; Financial
              stability"
}

@ARTICLE{Badrinath2002-gc,
  title     = "Momentum Trading by Institutions",
  author    = "Badrinath, S G and Wahal, Sunil",
  abstract  = "We document the equity trading practices of approximately 1,200
               institutions from the third quarter of 1987 through the third
               quarter of 1995. We decompose trading by institutions into the
               initiation of new positions (entry), the termination of previous
               positions (exit), and adjustments to ongoing holdings.
               Institutions act as momentum traders when they enter stocks but
               as contrarian traders when they exit or make adjustments to
               ongoing holdings. We find significant differences in trading
               practices among different types of institutions.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing, Inc.",
  volume    =  57,
  number    =  6,
  pages     = "2449--2478",
  month     =  dec,
  year      =  2002
}

@ARTICLE{Serban2010-md,
  title     = "Combining mean reversion and momentum trading strategies in
               foreign exchange markets",
  author    = "Serban, Alina F",
  abstract  = "The literature on equity markets documents the existence of mean
               reversion and momentum phenomena. Researchers in foreign
               exchange markets find that foreign exchange rates also display
               behaviors akin to momentum and mean reversion. This paper
               implements a trading strategy combining mean reversion and
               momentum in foreign exchange markets. The strategy was
               originally designed for equity markets, but it also generates
               abnormal returns when applied to uncovered interest parity
               deviations for five countries. I find that the pattern for the
               positions thus created in the foreign exchange markets is
               qualitatively similar to that found in the equity markets.
               Quantitatively, this strategy performs better in foreign
               exchange markets than in equity markets. Also, it outperforms
               traditional foreign exchange trading strategies, such as carry
               trades and moving average rules.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  34,
  number    =  11,
  pages     = "2720--2727",
  month     =  nov,
  year      =  2010,
  keywords  = "Uncovered interest parity; Mean reversion; Momentum; Foreign
               exchange; Trading strategies"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{AgyeiAmpomah2007-nh,
  title     = "The {Post‚ÄêCost} Profitability of Momentum Trading Strategies:
               Further Evidence from the {UK}",
  author    = "Agyei‚ÄêAmpomah, S",
  abstract  = "Abstract This paper examines the post-cost profitability of
               momentum trading strategies in the UK over the period 1988--2003
               and provides direct evidence on stock concentration, turnover
               and trading cost associated with the strategy. We find that
               after factoring out ...",
  journal   = "European Financial Management",
  publisher = "Wiley Online Library",
  year      =  2007
}

@ARTICLE{Johnson2002-oa,
  title     = "Rational Momentum Effects",
  author    = "Johnson, Timothy C",
  abstract  = "Momentum effects in stock returns need not imply investor
               irrationality, heterogeneous information, or market frictions. A
               simple, single-firm model with a standard pricing kernel can
               produce such effects when expected dividend growth rates vary
               over time. An enhanced model, under which persistent growth rate
               shocks occur episodically, can match many of the features
               documented by the empirical research. The same basic mechanism
               could potentially account for underreaction anomalies in
               general.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishers, Inc.",
  volume    =  57,
  number    =  2,
  pages     = "585--608",
  month     =  apr,
  year      =  2002
}

@ARTICLE{Chan2007-cd,
  title     = "Valuation of global {IPOs}: a stochastic frontier approach",
  author    = "Chan, Yue-Cheong and Wu, Congsheng and Kwok, Chuck C Y",
  abstract  = "This paper studies the impact of global offerings on US IPO
               firms' offer price using the stochastic frontier approach. We
               find that the offer price valuation efficiency for global IPOs
               exceeds that of IPOs with purely domestic offers by 3.1\%. In
               particular, the global offering approach is most appropriate to
               those IPO firms, which offer larger proportion of new shares to
               international investors, underwritten by less prestigious
               investment banks and with larger firm-specific return variance.
               Our findings are consistent with the demand inelasticity,
               certification effect and investor recognition arguments that
               account for the benefits of global offering.",
  journal   = "Rev Quant Finan Acc",
  publisher = "Springer US",
  volume    =  29,
  number    =  3,
  pages     = "267--284",
  month     =  oct,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Peng2007-rs,
  title     = "{IPO} underpricing and flotation methods in Taiwan -- a
               stochastic frontier approach",
  author    = "Peng, Yahui and Wang, Kehluh",
  abstract  = "Adopting stochastic frontier analysis, this article studies the
               pricing model and underpricing phenomenon of the initial public
               offerings (IPOs) in Taiwan and further elucidates the potential
               impact of offering mechanisms on underpricing. The sampling
               period is from 1996 to 2003, in which 647 IPOs are selected.
               Empirical results suggest that issuing firms with greater
               earning potentials, less risk or less asymmetric information
               have lower underpricing. Furthermore, the variables included to
               explain underpricing are mostly significant, especially the
               proxy variable for flotation method. Observed mean IPO
               underpricing is 20.59\% in the sample period, compared to
               17.12\% for the subgroup using the auction method. This
               statistically significant difference implies that the
               introduction of the auction method can help reduce IPO
               underpricing.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  39,
  number    =  21,
  pages     = "2785--2796",
  month     =  dec,
  year      =  2007
}

@ARTICLE{Zakaria2014-fh,
  title     = "A {NEW} {FRAMEWORK} {OF} {BANKING} {RISK} {MANAGEMENT}
               {EFFICIENCY} {ANALYSIS}: {AN} {INTEGRATED} {APPROACH} {BASED}
               {ON} {HEDGE} {ACCOUNTING} {AND} {DATA} {ENVELOPMENT} {ANALYSIS}
               ({DEA})",
  author    = "Zakaria, Shahsuzan and Islam, Sardar M N",
  abstract  = "... However, this author admits that dollar offsets are the
               simplest approach in testing for hedge ... when a longer
               cumulative period is recommended for analysis of the ratio for
               stability. ... and Grant (2003) believe that measuring risk
               management using the dollar - offset method is ...",
  journal   = "Recent Developments in Data Envelopment Analysis and its
               Applications",
  publisher = "researchgate.net",
  pages     = "21",
  year      =  2014
}

@ARTICLE{Nieboer2015-ef,
  title    = "Group member characteristics and risk taking by consensus",
  author   = "Nieboer, Jeroen",
  abstract = "I investigate the effect of group members' individual
              characteristics on risk taking by groups in an investment
              experiment. I find that gender is the only of the characteristics
              that significantly affects risk taking, both for individual
              investments and group investment decisions by consensus. In
              individual decisions, women are more risk averse than men. In
              groups, risk aversion is increasing in the number of female group
              members. I make out-of-sample predictions of group decisions for
              different gender compositions based on the sample of individual
              preferences using simulation of various `social decision
              schemes'. Generally, none of the schemes predicts group decisions
              well. These results pose new challenges for theories of
              preference aggregation in groups and have practical implications
              for organizations that rely on teams to make decisions under
              risk.",
  journal  = "Journal of Behavioral and Experimental Economics",
  volume   =  57,
  number   =  0,
  pages    = "81--88",
  month    =  aug,
  year     =  2015,
  keywords = "Experiments; Choice under risk; Groups; Teams; Consensus"
}

@ARTICLE{OHara2015-rr,
  title    = "High frequency market microstructure",
  author   = "O'Hara, Maureen",
  abstract = "Markets are different now, transformed by technology and high
              frequency trading. In this paper, I investigate the implications
              of these changes for high frequency market microstructure (HFT).
              I describe the new high frequency world, with a particular focus
              on how HFT affects the strategies of traders and markets. I
              discuss some of the gaps that arise when thinking about
              microstructure research issues in the high frequency world. I
              suggest that, like everything else in the markets, research must
              also change to reflect the new realities of the high frequency
              world. I propose some topics for this new research agenda in high
              frequency market microstructure.",
  journal  = "J. financ. econ.",
  volume   =  116,
  number   =  2,
  pages    = "257--270",
  month    =  may,
  year     =  2015,
  keywords = "High frequency trading; Market microstructure; Algorithmic
              trading"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Fama2015-op,
  title    = "A five-factor asset pricing model",
  author   = "Fama, Eugene F and French, Kenneth R",
  abstract = "A five-factor model directed at capturing the size, value,
              profitability, and investment patterns in average stock returns
              performs better than the three-factor model of Fama and French
              (FF, 1993). The five-factor model◊≥s main problem is its failure
              to capture the low average returns on small stocks whose returns
              behave like those of firms that invest a lot despite low
              profitability. The model◊≥s performance is not sensitive to the
              way its factors are defined. With the addition of profitability
              and investment factors, the value factor of the FF three-factor
              model becomes redundant for describing average returns in the
              sample we examine.",
  journal  = "J. financ. econ.",
  volume   =  116,
  number   =  1,
  pages    = "1--22",
  month    =  apr,
  year     =  2015,
  keywords = "Asset pricing model; Factor model; Dividend discount model;
              Profitability; Investment"
}

@ARTICLE{Berk2015-qr,
  title    = "Measuring skill in the mutual fund industry",
  author   = "Berk, Jonathan B and van Binsbergen, Jules H",
  abstract = "Using the value that a mutual fund extracts from capital markets
              as the measure of skill, we find that the average mutual fund has
              used this skill to generate about \$3.2 million per year. Large
              cross-sectional differences in skill persist for as long as ten
              years. Investors recognize this skill and reward it by investing
              more capital with better funds. Better funds earn higher
              aggregate fees, and a strong positive correlation exists between
              current compensation and future performance. The cross-sectional
              distribution of managerial skill is predominantly reflected in
              the cross-sectional distribution of fund size, not gross alpha.",
  journal  = "J. financ. econ.",
  volume   =  118,
  number   =  1,
  pages    = "1--20",
  month    =  oct,
  year     =  2015,
  keywords = "Keywords; Mutual funds; Managerial skill; Alpha"
}

@ARTICLE{noauthor_undated-pj,
  title = "{Essays\_on\_Diffusion\_and\_Categories.pdf}"
}

@TECHREPORT{Basel_Committee_on_Banking_Supervision2011-wh,
  title       = "Consultative Document on Core Principles of Effective Banking
                 Supervision",
  author      = "{Basel Committee on Banking Supervision}",
  institution = "Bank for International Settlements",
  year        =  2011
}

@ARTICLE{Nguyen_undated-km,
  title  = "Review of e fficiency measurement methodologies to inform hospital
            resource allocation decisions in {NSW}: a rapid review",
  author = "Nguyen, Chris O'donnell Kim"
}

@BOOK{Zhu2007-wm,
  title     = "Modeling data irregularities and structural complexities in data
               envelopment analysis",
  author    = "Zhu, Joe and Cook, Wade D",
  publisher = "Springer Science \& Business Media",
  year      =  2007,
  keywords  = "Econometrics; Modeling Data Irregularities and Structural
               Comple; Operation Research / Decision Theory"
}

@ARTICLE{Bernanke1989-jd,
  title     = "Agency Costs, Net Worth, and Business Fluctuations",
  author    = "Bernanke, Ben and Gertler, Mark",
  abstract  = "This paper develops a simple neoclassical model of the business
               cycle in which the condition of borrowers' balance sheets is a
               source of output dynamics. The mechanism is that higher borrower
               net worth reduces the agency costs of financing real capital
               investments. Business upturns improve net worth, lower agency
               costs, and increase investment, which amplifies the upturn; vice
               versa, for downturns. Shocks that affect net worth (as in a
               debt-deflation) can initiate fluctuations.",
  journal   = "Am. Econ. Rev.",
  publisher = "American Economic Association",
  volume    =  79,
  number    =  1,
  pages     = "14--31",
  year      =  1989
}

@ARTICLE{Binswanger1974-kp,
  title     = "A Cost Function Approach to the Measurement of Elasticities of
               Factor Demand and Elasticities of Substitution",
  author    = "Binswanger, Hans P",
  journal   = "Am. J. Agric. Econ.",
  publisher = "[Agricultural \& Applied Economics Association, Oxford
               University Press]",
  volume    =  56,
  number    =  2,
  pages     = "377--386",
  year      =  1974
}

@ARTICLE{Caves1980-yc,
  title     = "Flexible Cost Functions for Multiproduct Firms",
  author    = "Caves, Douglas W and Christensen, Laurits R and Tretheway,
               Michael W",
  journal   = "Rev. Econ. Stat.",
  publisher = "The MIT Press",
  volume    =  62,
  number    =  3,
  pages     = "477--481",
  year      =  1980
}

@ARTICLE{Kuosmanen2001-ak,
  title    = "Measuring economic efficiency with incomplete price information:
              With an application to European commercial banks",
  author   = "Kuosmanen, Timo and Post, Thierry",
  abstract = "Measuring economic efficiency requires complete price
              information, while resorting to technical efficiency exclusively
              does not allow one to utilise any price information. In most
              studies, at least some information on the prices is available
              from theory or practical knowledge of the industry under
              evaluation. In this paper we extend the theory of efficiency
              measurement to accommodate incomplete price information by
              deriving upper and lower bounds for Farrell's overall economic
              efficiency. The bounds typically give a better approximation for
              economic efficiency than technical efficiency measures that use
              no price data whatsoever. From an operational point of view, we
              derive new data envelopment analysis (DEA) models for computing
              these bounds using standard linear programming. The practical
              application of these estimators is illustrated with an empirical
              application to large European Union commercial banks.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  134,
  number   =  1,
  pages    = "43--58",
  month    =  oct,
  year     =  2001,
  keywords = "Economic efficiency measurement; Imperfect price information;
              Data envelopment analysis (DEA); Weight-restricted models; Free
              disposable hull (FDH)"
}

@ARTICLE{Barr2006-oi,
  title     = "The Economics of Pensions",
  author    = "Barr, Nicholas and Diamond, Peter",
  abstract  = "This paper sets out the economic analytics of pensions. After
               introductory discussion, successive sections consider the
               effects of different pension arrangements on labour markets, on
               national savings and growth, and on the distribution of burdens
               and benefits. These areas are controversial and politically
               highly salient. While we are open about expressing our own
               views, the main purpose of the paper is to set out the
               analytical process by which we reach them, to enable readers to
               form their own conclusions.",
  journal   = "Oxf Rev Econ Policy",
  publisher = "Oxford University Press",
  volume    =  22,
  number    =  1,
  pages     = "15--39",
  month     =  mar,
  year      =  2006
}

@ARTICLE{Do2010-ra,
  title     = "Does Simple Pairs Trading Still Work?",
  author    = "Do, Binh and Faff, Robert",
  abstract  = "Despite confirming the continuing downward trend in
               profitability of pairs trading, this study found that the
               strategy performs strongly during periods of prolonged
               turbulence, including the recent global financial crisis.
               Moreover, alternative algorithms combined with other measures
               enhance trading profits considerably, by 22 bps a month for bank
               stocks.",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  66,
  number    =  4,
  pages     = "83--95",
  month     =  jun,
  year      =  2010,
  keywords  = "Accounting"
}

@ARTICLE{Wu2001-oj,
  title     = "The Determinants of Asymmetric Volatility",
  author    = "Wu, Guojun",
  abstract  = "Volatility in equity markets is asymmetric: contemporaneous
               return and conditional return volatility are negatively
               correlated. In this article I develop an asymmetric volatility
               model where dividend growth and dividend volatility are the two
               state variables of the economy. The model allows both the
               leverage effect and the volatility feedback effect, the two
               popular explanations of asymmetry. The model is estimated by the
               simulated method of moments. I find that both the leverage
               effect and volatility feedback are important determinants of
               asymmetric volatility, and volatility feedback is significant
               both statistically and economically.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  14,
  number    =  3,
  pages     = "837--859",
  month     =  jul,
  year      =  2001
}

@ARTICLE{Glosten1993-fs,
  title     = "On the Relation between the Expected Value and the Volatility of
               the Nominal Excess Return on Stocks",
  author    = "Glosten, Lawrence R and Jagannathan, Ravi and Runkle, David E",
  abstract  = "We find support for a negative relation between conditional
               expected monthly return and conditional variance of monthly
               return, using a GARCH-M model modified by allowing (1) seasonal
               patterns in volatility, (2) positive and negative innovations to
               returns having different impacts on conditional volatility, and
               (3) nominal interest rates to predict conditional variance.
               Using the modified GARCH-M model, we also show that monthly
               conditional volatility may not be as persistent as was thought.
               Positive unanticipated returns appear to result in a downward
               revision of the conditional volatility whereas negative
               unanticipated returns result in an upward revision of
               conditional volatility.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  48,
  number    =  5,
  pages     = "1779--1801",
  month     =  dec,
  year      =  1993
}

@UNPUBLISHED{Demirguc-Kunt2003-ph,
  title       = "Regulations, Market Structure, Institutions, and the Cost of
                 Financial Intermediation",
  author      = "Demirguc-Kunt, Asli and Laeven, Luc and Levine, Ross",
  abstract    = "This paper examines the impact of bank regulations, market
                 structure, and national institutions on bank net interest
                 margins and overhead costs using data on over 1,400 banks
                 across 72 countries while controlling for bank-specific
                 characteristics. The data indicate that tighter regulations on
                 bank entry and bank activities boost the cost of financial
                 intermediation. Inflation also exerts a robust, positive
                 impact on bank margins and overhead costs. While concentration
                 is positively associated with net interest margins, this
                 relationship breaks down when controlling for regulatory
                 impediments to competition and inflation. Furthermore, bank
                 regulations become insignificant when controlling for national
                 indicators of economic freedom or property rights protection,
                 while these institutional indicators robustly explain
                 cross-bank net interest margins and overhead expenditures.
                 Thus, bank regulations cannot be viewed in isolation; they
                 reflect broad, national approaches to private property and
                 competition.",
  journal     = "J. Money Credit Bank.",
  number      =  9890,
  pages       = "593--622",
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  aug,
  year        =  2003
}

@ARTICLE{George_Assaf2011-ju,
  title    = "Technical efficiency in Saudi banks",
  author   = "George Assaf, A and Barros, Carlos P and Matousek, Roman",
  abstract = "This study analyses the technical efficiency of Saudi banks using
              a two-stage DEA-data envelopment analysis approach. In the first
              stage, we use a bootstrapped DEA--VRS model to identify the
              efficiency scores, and in the second stage, we use a bootstrapped
              truncated regression model to identify the covariates that
              explain technical efficiency. Policy implications are derived.",
  journal  = "Expert Syst. Appl.",
  volume   =  38,
  number   =  5,
  pages    = "5781--5786",
  month    =  may,
  year     =  2011,
  keywords = "DEA-data envelopment analysis; Double bootstrap; Saudi banks;
              Truncated regression"
}

@ARTICLE{Bahari2014-sq,
  title     = "Influential {DMUs} and outlier detection in data envelopment
               analysis with an application to health care",
  author    = "Bahari, Ali Reza and Emrouznejad, Ali",
  abstract  = "This paper explains some drawbacks on previous approaches for
               detecting influential observations in deterministic
               nonparametric data envelopment analysis models as developed by
               Yang et al. (Annals of Operations Research 173:89--103, 2010).
               For example efficiency scores and relative entropies obtained in
               this model are unimportant to outlier detection and the
               empirical distribution of all estimated relative entropies is
               not a Monte-Carlo approximation. In this paper we developed a
               new method to detect whether a specific DMU is truly influential
               and a statistical test has been applied to determine the
               significance level. An application for measuring efficiency of
               hospitals is used to show the superiority of this method that
               leads to significant advancements in outlier detection.",
  journal   = "Ann. Oper. Res.",
  publisher = "Springer US",
  volume    =  223,
  number    =  1,
  pages     = "95--108",
  month     =  dec,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Curi2013-vc,
  title     = "Is foreign-bank efficiency in financial centers driven by home
               or host country characteristics?",
  author    = "Curi, Claudia and Guarda, Paolo and Lozano-Vivas, Ana and
               Zelenyuk, Valentin",
  abstract  = "This paper investigates whether home or host country factors can
               explain differences in technical efficiency among foreign banks
               operating in the Luxembourg financial center. We first address
               heterogeneity across banks by using the group-wise bootstrap to
               compare DEA measures of bank efficiency between branches and
               subsidiaries, focused and diversified banks, and euro area and
               non-euro area banks. We then control for these factors in a
               second-stage regression indentifying the impact of
               country-specific regulatory and macroeconomic variables on
               individual bank efficiency scores. Our regulatory indicators
               capture the strictness of capital requirements, private
               monitoring, official disciplinary power and restrictions on bank
               activities. Our macroeconomic indicators capture GDP per capita
               in the home country and its position in the business cycle. Our
               results carry policy implications for bank regulators in both
               home and host countries and provide insight into banks' choice
               between establishing a branch or a subsidiary to develop
               cross-border activities through international financial centers.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  40,
  number    =  3,
  pages     = "367--385",
  month     =  dec,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Le2017-iv,
  title     = "Testing for differences in technical efficiency among groups
               within an industry",
  author    = "Le, Phuong Thanh and Harvie, Charles and Arjomandi, Amir",
  abstract  = "ABSTRACTThis study generalizes the test performed by Simar and
               Zelenyuk (2007) to examine differences in the technical
               efficiency among groups within an industry (where ). For this
               purpose, the groups are divided into pairs and each group is
               compared with all other groups. The groups can then be
               classified into three cohorts: those performing better, equally
               and worse, relative to the benchmark group. For illustration
               purposes, annual data for Vietnamese banks covering the period
               2005?2012 is used.",
  journal   = "Appl. Econ. Lett.",
  publisher = "Routledge",
  volume    =  24,
  number    =  3,
  pages     = "159--162",
  month     =  feb,
  year      =  2017
}

@MISC{Loughran2005-ld,
  title    = "Liquidity: Urban versus rural firms",
  author   = "Loughran, Tim and Schultz, Paul",
  abstract = "Our paper examines the impact of geographic location on liquidity
              for U.S. rural- and urban-based companies. Even after adjusting
              for size and other factors, rural firms trade much less, are
              covered by fewer analysts, and are owned by fewer institutions
              than urban firms. Trading costs are higher for rural Nasdaq
              firms, and volume that can be attributed to marketwide factors is
              lower for rural stocks. The findings add to our understanding of
              the way that access to information and familiarity affect
              liquidity",
  journal  = "J. financ. econ.",
  volume   =  78,
  pages    = "341--374",
  year     =  2005
}

@TECHREPORT{Basel_Committee_on_Banking_Supervision2011-zt,
  title       = "Core Principles for Effective Banking Supervision",
  author      = "{Basel Committee on Banking Supervision}",
  institution = "Bank of International Settlements",
  year        =  2011
}

@MISC{noauthor_undated-fo,
  title = "oxfordhb-9780199559084-e-2.pdf"
}

@ARTICLE{Kandasamy2014-kj,
  title    = "Cortisol shifts financial risk preferences",
  author   = "Kandasamy, Narayanan and Hardy, Ben and Page, Lionel and
              Schaffner, Markus and Graggaber, Johann and Powlson, Andrew S and
              Fletcher, Paul C and Gurnell, Mark and Coates, John",
  abstract = "Risk taking is central to human activity. Consequently, it lies
              at the focal point of behavioral sciences such as neuroscience,
              economics, and finance. Many influential models from these
              sciences assume that financial risk preferences form a stable
              trait. Is this assumption justified and, if not, what causes the
              appetite for risk to fluctuate? We have previously found that
              traders experience a sustained increase in the stress hormone
              cortisol when the amount of uncertainty, in the form of market
              volatility, increases. Here we ask whether these elevated
              cortisol levels shift risk preferences. Using a double-blind,
              placebo-controlled, cross-over protocol we raised cortisol levels
              in volunteers over 8 d to the same extent previously observed in
              traders. We then tested for the utility and probability weighting
              functions underlying their risk taking and found that
              participants became more risk-averse. We also observed that the
              weighting of probabilities became more distorted among men
              relative to women. These results suggest that risk preferences
              are highly dynamic. Specifically, the stress response calibrates
              risk taking to our circumstances, reducing it in times of
              prolonged uncertainty, such as a financial crisis.
              Physiology-induced shifts in risk preferences may thus be an
              underappreciated cause of market instability.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  111,
  number   =  9,
  pages    = "3608--3613",
  month    =  mar,
  year     =  2014,
  keywords = "Adult; Cross-Over Studies; Female; Financial Management; Humans;
              Hydrocortisone; Hydrocortisone: administration \& dosage;
              Hydrocortisone: blood; Hydrocortisone: metabolism;
              Hydrocortisone: pharmacology; Male; Risk-Taking; Saliva; Saliva:
              metabolism; Sex Factors; Stress, Physiological; Stress,
              Physiological: physiology",
  language = "en"
}

@BOOK{Box2005-qz,
  title     = "Statistics for experimenters: design, innovation, and discovery",
  author    = "Box, George E P and Stuart Hunter, J and Hunter, William Gordon",
  abstract  = "A Classic adapted to modern timesRewritten and updated, this new
               edition of Statistics for Experimenters adopts the same
               approaches as the landmark First Edition by teaching with
               examples, readily understood graphics, and the appropriate use
               of computers. Catalyzing innovation, problem solving, and
               discovery, the Second Edition provides experimenters with the
               scientific and statistical tools needed to maximize the
               knowledge gained from research data, illustrating how these
               tools may best be utilized during all stages of the
               investigative process. The authors' practical approach starts
               with a problem that needs to be solved and then examines the
               appropriate statistical methods of design and analysis.Providing
               even greater accessibility for its users, the Second Edition is
               thoroughly revised and updated to reflect the changes in
               techniques and technologies since the publication of the classic
               First Edition.Among the new topics included are:Graphical
               Analysis of VarianceComputer Analysis of Complex
               DesignsSimplification by transformationHands-on experimentation
               using Response Service MethodsFurther development of robust
               product and process design using split plot arrangements and
               minimization of error transmissionIntroduction to Process
               Control, Forecasting and Time SeriesIllustrations demonstrating
               how multi-response problems can be solved using the concepts of
               active and inert factor spaces and canonical spacesBayesian
               approaches to model selection and sequential experimentationAn
               appendix featuring Quaquaversal quotes from a variety of sources
               including noted statisticians and scientists to famous
               philosophers is provided to illustrate key concepts and enliven
               the learning process.All the computations in the Second Edition
               can be done utilizing the statistical language R. Functions for
               displaying ANOVA and lamba plots, Bayesian screening, and model
               building are all included and R packages are available online.
               All theses topics can also be applied utilizing easy-to-use
               commercial software packages.Complete with applications covering
               the physical, engineering, biological, and social sciences,
               Statistics for Experimenters is designed for individuals who
               must use statistical approaches to conduct an experiment, but do
               not necessarily have formal training in statistics.
               Experimenters need only a basic understanding of mathematics to
               master all the statistical methods presented. This text is an
               essential reference for all researchers and is a highly
               recommended course book for undergraduate and graduate students.",
  publisher = "Wiley-Interscience",
  pages     = "1--664",
  edition   = "2nd",
  month     =  may,
  year      =  2005,
  address   = "New Jersey",
  language  = "en"
}

@ARTICLE{Imbens2009-sg,
  title     = "Recent Developments in the Econometrics of Program Evaluation",
  author    = "Imbens, Guido W and Wooldridge, Jeffrey M",
  abstract  = "Many empirical questions in economics and other social sciences
               depend on causal effects of programs or policies. In the last
               two decades, much research has been done on the econometric and
               statistical analysis of such causal effects. This recent
               theoretical literature has built on, and combined features of,
               earlier work in both the statistics and econometrics
               literatures. It has by now reached a level of maturity that
               makes it an important tool in many areas of empirical research
               in economics, including labor economics, public finance,
               development economics, industrial organization, and other areas
               of empirical microeconomics. In this review, we discuss some of
               the recent developments. We focus primarily on practical issues
               for empirical researchers, as well as provide a historical
               overview of the area and give references to more technical
               research.",
  journal   = "J. Econ. Lit.",
  publisher = "American Economic Association",
  volume    =  47,
  number    =  1,
  pages     = "5--86",
  series    = "OSA Technical Digest (CD)",
  month     =  mar,
  year      =  2009
}

@ARTICLE{Halsteinli2010-pk,
  title    = "Productivity growth in outpatient child and adolescent mental
              health services: the impact of case-mix adjustment",
  author   = "Halsteinli, Vidar and Kittelsen, Sverre A and Magnussen, Jon",
  abstract = "The performance of health service providers may be monitored by
              measuring productivity. However, the policy value of such
              measures may depend crucially on the accuracy of input and output
              measures. In particular, an important question is how to adjust
              adequately for case-mix in the production of health care. In this
              study, we assess productivity growth in Norwegian outpatient
              child and adolescent mental health service units (CAMHS) over a
              period characterized by governmental utilization of simple
              productivity indices, a substantial increase in capacity and a
              concurrent change in case-mix. We analyze the sensitivity of the
              productivity growth estimates using different specifications of
              output to adjust for case-mix differences. Case-mix adjustment is
              achieved by distributing patients into eight groups depending on
              reason for referral, age and gender, as well as correcting for
              the number of consultations. We utilize the nonparametric Data
              Envelopment Analysis (DEA) method to implicitly calculate weights
              that maximize each unit's efficiency. Malmquist indices of
              technical productivity growth are estimated and bootstrap
              procedures are performed to calculate confidence intervals and to
              test alternative specifications of outputs. The dataset consist
              of an unbalanced panel of 48-60 CAMHS in the period 1998-2006.
              The mean productivity growth estimate from a simple unadjusted
              patient model (one single output) is 35\%; adjusting for case-mix
              (eight outputs) reduces the growth estimate to 15\%. Adding
              consultations increases the estimate to 28\%. The latter reflects
              an increase in number of consultations per patient. We find that
              the governmental productivity indices strongly tend to
              overestimate productivity growth. Case-mix adjustment is of major
              importance and governmental utilization of performance indicators
              necessitates careful considerations of output specifications.",
  journal  = "Soc. Sci. Med.",
  volume   =  70,
  number   =  3,
  pages    = "439--446",
  month    =  feb,
  year     =  2010,
  keywords = "Adolescent; Adolescent Health Services; Adolescent Health
              Services: organization \& adminis; Ambulatory Care; Ambulatory
              Care: organization \& administration; Child; Child Health
              Services; Child Health Services: organization \& administrati;
              Confidence Intervals; Efficiency, Organizational; Female; Humans;
              Male; Mental Health Services; Mental Health Services:
              organization \& administrat; Models, Statistical; Norway;
              Referral and Consultation; Referral and Consultation: statistics
              \& numerical; Risk Adjustment; State Medicine",
  language = "en"
}

@ARTICLE{Holland2002-kp,
  title    = "American Statistical Association Conference on Radiation and
              Health. Deerfield Beach, Florida, {USA}. June 23-26, 2002.
              Abstracts",
  author   = "Holland, P W",
  abstract = "Problems involving casual inference have dogged at the heels of
              statistics since its earliest days. Correlation does not imply
              causation, and yet casual conclusions drawn from a carefully
              designed experiment are often valid. What can a satistical model
              say about causation? This question is addressed by using a
              particular model for casual inference. (Holland and Rubin 1983;
              Rubin 1974) to critique the discussion of other writers on
              causation and casual inference. These include selected
              philosophers, medical researchers, statisticians,
              econometericians, and proponents of casual modeling.",
  journal  = "Radiat. Res.",
  volume   =  158,
  number   =  6,
  pages    = "782--808",
  month    =  dec,
  year     =  2002,
  keywords = "Association; Casual effect; Casual model; Experiments; Granger
              casuality; Hill's nine factors; Koch's postulates; Mill's
              methods; Path diagrams; Philosophy; Probabilistic casuality.",
  language = "en"
}

@MISC{Long1990-hr,
  title    = "Noise Trader Risk in Financial Markets",
  author   = "Long, J Bradford De and Shleifer, Andrei and Summers, Lawrence H
              and Waldmann, Robert J",
  abstract = "We present a simple overlapping generations model of an asset
              mar- ket in which irrational noise traders with erroneous
              stochastic beliefs both affect prices and earn higher expected
              returns. The unpredict- ability of noise traders' beliefs creates
              a risk in the price of the asset that deters rational
              arbitrageurs from aggressively betting against them. As a result,
              prices can diverge significantly from fundamental values even in
              the absence of fundamental risk. Moreover, bearing a
              disproportionate amount of risk that they themselves create
              enables noise traders to earn a higher expected return than
              rational inves- tors do. The model sheds light on a number of
              financial anomalies, including the excess volatility of asset
              prices, the mean reversion of stock returns, the underpricing of
              closed-end mutual funds, and the Mehra-Prescott equity premium
              puzzle.",
  journal  = "J. Polit. Econ.",
  volume   =  98,
  pages    = "703",
  year     =  1990
}

@ARTICLE{Damodaran2009-we,
  title     = "Equity Risk Premiums ({ERP)}: Determinants, Estimation and
               Implications -- A {Post-Crisis} Update",
  author    = "Damodaran, Aswath",
  abstract  = "Equity risk premiums are a central component of every risk and
               return model in finance and are a key input into estimating
               costs of equity and capital in both corporate finance and
               valuation. Given their importance, it is surprising how
               haphazard the estimation of equity risk premiums remains in
               practice. We begin this paper by looking at the economic
               determinants of equity risk premiums, including investor risk
               aversion, information uncertainty and perceptions of
               macroeconomic risk. In the standard approach to estimating
               equity risk premiums, historical returns are used, with the
               difference in annual returns on stocks versus bonds over a long
               time period comprising the expected risk premium. We note the
               limitations of this approach, even in markets like the United
               States, which have long periods of historical data available,
               and its complete failure in emerging markets, where the
               historical data tends to be limited and volatile. We look at two
               other approaches to estimating equity risk premiums -- the
               survey approach, where investors and managers are asked to
               assess the risk premium and the implied approach, where a
               forward-looking estimate of the premium is estimated using
               either current equity prices or risk premiums in non-equity
               markets. We also look at the relationship between the equity
               risk premium and risk premiums in the bond market (default
               spreads) and in real estate (cap rates) and how that
               relationship can be mined to generated expected equity risk
               premiums. We close the paper by examining why different
               approaches yield different values for the equity risk premium,
               and how to choose the ``right'' number to use in analysis.",
  journal   = "Financial Markets, Institutions \& Instruments",
  publisher = "Blackwell Publishing Inc",
  volume    =  18,
  number    =  5,
  pages     = "289--370",
  month     =  dec,
  year      =  2009
}

@ARTICLE{Shleifer1997-ju,
  title     = "The Limits of Arbitrage",
  author    = "Shleifer, Andrei and Vishny, Robert W",
  abstract  = "Textbook arbitrage in financial markets requires no capital and
               entails no risk. In reality, almost all arbitrage requires
               capital, and is typically risky. Moreover, professional
               arbitrage is conducted by a relatively small number of highly
               specialized investors using other people's capital. Such
               professional arbitrage has a number of interesting implications
               for security pricing, including the possibility that arbitrage
               becomes ineffective in extreme circumstances, when prices
               diverge far from fundamental values. The model also suggests
               where anomalies in financial markets are likely to appear, and
               why arbitrage fails to eliminate them.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  52,
  number    =  1,
  pages     = "35--55",
  month     =  mar,
  year      =  1997
}

@ARTICLE{Poso1978-go,
  title    = "Stabilization of ornithine decarboxylase in rat liver",
  author   = "P{\"o}s{\"o}, H and Guha, S K and J{\"a}nne, J",
  abstract = "This paper considers whether it is possible to devise a
              nonexperimental procedure for evaluating a prototypical job
              training programme. Using rich nonexperimental data, we examine
              the performance of a two-stage evaluation methodology that (a)
              estimates the probability that a person participates in a
              programme and (b) uses the estimated probability in extensions of
              the classical method of matching. We decompose the conventional
              measure of programme evaluation bias into several components and
              find that bias due to selection on unobservables, commonly called
              selection bias in econometrics, is empirically less important
              than other components, although it is still a sizeable fraction
              of the estimated programme impact. Matching methods applied to
              comparison groups located in the same labour markets as
              participants and administered the same questionnaire eliminate
              much of the bias as conventionally measured, but the remaining
              bias is a considerable fraction of experimentally-determined
              programme impact estimates. We test and reject the identifying
              assumptions that justify the classical method of matching. We
              present a nonparametric conditional difference-in-differences
              extension of the method of matching that is consistent with the
              classical index-sufficient sample selection model and is not
              rejected by our tests of identifying assumptions. This estimator
              is effective in eliminating bias, especially when it is due to
              temporally-invariant omitted variables.",
  journal  = "Biochim. Biophys. Acta",
  volume   =  524,
  number   =  2,
  pages    = "466--473",
  month    =  jun,
  year     =  1978,
  language = "en"
}

@MISC{Imbens2010-ae,
  title    = "Better {LATE} Than Nothing: Some Comments on Deaton (2009) and
              Heckman and Urzua (2009)",
  author   = "Imbens, Guido W",
  abstract = "Two recent papers, Deaton (2009) and Heckman and Urzua (2009),
              argue against what they see as an excessive and inappropriate use
              of experimental and quasi-experimental methods in empirical work
              in economics in the last decade. They specifically question the
              increased use of instrumental variables and natural experiments
              in labor economics and of randomized experiments in development
              economics. In these comments, I will make the case that this move
              toward shoring up the internal validity of estimates, and toward
              clarifying the description of the population these estimates are
              relevant for, has been important and beneficial in increasing the
              credibility of empirical work in economics. I also address some
              other concerns raised by the Deaton and Heckman--Urzua papers.
              (JEL C21, C31)",
  journal  = "J. Econ. Lit.",
  volume   =  48,
  pages    = "399--423",
  year     =  2010
}

@ARTICLE{Barth2002-fu,
  title    = "A {Cross-Country} Analysis of the Bank Supervisory Framework and
              Bank Performance",
  author   = "Barth, James R and Nolle, Daniel E and Phumiwasana, Triphon and
              Yago, Glenn",
  abstract = "Ongoing changes in the structure and nature of banking, as well
              as banking crises across the globe have focused the attention of
              policy makers on the appropriat",
  journal  = "Financial Markets, Institutions \& Instruments",
  volume   =  12,
  pages    = "205--248",
  month    =  aug,
  year     =  2002,
  keywords = "bank performance; bank safety and soundness; banking; banking
              crises; banking crisis; banking industry; development of the
              banking system; financial service; independence; scope;
              structure; supervisory structure"
}

@INCOLLECTION{Hughes2010-xb,
  title     = "Efficiency in Banking: Theory, Practice and Evidence",
  booktitle = "The Oxford Handbook of Banking",
  author    = "Hughes, Joseph P and Mester, Loretta J",
  editor    = "Berger, Alan and Molyneux, Phil and Wilson, John",
  publisher = "Oxford Handbooks",
  year      =  2010,
  keywords  = "bank efficiency measurement; bank performance; consolidation;
               structural approaches"
}

@ARTICLE{Glass2006-lr,
  title     = "The impact of differing operating environments on {US} Credit
               Union Performance, 1993--2001",
  author    = "Glass, J Colin and McKillop, Donal G",
  abstract  = "Today US credit unions operate within a highly competitive
               financial market place. Set against this competitive operating
               environment, the present study employs stochastic frontier
               analysis to evaluate the performance of large credit unions
               (assets greater than \$50 million) over the period 1993 to 2001.
               Although credit unions may share a common co-operative
               philosophy, differences between credit unions are also apparent
               across a range of operational, structural and locational
               characteristics (environmental conditions). The impact of these
               different environmental influences is modelled in two ways. One
               assumes that environmental factors affect the efficiency with
               which the production process is operated, while the second
               assumes that the environment affects the production process
               itself. Net and gross cost efficiency measures are obtained for
               both models, with the differences between these measures for a
               specific credit union being viewed as the impact that
               environmental variables have on the inefficiency of that credit
               union. In addition, if it is assumed that the main environmental
               factors are accounted for in the modelling, then a credit
               union's net efficiency measure may be interpreted as a measure
               of managerial performance when operating in equivalent
               environments. The analysis revealed that different environments
               (the age of the credit union; the potential for expansion within
               the existing common bond; whether the credit union has the
               option of expansion through the addition of select employee
               groups; whether the credit union is state or federally
               regulated; whether insurance is provided at state or federal
               level; as well as regional characteristics such as per capita
               income and the level of unemployment) account for much of the
               variability in cost efficiency between credit unions and once
               credit unions are placed in broadly equivalent operating
               environments only marginal differences are apparent in their
               managerial performance.",
  journal   = "Applied Financial Economics",
  publisher = "Routledge",
  volume    =  16,
  number    =  17,
  pages     = "1285--1300",
  month     =  nov,
  year      =  2006
}

@ARTICLE{Fama1992-yv,
  title     = "The {Cross-Section} of Expected Stock Returns",
  author    = "Fama, Eugene F and French, Kenneth R",
  abstract  = "Two easily measured variables, size and book-to-market equity,
               combine to capture the cross-sectional variation in average
               stock returns associated with market $\beta$, size, leverage,
               book-to-market equity, and earnings-price ratios. Moreover, when
               the tests allow for variation in $\beta$ that is unrelated to
               size, the relation between market $\beta$ and average return is
               flat, even when $\beta$ is the only explanatory variable.",
  journal   = "J. Finance",
  publisher = "[American Finance Association, Wiley]",
  volume    =  47,
  number    =  2,
  pages     = "427--465",
  year      =  1992
}

@ARTICLE{Datta2015-hs,
  title    = "On {post-IPO} stock price performance: A comparative analysis of
              {RLBOs} and {IPOs}",
  author   = "Datta, Sudip and Gruskin, Mark and Iskandar-Datta, Mai",
  abstract = "This is the first study to examine the post-IPO stock price
              performance by differentiating between IPOs and three types of
              RLBOs (i.e. public-to-private (or re-IPOs), division-to-private,
              and private-to-private deals). We document that public-to-private
              RLBOs outperform their industry rivals, IPOs, mature firms in
              comparable industries, and a propensity-score matched control
              group for up to five years post-offering. Further, we document
              that, within RLBOs, public-to-private RLBOs, outperform
              private-to-private and division-to-private RLBOs. We also find
              support for the underwriter signaling effect for
              public-to-private RLBOs. Our analysis identifies for the first
              time what private period restructuring activities contribute to
              superior post-re-IPO stock price performance. Further, the
              beneficial effects of private period restructurings are enhanced
              for deals associated with prestigious underwriters. Our findings
              suggest that first IPOs and re-IPOs differ substantially in term
              of post-offer performance, the impact of prestigious underwriters
              on performance, and performance over time.",
  journal  = "Journal of Banking \& Finance",
  volume   =  55,
  number   =  0,
  pages    = "187--203",
  month    =  jun,
  year     =  2015,
  keywords = "Post-RLBO performance; Difference between public-to-private
              (re-IPOs); Division-to-private; Private-to-private RLBO and IPO
              performance; Underwriter signaling; Private period restructuring"
}

@ARTICLE{Azadeh2010-gq,
  title    = "An adaptive network-based fuzzy inference system for short-term
              natural gas demand estimation: Uncertain and complex environments",
  author   = "Azadeh, A and Asadzadeh, S M and Ghanbari, A",
  abstract = "Accurate short-term natural gas (NG) demand estimation and
              forecasting is vital for policy and decision-making process in
              energy sector. Moreover, conventional methods may not provide
              accurate results. This paper presents an adaptive network-based
              fuzzy inference system (ANFIS) for estimation of NG demand.
              Standard input variables are used which are day of the week,
              demand of the same day in previous year, demand of a day before
              and demand of 2 days before. The proposed ANFIS approach is
              equipped with pre-processing and post-processing concepts.
              Moreover, input data are pre-processed (scaled) and finally
              output data are post-processed (returned to its original scale).
              The superiority and applicability of the ANFIS approach is shown
              for Iranian NG consumption from 22/12/2007 to 30/6/2008. Results
              show that ANFIS provides more accurate results than artificial
              neural network (ANN) and conventional time series approach. The
              results of this study provide policy makers with an appropriate
              tool to make more accurate predictions on future short-term NG
              demand. This is because the proposed approach is capable of
              handling non-linearity, complexity as well as uncertainty that
              may exist in actual data sets due to erratic responses and
              measurement errors.",
  journal  = "Energy Policy",
  volume   =  38,
  number   =  3,
  pages    = "1529--1536",
  month    =  mar,
  year     =  2010,
  keywords = "Short-term natural gas demand; Adaptive network-based fuzzy
              inference systems; Comparison"
}

@ARTICLE{Soldo2012-bw,
  title    = "Forecasting natural gas consumption",
  author   = "Soldo, Bo{\v z}idar",
  abstract = "Publishing papers in the area of forecasting natural gas
              consumption has begun in the middle of last century and led to a
              tremendous surge in research activities in the past decade. This
              paper presents a state-of-the-art survey of forecasting natural
              gas consumption. Purpose of this paper is to provide analysis and
              synthesis of published research in this area from beginning to
              the end of 2010, insights on applied area, used data, models and
              tools to achieve usable results, in order to be helpful base for
              future researchers.",
  journal  = "Appl. Energy",
  volume   =  92,
  number   =  0,
  pages    = "26--37",
  month    =  apr,
  year     =  2012,
  keywords = "Forecasting natural gas consumption; Predicting natural gas
              demand; Natural gas forecasting models"
}

@ARTICLE{Azadeh2011-cn,
  title    = "A Neuro-fuzzy-stochastic frontier analysis approach for long-term
              natural gas consumption forecasting and behavior analysis: The
              cases of Bahrain, Saudi Arabia, Syria, and {UAE}",
  author   = "Azadeh, A and Asadzadeh, S M and Saberi, M and Nadimi, V and
              Tajvidi, A and Sheikalishahi, M",
  abstract = "This paper presents an adaptive network-based fuzzy inference
              system-stochastic frontier analysis (ANFIS-SFA) approach for
              long-term natural gas (NG) consumption prediction and analysis of
              the behavior of NG consumption. The proposed models consist of
              input variables of Gross Domestic Product (GDP) and population
              (POP). Six distinct models based on different inputs are defined.
              All of trained ANFIS are then compared with respect to mean
              absolute percentage error (MAPE). To meet the best performance of
              the intelligent based approaches, data are pre-processed (scaled)
              and finally the outputs are post-processed (returned to its
              original scale). To show the applicability and superiority of the
              integrated ANFIS-SFA approach, gas consumption in four Middle
              Eastern countries i.e. Bahrain, Saudi Arabia, Syria, and United
              Arab Emirates is forecasted and analyzed based on the data of the
              time period 1980--2007. With the aid of autoregressive model, GDP
              and population are projected for the period 2008--2015. These
              projected data are used as the input of ANFIS model to predict
              the gas consumption in the selected countries for 2008--2015. SFA
              is then used to examine the behavior of gas consumption in the
              past and also to make insights for the forthcoming years. The
              ANFIS-SFA approach is capable of dealing with complexity,
              uncertainty, and randomness as well as several other unique
              features discussed in this paper.",
  journal  = "Appl. Energy",
  volume   =  88,
  number   =  11,
  pages    = "3850--3859",
  month    =  nov,
  year     =  2011,
  keywords = "Natural gas demand; Long-term prediction; Adaptive network-based
              fuzzy inference system (ANFIS); Stochastic frontier analysis
              (SFA); Stochastic data"
}

@ARTICLE{Akkurt2010-vh,
  title   = "Forecasting Turkey's natural gas consumption by using time series
             methods",
  author  = "Akkurt, Mustafa and Demirel, Omer F and Zaim, Selim",
  journal = "European Journal of Economic and Political Studies",
  volume  =  3,
  number  =  2,
  pages   = "1--21",
  year    =  2010
}

@ARTICLE{Azadeh2013-sr,
  title    = "A neuro-fuzzy-multivariate algorithm for accurate gas consumption
              estimation in South America with noisy inputs",
  author   = "Azadeh, Ali and Saberi, Morteza and Asadzadeh, Seyed Mohammad and
              Hussain, Omar Khadeer and Saberi, Zahra",
  abstract = "This paper presents an adaptive-network-based fuzzy inference
              system (ANFIS)-fuzzy data envelopment analysis (FDEA) algorithm
              for improvement of long-term natural gas (NG) consumption
              forecasting and analysis. Two types of ANFIS (Types 1 and 2) have
              been proposed to forecast annual NG demand. For each type,
              several ANFIS models have been constructed and tested in order to
              find the best ANFIS for NG consumption. Two parameters have been
              considered in construction and examination of plausible ANFIS
              models (Type 1). Six different membership functions and several
              linguistic variables are considered in building ANFIS. Also
              different value of cluster radius has been used to construct
              ANFIS (Type 2) models. The proposed models consist of two input
              variables, namely, Gross Domestic Product (GDP) and Population.
              All trained ANFIS are then compared with respect to mean absolute
              percentage error (MAPE), Root mean square normalized error (RMSE)
              and correlation coefficient (R) using data envelopment analysis
              (DEA). To meet the best performance of the intelligent based
              approaches, data are pre-processed (scaled) and finally our
              outputs are post-processed (returned to its original scale). FDEA
              is used to examine the behavior of gas consumption. To show the
              applicability and superiority of the ANFIS--FDEA algorithm,
              actual NG consumption in six Southern America countries from 1980
              to 2007 is considered. NG consumption is then forecasted up to
              2015. The ANFIS--FDEA algorithm is capable of dealing both
              complexity and uncertainty as well several other unique features
              discussed in this paper.",
  journal  = "Int. J. Electr. Power Energy Syst.",
  volume   =  46,
  number   =  0,
  pages    = "315--325",
  month    =  mar,
  year     =  2013,
  keywords = "Long-term; Gas consumption; Forecasting; Adaptive network based
              fuzzy inference system (ANFIS); Data envelopment analysis (DEA);
              Fuzzy data envelopment analysis (FDEA)"
}

@ARTICLE{Gil2004-mu,
  title     = "Generalized Model of Prediction of Natural Gas Consumption",
  author    = "Gil, S and Deferrari, J",
  journal   = "J. Energy Res. Technol.",
  publisher = "American Society of Mechanical Engineers",
  volume    =  126,
  number    =  2,
  pages     = "90--98",
  month     =  jun,
  year      =  2004,
  keywords  = "Temperature; Stress; Natural gas; Transportation systems"
}

@ARTICLE{Cho2013-ba,
  title     = "Modeling and Forecasting Daily Electricity Load Curves: A Hybrid
               Approach",
  author    = "Cho, Haeran and Goude, Yannig and Brossat, Xavier and Yao, Qiwei",
  abstract  = "We propose a hybrid approach for the modeling and the short-term
               forecasting of electricity loads. Two building blocks of our
               approach are (1) modeling the overall trend and seasonality by
               fitting a generalized additive model to the weekly averages of
               the load and (2) modeling the dependence structure across
               consecutive daily loads via curve linear regression. For the
               latter, a new methodology is proposed for linear regression with
               both curve response and curve regressors. The key idea behind
               the proposed methodology is dimension reduction based on a
               singular value decomposition in a Hilbert space, which reduces
               the curve regression problem to several ordinary (i.e., scalar)
               linear regression problems. We illustrate the hybrid method
               using French electricity loads between 1996 and 2009, on which
               we also compare our method with other available models including
               the {\'E}lectricit{\'e} de France operational model.
               Supplementary materials for this article are available online.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  108,
  number    =  501,
  pages     = "7--21",
  month     =  mar,
  year      =  2013
}

@ARTICLE{Hong2014-ly,
  title    = "Global Energy Forecasting Competition 2012",
  author   = "Hong, Tao and Pinson, Pierre and Fan, Shu",
  abstract = "The Global Energy Forecasting Competition (GEFCom2012) attracted
              hundreds of participants worldwide, who contributed many novel
              ideas to the energy forecasting field. This paper introduces both
              tracks of GEFCom2012, hierarchical load forecasting and wind
              power forecasting, with details on the aspects of the problem,
              the data, and a summary of the methods used by selected top
              entries. We also discuss the lessons learned from this
              competition from the organizers' perspective. The complete data
              set, including the solution data, is published along with this
              paper, in an effort to establish a benchmark data pool for the
              community.",
  journal  = "Int. J. Forecast.",
  volume   =  30,
  number   =  2,
  pages    = "357--363",
  month    =  apr,
  year     =  2014
}

@ARTICLE{Lloyd2014-sv,
  title    = "{GEFCom2012} hierarchical load forecasting: Gradient boosting
              machines and Gaussian processes",
  author   = "Lloyd, James Robert",
  abstract = "This report discusses methods for forecasting hourly loads of a
              US utility as part of the load forecasting track of the Global
              Energy Forecasting Competition 2012 hosted on Kaggle. The methods
              described (gradient boosting machines and Gaussian processes) are
              generic machine learning/regression algorithms, and few
              domain-specific adjustments were made. Despite this, the
              algorithms were able to produce highly competitive predictions,
              which can hopefully inspire more refined techniques to compete
              with state-of-the-art load forecasting methodologies.",
  journal  = "Int. J. Forecast.",
  volume   =  30,
  number   =  2,
  pages    = "369--374",
  month    =  apr,
  year     =  2014,
  keywords = "Load forecasting; Gradient boosting machines; Gaussian processes"
}

@ARTICLE{Charlton2014-he,
  title    = "A refined parametric model for short term load forecasting",
  author   = "Charlton, Nathaniel and Singleton, Colin",
  abstract = "We present a refined parametric model for forecasting electricity
              demand which performed particularly well in the recent Global
              Energy Forecasting Competition (GEFCom 2012). We begin by
              motivating and presenting a simple parametric model, treating the
              electricity demand as a function of the temperature and day of
              the data. We then set out a series of refinements of the model,
              explaining the rationale for each, and using the competition
              scores to demonstrate that each successive refinement step
              increases the accuracy of the model's predictions. These
              refinements include combining models from multiple weather
              stations, removing outliers from the historical data, and special
              treatments of public holidays.",
  journal  = "Int. J. Forecast.",
  volume   =  30,
  number   =  2,
  pages    = "364--368",
  month    =  apr,
  year     =  2014,
  keywords = "Electricity; Regression; Forecasting competitions; Combining
              forecasts; Demand forecasting"
}

@ARTICLE{Nedellec2014-hm,
  title    = "{GEFCom2012}: Electric load forecasting and backcasting with
              semi-parametric models",
  author   = "Nedellec, Raphael and Cugliari, Jairo and Goude, Yannig",
  abstract = "We sum up the methodology of the team tololo for the Global
              Energy Forecasting Competition 2012: Load Forecasting. Our
              strategy consisted of a temporal multi-scale model that combines
              three components. The first component was a long term trend
              estimated by means of non-parametric smoothing. The second was a
              medium term component describing the sensitivity of the
              electricity demand to the temperature at each time step. We use a
              generalized additive model to fit this component, using calendar
              information as well. Finally, a short term component models local
              behaviours. As the factors that drive this component are unknown,
              we use a random forest model to estimate it.",
  journal  = "Int. J. Forecast.",
  volume   =  30,
  number   =  2,
  pages    = "375--381",
  month    =  apr,
  year     =  2014,
  keywords = "Demand forecasting; Forecasting competitions; Multivariate time
              series; Nonlinear time series; Regression"
}

@ARTICLE{Fan2012-sm,
  title    = "{Short-Term} Load Forecasting Based on a {Semi-Parametric}
              Additive Model",
  author   = "Fan, S and Hyndman, R J",
  abstract = "Short-term load forecasting is an essential instrument in power
              system planning, operation, and control. Many operating decisions
              are based on load forecasts, such as dispatch scheduling of
              generating capacity, reliability analysis, and maintenance
              planning for the generators. Overestimation of electricity demand
              will cause a conservative operation, which leads to the start-up
              of too many units or excessive energy purchase, thereby supplying
              an unnecessary level of reserve. On the other hand,
              underestimation may result in a risky operation, with
              insufficient preparation of spinning reserve, causing the system
              to operate in a vulnerable region to the disturbance. In this
              paper, semi-parametric additive models are proposed to estimate
              the relationships between demand and the driver variables.
              Specifically, the inputs for these models are calendar variables,
              lagged actual demand observations, and historical and forecast
              temperature traces for one or more sites in the target power
              system. In addition to point forecasts, prediction intervals are
              also estimated using a modified bootstrap method suitable for the
              complex seasonality seen in electricity demand data. The proposed
              methodology has been used to forecast the half-hourly electricity
              demand for up to seven days ahead for power systems in the
              Australian National Electricity Market. The performance of the
              methodology is validated via out-of-sample experiments with real
              data from the power system, as well as through on-site
              implementation by the system operator.",
  journal  = "IEEE Trans. Power Syst.",
  volume   =  27,
  number   =  1,
  pages    = "134--141",
  month    =  feb,
  year     =  2012,
  keywords = "load forecasting;power markets;power system control;power system
              economics;power system faults;power system planning;statistical
              analysis;Australian National Electricity Market;electricity
              demand data;electricity demand overestimation;generating capacity
              dispatch scheduling;generator maintenance planning;modified
              bootstrap method;power system control;power system
              disturbance;power system operation;power system
              planning;prediction interval estimation;reliability
              analysis;semiparametric additive model;short-term load
              forecasting;spinning reserve preparation;Computational
              modeling;Electricity;Forecasting;Input variables;Load
              forecasting;Load modeling;Predictive models;Additive
              model;forecast distribution;short-term load forecasting;time
              series"
}

@ARTICLE{Ter_Horst2004-oe,
  title    = "Evaluating style analysis",
  author   = "ter Horst, Jenke R and Nijman, Theo E and de Roon, Frans A",
  abstract = "In this paper we analyze the use and implications of
              (return-based) style analysis. First, style analysis may be used
              to estimate the relevant factor exposures of a fund. We use a
              simple simulation experiment to show that imposing portfolio and
              positivity constraints in style analysis leads to significant
              efficiency gains if the factor loadings are indeed positively
              weighted portfolios, in particular when the factors have low
              cross-correlations. If this is not the case though, imposing the
              constraints can lead to biased exposure estimates. Second, style
              analysis may be used in performance measurement. If the actual
              factor exposures are a positively weighted portfolio and if the
              risk-free rate is one of the benchmarks, then the intercept
              coincides with the Jensen measure. In general, the intercept in
              the style regression can only be interpreted as a special case of
              the familiar Jensen measure. Third, style estimates may be
              compared with actual portfolio holdings. We show that the actual
              portfolio holdings will in general not reveal the actual
              investment style of a fund because of cross exposures between the
              asset classes and because fund managers may hold securities that
              on average do not have a beta of one relative to their own asset
              class. Although return-based style analysis is less suitable to
              predict future portfolio holdings, our empirical analysis
              suggests that it performs better than holding-based style
              analysis in predicting future fund returns.",
  journal  = "Journal of Empirical Finance",
  volume   =  11,
  number   =  1,
  pages    = "29--53",
  month    =  jan,
  year     =  2004,
  keywords = "Style analysis; Mutual funds"
}

@TECHREPORT{Peeters2012-zh,
  title     = "Vertical restraints in soccer: Financial fair play and the
               English Premier League",
  author    = "Peeters, Thomas and Szymanski, Stefan",
  abstract  = "In 2010 UEFA, the governing body of European soccer, announced a
               set of financial restraints, that clubs must observe when
               seeking to enter its competitions, notably the UEFA Champions
               League. We characterize these ``Financial Fair Play'' (FFP)
               regulations as a form of vertical restraint and assess their
               impact on the intensity of competition in the English Premier
               League. We build a structural empirical model to show that
               introducing FFP would substantially reduce competition,
               resulting in lower average payrolls, while average revenues
               would hardly be affected. Depending on the exact regime, wage to
               turnover ratios would decline by 8\% to 15\%.",
  publisher = "University of Antwerp, Faculty of Applied Economics",
  number    =  2012028,
  month     =  dec,
  year      =  2012,
  keywords  = "Vertical restraints; Soccer; Financial Fair Play"
}

@ARTICLE{Vopel2011-sn,
  title     = "Do we really need financial fair play in european club football?
               an economic analysis",
  author    = "V{\"o}pel, H",
  abstract  = "The UEFA (Union des Associations Europ{\'e}ennes de Football )
               and above all the UEFA president Michel Platini are very
               concerned about recent developments in European club football .
               Many clubs have reported repeated and worsening deficits which
               have led to ...",
  journal   = "CESifo DICE Report",
  publisher = "hwwi.org",
  year      =  2011
}

@ARTICLE{Muller2012-yo,
  title     = "The Financial Fair Play regulations of {UEFA}: an adequate
               concept to ensure the long-term viability and sustainability of
               European club football",
  author    = "M{\"u}ller, J Christian and Lammert, Joachim and Hovemann,
               Gregor",
  abstract  = "Abstract In response to the severe financial plight of many
               clubs that regularly take part in European competitions, UEFA
               developed the concept of Financial Fair Play as an extension of
               its licensing regulations. The aim of the concept is to curtail
               financial foul play in ...",
  journal   = "Int. J. Sports Financ.",
  publisher = "Fitness Information Technology",
  volume    =  7,
  number    =  2,
  pages     = "117--140",
  year      =  2012
}

@TECHREPORT{Budzinski2014-ax,
  title       = "The Competition Economics of Financial Fair Play",
  author      = "Budzinski, Oliver",
  number      =  85,
  institution = "Ilmenau Economics Discussion Papers, Vol. 19, No.85",
  month       =  mar,
  year        =  2014,
  keywords    = "financial fair play, sports economics, competition economics,
                 European competition policy, football, soccer, overinvestment,
                 rat race"
}

@ARTICLE{Buraimo2015-vb,
  title     = "Uncertainty of Outcome or Star Quality? Television Audience
               Demand for English Premier League Football",
  author    = "Buraimo, Babatunde and Simmons, Rob",
  abstract  = "AbstractThis paper presents new evidence on the relevance of
               uncertainty of outcome for demand for sports viewing. Using
               television viewing figures for eight seasons from the English
               Premier League, we show that uncertainty of outcome does not
               have the hypothesised effect on television audience demand.
               Separating uncertainty of outcome effects by season, the results
               show that, at best, uncertainty of outcome had imprecise effects
               on audiences in earlier seasons, but zero effects in later
               seasons. Television audiences have evolved to exhibit
               preferences for talent. We suggest that the notion of a pure
               sporting contest in which uncertainty of outcome matters is no
               longer relevant and more important is the extent to which sports
               teams and leagues can increase the quality of the talent on
               show.",
  journal   = "International Journal of the Economics of Business",
  publisher = "Routledge",
  volume    =  22,
  number    =  3,
  pages     = "449--469",
  month     =  sep,
  year      =  2015
}

@PHDTHESIS{Evans2014-cz,
  title     = "Economic model of financial fair play in professional football",
  author    = "Evans, Richard",
  abstract  = "The organisers of most professional sports leagues now employ
               one or more forms of policy intervention such as revenue sharing
               and salary capping schemes. The focus of the sports economic
               literature was initially directed towards the theoretical
               effects of these policies on competitive balance, wage rates and
               owner profits in the context of Major US sports leagues. That
               work has since been broadened in the literature to include other
               types of policy intervention and other model assumptions such as
               `win maximising' owners and `open' labour markets that
               characterise other professional leagues such as for association
               football. More recent policy intervention has included the
               regulation of financial performance of professional
               (association) football clubs. Hitherto, the literature has not
               addressed the implications of `Financial Fair Play' (FFP)
               regulation in a dynamic context. This paper provides a basic
               theoretical model to address that requirement. It shows, for
               example, the conditions that would result in a stable or an
               unstable league and the effect of introducing FFP on the total
               expenditure of teams in the league.",
  publisher = "Birkbeck College, University of London",
  month     =  sep,
  year      =  2014,
  address   = "London, UK",
  school    = "Birkbeck College, University of London"
}

@ARTICLE{Burdekin2015-js,
  title     = "Transfer Spending in the English Premier League: The Haves and
               the Have Nots",
  author    = "Burdekin, Richard C K and Franklin, Michael",
  abstract  = "Transfer spending among English Premier League clubs has
               increased drastically since the inception of the league, often
               funded by extremely wealthy owners who began purchasing majority
               stakes in clubs. Using data from the Deloitte Annual Review of
               Football Finance, we allow for different tiers of football clubs
               representing the haves and the have nots as well as a comparison
               between the first and second decades of the Premier League. Our
               finding that heightened spending has improved on-field
               performance only at the expense of hurting profitability is in
               line with win maximisation surmounting profit maximisation in
               today's Premier League.",
  journal   = "Natl. Inst. Econ. Rev.",
  publisher = "SAGE PublicationsSage UK: London, England",
  volume    =  232,
  number    =  1,
  pages     = "R4--R17",
  month     =  may,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Maxcy2014-dp,
  title     = "The American View on Financial Fair Play",
  author    = "Maxcy, Joel G",
  abstract  = "Financial Fair Play (FFP) regulations restrict the total
               spending by football clubs on salaries and other expenses. The
               policy resembles the restraints on playe",
  journal   = "ESEA Conference Volume. Budzinski, O. and",
  publisher = "papers.ssrn.com",
  month     =  jan,
  year      =  2014,
  keywords  = "Financial Regulation, Football Economics, Labor Markets"
}

@INCOLLECTION{Carmichael2014-ad,
  title     = "Team performance: production and efficiency in football",
  booktitle = "Handbook on the Economics of Professional Football",
  author    = "Carmichael, Fiona and Thomas, Dennis",
  editor    = "Goddard, John and Sloane, Peter",
  publisher = "Edward Elgar",
  pages     = "143--165",
  year      =  2014,
  keywords  = "Economics and Finance"
}

@ARTICLE{Wu2003-st,
  title     = "The pricing of global and domestic initial public offerings by
               {US} companies",
  author    = "Wu, Congsheng and Kwok, Chuck C Y",
  abstract  = "This study examines the pricing of global initial public
               offerings made by US companies as compared to purely domestic
               offerings. We find that global participation can significantly
               reduce underpricing by about four percentage points. Moreover,
               the degree of underpricing declines as larger proportions of
               shares are allocated to foreign investors. Our results suggest
               that US companies time their global offerings when foreign
               demand for US shares is high. There is also evidence that global
               offerings alleviate the downward pricing pressure associated
               with new share offerings.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  27,
  number    =  6,
  pages     = "1167--1184",
  month     =  jun,
  year      =  2003,
  keywords  = "Global initial public offerings"
}

@ARTICLE{Palomaki2011-yz,
  title    = "{DNA} sequencing of maternal plasma to detect Down syndrome: an
              international clinical validation study",
  author   = "Palomaki, Glenn E and Kloza, Edward M and Lambert-Messerlian,
              Geralyn M and Haddow, James E and Neveux, Louis M and Ehrich,
              Mathias and van den Boom, Dirk and Bombard, Allan T and Deciu,
              Cosmin and Grody, Wayne W and Nelson, Stanley F and Canick, Jacob
              A",
  abstract = "PURPOSE: Prenatal screening for Down syndrome has improved, but
              the number of resulting invasive diagnostic procedures remains
              problematic. Measurement of circulating cell-free DNA in maternal
              plasma might offer improvement. METHODS: A blinded, nested
              case-control study was designed within a cohort of 4664
              pregnancies at high risk for Down syndrome. Fetal karyotyping was
              compared with an internally validated, laboratory-developed test
              based on next-generation sequencing in 212 Down syndrome and 1484
              matched euploid pregnancies. None had been previously tested.
              Primary testing occurred at a CLIA-certified commercial
              laboratory, with cross validation by a CLIA-certified university
              laboratory. RESULTS: Down syndrome detection rate was 98.6\%
              (209/212), the false-positive rate was 0.20\% (3/1471), and the
              testing failed in 13 pregnancies (0.8\%); all were euploid.
              Before unblinding, the primary testing laboratory also reported
              multiple alternative interpretations. Adjusting chromosome 21
              counts for guanine cytosine base content had the largest impact
              on improving performance. CONCLUSION: When applied to high-risk
              pregnancies, measuring maternal plasma DNA detects nearly all
              cases of Down syndrome at a very low false-positive rate. This
              method can substantially reduce the need for invasive diagnostic
              procedures and attendant procedure-related fetal losses. Although
              implementation issues need to be addressed, the evidence supports
              introducing this testing on a clinical basis.",
  journal  = "Genet. Med.",
  volume   =  13,
  number   =  11,
  pages    = "913--920",
  month    =  nov,
  year     =  2011,
  language = "en"
}

@ARTICLE{Mason2012-oj,
  title     = "Style Analysis for Diversified {US} Equity Funds",
  author    = "Mason, A and McGroarty, F J and Thomas, S H",
  abstract  = "In this study we consider two methods of returns based style
               analysis for classification of investment styles for a single
               asset class, US Diversified Equity Funds. We extend Sharpe's
               (1992) style Returns Based Style Analysis (RBSA) by forming
               style groups using cluster analysis and RBSA factors. We also
               introduce a parsimonious Best Fit Index (BFI) of style
               classification which explicitly acknowledges the existence of
               market segmentation and practitioner benchmarking. The methods
               provide complementary information about mutual fund returns.
               Both methodologies explain a significant proportion of the cross
               section of out of sample returns, but the BFI method performs
               better out-of-sample is more transparent and more closely
               aligned to investment practice.",
  journal   = "The Journal of Asset Management",
  publisher = "Palgrave Macmillan",
  volume    =  13,
  number    =  3,
  pages     = "170--185",
  year      =  2012,
  keywords  = "Style; Investment; Benchmark; Portfolio; Value; Factors",
  language  = "en"
}

@ARTICLE{noauthor_undated-nq,
  title = "[{PDF]Style} Analysis for Diversified {US} Equity Funds - Surrey .."
}

@ARTICLE{Immonen2014-px,
  title  = "Style analysis and performance evaluation of Finnish equity mutual
            funds",
  author = "Immonen, Aapo and Kahra, Hannu",
  year   =  2014
}

@ARTICLE{Mason2013-li,
  title     = "Complementary or contradictory? Combining returns-based and
               characteristics-based investment style analysis",
  author    = "Mason, Andrew and McGroarty, Frank and Thomas, Steve",
  abstract  = "This study is the first to combine returns-based (RBS) and
               characteristics-based (CBS) style analysis into a single style
               analysis model. We address the issue of whether RBS and CBS
               analysis are complementary. Out-of-sample tests confirmed two
               things: membership of style groups explains a significant degree
               of cross-sectional performance of mutual funds, and the
               cumulative effect of combining BFI (Best Fit Index) and CBS
               analysis significantly improves on the CBS and BFI models in
               isolation. The ex post explanatory power of the combined model
               is greater than the individual parts.",
  journal   = "J Asset Manag",
  publisher = "Palgrave Macmillan UK",
  volume    =  14,
  number    =  6,
  pages     = "423--438",
  month     =  dec,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Makarov2015-zd,
  title    = "Rewarding Trading Skills without Inducing Gambling",
  author   = "Makarov, Igor and Plantin, Guillaume",
  abstract = "This paper develops a model of active asset management in which
              fund managers may forgo alpha-generating strategies, preferring
              instead to make negative-alpha trades that enable them to
              temporarily manipulate investors' perceptions of their skills. We
              show that such trades are optimally generated by taking on hidden
              tail risk, and are more likely to occur when fund managers are
              impatient and when their trading skills are scalable, and
              generate a high profit per unit of risk. We propose long-term
              contracts that deter this behavior by dynamically adjusting the
              dates on which the manager is compensated in response to her
              cumulative performance.",
  journal  = "J. Finance",
  volume   =  70,
  number   =  3,
  pages    = "925--962",
  month    =  jun,
  year     =  2015
}

@ARTICLE{Patton2015-lx,
  title    = "Change You Can Believe In? Hedge Fund Data Revisions",
  author   = "Patton, Andrew J and Ramadorai, Tarun and Streatfield, Michael",
  abstract = "We analyze the reliability of voluntary disclosures of financial
              information, focusing on widely-employed publicly-available hedge
              fund databases. Tracking changes to statements of historical
              performance recorded between 2007 and 2011, we find that
              historical returns are routinely revised. These revisions are not
              merely random or corrections of earlier mistakes; they are partly
              forecastable by fund characteristics. Funds that revise their
              performance histories significantly and predictably underperform
              those that have never revised, suggesting that unreliable
              disclosures constitute a valuable source of information for
              investors. These results speak to current debates about mandatory
              disclosures by financial institutions to market regulators.",
  journal  = "J. Finance",
  volume   =  70,
  number   =  3,
  pages    = "963--999",
  month    =  jun,
  year     =  2015
}

@ARTICLE{Beshears2015-no,
  title    = "The Effect of Providing Peer Information on Retirement Savings
              Decisions",
  author   = "Beshears, John and Choi, James J and Laibson, David and Madrian,
              Brigitte C and Milkman, Katherine L",
  abstract = "Using a field experiment in a 401(k) plan, we measure the effect
              of disseminating information about peer behavior on savings.
              Low-saving employees received simplified plan enrollment or
              contribution increase forms. A randomized subset of forms stated
              the fraction of age-matched coworkers participating in the plan
              or age-matched participants contributing at least 6\% of pay to
              the plan. We document an oppositional reaction: the presence of
              peer information decreased the savings of nonparticipants who
              were ineligible for 401(k) automatic enrollment, and higher
              observed peer savings rates also decreased savings.
              Discouragement from upward social comparisons seems to drive this
              reaction.",
  journal  = "J. Finance",
  volume   =  70,
  number   =  3,
  pages    = "1161--1120",
  month    =  jun,
  year     =  2015,
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Stevenson2007-ne,
  title    = "A comparison of the forecasting ability of {ARIMA} models",
  author   = "Stevenson, Simon",
  abstract = "Purpose -- ARIMA models have been extensively examined in the
              context of the real estate market. The purpose of this paper is
              to examine issues relating to their application in a forecasting
              context. Specifically, the paper seeks to examine whether
              in‚Äêsample measures of best‚Äêfit and also past forecasting accuracy
              bear any relation to future forecasting
              performance.Design/methodology/approach -- The forecasting
              performance of alternative ARIMA specifications are compared over
              rolling estimation and forecasting windows. The forecasting
              accuracy of the alternative specifications is compared with
              specific attention placed on the accuracy of the respective
              specification that in‚Äêsample provides the best fitting
              model.Findings -- The results highlight the limitations in using
              the conventional approach to identifying the best‚Äêspecified ARIMA
              model in sample, when the purpose of the analysis is to provide
              forecasts. The results show that while ARIMA models can be useful
              in anticipating broad market trends, there are substantial
              differences in the forecasts obtained using alternative
              specifications. The use of conventional measures of best‚Äêfit
              provide little indication as to future forecasting ability, nor
              does the forecasting performance of a specification in previous
              periods.Originality/value -- ARIMA modelling has frequently been
              highlighted as a useful forecasting approach. This paper
              illustrates that care needs to be paid in their use in a
              forecasting context and full appreciation of the strengths and
              limitations of the ARIMA approach.",
  journal  = "Journal of Property Investment \& Finance",
  volume   =  25,
  number   =  3,
  pages    = "223--240",
  year     =  2007
}

@ARTICLE{Sullivan1999-gz,
  title     = "{Data-Snooping}, Technical Trading Rule Performance, and the
               Bootstrap",
  author    = "Sullivan, Ryan and Timmermann, Allan and White, Halbert",
  abstract  = "In this paper we utilize White's Reality Check bootstrap
               methodology (White (1999)) to evaluate simple technical trading
               rules while quantifying the data-snooping bias and fully
               adjusting for its effect in the context of the full universe
               from which the trading rules were drawn. Hence, for the first
               time, the paper presents a comprehensive test of performance
               across all technical trading rules examined. We consider the
               study of Brock, Lakonishok, and LeBaron (1992), expand their
               universe of 26 trading rules, apply the rules to 100 years of
               daily data on the Dow Jones Industrial Average, and determine
               the effects of data-snooping.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishers, Inc.",
  volume    =  54,
  number    =  5,
  pages     = "1647--1691",
  month     =  oct,
  year      =  1999
}

@BOOK{Boyd2006-hx,
  title     = "Bank risk-taking and competition revisited [electronic
               resource]: new theory and new evidence",
  author    = "Boyd, John Harvey and De Nicol{\'o}, Gianni",
  abstract  = "... Thus, the banking industry's optimal portfolio choice will
               depend on the degree of competition . ... strategy relative to
               profits under the MH strategy, it is evident that bank expected
               profits ... Entrepreneurs may borrow from banks , who cannot
               observe their risk shifting choice S, but take ...",
  publisher = "International Monetary Fund",
  volume    =  06,
  year      =  2006,
  language  = "en"
}

@TECHREPORT{Cihak2007-mf,
  title     = "Cooperative Banks and Financial Stability",
  author    = "Cihak, Martin and Hesse, Heiko",
  abstract  = "Cooperative banks are an important, and growing, part of many
               financial systems. This paper empirically analyzes the role of
               cooperative banks in financial stability. Contrary to some
               suggestions in the literature, we find that cooperative banks
               are more stable than commercial banks. This finding is due to
               the lower volatility of the cooperative banks' returns, which
               more than offsets their lower profitability and capitalization.
               This is most likely due to cooperative banks' ability to use
               customer surplus as a cushion in weaker periods. We also find
               that in systems with a high presence of cooperative banks, weak
               commercial banks are less stable than they would be otherwise.
               The overall impact of a higher cooperative presence on bank
               stability is positive on average but insignificant in some
               specifications.",
  publisher = "International Monetary Fund",
  number    = "07/2",
  month     =  jan,
  year      =  2007,
  keywords  = "Commercial banks; Financial stability; Economic models; banking;
               savings banks; cooperative bank; savings bank; banking system;
               banking sector; bank size; cooperative banking; bank data; bank
               risk; return on assets; national bank; bank stability; banking
               systems; investment banking; deposit insurance; banking system
               assets; credit unions; banking activities; bank share; bank risk
               taking; depository institutions; bank risk-taking; banking
               industry; capital markets; banking sector assets; bank policy;
               financial intermediation; banking reform; retail banking; bank
               soundness; bank profitability; banking risk; regulatory
               framework; corporate banking; bank privatization; bank
               liability; bank statements; bank ownership; equity markets;
               banking stability; bank for international settlements; financial
               sector stability; cooperative banks; Cooperative Enterprises"
}

@ARTICLE{Papke1996-dz,
  title     = "Econometric Methods for Fractional Response Variables with an
               Application to 401(K) Plan Participation Rates",
  author    = "Papke, Leslie E and Wooldridge, Jeffrey M",
  abstract  = "We develop attractive functional forms and simple
               quasi-likelihood estimation methods for regression models with a
               fractional dependent variable. Compared with log-odds type
               procedures, there is no difficulty in recovering the regression
               function for the fractional variable, and there is no need to
               use ad hoc transformations to handle data at the extreme values
               of zero and one. We also offer some new, robust specification
               tests by nesting the logit or probit function in a more general
               functional form. We apply these methods to a data set of
               employee participation rates in 401(k) pension plans. Copyright
               1996 by John Wiley \& Sons, Ltd.",
  journal   = "J. Appl. Econometrics",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  11,
  number    =  6,
  pages     = "619--632",
  year      =  1996
}

@ARTICLE{Berger2014-uw,
  title    = "Do small businesses still prefer community banks?",
  author   = "Berger, Allen N and Goulding, William and Rice, Tara",
  abstract = "We formulate and test hypotheses about the role of bank type --
              small versus large, single-market versus multimarket, and local
              versus nonlocal banks -- in banking relationships. The
              conventional paradigm suggests that ``community banks'' -- small,
              single-market, local institutions -- are better able to form
              strong relationships with informationally opaque small
              businesses, while ``megabanks'' -- large, multimarket, nonlocal
              institutions -- tend to serve more transparent firms. Using the
              2003 Survey of Small Business Finance (SSBF), we conduct two sets
              of tests. First, we test for the type of bank serving as the
              ``main'' relationship bank for small businesses with different
              firm and owner characteristics. Second, we test for the strength
              of these main relationships by examining the probability of an
              exclusive relationship and main bank relationship length as
              functions of main bank type and financial fragility, as well as
              firm and owner characteristics. The results are often not
              consistent with the conventional paradigm, perhaps because of
              changes in lending technologies and deregulation of the banking
              industry.",
  journal  = "Journal of Banking \& Finance",
  volume   =  44,
  number   =  0,
  pages    = "264--278",
  month    =  jul,
  year     =  2014,
  keywords = "Banks; Relationships; Small business; Government policy"
}

@ARTICLE{noauthor_undated-iz,
  title = "Cooperative Banks and Financial Stability; Heiko Hesse and Martin
           {\v C}ih{\'a}k; {IMF} Working Paper 07/02; January 1, 2007"
}

@INCOLLECTION{Goddard2014-lz,
  title     = "Club Objectives",
  booktitle = "Handbook on the Economics of Professional Football",
  author    = "Goddard, John and Sloane, Peter",
  publisher = "Edward Elgar Publishing",
  pages     = "13--22",
  year      =  2014
}

@INCOLLECTION{Morrow2014-iz,
  title     = "Football finances",
  booktitle = "Handbook on the Economics of Professional Football",
  author    = "Morrow, Stephen and Stephen, Morrow",
  editor    = "Goddard, John and Sloane, Peter",
  publisher = "Edward Elgar Publishing",
  pages     = "80--99",
  year      =  2014
}

@INCOLLECTION{Szymanski2014-iw,
  title     = "Insolvency in English football",
  booktitle = "Handbook of the Economics of Professional Football",
  author    = "Szymanski, Stefan",
  editor    = "Dobson, Stephen and Goddard, John",
  pages     = "100--116",
  year      =  2014
}

@MISC{noauthor_undated-qt,
  title = "Handbook on the Economics of Professional Football\_ Introduction"
}

@INCOLLECTION{Owen2014-pg,
  title     = "Measurement of competitive balance and uncertainty of outcome",
  booktitle = "Handbook on the Economics of Professional Football",
  author    = "Owen, Dorian",
  editor    = "Goddard, John and Sloane, Peter",
  publisher = "Edward Elgar Publishing",
  pages     = "41--59",
  year      =  2014
}

@INCOLLECTION{Buraimo2014-ly,
  title     = "Spectator demand and attendance in English league football",
  booktitle = "Handbook on the Economics of Professional Football",
  author    = "Buraimo, Babatunde",
  editor    = "Goddard, John and Sloane, Peter",
  publisher = "Edward Elgar Publishing",
  pages     = "60--72",
  year      =  2014
}

@INCOLLECTION{Bridgewater2014-bg,
  title     = "Sponsorship and Football",
  booktitle = "Handbook on the Economics of Professional Football",
  author    = "Bridgewater, Sue",
  editor    = "Goddard, John and Sloane, Peter",
  publisher = "Edward Elgar Publishing",
  pages     = "117--129",
  year      =  2014
}

@INCOLLECTION{Goddard2014-zj,
  title     = "Introduction",
  booktitle = "Handbook on the Economics of Professional Football",
  author    = "Goddard, John and Sloane, Peter",
  editor    = "Goddard, John and Sloane, Peter",
  publisher = "Edward Elgar Publishing",
  pages     = "73--79",
  year      =  2014
}

@INCOLLECTION{Goddard2014-uy,
  title     = "The promotion and relegation system",
  booktitle = "Handbook on the Economics of Professional Football",
  author    = "Goddard, John",
  editor    = "Goddard, John and Sloane, Peter",
  publisher = "Edward Elgar Publishing",
  pages     = "23--40",
  year      =  2014
}

@TECHREPORT{Daouia2013-uz,
  title       = "Measuring firm performance using nonparametric quantile-type
                 distances",
  author      = "Daouia, Abdelaati and Simar, L{\'e}opold and Wilson, Paul W
                 and {Others}",
  institution = "Toulouse School of Economics (TSE)",
  year        =  2013
}

@TECHREPORT{Basel_Committee_on_Banking_Supervision1999-gs,
  title       = "Core Principles Methodology",
  author      = "{Basel Committee on Banking Supervision}",
  institution = "Bank of International Settlements",
  year        =  1999
}

@ARTICLE{Kesenne2015-ry,
  title     = "12 The growing gap between small-and large-country football
               teams in Europe",
  author    = "Kesenne, Stefan",
  abstract  = "It is a well-known fact that professional football clubs of
               small European countries can no longer compete with top clubs in
               large countries. The gap, both in budget and in international
               performance, has grown dramatically over the last decennia. Many
               ...",
  journal   = "Ethics and Governance in Sport: The Future of Sport Imagined",
  publisher = "Routledge",
  pages     = "113",
  year      =  2015
}

@TECHREPORT{noauthor_2015-mf,
  title       = "Global Shadow Banking Monitoring Report 2015",
  institution = "Financial Stability Board",
  year        =  2015
}

@ARTICLE{Christensen1997-mb,
  title   = "The Innovator's Dilemma: When New Technologies Cause Great Firms
             to Fail",
  author  = "Christensen, Clayton",
  journal = "Harvard Business Review Press",
  year    =  1997
}

@ARTICLE{Frick2014-bs,
  title     = "The footballers' labour market after the Bosman ruling",
  author    = "Frick, B and Simmons, R",
  abstract  = "This chapter will chart the development of the footballers '
               labour market over the last 30 years. We will contrast the
               features of the player labour market after the landmark Bosman
               ruling with the restricted labour market regime that preceded
               it. The chapter will focus on ...",
  journal   = "on the Economics of Professional Football",
  publisher = "books.google.com",
  year      =  2014
}

@ARTICLE{Tiedemann2011-yg,
  title     = "Assessing the performance of German Bundesliga football players:
               a non-parametric metafrontier approach",
  author    = "Tiedemann, Torben and Francksen, Tammo and Latacz-Lohmann, Uwe",
  abstract  = "This article presents a novel model for evaluating the
               performance of field players in football. Based upon Data
               Envelopment Analysis (DEA), we employ a non-concave metafrontier
               approach that permits estimation of players' efficiency scores
               under consideration of their playing positions. The model is
               applied to a data set of Germany's premier league football
               players covering the playing seasons 2002/03 to 2008/09. The
               results reveal a clear positive relationship between a team's
               average player efficiency score and its rank in the league table
               at the end of the season. In addition, the metafrontier approach
               is used to identify a footballer's optimal playing position in
               the team and to quantify the performance increase from moving to
               that position.",
  journal   = "CEJOR Cent. Eur. J. Oper. Res.",
  publisher = "Springer-Verlag",
  volume    =  19,
  number    =  4,
  pages     = "571--587",
  month     =  dec,
  year      =  2011,
  language  = "en"
}

@ARTICLE{Zambom-Ferraresi2015-pi,
  title     = "Performance Evaluation in the {UEFA} Champions League",
  author    = "Zambom-Ferraresi, Fab{\'\i}ola and Garc{\'\i}a-Cebri{\'a}n,
               Luc{\'\i}a Isabel and Lera-L{\'o}pez, Fernando and Ir{\'a}izoz,
               Bel{\'e}n",
  abstract  = "This article aims to evaluate the sports performance of teams
               that have participated in the Union of European Football
               Associations (UEFA) Champions League (UCL) during the last 10
               seasons (2004-2005 to 2013-2014). Technical efficiency is
               estimated using well-known data envelopment analysis (DEA)
               approaches and a bootstrapped DEA model. To solve the problem of
               measuring sporting results as output in knockout competitions,
               we propose the use of the coefficients applied by the UEFA from
               UCL revenue distribution. The results obtained show first that
               there is a high level of inefficiency in UCL over the period
               studied: Only 10\% of the teams seem to be efficient. Also, the
               teams have many problems in maintaining their efficiency during
               the seasons. Second, the champion is always efficient. Third, we
               identify two sources of inefficiency: waste of sports resources
               and the selection of sporting tactics. Finally, from a
               methodological perspective, the output measure proposed seems to
               be suitable to represent relia...",
  journal   = "J. Sports Econom.",
  publisher = "SAGE PublicationsSage CA: Los Angeles, CA",
  month     =  jun,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Keefer2015-cf,
  title     = "The {Sunk-Cost} Fallacy in the National Football League",
  author    = "Keefer, Quinn A W",
  abstract  = "The National Football League (NFL) draft is used to examine the
               presence of the sunk-cost fallacy in teams' playing time
               decisions. In the NFL, salary cap value represents a significant
               sunk cost to teams. We use the structure of the NFL draft to
               conduct a fuzzy regression discontinuity design. Optimal
               bandwidth local linear results suggest a 10\% increase in salary
               cap value yields an additional 2.7 games started, for players
               selected near the cutoff between the first two rounds. Despite
               being no more productive, the first round selections receive a
               compensation premium, which leads to them starting significantly
               more games.",
  journal   = "J. Sports Econom.",
  publisher = "SAGE PublicationsSage CA: Los Angeles, CA",
  month     =  mar,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Cooper2014-ii,
  title   = "Two-stage financial risk tolerance assessment using data
             envelopment analysis",
  author  = "Cooper, W W and Kingyens, Angela T and Paradi, Joseph C",
  journal = "Eur. J. Oper. Res.",
  volume  =  233,
  number  =  1,
  pages   = "273--280",
  month   =  feb,
  year    =  2014
}

@ARTICLE{Muniz2006-nj,
  title    = "Evaluating alternative {DEA} models used to control for
              non-discretionary inputs",
  author   = "Mu{\~n}iz, Manuel and Paradi, Joseph and Ruggiero, John and Yang,
              Zijiang",
  abstract = "Evaluation of performance using DEA requires models consistent
              with the underlying technology. There have been a number of
              models proposed for analyzing performance in the presence of
              non-discretionary inputs. Banker and Morey (Operations Research
              34 (1986) 513--521) provided the first DEA model to measure
              technical efficiency. Other single- and multiple-stage models
              that incorporate DEA have been developed. This paper discusses
              the various approaches and provides a simulation analysis to
              compare the relative performance of each.",
  journal  = "Comput. Oper. Res.",
  volume   =  33,
  number   =  5,
  pages    = "1173--1183",
  month    =  may,
  year     =  2006,
  keywords = "Non-discretionary variables; Data envelopment analysis"
}

@ARTICLE{Hadley2000-jh,
  title     = "Performance evaluation of National Football League teams",
  author    = "Hadley, Lawrence and Poitras, Marc and Ruggiero, John and
               Knowles, Scott",
  abstract  = "Most recent empirical analyses of production in the sports
               economic literature have focused on Major League Baseball. This
               paper extends that literature by analysing football production
               in the National Football League (NFL). Using the Poisson
               regression model, we measure the performance of NFL teams and
               head coaches. The measure is based on a production process where
               player skills are converted into games won. The evidence reveals
               that quality coaching is an important component in the
               production process. It appears that efficient coaching can
               account for an additional three to four victories in a given
               season. Copyright \copyright{} 2000 John Wiley \& Sons, Ltd.",
  journal   = "Manage. Decis. Econ.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  21,
  number    =  2,
  pages     = "63--70",
  month     =  mar,
  year      =  2000
}

@ARTICLE{Ruggiero2004-eo,
  title     = "Data Envelopment Analysis with Stochastic Data",
  author    = "Ruggiero, John",
  abstract  = "Data envelopment analysis (DEA) has proven to be a useful
               technique in evaluating the efficiency of decision making units
               that produce multiple-outputs using multiple-inputs. However,
               the ability to estimate efficiency reliably is hampered in the
               presence of measurement error and other statistical noise. A
               main and legitimate criticism of all deterministic models is the
               inability to separate out measurement error from inefficiency,
               both of which are unobserved. In this paper, we consider panel
               data models of efficiency estimation. One DEA model that has
               been used averages cross-sectional efficiency estimates across
               time and has been shown to work relatively well. In this paper,
               it is shown that this approach leads to biased efficiency
               estimates and provide an alternative model that corrects this
               problem. The approaches are compared using simulated data for
               illustrative purposes.",
  journal   = "J. Oper. Res. Soc.",
  publisher = "Palgrave Macmillan Journals",
  volume    =  55,
  number    =  9,
  pages     = "1008--1012",
  year      =  2004
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ivicic_undated-wm,
  title  = "Measuring Bank Insolvency Risk in {CEE} Countries",
  author = "Ivi$\mu$ci¬•c, Lana and Kunovac, Davor and Ljubaj, Igor"
}

@ARTICLE{Goddard2016-nq,
  title     = "Regulatory Change and Capital Adjustment of {US} Credit Unions",
  author    = "Goddard, John and McKillop, Donal and Wilson, John O S",
  abstract  = "We investigate the determinants of US credit union
               capital-to-assets ratios, before and after the implementation of
               the current capital adequacy regulatory framework in 2000.
               Capitalization varies pro-cyclically, and until the financial
               crisis credit unions classified as adequately capitalized or
               below followed a faster adjustment path than well capitalized
               credit unions. This pattern was reversed, however, in the
               aftermath of the crisis. The introduction of the PCA regulatory
               regime achieved a reduction in the proportion of credit unions
               classified as adequately capitalized or below that continued
               until the onset of the crisis. Since the crisis, the speed of
               recovery of credit unions in this category following an adverse
               capitalization shock was sharply reduced.",
  journal   = "J Financ Serv Res",
  publisher = "Springer US",
  volume    =  50,
  number    =  1,
  pages     = "29--55",
  month     =  aug,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Talbot2015-at,
  title     = "Can credit unions bridge the gap in lending to {SMEs}?",
  author    = "Talbot, Steve and Mac an Bhaird, Ciar{\'a}n and Whittam, Geoff",
  abstract  = "Small firms continue to experience difficulty accessing adequate
               finance from formal external sources, notwithstanding many and
               varied institutional and policy initiatives introduced to
               address this seemingly perennial problem. Underpinning research
               indicates that information asymmetry is the principal reason for
               the finance gap, particularly for young firms. The aim of
               legislation introduced in the UK in 2012 is to utilise the
               credit union sector to increase the amount of new lending to
               SMEs. The rationale for this legislative change arises because
               credit unions typically operate within a defined geographic
               region wherefrom they can compile detailed local knowledge of
               small businesses and be therefore uniquely placed to minimise
               information asymmetries thereby reducing the funding gap for
               small firms. Despite this perceived advantage, credit unions
               have been reluctant to take advantage of this legislative, and
               therefore lending by credit unions to SMEs has been negligent to
               date. We investigate the reasons for this lack of engagement in
               SME lending by interviewing the chief executives of five credit
               unions in Scotland. Our findings reveal that the CEOs of the
               credit unions are reluctant to lend to SMEs at present as they
               are uncomfortable with the level of risk associated with lending
               to a sector of which they have little experience or expertise.
               Furthermore, credit unions will need to offer attractive
               interest rates to compete with high street banks and an
               increasing number of microcredit providers. Policy makers need
               to better understand the structure and function of credit unions
               before assigning a greater role in SME lending. It is too early
               to say whether credit unions can play a significant role in SME
               lending, and our evidence suggests that structural issues must
               first be resolved before they become an established presence in
               the SME lending ecosystem.",
  journal   = "Venture Capital",
  publisher = "Routledge",
  volume    =  17,
  number    = "1-2",
  pages     = "113--128",
  month     =  jan,
  year      =  2015
}

@ARTICLE{Edirisinghe2007-ft,
  title    = "Generalized {DEA} model of fundamental analysis and its
              application to portfolio optimization",
  author   = "Edirisinghe, N C P and Zhang, X",
  abstract = "Fundamental analysis is used in asset selection for equity
              portfolio management. In this paper, a generalized data
              envelopment analysis (DEA) model is developed to analyze a firm's
              financial statements over time in order to determine a relative
              financial strength indicator (RFSI) that is predictive of firm's
              stock price returns. RFSI is based on maximizing the correlation
              between the DEA-based score of financial strength and the stock
              market performance. This maximization involves a difficult binary
              nonlinear program that requires iterative re-configuration of
              parameters of financial statements as inputs and outputs. We
              utilize a two-step heuristic algorithm that combines random
              sampling and local search optimization. The proposed approach is
              tested with 230 firms from various US technology-industries to
              determine optimized RFSI indicators for stock selection. Then,
              those selected stocks are used within portfolio optimization
              models to demonstrate the usefulness of the scheme for portfolio
              risk management.",
  journal  = "Journal of Banking \& Finance",
  volume   =  31,
  number   =  11,
  pages    = "3311--3335",
  month    =  nov,
  year     =  2007,
  keywords = "Portfolio optimization; Fundamental analysis; Relative financial
              strength; Data envelopment analysis"
}

@ARTICLE{Goddard2014-ng,
  title     = "{U.S}. {CREDIT} {UNIONS}: {SURVIVAL}, {CONSOLIDATION}, {AND}
               {GROWTH}",
  author    = "Goddard, John and Mckillop, Donal and Wilson, John O S",
  abstract  = "This study uses hazard function estimations and time-series and
               cross-sectional growth regressions to examine the impact of exit
               through merger and acquisition (M\&A) or failure, and internally
               generated growth, on the firm-size distribution within the U.S.
               credit union sector. Consolidation through M\&A was the
               principal cause of a reduction in the number of credit unions,
               but impact on concentration was small. Divergence between the
               average internally generated growth of smaller and larger credit
               unions was the principal driver of the rise in concentration.
               (JEL G21)",
  journal   = "Econ. Inq.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  52,
  number    =  1,
  pages     = "304--319",
  month     =  jan,
  year      =  2014
}

@ARTICLE{Smith2014-pq,
  title   = "{WIthstanding} a firestorm: Credit unions vs Banks",
  author  = "Smith, David M and {Woodbury}",
  journal = "Filene Research Institute",
  year    =  2014
}

@ARTICLE{Fukuyama2017-cd,
  title     = "Measuring bank performance with a dynamic network Luenberger
               indicator",
  author    = "Fukuyama, Hirofumi and Weber, William L",
  abstract  = "We construct and estimate a dynamic network Luenberger
               productivity indicator for Japanese banks during fiscal years
               2006--2012. The network aspect to the model recognizes that
               banks produce deposits in the first stage of production using
               inputs such as labor, physical capital, and equity capital and
               then in the second stage use those deposits to generate a
               portfolio of loans and securities investments. Because of
               asymmetric information between borrower and lender and
               uncertainty about the future state of the economy the second
               stage of production also generates an undesirable by-product:
               some loans become nonperforming. The dynamic aspect to the model
               recognizes that nonperforming loans generated in one period will
               typically constrain production in a subsequent period. Moreover,
               bank managers have discretion over when to transform deposits
               into the portfolio of loans and investments so that when faced
               with a high risk lending environment managers can choose to save
               some deposits as excess reserves for use in a subsequent period
               when they anticipate a more favorable lending environment.",
  journal   = "Ann. Oper. Res.",
  publisher = "Springer US",
  volume    =  250,
  number    =  1,
  pages     = "85--104",
  month     =  mar,
  year      =  2017
}

@ARTICLE{Guidara2013-xl,
  title     = "Banks' capital buffer, risk and performance in the Canadian
               banking system: Impact of business cycles and regulatory changes",
  author    = "Guidara, Alaa and Lai, Van Son and Soumar{\'e}, Issouf and
               Tchana, Fulbert Tchana",
  abstract  = "Using quarterly financial statements and stock market data from
               1982 to 2010 for the six largest Canadian chartered banks, this
               paper documents positive co-movement between Canadian banks'
               capital buffer and business cycles. The adoption of Basel
               Accords and the balance sheet leverage cap imposed by Canadian
               banking regulations did not change this cyclical behavior of
               Canadian bank capital. We find Canadian banks to be
               well-capitalized and that they hold a larger capital buffer in
               expansion than in recession, which may explain how they
               weathered the recent subprime financial crisis so well. This
               evidence that Canadian banks ride the business and regulatory
               periods underscores the appropriateness of a both micro- and a
               macro-prudential ``through-the-cycle'' approach to capital
               adequacy as advocated in the proposed Basel III framework to
               strengthen the resilience of the banking sector.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  37,
  number    =  9,
  pages     = "3373--3387",
  month     =  sep,
  year      =  2013,
  keywords  = "Capital buffer; Risk; Performance; Basel accords; Regulation;
               Business cycles; Canadian banks"
}

@ARTICLE{Gravelle2013-rc,
  title     = "Measuring systemic importance of financial institutions: An
               extreme value theory approach",
  author    = "Gravelle, Toni and Li, Fuchun",
  abstract  = "This paper proposes a set of market-based measures on the
               systemic importance of a financial institution or a group of
               financial institutions, each designed to capture different
               aspects of systemic importance of financial institutions.
               Multivariate extreme value theory approach is used to estimate
               these measures. Using six big Canadian banks as the proxy for
               Canadian banking sector, we apply these measures to identify
               systemically important banks in Canadian banking sector and
               major risk contributors from international financial
               institutions to Canadian banking sector. The empirical evidence
               reveals that (i) the top three banks, RBC Financial Group, TD
               Bank Financial Group, and Scotiabank, are more systemically
               important than other banks, while we also find that the size of
               a financial institution should not be considered as a proxy of
               systemic importance; (ii) compared to the European and Asian
               banks, the crashes of the U.S. banks, on average, are the most
               damaging to Canadian banking sector, while the risk contribution
               to the Canadian banking sector from Asian banks is quite lower
               than that from banks in the U.S. and euro area; (iii) the risk
               contribution to Canadian banking sector exhibits ``home bias'',
               that is, cross-country risk contribution tends to be smaller
               than domestic risk contribution.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  37,
  number    =  7,
  pages     = "2196--2209",
  month     =  jul,
  year      =  2013,
  keywords  = "Systemic risk; Systemic importance of Financial Institutions;
               Extreme value theory approach"
}

@ARTICLE{Beyhaghi2014-uy,
  title     = "Funding advantage and market discipline in the Canadian banking
               sector",
  author    = "Beyhaghi, Mehdi and D'Souza, Chris and Roberts, Gordon S",
  abstract  = "We employ a comprehensive data set and a variety of methods to
               provide evidence on the magnitude of large banks' funding
               advantage in Canada in addition to the extent to which market
               discipline exists across different securities issued by the
               Canadian banks. The banking sector in Canada provides a unique
               setting in which to examine market discipline along with the
               prospects of proposed reforms because Canada has no history of
               government bailouts, and an implicit government guarantee has
               been in effect consistently since the 1920s. We find that large
               banks have a funding advantage over small banks after
               controlling for bank-specific and market risk factors. Large
               banks on average pay 80 basis points and 70 basis points less,
               respectively, on their deposits and subordinated debt. Working
               with hand-collected market data on debt issues by large banks,
               we also find that market discipline exists for subordinated debt
               and not for senior debt.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  48,
  number    =  0,
  pages     = "396--410",
  month     =  nov,
  year      =  2014,
  keywords  = "Bail-in; Contingent capital; Market discipline; Funding
               advantage; Subordinated debt; Financial regulation; Bank
               resolution"
}

@ARTICLE{Liu2009-xn,
  title    = "The worst-practice {DEA} model with slack-based measurement",
  author   = "Liu, Fuh-Hwa Franklin and Chen, Cheng-Li",
  abstract = "An original data envelopment analysis (DEA) model is to evaluate
              each decision-making unit (DMU) with a set of most favorable
              weights of performance indices. The efficient DMUs obtained from
              the original DEA construct an efficient (best-practice) frontier.
              The original DEA can be considered to identify good (efficient)
              performers in the most favorable scenario. For the purpose of
              identifying bad performers such as bankrupt firms in the most
              unfavorable (worst-case) scenario, radial worst-practice frontier
              DEA (WPF--DEA) model in which the ``worst efficient'' DMUs
              construct a worst-practice frontier has been proposed. To
              identify bad performers together with the slack values we
              formulate another model called WPF--SBM. Then we develop the
              HypoSBM model to distinguish the worst performers from the bad
              ones. Finally, a solution approach is suggested to fully rank
              worst efficiencies in the worst-case scenario.",
  journal  = "Comput. Ind. Eng.",
  volume   =  57,
  number   =  2,
  pages    = "496--505",
  month    =  sep,
  year     =  2009,
  keywords = "Data envelopment analysis; Worst-case scenario; Worst-practice
              frontier; Worst efficiency; Slack-based efficiency measure"
}

@ARTICLE{Yang2009-ku,
  title     = "Assessing the performance of Canadian bank branches using data
               envelopment analysis",
  author    = "Yang, Z",
  abstract  = "This paper examines the performance of 758 branches of one big
               Canadian bank nationwide using Data Envelopment Analysis (DEA)
               from several perspectives. The system-differentiated DEA models
               used in the paper consider the systematic difference among
               different geographical areas. Five alternatives to express
               outputs are presented in order to provide complementary
               information to the bank management and further investigation of
               the cause of inefficiency when assessing bank branches is given.
               Moreover, the correlation analysis of different models is
               provided. The potential management uses of DEA results are also
               presented.",
  journal   = "J. Oper. Res. Soc.",
  publisher = "Palgrave Macmillan UK",
  volume    =  60,
  number    =  6,
  pages     = "771--780",
  month     =  jun,
  year      =  2009,
  language  = "en"
}

@PHDTHESIS{Pille_undated-mj,
  title  = "Performance Analysis of Ontario Credit Unions",
  author = "Pille, Peter",
  school = "University of Toronto"
}

@ARTICLE{Garcia-del-Barrio2009-jr,
  title     = "Goal! Profit Maximization Versus Win Maximization in Soccer",
  author    = "Garcia-del-Barrio, Pedro and Szymanski, Stefan",
  abstract  = "In this paper we estimate the best responses of soccer clubs to
               the choices of other clubs in Spanish and English leagues over
               the period 1994--2004. We find that choices are more closely
               approximated by win maximization than by profit maximization in
               both leagues. We examine club characteristics that might explain
               variations in choices between Spanish clubs.",
  journal   = "Rev Ind Organ",
  publisher = "Springer US",
  volume    =  34,
  number    =  1,
  pages     = "45--68",
  month     =  feb,
  year      =  2009,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kesenne2003-lh,
  title     = "The salary cap proposal of the G‚Äê14 in European football",
  author    = "Kesenne, Stefan",
  abstract  = "In this paper we try to show that a salary cap, as it is
               proposed by the G?14, the association of the 18 most successful
               clubsin European football, is fundamentally different from the
               salary cap as it has been introduced in some major leagues in
               the U.S. Whatever the objectives, the impact of these two types
               of salary caps on the distribution of playing, talent, which is
               the most important determinant of the competitive balance in a
               sports league,? can be very different, depending, among other
               things, on the cost structure of the large and the small market
               clubs.",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  3,
  number    =  2,
  pages     = "120--128",
  month     =  jun,
  year      =  2003
}

@ARTICLE{Kesenne2001-mi,
  title     = "The different impact of different revenue sharing systems on the
               competitive balance in professional team sports",
  author    = "Kesenne, Stefan",
  abstract  = "The aim of this contribution is to show that different systems
               of revenue sharing among sports clubs in a league can have
               differential impacts on the distribution of playing talent among
               clubs. The main conclusion is that in a profit maximizing league
               where different sharing systems exist for different categories
               of club revenue, the impact of revenue sharing on the
               competitive balance is theoretically indeterminate.",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  1,
  number    =  3,
  pages     = "210--218",
  month     =  sep,
  year      =  2001
}

@TECHREPORT{Page2016-mg,
  title       = "Systemic Risk and Shadow Banks",
  author      = "Page, Frank and Gong, Rui",
  number      = "Discussion Paper 55",
  institution = "London School of Economics",
  year        =  2016
}

@ARTICLE{Alexander1999-xk,
  title     = "Optimal hedging using cointegration",
  author    = "Alexander, Carol",
  journal   = "Philosophical Transactions of the Royal Society of London A:
               Mathematical, Physical and Engineering Sciences",
  publisher = "The Royal Society",
  volume    =  357,
  number    =  1758,
  pages     = "2039--2058",
  year      =  1999
}

@ARTICLE{Fan1996-dm,
  title     = "Semiparametric Estimation of Stochastic Production Frontier
               Models",
  author    = "Fan, Yanqin and Li, Qi and Weersink, Alfons",
  abstract  = "This article extends the linear stochastic frontier model
               proposed by Aigner, Lovell, and Schmidt to a semiparametric
               frontier model in which the functional form of the production
               frontier is unspecified and the distributions of the composite
               error terms are of known form. Pseudolikelihood estimators of
               the parameters characterizing the two error terms of the model
               are constructed based on kernel estimation of the conditional
               mean function. The Monte Carlo results show that the proposed
               estimators perform well in finite samples. An empirical
               application is presented. Extensions to a partially linear
               frontier function and to more flexible one-sided error
               distributions than the half-normal are discussed.",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  14,
  number    =  4,
  pages     = "460--468",
  year      =  1996
}

@ARTICLE{Adelino2015-bb,
  title    = "Investment Decisions of Nonprofit Firms: Evidence from Hospitals",
  author   = "Adelino, Manuel and Lewellen, Katharina and Sundaram, Anant",
  abstract = "This paper examines investment choices of nonprofit hospitals. It
              tests how shocks to cash flows caused by the performance of the
              hospitals' financial assets affect hospital expenditures. Capital
              expenditures increase, on average, by 10 to 28 cents for every
              dollar received from financial assets. The sensitivity is similar
              to that found earlier for shareholder-owned corporations.
              Executive compensation, other salaries, and perks do not respond
              significantly to cash flow shocks. Hospitals with an apparent
              tendency to overspend on medical procedures do not exhibit higher
              investment-cash flow sensitivities. The sensitivities are higher
              for hospitals that appear financially constrained.",
  journal  = "J. Finance",
  volume   =  70,
  number   =  4,
  pages    = "1583--1628",
  month    =  aug,
  year     =  2015
}

@ARTICLE{Murray1983-wt,
  title     = "Economies of Scale and Economies of Scope in Multiproduct
               Financial Institutions: A Study of British Columbia Credit
               Unions",
  author    = "Murray, John D and White, Robert W",
  abstract  = "This paper investigates the production technology facing
               computerized credit unions in Canada. A full system of translog
               cost equations is estimated in order to test for economies of
               scale, economies of scope, and other production characteristics
               in a multiproduct context. The regression results indicate that
               most of the credit unions in our sample experience significant
               increasing returns to scale as they expand their level of
               output. There is also evidence of cost complementarity or
               economies of scope in their mortgage and other lending
               activities. As a result, legislation which limits the ability of
               credit unions to grow and diversify will likely raise the
               operating costs of this important group of financial
               institutions. Additional structural tests of the most general
               translog specification suggest that none of the restrictive
               production conditions commonly imposed by other researchers
               using Cobb-Douglas and CES specifications provide a valid
               representation of credit union technology. The results of many
               earlier studies are therefore open to question.",
  journal   = "J. Finance",
  publisher = "[American Finance Association, Wiley]",
  volume    =  38,
  number    =  3,
  pages     = "887--902",
  year      =  1983
}

@ARTICLE{Mekaroonreung2012-dp,
  title    = "Estimating the shadow prices of {SO2} and {NOx} for {U.S}. coal
              power plants: A convex nonparametric least squares approach",
  author   = "Mekaroonreung, Maethee and Johnson, Andrew L",
  abstract = "Weak disposability between outputs and pollutants, defined as a
              simultaneous proportional reduction of both outputs and
              pollutants, assumes that pollutants are byproducts of the output
              generation process and that a firm can ``freely dispose'' of both
              by scaling down production levels, leaving some inputs idle.
              Based on the production axioms of monotonicity, convexity and
              weak disposability, we formulate a convex nonparametric least
              squares (CNLS) quadratic optimization problem to estimate a
              frontier production function assuming either a deterministic
              disturbance term consisting only of inefficiency, or a composite
              disturbance term composed of both inefficiency and noise. The
              suggested methodology extends the stochastic semi-nonparametric
              envelopment of data (StoNED) described in Kuosmanen and
              Kortelainen (2011). Applying the method to estimate the shadow
              prices of SO2 and NOx generated by U.S. coal power plants, we
              conclude that the weak disposability StoNED method provides more
              consistent estimates of market prices.",
  journal  = "Energy Econ.",
  volume   =  34,
  number   =  3,
  pages    = "723--732",
  month    =  may,
  year     =  2012,
  keywords = "Frontier estimation; Nonparametric regression; Parametric
              programming; Shadow pricing"
}

@ARTICLE{Smith2005-aq,
  title     = "Measuring the efficiency of public services: the limits of
               analysis",
  author    = "Smith, Peter C and Street, Andrew",
  abstract  = "Summary. Policy makers are increasingly seeking to develop
               overall measures of the effi-ciency of public service
               organizations. For that, the use of `off-the-shelf' statistical
               tools such as data envelopment analysis and stochastic frontier
               analysis have been advocated as tools to measure organizational
               efficiency. The analytical sophistication of such methods has
               reached an advanced stage of development. We discuss the context
               within which such models are deployed, their underlying
               assumptions and their usefulness for a regulator of public
               services. Four specific model building issues are discussed: the
               weights that are attached to public service outputs; the
               specification of the statistical model; the treatment of
               environmental influences on performance; the treatment of
               dynamic effects. The paper concludes with recommendations for
               policy makers and researchers on the development and use of
               efficiency measurement techniques.",
  journal   = "J. R. Stat. Soc. Ser. A Stat. Soc.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  168,
  number    =  2,
  pages     = "401--417",
  month     =  mar,
  year      =  2005,
  keywords  = "Data envelopment analysis; Efficiency measurement; Public
               policy; Public services; Stochastic frontier analysis"
}

@ARTICLE{Halvorsen1991-rn,
  title     = "A Test of the Theory of Exhaustible Resources",
  author    = "Halvorsen, Robert and Smith, Tim R",
  abstract  = "An empirical test of the theory of exhaustible resources
               requires an estimate of the time path of the shadow price of the
               unextracted resource that generally is not observable because of
               the prevalence of vertical integration in natural resource
               industries. In this paper we use duality theory to derive an
               econometric model that provides a statistical test of the theory
               of exhaustible resources. A restricted cost function is used to
               obtain estimates of the shadow prices of unextracted resources.
               The procedure is illustrated with data for the Canadian metal
               mining industry. For this industry the empirical implications of
               the theory of exhaustible resources are strongly rejected.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  106,
  number    =  1,
  pages     = "123--140",
  year      =  1991
}

@ARTICLE{Atkinson2005-qn,
  title     = "Multiple Comparisons with the Best: Bayesian Precision Measures
               of Efficiency Rankings",
  author    = "Atkinson, Scott E and Dorfman, Jeffrey H",
  abstract  = "A large literature measures the allocative and technical
               efficiency of a set of firms using econometric techniques to
               estimate stochastic production frontiers or distance functions.
               Typically, researchers compute only the precision of individual
               efficiency rankings. Recently, Horrace and Schmidt (Journal of
               Applied Economics 15, 1--26, 2000) have applied sampling
               theoretic statistical techniques known as multiple comparisons
               with a control (MCC) and multiple comparisons with the best
               (MCB) to make statistical comparisons of efficiency rankings. As
               an alternative, this paper offers a Bayesian multiple comparison
               procedure that we argue is simpler to implement, gives the
               researcher increased flexibility over the type of comparison,
               and provides greater, and more intuitive, information content.
               For these methods and a parametric bootstrap technique, we carry
               out multiple comparisons of technical efficiency rankings for a
               set of U.S. electric generating firms, estimated using a
               distance function framework. We find that the Bayesian method
               provides substantially more precise inferences than obtained
               using the MCB and MCC methods.",
  journal   = "J Prod Anal",
  publisher = "Kluwer Academic Publishers",
  volume    =  23,
  number    =  3,
  pages     = "359--382",
  month     =  jul,
  year      =  2005,
  language  = "en"
}

@ARTICLE{Griffin2007-ex,
  title     = "Bayesian stochastic frontier analysis using {WinBUGS}",
  author    = "Griffin, Jim E and Steel, Mark F J",
  abstract  = "Markov chain Monte Carlo (MCMC) methods have become a ubiquitous
               tool in Bayesian analysis. This paper implements MCMC methods
               for Bayesian analysis of stochastic frontier models using the
               WinBUGS package, a freely available software. General code for
               cross-sectional and panel data are presented and various ways of
               summarizing posterior inference are discussed. Several examples
               illustrate that analyses with models of genuine practical
               interest can be performed straightforwardly and model changes
               are easily implemented. Although WinBUGS may not be that
               efficient for more complicated models, it does make Bayesian
               inference with stochastic frontier models easily accessible for
               applied researchers and its generic structure allows for a lot
               of flexibility in model specification.",
  journal   = "J Prod Anal",
  publisher = "Kluwer Academic Publishers-Plenum Publishers",
  volume    =  27,
  number    =  3,
  pages     = "163--176",
  month     =  jun,
  year      =  2007,
  language  = "en"
}

@ARTICLE{ODonnell2016-ym,
  title    = "Using information about technologies, markets and firm behaviour
              to decompose a proper productivity index",
  author   = "O'Donnell, C J",
  abstract = "This paper uses distance functions to define new output and input
              quantity indexes that satisfy important axioms from index number
              theory (e.g., identity, transitivity, proportionality and
              time-space reversal). Dividing the output index by the input
              index yields a new productivity index that can be decomposed into
              a measure of technical change, a measure of environmental change,
              and several measures of efficiency change. A problem with this
              new index is that it cannot be computed without estimating the
              production frontier. The paper shows how assumptions concerning
              technologies, markets and firm behaviour can be used to inform
              the estimation process. The focus is on the asymptotic properties
              of least squares estimators when the explanatory variables in the
              production frontier model are endogenous. In this case, the
              ordinary least squares estimator is usually inconsistent.
              However, there is one situation where it is super-consistent. A
              fully-modified ordinary least squares estimator is also available
              in this case. To illustrate the main ideas, the paper uses US
              state-level farm data to estimate a stochastic production
              frontier. The parameter estimates are then used to obtain
              estimates of the economically-relevant components of productivity
              change.",
  journal  = "J. Econom.",
  volume   =  190,
  number   =  2,
  pages    = "328--340",
  month    =  feb,
  year     =  2016
}

@ARTICLE{Zott2011-pg,
  title     = "The Business Model: Recent Developments and Future Research",
  author    = "Zott, Christoph and Amit, Raphael and Massa, Lorenzo",
  abstract  = "This article provides a broad and multifaceted review of the
               received literature on business models in which the authors
               examine the business model concept through multiple
               subject-matter lenses. The review reveals that scholars do not
               agree on what a business model is and that the literature is
               developing largely in silos, according to the phenomena of
               interest of the respective researchers. However, the authors
               also found emerging common themes among scholars of business
               models. Specifically, (1) the business model is emerging as a
               new unit of analysis; (2) business models emphasize a
               system-level, holistic approach to explaining how firms ``do
               business''; (3) firm activities play an important role in the
               various conceptualizations of business models that have been
               proposed; and (4) business models seek to explain how value is
               created, not just how it is captured. These emerging themes
               could serve as catalysts for a more unified study of business
               models.",
  journal   = "J. Manage.",
  publisher = "SAGE PublicationsSage CA: Los Angeles, CA",
  volume    =  37,
  number    =  4,
  pages     = "1019--1042",
  month     =  may,
  year      =  2011,
  language  = "en"
}

@ARTICLE{Ferri2014-li,
  title    = "Does bank ownership affect lending behavior? Evidence from the
              Euro area",
  author   = "Ferri, Giovanni and Kalmi, Panu and Kerola, Eeva",
  abstract = "We analyze the differences in lending policies across banks
              characterized by different types of ownership, using micro-level
              data on Euro area banks during the period 1999--2011 to detect
              possible variations in bank lending supply responses to changes
              in monetary policy. Our results identify a general difference
              between stakeholder and shareholder banks: following a monetary
              policy contraction, stakeholder banks decrease their loan supply
              to a lesser extent than shareholder banks. A detailed analysis of
              the effect among stakeholder banks reveals that cooperative banks
              continued to smooth the impact of tighter monetary policy on
              their lending during the crisis period (2008--2011), whereas
              savings banks did not. Stakeholder banks' propensity to smooth
              their lending cycles suggests that their presence in the economy
              has the potential to reduce credit supply volatility.",
  journal  = "Journal of Banking \& Finance",
  volume   =  48,
  pages    = "194--209",
  month    =  nov,
  year     =  2014,
  keywords = "European banks; Monetary policy transmission; Commercial banks;
              Savings banks; Cooperative banks; Lending cyclicality"
}

@ARTICLE{Cross2013-hx,
  title    = "Valuing Vineyards: A Directional Distance Function Approach",
  author   = "Cross, Robin and F{\"a}re, Rolf and Grosskopf, Shawna and Weber,
              William L",
  journal  = "Journal of Wine Economics",
  volume   =  8,
  number   =  01,
  pages    = "69--82",
  year     =  2013,
  keywords = "Hedonic prices, directional distance function"
}

@ARTICLE{Birkhauser_undated-kf,
  title  = "Investor Presence and Competition in Major European football
            Leagues",
  author = "Birkh{\"a}user, Stephan and Kaserer, Christoph and Urban, Daniel"
}

@ARTICLE{Szymanski2014-ic,
  title     = "Fair is Foul: A Critical Analysis of {UEFA} Financial Fair Play",
  author    = "Szymanski, Stefan",
  abstract  = "This paper critically analyzes the rules of the UEFA financial
               regulatory system for football clubs known as Financial Fair
               Play (FFP). I argue that the objectives of FFP are not really
               fairness but financial efficiency and that the rules are
               unlikely to achieve efficiency. I also contend that even from
               the perspective of fairness, the rules do little more than
               substitute one form of inequality for another. Finally I briefly
               assess the implications for the competition law challenge that
               was launched in May 2013 against the FFP breakeven rule.",
  journal   = "Int. J. Sports Financ.",
  publisher = "Fitness Information Technology",
  volume    =  9,
  number    =  3,
  pages     = "218--229",
  year      =  2014,
  keywords  = "UEFA; financial fair play; football"
}

@ARTICLE{Luenberger1992-jv,
  title    = "Benefit functions and duality",
  author   = "Luenberger, David G",
  abstract = "This paper studies a new representation of individual preferences
              termed the benefit function. The benefit function b(g; x,u)
              measures the amount that an individual is willing to trade, in
              terms of a specific reference commodity bundle g, for the
              opportunity to move from utility level u to a consumption bundle
              x. The benefit function is therefore a generalization of the
              willingness-to-pay concept. This paper studies properties of this
              function, including its continuity and structural properties and
              its indirect relation to the underlying utility function. A very
              important property of the benefit function is that it is the
              natural precursor of the expenditure function, in the sense that
              the expenditure function is a (special) dual of the benefit
              function. This duality is shown to be complete by proving that
              when appropriate convexity properties hold, the (correspondingly
              special) dual of the expenditure function is, in fact, the
              benefit function. The duality makes the benefit function a
              powerful tool for analysis of welfare issues.",
  journal  = "J. Math. Econ.",
  volume   =  21,
  number   =  5,
  pages    = "461--481",
  month    =  jan,
  year     =  1992
}

@ARTICLE{Kuosmanen2015-wt,
  title    = "Orthogonality Conditions for Identification of Joint Production
              Technologies: Axiomatic Nonparametric Approach to the Estimation
              of Stochastic Distance Functions",
  author   = "Kuosmanen, Timo and Johnson, Andrew L and Parmeter, Christopher",
  abstract = "The regression residual is commonly used as a productivity
              indicator. However, the observed input demands are endogenous if
              rational managers adjust their input",
  month    =  may,
  year     =  2015,
  keywords = "Economies of scope, Efficiency analysis, Endogeneity, Frontier
              estimation, Simultaneity bias, Productivity measurement"
}

@ARTICLE{Lee2013-kv,
  title    = "A more efficient algorithm for Convex Nonparametric Least Squares",
  author   = "Lee, Chia-Yen and Johnson, Andrew L and Moreno-Centeno, Erick and
              Kuosmanen, Timo",
  abstract = "Convex Nonparametric Least Squares (CNLSs) is a nonparametric
              regression method that does not require a priori specification of
              the functional form. The CNLS problem is solved by mathematical
              programming techniques; however, since the CNLS problem size
              grows quadratically as a function of the number of observations,
              standard quadratic programming (QP) and Nonlinear Programming
              (NLP) algorithms are inadequate for handling large samples, and
              the computational burdens become significant even for relatively
              small samples. This study proposes a generic algorithm that
              improves the computational performance in small samples and is
              able to solve problems that are currently unattainable. A Monte
              Carlo simulation is performed to evaluate the performance of six
              variants of the proposed algorithm. These experimental results
              indicate that the most effective variant can be identified given
              the sample size and the dimensionality. The computational
              benefits of the new algorithm are demonstrated by an empirical
              application that proved insurmountable for the standard QP and
              NLP algorithms.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  227,
  number   =  2,
  pages    = "391--400",
  month    =  jun,
  year     =  2013,
  keywords = "Convex Nonparametric Least Squares; Frontier estimation;
              Productive efficiency analysis; Model reduction; Computational
              complexity"
}

@ARTICLE{Henningsen2009-me,
  title     = "Imposing regional monotonicity on translog stochastic production
               frontiers with a simple three-step procedure",
  author    = "Henningsen, Arne and {Christian H C}",
  abstract  = "We show that the monotonicity condition is conceptually
               important in Stochastic Frontier Analysis (SFA). Despite its
               importance, most empirical studies do not impose
               monotonicity---probably because existing approaches are rather
               complex and laborious. Therefore, we propose a three-step
               procedure that is much simpler than existing approaches. We
               demonstrate how monotonicity of a translog function can be
               imposed regionally at a connected set (region) of input
               quantities. Our method can be applied not only to impose
               monotonicity on translog production frontiers but also to impose
               other restrictions on cost, distance, or profit frontiers.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  32,
  number    =  3,
  pages     = "217",
  month     =  dec,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Shehzad2015-wz,
  title    = "Supervisory powers and bank risk taking",
  author   = "Shehzad, Choudhry Tanveer and De Haan, Jakob",
  abstract = "We examine the effect of different types of bank supervisory
              powers in place before the crisis on bank risk-taking during the
              crisis. We employ data of more than 8000 banks from high-income
              OECD countries for the 2007--2011 period and impaired loans to
              gross loans ratio as proxy for bank risk-taking. Our
              Hausman--Taylor estimates indicate that the powers of bank
              supervisors to shake up the organizational structure of banks are
              more effective than powers to issue monetary penalties. Our
              results also suggest that supervisory powers do not affect
              risk-taking behavior of systemically important banks.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  39,
  pages    = "15--24",
  month    =  nov,
  year     =  2015,
  keywords = "Bank supervision; Bank regulation; Financial soundness; Financial
              fragility"
}

@ARTICLE{Fare2014-eq,
  title     = "Pricing Nonmarketed Outputs with an Application to Community
               Colleges",
  author    = "F{\"a}re, Rolf and Grosskopf, Shawna and Weber, William L",
  abstract  = "We use the directional output distance function in cost space to
               recover absolute prices of nonmarket goods such that the ratio
               of prices equals the marginal rate of transformation of outputs
               in production. As such, the method is quite useful for valuing
               nonmarketed goods such as those produced by government entities
               such as the National Park Service, public schools and
               universities, and municipalities. We provide an empirical
               illustration of our method using fourteen Missouri community
               colleges that produced full-time equivalent students,
               certificates, associate's degrees, and real spending on public
               service and auxiliary enterprises during the period 2005--2006
               to 2011--2012.",
  journal   = "Public Finance Review",
  publisher = "SAGE PublicationsSage CA: Los Angeles, CA",
  month     =  oct,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Ayadi2015-hz,
  title   = "Regulations of European Banks and Business Models: Towards a new
             paradigm",
  author  = "Ayadi, R and Arbak, M and Greon,W,P, De",
  journal = "CEPS",
  year    =  2015
}

@ARTICLE{Curi2015-fv,
  title    = "Foreign bank diversification and efficiency prior to and during
              the financial crisis: Does one business model fit all?",
  author   = "Curi, Claudia and Lozano-Vivas, Ana and Zelenyuk, Valentin",
  abstract = "Diversified and focused business models may affect foreign bank
              efficiency differently. We investigate whether there is an
              optimal business model along three business dimensions---assets,
              funding and income---and which business model is optimal for
              foreign banks in a financial center. We apply recently developed
              non-parametric methods with bootstrap to estimate group
              efficiency, to test for differences across groups and finally to
              analyze the link between bank efficiency and diversification
              measures. Using Luxembourg bank data that include the financial
              crisis, we find that there is no unique business model. The most
              efficient business model appears to be a focused asset, funding
              and income strategy. Banks' organizational forms play a role;
              branches may be preferable to subsidiaries prior to the financial
              crisis, whereas bank subsidiaries perform better than branches
              during the financial crisis. However, branches diversified in
              assets, funding and income exploit efficiency advantages during
              the financial crisis.",
  journal  = "Journal of Banking \& Finance",
  volume   =  61,
  pages    = "S22--S35",
  month    =  dec,
  year     =  2015,
  keywords = "Foreign banks; Organizational form; Asset, funding and income
              diversification; Financial crisis; DEA group-efficiency;
              Heterogeneous bootstrap"
}

@TECHREPORT{noauthor_2012-wk,
  title       = "Report of the Commission on Credit Unions",
  institution = "Irish Finance Department",
  month       =  mar,
  year        =  2012
}

@TECHREPORT{noauthor_2013-du,
  title       = "Consultation on the Introduction of a Tiered Regulatory
                 Approach for Credit Unions",
  number      = " CP 76",
  institution = "Central Bank of Ireland",
  month       =  dec,
  year        =  2013
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Parmeter2014-pn,
  title     = "Efficiency Analysis: A Primer on Recent Advances1",
  author    = "Parmeter, Christopher F and Kumbhakar, Subal C",
  abstract  = "... EFFICIENCY ANALYSIS ... estimates, aside from ÃÇ$\beta$0 are
               robust to distri- butional assumptions as the OLS estimator is
               semiparametrically efficient when no ... After the model
               parameters are estimated, we can proceed to estimate
               observation-specific efficiency , which is one of the ...",
  journal   = "Foundations and Trends (R) in Econometrics",
  publisher = "now publishers",
  volume    =  7,
  number    = "3-4",
  pages     = "191--385",
  year      =  2014
}

@ARTICLE{Barros2007-jd,
  title    = "Analysing the determinants of performance of best and worst
              European banks: A mixed logit approach",
  author   = "Barros, Carlos Pestana and Ferreira, Candida and Williams,
              Jonathan",
  abstract = "Using a dataset of 7635 observations on 1384 commercial banks
              operating in the EU between 1993 and 2001, we utilise a mixed
              logit model to identify factors that explain the probability of a
              bank being a best [worst] performer. The empirical evidence
              confirms the importance of country-level characteristics
              (location and legal tradition), and firm-level features (bank
              ownership, balance sheet structure and size). Specifically,
              smaller sized banks with higher loan-intensity, and foreign banks
              from countries upholding common law traditions have a higher
              probability of best performance.",
  journal  = "Journal of Banking \& Finance",
  volume   =  31,
  number   =  7,
  pages    = "2189--2203",
  month    =  jul,
  year     =  2007,
  keywords = "Bank performance; Mixed Logit model; Managerial implications;
              Legal tradition; Foreign banks; Efficiency; Deregulation;
              Integration"
}

@ARTICLE{Bos2007-ag,
  title   = "Is there a single frontier in a single European banking market ?",
  author  = "Bos, J W B and Schmeidel, H",
  journal = "Journal of Banking \& Finance",
  volume  =  31,
  pages   = "2081--2012",
  year    =  2007
}

@ARTICLE{Boot2000-qu,
  title     = "Can Relationship Banking Survive Competition?",
  author    = "Boot, Arnoud W A and Thakor, Anjan V",
  abstract  = "How will banks evolve as competition increases from other banks
               and from the capital market? Will banks become more like capital
               market underwriters and offer passive transaction loans or
               return to their roots as relationship lending experts? These are
               the questions we address. Our key result is that as interbank
               competition increases, banks make more relationship loans, but
               each has lower added value for borrowers. Capital market
               competition reduces relationship lending (and bank lending
               shrinks), but each relationship loan has greater added value for
               borrowers. In both cases, welfare increases for some borrowers
               but not necessarily for all.",
  journal   = "J. Finance",
  publisher = "Wiley for the American Finance Association",
  volume    =  55,
  number    =  2,
  pages     = "679--713",
  month     =  apr,
  year      =  2000
}

@ARTICLE{Thakor2013-tm,
  title    = "The Economics of Higher Purpose",
  author   = "Thakor, Anjan V and Quinn, Robert E",
  abstract = "We develop a theory of how the intersection of business goals and
              the pursuit of ``higher purpose'' --- something that produces a
              non-pecuniary social ben",
  month    =  dec,
  year     =  2013,
  keywords = "Higher purpose, budget constraints, wealth maximization"
}

@ARTICLE{Altunbas2011-kp,
  title   = "Bank risk-taking during the financial crisis: Do Business Models
             matter ?",
  author  = "Altunbas, Yener and Manganelli, Simone and Marques-Ibanez, David",
  journal = "European Central Bank Working Paper Series",
  volume  = "No. 1394",
  year    =  2011
}

@ARTICLE{Calem1999-sd,
  title    = "The Impact of {Capital-Based} Regulation on Bank {Risk-Taking}",
  author   = "Calem, Paul and Rob, Rafael",
  abstract = "In this paper we model the dynamic portfolio choice problem
              facing banks, calibrate the model using empirical data from the
              banking industry for 1984--1993, and assess quantitatively the
              impact of recent regulatory developments related to bank capital.
              The model implies a U-shaped relationship between capital and
              risk-taking: As a bank's capital increases it first takes less
              risk, then more risk. A deposit insurance premium surcharge on
              undercapitalized banks induces them to take more risk. An
              increased capital requirement, whether flat or risk-based, tends
              to induce more risk-taking by ex-ante well-capitalized banks that
              comply with the new standard. Journal of Economic Literature
              Classification Numbers: G20, G28.",
  journal  = "Journal of Financial Intermediation",
  volume   =  8,
  number   =  4,
  pages    = "317--352",
  month    =  oct,
  year     =  1999
}

@ARTICLE{Ayadi2014-gz,
  title   = "Banking Business Models Monitor 2014 Europe",
  author  = "Ayadi, Rym and DeGreon, Willem Pieter",
  journal = "Centre for European Policy Studies",
  year    =  2014
}

@ARTICLE{Birchall2013-mo,
  title   = "Resilience in a downturn: the power of financial cooperatives",
  author  = "Birchall, J",
  journal = "Geneva, International Labour Organisation",
  year    =  2013
}

@BOOK{Croteau1963-az,
  title     = "The economics of the credit union",
  author    = "Croteau, John Tougas",
  publisher = "Wayne State University Press",
  year      =  1963
}

@ARTICLE{Thakor2015-jl,
  title    = "Corporate Culture in Banking",
  author   = "Thakor, Anjan V",
  abstract = "In light of the erosion of public trust in banking and the
              massive fines imposed on banks since the 2007-09 financial
              crisis, the issue of culture in banking as",
  month    =  feb,
  year     =  2015,
  keywords = "corporate culture, banking stability, regulation, corporate
              governance"
}

@ARTICLE{Lucey2014-iz,
  title     = "Learning from the Irish Experience -- A Clinical Case Study in
               Banking Failure",
  author    = "Lucey, Brian M and Larkin, Charles and Gurdgiev, Constantin",
  abstract  = "We present a review of the Irish banking collapse, detailing its
               origins in a confluence of events. We suggest that the very
               concentrated nature of the Irish banking sector, which will
               emerge from the policy decisions taken as a consequence of the
               collapse, runs a risk of a second crisis. We survey the
               literature on size and efficiency and suggest some alternative
               policy approaches.",
  journal   = "Comp. Econ. Stud.",
  publisher = "Palgrave Macmillan UK",
  volume    =  56,
  number    =  2,
  pages     = "295--312",
  month     =  jun,
  year      =  2014,
  language  = "en"
}

@BOOK{Pasiouras2013-iy,
  title     = "Efficiency and Productivity Growth: Modelling in the Financial
               Services Industry",
  author    = "Pasiouras, Fotios",
  abstract  = "An authoritative introduction to efficiency and productivity
               analysis with applications in both the banking and finance
               industry In light of the recent global financial crisis, several
               studies have examined the efficiency of financial institutions.
               A number of open questions remain and this book reviews recent
               issues and state-of-the-art techniques in the assessment of the
               efficiency and productivity of financial institutions. Written
               by an international team of experts, the first part of the book
               links efficiency with a variety of topics like Latin American
               banking, market discipline and governance, economics of scale,
               off-balance-sheet activities, productivity of foreign banks,
               mergers and acquisitions, and mutual fund ratings. The second
               part of the book compares existing techniques and
               state-of-the-art techniques in the bank efficiency literature,
               including among others, network data envelopment analysis and
               quantile regression. The book is suitable for academics and
               professionals as well as postgraduate research students working
               in banking and finance. Efficiency and Productivity Growth:
               Provides an authoritative introduction to efficiency and
               productivity analysis with applications in both the banking and
               mutual funds industry such as efficiency of Asian banks,
               cooperatives and not-for-profit credit associations. Explores
               contemporary research issues in the area of efficiency and
               productivity measurement in the financial sector. Evaluates the
               most suitable approaches to selecting inputs and outputs as well
               as selecting the most efficient techniques, such as parametric
               and non-parametric, to estimate the models.",
  publisher = "John Wiley \& Sons",
  month     =  mar,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Affinito2010-zh,
  title    = "Why do (or did?) banks securitize their loans? Evidence from
              Italy",
  author   = "Affinito, Massimiliano and Tagliaferri, Edoardo",
  abstract = "This paper investigates the ex ante determinants of bank loan
              securitization by using different econometric methods on Italian
              individual bank data from 2000 to 2006. Our results show that
              bank loan securitization is a composite decision. Banks that are
              less capitalized, less profitable, less liquid and burdened with
              troubled loans are more likely to perform securitization, for a
              larger amount and earlier.",
  journal  = "Journal of Financial Stability",
  volume   =  6,
  number   =  4,
  pages    = "189--202",
  month    =  dec,
  year     =  2010,
  keywords = "Originate-to-distribute (O\&D); Loan; Securitization"
}

@ARTICLE{Carbo-Valverde2015-oo,
  title    = "The impact of securitization on credit rationing: Empirical
              evidence",
  author   = "Carbo-Valverde, Santiago and Degryse, Hans and
              Rodr{\'\i}guez-Fern{\'a}ndez, Francisco",
  abstract = "We study whether banks' involvement into different types of
              securitization activity -- asset backed securities (ABS) and
              covered bonds -- in Spain influences credit supply before and
              during the financial crisis. While both ABS and covered bonds
              were hit by the crisis, the former were hit more severely.
              Employing a disequilibrium model to identify credit rationing, we
              find that firms with banks that were more involved in
              securitization see their credit constraints more relaxed in
              normal periods. In contrast, only greater covered bonds issuance
              reduces credit rationing during crisis periods whereas ABS
              aggravates these firms' credit rationing in crisis periods. Our
              results are in line with the theoretical predictions that a
              securitization instrument that retains risk (covered bond) may
              induce a more prudent risk behavior of banks than an instrument
              that provides risk transferring (ABS).",
  journal  = "Journal of Financial Stability",
  volume   =  20,
  pages    = "36--50",
  month    =  oct,
  year     =  2015,
  keywords = "Financial crisis; Securitization; Credit rationing"
}

@ARTICLE{Kiff2014-rw,
  title    = "A shot at regulating securitization",
  author   = "Kiff, John and Kisser, Michael",
  abstract = "In order to incentivize stronger issuer due diligence effort,
              European and U.S. authorities are amending securitization-related
              regulations to force issuers to retain an economic interest in
              the securitization products they issue. This paper contributes to
              the process by exploring the economics of equity and mezzanine
              tranche retention in the context of systemic risk, moral hazard,
              accounting frictions and funding distortions. It shows that loan
              screening activity is maximized when the loan originating bank
              retains the equity tranche. However, in case capital structure
              irrelevance does not hold a profit maximizing bank is likely to
              favor retention of the less risky mezzanine tranche. From a
              regulator's perspective this is a problem because the implied
              loan screening activity is substantially lower in this case.
              Policy attention is even more warranted if performing due
              diligence is costly, the economic outlook is positive or loan
              profitability is high.",
  journal  = "Journal of Financial Stability",
  volume   =  10,
  pages    = "32--49",
  month    =  feb,
  year     =  2014,
  keywords = "Financial crisis; Securitization; Moral hazard; Regulation"
}

@ARTICLE{Diamond1991-de,
  title     = "Monitoring and Reputation: The Choice between Bank Loans and
               Directly Placed Debt",
  author    = "Diamond, Douglas W",
  abstract  = "This paper determines when a debt contract will be monitored by
               lenders. This is the choice between borrowing directly (issuing
               a bond, without monitoring) and borrowing through a bank that
               monitors to alleviate moral hazard. This provides a theory of
               bank loan demand and of the role of monitoring in circumstances
               in which reputation effects are important. A key result is that
               borrowers with credit ratings toward the middle of the spectrum
               rely on bank loans, and in periods of high interest rates or low
               future profitability, higher-rated borrowers choose to borrow
               from banks.",
  journal   = "J. Polit. Econ.",
  publisher = "University of Chicago Press",
  volume    =  99,
  number    =  4,
  pages     = "689--721",
  year      =  1991
}

@ARTICLE{Diamond1984-rw,
  title     = "Financial Intermediation and Delegated Monitoring",
  author    = "Diamond, Douglas W",
  abstract  = "This paper develops a theory of financial intermediation based
               on minimizing the cost of monitoring information which is useful
               for resolving incentive problems between borrowers and lenders.
               It presents a characterization of the costs of providing
               incentives for delegated monitoring by a financial intermediary.
               Diversification within an intermediary serves to reduce these
               costs, even in a risk neutral economy. The paper presents some
               more general analysis of the effect of diversification on
               resolving incentive problems. In the environment assumed in the
               model, debt contracts with costly bankruptcy are shown to be
               optimal. The analysis has implications for the portfolio
               structure and capital structure of intermediaries.",
  journal   = "Rev. Econ. Stud.",
  publisher = "Oxford University Press",
  volume    =  51,
  number    =  3,
  pages     = "393--414",
  month     =  jul,
  year      =  1984
}

@ARTICLE{Winton1995-lv,
  title     = "Delegated Monitoring and Bank Structure in a Finite Economy",
  author    = "Winton, Andrew",
  abstract  = "When banks act as delegated monitors of borrowing firms in a
               finite economy, two factors help banks dominate direct lending:
               portfolio diversification, which increases with bank size, and
               bank capitalization, which diminishes with size. With free entry
               into banking, intermediated equilibria are possible even when
               direct lending cannot overcome autarky. There are usually
               multiple intermediated equilibria; these may not be
               Pareto-ranked by bank size, since smaller banks are better
               captialized and may Pareto-dominate larger banks. Even when one
               large bank would be most efficient, assigning a monopoly bank
               charter to coordinate beliefs on the single bank equilibrium may
               be unattractive: in some cases, a monopoly bank cannot overcome
               autarky even though free-entry banking can, and in other cases,
               the monopoly bank reduces production from the direct lending
               level. Journal of Economic Literature Classification Numbers:
               G21, L13, O16.",
  journal   = "Journal of Financial Intermediation",
  publisher = "Elsevier",
  volume    =  4,
  number    =  2,
  pages     = "158--187",
  month     =  apr,
  year      =  1995
}

@ARTICLE{Diamond1983-sv,
  title     = "Bank Runs, Deposit Insurance, and Liquidity",
  author    = "Diamond, Douglas W and Dybvig, Philip H",
  abstract  = "This paper shows that bank deposit contracts can provide
               allocations superior to those of exchange markets, offering an
               explanation of how banks subject to runs can attract deposits.
               Investors face privately observed risks which lead to a demand
               for liquidity. Traditional demand deposit contracts which
               provide liquidity have multiple equilibria, one of which is a
               bank run. Bank runs in the model cause real economic damage,
               rather than simply reflecting other problems. Contracts which
               can prevent runs are studied, and the analysis shows that there
               are circumstances when government provision of deposit insurance
               can produce superior contracts.",
  journal   = "J. Polit. Econ.",
  publisher = "University of Chicago Press",
  volume    =  91,
  number    =  3,
  pages     = "401--419",
  year      =  1983
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bhattacharya1995-ke,
  title     = "Proprietary Information, Financial Intermediation, and Research
               Incentives",
  author    = "Bhattacharya, Sudipto and Chiesa, Gabriella",
  abstract  = "We contrast equilibria in loan markets with bilateral
               bank-borrower ties, in which proprietary technological knowledge
               of borrowers is not revealed to product market competitors, with
               equilibria under multilateral financing in which such knowledge
               may be shared among competing borrowing firms. Using each of
               these two institutional arrangements, we examine the conditions
               for existence of equilibrium, its ex ante optimality, and
               borrowing firms‚Ä≤ incentives to engage in private costly
               research. Also explored is the potential for lending banks to
               coordinate postinvention collusion in product markets by
               multiple inventing firms. Journal of Economic Literature
               Classification Numbers: 310, 312, 314.",
  journal   = "Journal of Financial Intermediation",
  publisher = "Elsevier",
  volume    =  4,
  number    =  4,
  pages     = "328--357",
  month     =  oct,
  year      =  1995
}

@ARTICLE{Yosha1995-sh,
  title     = "Information Disclosure Costs and the Choice of Financing Source",
  author    = "Yosha, Oved",
  abstract  = "Small- and medium-size, high quality, entrepreneurial firms may
               prefer bilateral to multilateral financing arrangements, in
               order to avoid disclosure of private information which might
               leak to competitors. In the presence of a cost differential
               between these forms of financing, the higher quality firms
               (those with more to lose from disclosure) prefer bilateral
               financing. The cost differential prevents competitors from
               unambiguously inferring that these firms are hiding information.
               Journal of Economic Literature Classification Numbers: D82, G21,
               G32, K22.",
  journal   = "Journal of Financial Intermediation",
  publisher = "Elsevier",
  volume    =  4,
  number    =  1,
  pages     = "3--20",
  month     =  jan,
  year      =  1995
}

@TECHREPORT{Berk2012-tn,
  title       = "Measuring managerial skill in the mutual fund industry",
  author      = "Berk, Jonathan B and Van Binsbergen, Jules H",
  institution = "National Bureau of Economic Research",
  year        =  2012
}

@BOOK{Quinn1994-uf,
  title     = "Credit unions in Ireland",
  author    = "Quinn, Anthony P",
  publisher = "Dublin: Oak Tree Press",
  year      =  1994
}

@UNPUBLISHED{Peeters2015-yh,
  title  = "Matching and Winning? The Impact of Upper and Middle Managers on
            Team Performance",
  author = "Peeters, Thomas",
  year   =  2015
}

@ARTICLE{Acharya2012-jh,
  title    = "Capital Shortfall: A New Approach to Ranking and Regulating
              Systemic Risks",
  author   = "Acharya, Viral and Engle, Robert and Richardson, Matthew",
  journal  = "Am. Econ. Rev.",
  volume   =  102,
  number   =  3,
  pages    = "59--64",
  month    =  may,
  year     =  2012
}

@ARTICLE{Des_Vereins_fur_Socialpolitik_2015_Okonomische_Entwicklung-Theorie_und_Politik-_Session_Finance_undated-pk,
  title  = "Regional Bank Efficiency and its Effect on Regional Growth in
            Normal and Bad Times",
  author = "des Vereins f{\"u}r Socialpolitik 2015: {\"O}konomische
            Entwicklung-Theorie und Politik - Session: Finance, Beitr{\"a}ge
            Zur Jahrestagung and Banks, I I and F09-V, No"
}

@TECHREPORT{Klomp2011-tx,
  title     = "Banking risk and regulation: Does one size fit all?",
  author    = "Klomp, Jeroen and de Haan, Jakob",
  abstract  = "Using data for more than 200 banks from 21 OECD countries for
               the period 2002 to 2008, we examine the impact of bank
               regulation and supervision on banking risk using quantile
               regressions. In contrast to most previous research, we find that
               banking regulation and supervision has an effect on the risks of
               high-risk banks. However, most measures for bank regulation and
               supervision do not have a significant effect on low-risk banks.
               As banking risk and bank regulation and supervision are
               multifaceted concepts, our measures for both concepts are
               constructed using factor analysis.",
  publisher = "Netherlands Central Bank, Research Department",
  number    =  323,
  month     =  nov,
  year      =  2011,
  keywords  = "Financial soundness; Bank regulation and supervision; Banking
               risk; Quantile regression"
}

@BOOK{Barth2012-ex,
  title     = "Guardians of Finance: Making Regulators Work for Us",
  author    = "Barth, James R and Caprio, Gerard and Levine, Ross",
  abstract  = "The recent financial crisis was an accident, a ``perfect storm''
               fueled by an unforeseeable confluence of events that
               unfortunately combined to bring down the global financial
               systems. Or at least this is the story told and retold by a
               chorus of luminaries that includes Timothy Geithner, Henry
               Paulson, Robert Rubin, Ben Bernanke, and Alan Greenspan. In
               Guardians of Finance, economists James Barth, Gerard Caprio, and
               Ross Levine argue that the financial meltdown of 2007 to 2009
               was no accident; it was negligent homicide. They show that
               senior regulatory officials around the world knew or should have
               known that their policies were destabilizing the global
               financial system and yet chose not to act until the crisis had
               fully emerged.Barth, Caprio, and Levine propose a reform to
               counter this systemic failure: the establishment of a
               ``Sentinel'' to provide an informed, expert, and independent
               assessment of financial regulation. Its sole power would be to
               demand information and to evaluate it from the perspective of
               the public--rather than that of the financial industry, the
               regulators, or politicians.",
  publisher = "MIT Press",
  month     =  feb,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Hofheimer_undated-gx,
  title  = "Innovation in the Payment Ecosystem",
  author = "Hofheimer, George"
}

@ARTICLE{Scully1974-ua,
  title     = "Pay and Performance in Major League Baseball",
  author    = "Scully, Gerald W",
  abstract  = "By Gerald W Scully; Pay and Performance in Major League Baseball",
  journal   = "Am. Econ. Rev.",
  publisher = "American Economic Association",
  volume    =  64,
  number    =  6,
  pages     = "915--930",
  year      =  1974
}

@ARTICLE{Dimitropoulos2012-rz,
  title    = "Financial Performance and Corporate Governance in the European
              Football Industry",
  author   = "Dimitropoulos, Panagiotis E and Tsagkanos, Athanasios",
  journal  = "International Journal of Sports Finance",
  number   =  2007,
  pages    = "280--308",
  year     =  2012,
  keywords = "corporate governance; financial performance; football clubs;
              profitability and viability; uefa financial fair play"
}

@ARTICLE{Lee2008-nn,
  title     = "A {RE-EXAMINATION} {OF} {PRODUCTION} {FUNCTIONS} {AND}
               {EFFICIENCY} {ESTIMATES} {FOR} {THE} {NATIONAL} {BASKETBALL}
               {ASSOCIATION}",
  author    = "Lee, Young Hoon and Berri, David",
  abstract  = "This paper seeks to re-examine the issue of estimating team
               efficiency for sports teams via an application of data from the
               National Basketball Association. This paper argues that the
               inputs the coaches allocate are the players the team employs.
               Therefore, this paper employs a measure of playing talent in
               modeling team production. Unlike previous studies, which only
               employed one measure of playing talent, we employ measures of
               guards, small forwards and big men in a study of basketball.
               This paper also argues that the time-varying stochastic frontier
               models with the identical temporal pattern assumption such as
               Lee and Schmidt and Battese and Coelli cannot be used in the
               analysis of team efficiency in sports. The evidence we present
               shows by hypothesis test that this argument holds.",
  journal   = "Scott. J. Polit. Econ.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  55,
  number    =  1,
  pages     = "51--66",
  month     =  feb,
  year      =  2008,
  keywords  = "M11; M21; L83; C23"
}

@ARTICLE{Tulkens1995-ve,
  title    = "Non-parametric efficiency, progress and regress measures for
              panel data: Methodological aspects",
  author   = "Tulkens, Henry and Vanden Eeckaut, Philippe",
  abstract = "This purely methodological paper deals with the r{\^o}le of time
              in non-parametric efficiency analysis. Using both FDH and DEA
              technologies, it first shows how each observation in a panel can
              be characterized in efficiency terms vis-{\`a}-vis three
              different kinds of frontiers: (i) `contemporaneous', (ii)
              `sequential', and (iii) `intertemporal'. These are then compared
              with window analysis. Next, frontier shifts `outward' and
              `inward', interpreted as progress or regress are considered for
              the two kinds of technologies, and computational methods are
              described in detail for evaluating such shifts in either case.
              These are also contrasted with what is measured by the
              `Malmquist' productivity index. Finally, an alternative way of
              identifying progress and regress, independent of the frontier
              notion and referring instead to some `benchmark' notion, is
              extended here to panel data.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  80,
  number   =  3,
  pages    = "474--499",
  month    =  feb,
  year     =  1995
}

@TECHREPORT{Goth2012-zt,
  title       = "Corporate Governance in Canadian and {US} Credit Unions",
  author      = "Goth, Peter and McKillop, Donal and Wilson, John",
  institution = "Filene Research Institute, Centre for Credit Union Research
                 Madison Wisconsin",
  year        =  2012
}

@ARTICLE{Becchetti2015-bd,
  title    = "Corporate social responsibility, stakeholder risk, and
              idiosyncratic volatility",
  author   = "Becchetti, Leonardo and Ciciretti, Rocco and Hasan, Iftekhar",
  abstract = "Idiosyncratic volatility (IV) is a measure of firm specific
              information that is correlated with lower stock returns. We
              explore the nexus between IV and corporate social responsibility
              (CSR) and document that IV is positively correlated with
              aggregate CSR and is negatively correlated with a CSR-specific
              (stakeholder) risk factor. Our findings are consistent with the
              view that CSR reduces flexibility in responding to productive
              shocks via the reduction of stakeholder well-being, thereby
              producing the combined effect of making earnings less predictable
              and reducing exposure to risk of conflicts with stakeholders.",
  journal  = "Journal of Corporate Finance",
  volume   =  35,
  pages    = "297--309",
  month    =  dec,
  year     =  2015,
  keywords = "idiosyncratic volatility; corporate social responsibility;
              stakeholder risk"
}

@ARTICLE{Ellul2013-if,
  title    = "Stronger Risk Controls, Lower Risk: Evidence from {U.S}. Bank
              Holding Companies",
  author   = "Ellul, Andrew and Yerramilli, Vijay",
  abstract = "We construct a risk management index (RMI) to measure the
              strength and independence of the risk management function at bank
              holding companies (BHCs). The U.S. BHCs with higher RMI before
              the onset of the financial crisis have lower tail risk, lower
              nonperforming loans, and better operating and stock return
              performance during the financial crisis years. Over the period
              1995 to 2010, BHCs with a higher lagged RMI have lower tail risk
              and higher return on assets, all else equal. Overall, these
              results suggest that a strong and independent risk management
              function can curtail tail risk exposures at banks.",
  journal  = "J. Finance",
  volume   =  68,
  number   =  5,
  pages    = "1757--1803",
  month    =  oct,
  year     =  2013
}

@TECHREPORT{Arjani2013-sl,
  title       = "Lessons from the financial crisis: bank performance and
                 regulatory reform",
  author      = "Arjani, Neville and Paulin, Graydon",
  institution = "Bank of Canada Discussion Paper",
  year        =  2013
}

@TECHREPORT{Beltratti2009-xw,
  title       = "Why Did Some Banks Perform Better During the Credit Crisis? A
                 {Cross-Country} Study of the Impact of Governance and
                 Regulation",
  author      = "Beltratti, Andrea and Stulz, Ren{\'e} M",
  number      = "w15180",
  institution = "National Bureau of Economic Research",
  month       =  jul,
  year        =  2009
}

@ARTICLE{Ratnovski2009-cz,
  title   = "Why Are Canadian Banks More Resilient?",
  author  = "Ratnovski, L and Huang, R",
  journal = "IMF Working Paper 09/152",
  year    =  2009
}

@TECHREPORT{McDonald2012-ny,
  title       = "The Big Banks' Big Secret: Estimating government support for
                 Canadian banks during the financial crisis",
  author      = "McDonald, D",
  institution = "Canadian Center for Policy Alternatives",
  month       =  apr,
  year        =  2012
}

@ARTICLE{Schmidt1984-vr,
  title     = "Production Frontiers and Panel Data",
  author    = "Schmidt, Peter and Sickles, Robin C",
  abstract  = "1. INTRODUCTION consistently. We can consistently estimate the
               (whole) error term for a given observation, but it contains This
               article considers estimation ofa stochasticfion- statistical
               noise as well as technical inefficiency. The tier production
               function-the type ...",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  2,
  number    =  4,
  pages     = "367--374",
  month     =  oct,
  year      =  1984
}

@ARTICLE{Wang2002-dn,
  title     = "{One-Step} and {Two-Step} Estimation of the Effects of Exogenous
               Variables on Technical Efficiency Levels",
  author    = "Wang, Hung-Jen and Schmidt, Peter",
  abstract  = "Consider a stochastic frontier model with one-sided inefficiency
               u, and suppose that the scale of u depends on some variables
               (firm characteristics) z. A ``one-step'' model specifies both
               the stochastic frontier and the way in which u depends on z, and
               can be estimated in a single step, for example by maximum
               likelihood. This is in contrast to a ``two-step'' procedure,
               where the first step is to estimate a standard stochastic
               frontier model, and the second step is to estimate the
               relationship between (estimated) u and z.In this paper we
               propose a class of one-step models based on the ``scaling
               property'' that u equals a function of z times a one-sided error
               u* whose distribution does not depend on z. We explain
               theoretically why two-step procedures are biased, and we present
               Monte Carlo evidence showing that the bias can be very severe.
               This evidence argues strongly for one-step models whenever one
               is interested in the effects of firm characteristics on
               efficiency levels.",
  journal   = "Journal of Productivity Analysis",
  publisher = "Kluwer Academic Publishers",
  volume    =  18,
  number    =  2,
  pages     = "129--144",
  month     =  sep,
  year      =  2002,
  language  = "en"
}

@ARTICLE{Khetrapal2014-pm,
  title   = "A Review of Benchmarking Approaches for Productivity and
             Efficiency Measurement in Electricity Distribution Sector",
  author  = "Khetrapal, Pavan and Thakur, Tripta",
  journal = "IJEEE",
  pages   = "214--221",
  year    =  2014
}

@ARTICLE{Bos2007-nu,
  title     = "Is there a single frontier in a single European banking market?",
  author    = "Bos, J W B and Schmiedel, H",
  abstract  = "This paper attempts to estimate comparable efficiency scores for
               European banks operating in the Single Market in the EU. Using a
               data set of more than 5000 large commercial banks from all major
               European banking markets over the period 1993--2004, the
               application of meta-frontiers enables us to assess the existence
               of a single and integrated European banking market. We find
               evidence in favor of a single European banking market
               characterized by cost and profit meta-frontiers. However,
               compared to the meta-frontier estimations, pooled frontier
               estimations tend to underestimate efficiency levels and
               correlate poorly with country-specific frontier efficiency
               ranks.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  31,
  number    =  7,
  pages     = "2081--2102",
  month     =  jul,
  year      =  2007,
  keywords  = "X-efficiency; Stochastic frontiers; Banking; Meta-frontiers;
               Technology gap ratios; -efficiency"
}

@TECHREPORT{Bisias2012-ql,
  title       = "A survey of systemic risk analytics",
  author      = "Bisias, Dimitrios and Flood, Mark D and Lo, Andrew W and
                 Valavanis, Stavros",
  institution = "Office of Financial Research, US Treasury Department",
  year        =  2012
}

@TECHREPORT{Brunnermeier2012-rh,
  title       = "Bubbles, Financial Crises, and Systemic Risk",
  author      = "Brunnermeier, Markus and Oehmke, Martin",
  publisher   = "National Bureau of Economic Research",
  institution = "National Bureau of Economic Research",
  month       =  sep,
  year        =  2012,
  address     = "Cambridge, MA"
}

@ARTICLE{Olesen2016-bu,
  title    = "Stochastic Data Envelopment {Analysis---A} review",
  author   = "Olesen, Ole B and Petersen, Niels Christian",
  abstract = "This paper provides a review of stochastic Data Envelopment
              Analysis (DEA). We discuss extensions of deterministic DEA in
              three directions: (i) deviations from the deterministic frontier
              are modeled as stochastic variables, (ii) random noise in terms
              of measurement errors, sample noise, and specification errors is
              made an integral part of the model, and (iii) the frontier is
              stochastic as is the underlying Production Possibility Set (PPS).
              Stochastic DEA utilizes non-parametric convex or conical hull
              reference technologies based upon axioms from production theory
              accompanied by a statistical foundation in terms of axioms from
              statistics or distributional assumptions. The approaches allow
              for an estimation of stochastic inefficiency compared to a
              deterministic or a stochastic PPS and for statistical inference
              while maintaining an axiomatic foundation. Focus is on bridges
              and differences between approaches within the field of Stochastic
              DEA including semi-parametric Stochastic Frontier Analysis (SFA)
              and Chance Constrained DEA (CCDEA). We argue that statistical
              inference based upon homogenous bootstrapping in contrast to a
              management science approach imposes a restrictive structure on
              inefficiency, which may not facilitate the communication of
              results of the analysis to decision makers. Semi-parametric SFA
              and CCDEA differ w.r.t. the modeling of noise and stochastic
              inefficiency. The two approaches are in spite of the inherent
              differences shown to be complements in the sense that the
              stochastic PPSs obtained by the two approaches share basic
              similarities in the case of one output and multiple inputs.
              Recent contributions related to (i) disentangling of random noise
              and random inefficiency and (ii) obtaining smooth shape
              constrained estimators of the frontier are discussed.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  251,
  number   =  1,
  pages    = "2--21",
  month    =  may,
  year     =  2016,
  keywords = "Data Envelopment Analysis; Stochastic DEA; Stochastic frontier
              analysis; Stochastic production possibility sets; Review of
              stochastic DEA"
}

@ARTICLE{Kuosmanen2009-iz,
  title     = "Neoclassical versus Frontier Production Models? Testing for the
               Skewness of Regression Residuals*",
  author    = "Kuosmanen, Timo and Fosgerau, Mogens",
  abstract  = "The empirical literature on production and cost functions is
               divided into two strands. The neoclassical approach concentrates
               on model parameters, while the frontier approach decomposes the
               disturbance term to a symmetric noise term and a positively
               skewed inefficiency term. We propose a theoretical justification
               for the skewness of the inefficiency term, arguing that this
               skewness is the key testable hypothesis of the frontier
               approach. We propose to test the regression residuals for
               skewness in order to distinguish the two competing approaches.
               Our test builds directly upon the asymmetry of regression
               residuals and does not require any prior distributional
               assumptions.",
  journal   = "Scand. J. Econ.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  111,
  number    =  2,
  pages     = "351--367",
  month     =  jun,
  year      =  2009,
  keywords  = "Firms and production; frontier estimation; hypothesis testing;
               production function; productive efficiency analysis; C12; C14;
               D24"
}

@INCOLLECTION{Parmeter2013-uq,
  title     = "Smooth Constrained Frontier Analysis",
  booktitle = "Recent Advances and Future Directions in Causality, Prediction,
               and Specification Analysis",
  author    = "Parmeter, Christopher F and Racine, Jeffrey S",
  abstract  = "Production frontiers (i.e., ``production functions'') specify
               the maximum output of firms, industries, or economies as a
               function of their inputs. A variety of innovative methods have
               been proposed for estimating both ``deterministic'' and
               ``stochastic'' frontiers. However, existing approaches are
               either parametric in nature, rely on nonsmooth nonparametric
               methods, or rely on nonparametric or semiparametric methods that
               ignore theoretical axioms of production theory, each of which
               can be problematic. In this chapter we propose a class of smooth
               constrained nonparametric and semiparametric frontier estimators
               that may be particularly appealing to practitioners who require
               smooth (i.e., continuously differentiable) estimates that, in
               addition, are consistent with theoretical axioms of production.",
  publisher = "Springer, New York, NY",
  pages     = "463--488",
  year      =  2013,
  language  = "en"
}

@ARTICLE{Barros2015-qp,
  title     = "The Brazilian Soccer Championship: an efficiency analysis",
  author    = "Barros, Carlos Pestana and Wanke, Peter and Figueiredo,
               Ot{\'a}vio",
  abstract  = "This article analyses the top Brazilian football league from
               2003 to 2011 by estimating a cost function and using a
               stochastic frontier model. Among the covariates, the number of
               fans per club and club remoteness is taken into account. The
               Brazilian clubs are then ranked according to their technical
               efficiency during the 2000?2011 period. Based on the results,
               the policy implication is presented, and the economic
               implications arising from the study are also considered.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  47,
  number    =  9,
  pages     = "906--915",
  month     =  feb,
  year      =  2015
}

@ARTICLE{Barros2016-gc,
  title     = "A performance assessment of the Angolan soccer league",
  author    = "Barros, C P and Figueiredo, O H dos S and Dumbo, Silvestre",
  abstract  = "ABSTRACTThis article analyses the technical efficiency of the
               Angolan soccer league from 2008 to 2014, using a translog
               distance stochastic frontier model. The Greene stochastic
               frontier model, presented in 2005, and Kumbhakar stochastic
               frontier model, presented in 1990, are adopted, and the
               covariates used include Luanda location, funding by the oil
               company Sonangol, club supported by rich fans and club relegated
               during the period. Policy implications are then derived.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  48,
  number    =  29,
  pages     = "2711--2720",
  month     =  jun,
  year      =  2016
}

@ARTICLE{Levinsohn2003-fd,
  title     = "Estimating Production Functions Using Inputs to Control for
               Unobservables",
  author    = "Levinsohn, James and Petrin, Amil",
  abstract  = "We add to the methods for conditioning out serially correlated
               unobserved shocks to the production technology. We build on
               ideas first developed in Olley and Pakes (1996). They show how
               to use investment to control for correlation between input
               levels and the unobserved firm-specific productivity process. We
               show that intermediate inputs (those inputs which are typically
               subtracted out in a value-added production function) can also
               solve this simultaneity problem. We discuss some theoretical
               benefits of extending the proxy choice set in this direction and
               our empirical results suggest these benefits can be important.",
  journal   = "Rev. Econ. Stud.",
  publisher = "[Oxford University Press, Review of Economic Studies, Ltd.]",
  volume    =  70,
  number    =  2,
  pages     = "317--341",
  year      =  2003
}

@ARTICLE{Cihak2010-qd,
  title     = "Islamic Banks and Financial Stability: An Empirical Analysis",
  author    = "{\v C}ih{\'a}k, Martin and Hesse, Heiko",
  abstract  = "The relative financial strength of Islamic banks is assessed
               empirically based on evidence covering individual Islamic and
               commercial banks in 19 banking systems with a substantial
               presence of Islamic banking. We find that (a) small Islamic
               banks tend to be financially stronger than small commercial
               banks; (b) large commercial banks tend to be financially
               stronger than large Islamic banks; and (c) small Islamic banks
               tend to be financially stronger than large Islamic banks, which
               may reflect challenges of credit risk management in large
               Islamic banks. We also find that the market share of Islamic
               banks does not have a significant impact on the financial
               strength of other banks.",
  journal   = "J Financ Serv Res",
  publisher = "Springer US",
  volume    =  38,
  number    = "2-3",
  pages     = "95--113",
  month     =  dec,
  year      =  2010,
  keywords  = "Islamic banking, Financial stability"
}

@ARTICLE{Devereux2008-fj,
  title     = "Do countries compete over corporate tax rates?",
  author    = "Devereux, Michael P and Lockwood, Ben and Redoano, Michela",
  abstract  = "This paper investigates whether OECD countries compete with each
               other over corporation taxes, and whether such competition can
               explain the fall in statutory tax rates in the 1980s and 1990s.
               We develop a model in which multinational firms choose their
               capital stock in response to an effective marginal tax rate
               (EMTR), and simultaneously choose the location of their profit
               in response to differences in statutory tax rates. Governments
               engage in two-dimensional tax competition: they simultaneously
               compete over EMTRs for capital and over statutory rates for
               profit. We estimate the parameters of their reaction functions
               using data from 21 countries between 1982 and 1999. We find
               evidence that countries compete over both measures, and
               moreover, that the estimated slopes of reaction functions are
               consistent with our theoretical predictions. We find that --
               consistent with our model, but not some other forms of
               competition -- evidence of strategic interaction is present only
               between open economies (i.e. those without capital controls in
               place). The Nash equilibrium average statutory rates implied by
               the empirical model fall substantially over the period, in line
               with falls in actual statutory rates. The reductions in
               equilibrium tax rates can be explained almost entirely by more
               intense competition generated by the relaxation of capital
               controls.",
  journal   = "J. Public Econ.",
  publisher = "Elsevier",
  volume    =  92,
  number    =  5,
  pages     = "1210--1235",
  month     =  jun,
  year      =  2008,
  keywords  = "Tax competition; Corporate taxes; Transfer pricing"
}

@ARTICLE{Smith1981-yz,
  title     = "{CREDIT} {UNIONS}: An Economic Theory of a Credit Union",
  author    = "Smith, Donald J and Cargill, Thomas F and Meyer, Robert A",
  abstract  = "... two principal characteristics of CUs that prevent this:
               First, in a CU (and cooperatives in general ... CU would need a
               (subjective) distribution function for TDM and choose a value
               for parameter ... Taylor, R. ``The Credit Union as a Cooperative
               Institution.'' Review of Social Economy 29 ...",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  36,
  number    =  2,
  pages     = "519--528",
  month     =  may,
  year      =  1981
}

@ARTICLE{Koot1978-fm,
  title     = "On Economies of Scale in Credit Unions",
  author    = "Koot, Ronald S",
  abstract  = "... returns to scale may ameliorate these potential conflicts
               and help preserve the cooperative nature of ... 103-106], a
               ``high value for PART could reflect a credit union that offers
               exceptionally ... Useable data were obtained from a sample for
               380 credit unions on their 1976operations. ...",
  journal   = "J. Finance",
  publisher = "[American Finance Association, Wiley]",
  volume    =  33,
  number    =  4,
  pages     = "1087--1094",
  year      =  1978
}

@ARTICLE{Posnett1996-zt,
  title    = "Indirect cost in economic evaluation: the opportunity cost of
              unpaid inputs",
  author   = "Posnett, J and Jan, S",
  abstract = "Unpaid time represents a potentially significant input into the
              health production function. The paper sets out the basis for
              valuation of time inputs consistent with the notion of
              opportunity cost. Such analysis requires consideration of whether
              time displaced in the production of health involves lost work or
              lost leisure. Furthermore, because valuation of opportunity cost
              requires the consistent treatment of costs and benefits, the
              study also considers the valuation of outputs. The basis for
              valuing the shadow price of work time is examined by firstly
              assuming perfect competition. The analysis then considers the
              presence of monopoly and monopsony in product markets and income
              and sales taxes. The basis for valuing the shadow price of
              leisure (``leisure' being all uses of time except paid
              employment) is restricted to an examination of methods previously
              used to value unpaid housework. The two methods examined are the
              replacement cost and the opportunity cost method. As the methods
              are not equivalent, the circumstances where each is appropriate
              vary depending on whether the output lost in producing health is
              replaced. Although not set out as the primary focus of the paper,
              the issues surrounding the valuation of outputs generated by
              non-market and quasi-market activity are examined. In particular,
              where activities such as informal care result in indirect utility
              to the carers (and patients) themselves, it is likely the full
              market wage provides a lower bound estimate of the value of
              marginal benefit. Finally the paper provides a practical approach
              to examining opportunity cost of unpaid inputs consistent with
              the concepts set out in preceding sections.",
  journal  = "Health Econ.",
  volume   =  5,
  number   =  1,
  pages    = "13--23",
  month    =  jan,
  year     =  1996,
  language = "en"
}

@TECHREPORT{noauthor_2010-xg,
  title       = "A Case for the Reduction in Corporation Tax in Northern
                 Ireland",
  institution = "Economic Reform Group of Northern Ireland",
  year        =  2010
}

@TECHREPORT{noauthor_2013-bv,
  title       = "Analysis of the Dynamic Effects of Corporation Tax Reduction",
  institution = "HM Treasury",
  year        =  2013
}

@TECHREPORT{noauthor_2010-hl,
  title       = "A comparison of the Varney Review and the {ERGNI} Report on
                 Corporation Tax Reform",
  institution = "NI Assembly Discussion Paper",
  year        =  2010
}

@ARTICLE{Gelman2007-cx,
  title     = "Struggles with Survey Weighting and Regression Modeling",
  author    = "Gelman, Andrew",
  abstract  = "The general principles of Bayesian data analysis imply that
               models for survey responses should be constructed conditional on
               all variables that affect the probability of inclusion and
               nonresponse, which are also the variables used in survey
               weighting and clustering. However, such models can quickly
               become very complicated, with potentially thousands of
               poststratification cells. It is then a challenge to develop
               general families of multilevel probability models that yield
               reasonable Bayesian inferences. We discuss in the context of
               several ongoing public health and social surveys. This work is
               currently open-ended, and we conclude with thoughts on how
               research could proceed to solve these problems.",
  journal   = "Stat. Sci.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  22,
  number    =  2,
  pages     = "153--164",
  year      =  2007,
  keywords  = "Multilevel modeling; poststratification; sampling weights;
               shrinkage"
}

@ARTICLE{Nakagawa2007-ot,
  title     = "Effect size, confidence interval and statistical significance: a
               practical guide for biologists",
  author    = "Nakagawa, Shinichi and Cuthill, Innes C",
  abstract  = "Null hypothesis significance testing (NHST) is the dominant
               statistical approach in biology, although it has many,
               frequently unappreciated, problems. Most importantly, NHST does
               not provide us with two crucial pieces of information: (1) the
               magnitude of an effect of interest, and (2) the precision of the
               estimate of the magnitude of that effect. All biologists should
               be ultimately interested in biological importance, which may be
               assessed using the magnitude of an effect, but not its
               statistical significance. Therefore, we advocate presentation of
               measures of the magnitude of effects (i.e. effect size
               statistics) and their confidence intervals (CIs) in all
               biological journals. Combined use of an effect size and its CIs
               enables one to assess the relationships within data more
               effectively than the use of p values, regardless of statistical
               significance. In addition, routine presentation of effect sizes
               will encourage researchers to view their results in the context
               of previous research and facilitate the incorporation of results
               into future meta-analysis, which has been increasingly used as
               the standard method of quantitative review in biology. In this
               article, we extensively discuss two dimensionless (and thus
               standardised) classes of effect size statistics: d statistics
               (standardised mean difference) and r statistics (correlation
               coefficient), because these can be calculated from almost all
               study designs and also because their calculations are essential
               for meta-analysis. However, our focus on these standardised
               effect size statistics does not mean unstandardised effect size
               statistics (e.g. mean difference and regression coefficient) are
               less important. We provide potential solutions for four main
               technical problems researchers may encounter when calculating
               effect size and CIs: (1) when covariates exist, (2) when bias in
               estimating effect size is possible, (3) when data have
               non-normal error structure and/or variances, and (4) when data
               are non-independent. Although interpretations of effect sizes
               are often difficult, we provide some pointers to help
               researchers. This paper serves both as a beginner's instruction
               manual and a stimulus for changing statistical practice for the
               better in the biological sciences.",
  journal   = "Biol. Rev. Camb. Philos. Soc.",
  publisher = "Wiley Online Library",
  volume    =  82,
  number    =  4,
  pages     = "591--605",
  month     =  nov,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Coval2005-du,
  title     = "Do Behavioral Biases Affect Prices?",
  author    = "Coval, Joshua D and Shumway, Tyler",
  abstract  = "This paper documents strong evidence for behavioral biases among
               Chicago Board of Trade proprietary traders and investigates the
               effect these biases have on prices. Our traders appear highly
               loss-averse, regularly assuming above-average afternoon risk to
               recover from morning losses. This behavior has important
               short-term consequences for afternoon prices, as losing traders
               actively purchase contracts at higher prices and sell contracts
               at lower prices than those that prevailed previously. However,
               the market appears to distinguish these risk-seeking trades from
               informed trading. Prices set by loss-averse traders are reversed
               significantly more quickly than those set by unbiased traders.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing, Inc.",
  volume    =  60,
  number    =  1,
  pages     = "1--34",
  month     =  feb,
  year      =  2005
}

@ARTICLE{Fellner2007-nj,
  title     = "Risk attitude and market behavior: Evidence from experimental
               asset markets",
  author    = "Fellner, Gerlinde and Maciejovsky, Boris",
  abstract  = "In this paper we relate individual risk attitude as elicited by
               binary lottery choices to market behavior. By analyzing 26
               independent experimental markets with a total of 280
               participants, we show that binary lottery choices are
               systematically correlated with market behavior: the higher the
               degree of risk aversion the lower the observed market activity.
               Our results also uncover gender differences in risk attitude,
               which moderate market behavior. We find that women are more risk
               averse than men, submit fewer offers, and engage less often in
               trades.",
  journal   = "J. Econ. Psychol.",
  publisher = "Elsevier",
  volume    =  28,
  number    =  3,
  pages     = "338--350",
  month     =  jun,
  year      =  2007,
  keywords  = "Individual risk attitude; Binary lottery choices; Experimental
               asset markets; Gender differences"
}

@ARTICLE{Ayadi2011-ai,
  title    = "Business Models in European Banking: A Pre-and {Post-Crisis}
              Screening",
  author   = "Ayadi, Rym and Arbak, Emrah and Pieter De Groen, Willem",
  abstract = "The next few years will be critical for Europe's banking industry
              as it faces a number of financial sector reforms that will have a
              decisive impact on the domin",
  journal  = "Centre for European Policy Studies",
  month    =  sep,
  year     =  2011,
  keywords = "crisis, financial crisis, banking industry, Europe, financial
              sector, reform, business, stability, risk, efficiency"
}

@ARTICLE{Timmermann2004-gi,
  title    = "Efficient market hypothesis and forecasting",
  author   = "Timmermann, Allan and Granger, Clive W J",
  abstract = "The efficient market hypothesis gives rise to forecasting tests
              that mirror those adopted when testing the optimality of a
              forecast in the context of a given information set. However,
              there are also important differences arising from the fact that
              market efficiency tests rely on establishing profitable trading
              opportunities in `real time'. Forecasters constantly search for
              predictable patterns and affect prices when they attempt to
              exploit trading opportunities. Stable forecasting patterns are
              therefore unlikely to persist for long periods of time and will
              self-destruct when discovered by a large number of investors.
              This gives rise to non-stationarities in the time series of
              financial returns and complicates both formal tests of market
              efficiency and the search for successful forecasting approaches.",
  journal  = "Int. J. Forecast.",
  volume   =  20,
  number   =  1,
  pages    = "15--27",
  month    =  jan,
  year     =  2004,
  keywords = "Efficient market hypothesis; Forecast evaluation; Model
              specification; Learning"
}

@ARTICLE{Jones2012-fp,
  title     = "Economies of Scale Versus Participation: A {Co-Operative}
               Dilemma?",
  author    = "Jones, Derek C and Kalmi, Panu",
  abstract  = "We examine the proposition that, in co-operatives, the need for
               democracy must clash with efficiency demands. To shed light on
               diverse issues surrounding this c",
  journal   = "Journal of Entrepreneurial and Organizational",
  publisher = "papers.ssrn.com",
  month     =  dec,
  year      =  2012,
  keywords  = "co-operatives, democracy, efficiency, corporate governance,
               Mondragon, Finland"
}

@ARTICLE{Akinsoyinu2015-qd,
  title     = "Efficiency Evaluation of European Financial Cooperative Sector.
               A Data Envelopment Analysis Approach",
  author    = "Akinsoyinu, Clements Adeyinka and {Others}",
  abstract  = "Abstract The co-operative banking sector has become a major
               force in the socio-economic development of the European
               continent's and is increasingly becoming an important part of
               the continents banking sector. By applying the Data Envelopment
               Analysis (DEA) approach, this paper ...",
  journal   = "International Journal of Academic Research in Accounting,
               Finance and Management Sciences",
  publisher = "Human Resource Management Academic Research Society,
               International Journal of Academic Research in Accounting,
               Finance and Management Sciences",
  volume    =  5,
  number    =  4,
  pages     = "11--21",
  year      =  2015
}

@ARTICLE{Nawrocki2001-yq,
  title     = "The Problems with Monte Carlo Simulation",
  author    = "Nawrocki, David",
  abstract  = "Abstract Monte Carlo simulation has enjoyed a resurgence in
               financial literature in recent years. This paper explores the
               reasons why implementing Monte Carlo simulation is very
               difficult at best and can lead to incorrect decisions at worst.
               The problem is that the typical ...",
  journal   = "Journal of Financial Planning",
  publisher = "search.ebscohost.com",
  volume    =  14,
  number    =  11,
  year      =  2001
}

@ARTICLE{Bos2015-ux,
  title    = "Practice What You Preach: Microfinance Business Models and
              Operational Efficiency",
  author   = "Bos, Jaap W B and Millone, Matteo",
  abstract = "The microfinance sector has room for pure for-profit microfinance
              institutions (MFIs), non-profit organizations, and ``social''
              for-profit firms that aim to pursue a double bottom line.
              Depending on their business model, these institutions target
              different types of borrowers, change the size of their loans and
              adjust their loan pricing. We introduce a simple approach that
              accommodates a wide range of business models and allows us to
              estimate the operational efficiency of MFIs. Our empirical
              results show that MFIs with a high depth of outreach are most
              efficient, resulting in higher levels of outreach and profits for
              the same input mix.",
  journal  = "World Dev.",
  volume   =  70,
  pages    = "28--42",
  month    =  jun,
  year     =  2015,
  keywords = "microfinance; output distance function; outreach; efficiency;
              global"
}

@ARTICLE{Chatelain2012-yw,
  title     = "The failure of financial macroeconomics and what to do about it",
  author    = "Chatelain, Jean-Bernard and Ralf, Kirsten",
  journal   = "Manchester Sch. Econ. Soc. Stud.",
  publisher = "Wiley Online Library",
  volume    =  80,
  number    = "s1",
  pages     = "21--53",
  year      =  2012
}

@ARTICLE{Clark2002-ja,
  title     = "{X-Efficiency} in Banking: Looking beyond the Balance Sheet",
  author    = "Clark, Jeffrey A and Siems, Thomas F",
  abstract  = "The distribution free and stochastic frontier estimation methods
               are used to derive bank specific measures of cost and profit
               X-efficiency. This is done to investigate the importance of
               including aggregate measures of off-balance-sheet (OBS)
               activities. The results indicate that economic cost and
               production cost X-efficiency estimates increase with the
               inclusion of the OBS measure. Profit X-efficiency estimates are
               largely unaffected. Further, the composition of banks' OBS
               activities appears to help explain interbank differences in cost
               and profit X-efficiency estimates, whereas bank size and the mix
               between on- and off-balance-sheet banking activities are largely
               uncorrelated with the X-efficiency estimates.",
  journal   = "J. Money Credit Bank.",
  publisher = "[Wiley, Ohio State University Press]",
  volume    =  34,
  number    =  4,
  pages     = "987--1013",
  year      =  2002
}

@ARTICLE{Hunter1986-vt,
  title     = "Technical Change, Organizational Form, and the Structure of Bank
               Production",
  author    = "Hunter, William C and Timme, Stephen G",
  journal   = "J. Money Credit Bank.",
  publisher = "[Wiley, Ohio State University Press]",
  volume    =  18,
  number    =  2,
  pages     = "152--166",
  year      =  1986
}

@ARTICLE{Matthews2009-mm,
  title     = "Nonperforming Loans and Productivity in Chinese Banks, 1997-2006",
  author    = "Matthews, Kent and Zhang, Xu and Guo, Jianguang",
  abstract  = "A bootstrap method for Malmquist index estimates of productivity
               growth is constructed with appropriate confidence intervals.
               This paper adjusts for the quality of the output by accounting
               for the nonperforming loans (NPLs) on balance sheets and tests
               the robustness of the results by examining alternative sets of
               outputs. The productivity growth of state-owned banks and
               joint-stock banks is compared, and the determinants are
               evaluated. It was found that the average productivity of Chinese
               banks improved modestly over this period. Adjusting for the
               quality of loans by treating NPLs as an undesirable output, the
               average productivity growth of the state-owned banks was zero or
               negative, while the productivity of the joint-stock banks was
               markedly higher.",
  journal   = "The Chinese Economy",
  publisher = "Routledge",
  volume    =  42,
  number    =  2,
  pages     = "30--47",
  month     =  mar,
  year      =  2009
}

@ARTICLE{Jiang2013-pk,
  title    = "Bank ownership, privatization, and performance: Evidence from a
              transition country",
  author   = "Jiang, Chunxia and Yao, Shujie and Feng, Genfu",
  abstract = "This paper combines the static effect of ownership and the
              dynamic effect of privatization on bank performance in China over
              1995--2010, reporting a significantly higher performance by
              private intermediaries -- joint stock commercial banks and city
              commercial banks -- relative to state-owned commercial banks.
              However, publicly traded banks, subject to multiple monitoring
              and vetting in capital markets, perform better regardless of
              ownership status. The privatization of banks has improved
              performance with respect to revenue inflow and efficiency gains
              in the short- or long-run (initial public offerings). The
              positive long-run effect is more relevant and significant for
              banking institutions with minority foreign ownership. Moreover,
              this paper innovatively estimates interest income efficiency and
              non-interest income efficiency at the same time. The results
              suggest that Chinese banks are much more efficient in generating
              interest income than raising non-interest revenue, although the
              latter aspect has improved significantly during the sample
              period.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  9,
  pages    = "3364--3372",
  month    =  sep,
  year     =  2013,
  keywords = "Bank performance; Privatization; Minority foreign ownership;
              Efficiency"
}

@ARTICLE{Simper2013-ar,
  title     = "How Relevant is the Choice of Risk Management Control Variable
               to {Non-Parametric} Bank Profit Efficiency Analysis? The Case of
               South Korean Banks",
  author    = "Simper, Richard and Hall, Max J B and Liu, Wenbin and Zelenyuk,
               Valentin and Zhou, Zhongbao",
  abstract  = "Adopting a profit-based approach to the estimation of the
               technical efficiency of South Korean banks, we systematically
               analyse, within a non-parametric DEA ana",
  journal   = "Nottingham University Business School Research Paper Series",
  publisher = "papers.ssrn.com",
  month     =  mar,
  year      =  2013,
  keywords  = "OR in banking, Korean Banks, Risk Management, Efficiency"
}

@ARTICLE{Epure2012-nw,
  title     = "Monitoring bank performance in the presence of risk",
  author    = "Epure, Mircea and Lafuente, Esteban",
  abstract  = "Abstract This paper proposes a managerial control tool that
               integrates risk in efficiency measures. Building on existing
               efficiency specifications, our proposal reflects the real
               banking technology and accurately models the relationship
               between desirable and ...",
  journal   = "Journal of Productivity Analysis",
  publisher = "Springer",
  pages     = "1--17",
  year      =  2012
}

@ARTICLE{Illueca2013-dn,
  title     = "Earnings quality and performance in the banking industry: A
               profit frontier approach",
  author    = "Illueca, Manuel and Prior, Diego and Tortosa-Ausina, Emili and
               Garc{\'\i}a-Alcober, Mapilar",
  abstract  = "Abstract The analysis of efficiency and productivity in banking
               has received a great deal of attention for almost three decades
               now. However, most of the existing literature to date has not
               explicitly accounted for risk when measuring efficiency. We
               propose an analysis of ...",
  journal   = "Research Paper. University of Jaume I",
  publisher = "econfin.massey.ac.nz",
  year      =  2013
}

@ARTICLE{Tsang2014-zz,
  title     = "Assessing productivity in the presence of negative data and
               undesirable outputs",
  author    = "Tsang, Seng-Su and Chen, Yi-Fen and Lu, Yung-Hsiang and Chiu,
               Ching-Ren",
  abstract  = "Few studies address the presence of negative data and
               undesirable outputs in productivity assessment. This study
               proposes a range-adjusted measure model that uses a non-radial
               Malmquist productivity index to estimate dynamic productivity in
               the presence of negative data and undesirable outputs.
               Banking-industry data are used to demonstrate the proposed
               model. The results show that during the 2007?2009 global
               financial crises, bank productivity deteriorated, mainly because
               of technical changes, and smaller banks suffered smaller
               financial losses. Finally, a decision making matrix based on the
               analysis results is presented to show the implications of the
               proposed method.",
  journal   = "The Service Industries Journal",
  publisher = "Routledge",
  volume    =  34,
  number    =  2,
  pages     = "162--174",
  month     =  jan,
  year      =  2014
}

@ARTICLE{Malikov2016-um,
  title     = "A Cost System Approach to the Stochastic Directional Technology
               Distance Function with Undesirable Outputs: The Case of us Banks
               in 2001--2010",
  author    = "Malikov, Emir and Kumbhakar, Subal C and Tsionas, Mike G",
  abstract  = "This paper offers a methodology to address the endogeneity of
               inputs in the directional technology distance function
               (DTDF)-based formulation of banking technology which explicitly
               accommodates the presence of undesirable nonperforming
               loans---an inherent characteristic of the bank's production due
               to its exposure to credit risk. Specifically, we model
               nonperforming loans as an undesirable output in the bank's
               production process. Since the stochastic DTDF describing banking
               technology is likely to suffer from the endogeneity of inputs,
               we propose addressing this problem by considering a system
               consisting of the DTDF and the first-order conditions from the
               bank's cost minimization problem. The first-order conditions
               also allow us to identify the `cost-optimal' directional vector
               for the banking DTDF, thus eliminating the uncertainty
               associated with an ad hoc choice of the direction. We apply our
               cost system approach to the data on large US commercial banks
               for the 2001--2010 period, which we estimate via Bayesian Markov
               chain Monte Carlo methods subject to theoretical regularity
               conditions. We document dramatic distortions in banks'
               efficiency, productivity growth and scale elasticity estimates
               when the endogeneity of inputs is assumed away and/or the DTDF
               is fitted in an arbitrary direction. Copyright \copyright{} 2015
               John Wiley \& Sons, Ltd.",
  journal   = "J. Appl. Econ.",
  publisher = "Wiley Online Library",
  volume    =  31,
  number    =  7,
  pages     = "1407--1429",
  month     =  nov,
  year      =  2016
}

@ARTICLE{McKillop2015-ga,
  title    = "A Sustainable Business Model Strategy for Irish Credit Unions:
              Does One Size Fit All?",
  author   = "McKillop, Donal G and Quinn, Barry",
  abstract = "This study examines the business model complexity of Irish credit
              unions using a latent class approach to measure structural
              performance over the period 2002 to",
  journal  = "Queen's Centre for Not-for-profit and Public sector Research
              Working Paper Series",
  month    =  sep,
  year     =  2015,
  keywords = "credit unions, business model, latent class frontier"
}

@ARTICLE{Corrado2011-sz,
  title   = "Where is the Economics in Spatial Econometrics?",
  author  = "Corrado, Luisa and Fingleton, Bernard",
  journal = "SERC DISCUSSION PAPER 71",
  year    =  2011
}

@TECHREPORT{Credit_Union_Central_of_Canada2015-sg,
  title  = "2015 Community and Economic Impact Report",
  author = "{Credit Union Central of Canada}",
  year   =  2015
}

@ARTICLE{Credit_Union_Central_of_Canada2013-rq,
  title   = "Canada's Credit Unions and the State of the System",
  author  = "{Credit Union Central of Canada}",
  journal = "System Brief",
  year    =  2013
}

@ARTICLE{Beck2014-pb,
  title    = "When Arm's Length is Too Far. Relationship Banking Over the
              Business Cycle",
  author   = "Beck, Thorsten and Degryse, Hans and de Haas, Ralph and van
              Horen, Neeltje",
  abstract = "Using a novel way to identify relationship and transaction banks,
              we study how banks' lending techniques affect funding to SMEs
              over the business cycle. For 21",
  journal  = "FEB working paper series",
  month    =  jul,
  year     =  2014,
  keywords = "relationship banking, credit constraints, business cycle"
}

@TECHREPORT{Brown2013-hh,
  title       = "Grounds for Benefit: Developing and protecting community
                 benefits in football stadia",
  author      = "Brown, Adam and McGee, Fiona",
  institution = "Supporters Direct",
  year        =  2013
}

@ARTICLE{Hirshleifer2001-sh,
  title     = "Investor Psychology and Asset Pricing",
  author    = "Hirshleifer, David",
  abstract  = "The basic paradigm of asset pricing is in vibrant flux. The
               purely rational approach is being subsumed by a broader approach
               based upon the psychology of investors. In this approach,
               security expected returns are determined by both risk and
               misvaluation. This survey sketches a framework for understanding
               decision biases, evaluates the a priori arguments and the
               capital market evidence bearing on the importance of investor
               psychology for security prices, and reviews recent models.",
  journal   = "J. Finance",
  publisher = "[American Finance Association, Wiley]",
  volume    =  56,
  number    =  4,
  pages     = "1533--1597",
  year      =  2001
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Edmans2007-oc,
  title     = "Sports Sentiment and Stock Returns",
  author    = "Edmans, Alex and Garc{\'\i}a, Diego and Norli, {\O}yvind",
  abstract  = "This paper investigates the stock market reaction to sudden
               changes in investor mood. Motivated by psychological evidence of
               a strong link between soccer outcomes and mood, we use
               international soccer results as our primary mood variable. We
               find a significant market decline after soccer losses. For
               example, a loss in the World Cup elimination stage leads to a
               next-day abnormal stock return of ‚àí49 basis points. This loss
               effect is stronger in small stocks and in more important games,
               and is robust to methodological changes. We also document a loss
               effect after international cricket, rugby, and basketball games.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  62,
  number    =  4,
  pages     = "1967--1998",
  month     =  aug,
  year      =  2007
}

@ARTICLE{Tetlock2007-ad,
  title     = "Giving Content to Investor Sentiment: The Role of Media in the
               Stock Market",
  author    = "Tetlock, Paul C",
  abstract  = "I quantitatively measure the interactions between the media and
               the stock market using daily content from a popular Wall Street
               Journal column. I find that high media pessimism predicts
               downward pressure on market prices followed by a reversion to
               fundamentals, and unusually high or low pessimism predicts high
               market trading volume. These and similar results are consistent
               with theoretical models of noise and liquidity traders, and are
               inconsistent with theories of media content as a proxy for new
               information about fundamental asset values, as a proxy for
               market volatility, or as a sideshow with no relationship to
               asset markets.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  62,
  number    =  3,
  pages     = "1139--1168",
  month     =  jun,
  year      =  2007
}

@ARTICLE{Lerner2004-jp,
  title     = "Heart strings and purse strings: Carryover effects of emotions
               on economic decisions",
  author    = "Lerner, Jennifer S and Small, Deborah A and Loewenstein, George",
  abstract  = "We examined the impact of specific emotions on the endowment
               effect, the tendency for selling prices to exceed buying or
               ``choice'' prices for the same object. As predicted by
               appraisal-tendency theory, disgust induced by a prior,
               irrelevant situation carried over to normatively unrelated
               economic decisions, reducing selling and choice prices and
               eliminating the endowment effect. Sadness also carried over,
               reducing selling prices but increasing choice prices--producing
               a ``reverse endowment effect'' in which choice prices exceeded
               selling prices. The results demonstrate that incidental emotions
               can influence decisions even when real money is at stake, and
               that emotions of the same valence can have opposing effects on
               such decisions.",
  journal   = "Psychol. Sci.",
  publisher = "pss.sagepub.com",
  volume    =  15,
  number    =  5,
  pages     = "337--341",
  month     =  may,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Kamstra2003-td,
  title     = "Winter Blues: A {SAD} Stock Market Cycle",
  author    = "Kamstra, Mark J and Kramer, Lisa A and Levi, Maurice D",
  abstract  = "Depression has been linked with seasonal affective disorder
               (SAD), a condition that affects many people during the seasons
               of relatively fewer hours of daylight. Experimental research in
               psychology has documented a clear link between depression and
               lowered risktaking ...",
  journal   = "Am. Econ. Rev.",
  publisher = "American Economic Association",
  volume    =  93,
  number    =  1,
  pages     = "324--343",
  year      =  2003
}

@ARTICLE{Garcia2013-hc,
  title     = "Sentiment during Recessions",
  author    = "Garc{\'\i}a, Diego",
  abstract  = "This paper studies the effect of sentiment on asset prices
               during the 20th century (1905 to 2005). As a proxy for
               sentiment, we use the fraction of positive and negative words in
               two columns of financial news from the New York Times. The main
               contribution of the paper is to show that, controlling for other
               well-known time-series patterns, the predictability of stock
               returns using news' content is concentrated in recessions. A one
               standard deviation shock to our news measure during recessions
               predicts a change in the conditional average return on the DJIA
               of 12 basis points over one day.",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  68,
  number    =  3,
  pages     = "1267--1300",
  month     =  jun,
  year      =  2013
}

@BOOK{Morgan2014-hg,
  title     = "Counterfactuals and Causal Inference",
  author    = "Morgan, Stephen L and Winship, Christopher",
  abstract  = "In this second edition of Counterfactuals and Causal Inference,
               completely revised and expanded, the essential features of the
               counterfactual approach to observational data analysis are
               presented with examples from the social, demographic, and health
               sciences. Alternative estimation techniques are first introduced
               using both the potential outcome model and causal graphs; after
               which, conditioning techniques, such as matching and regression,
               are presented from a potential outcomes perspective. For
               research scenarios in which important determinants of causal
               exposure are unobserved, alternative techniques, such as
               instrumental variable estimators, longitudinal methods, and
               estimation via causal mechanisms, are then presented. The
               importance of causal effect heterogeneity is stressed throughout
               the book, and the need for deep causal explanation via
               mechanisms is discussed.",
  publisher = "Cambridge University Press",
  month     =  nov,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Engle2002-vt,
  title     = "Dynamic conditional correlation: A simple class of multivariate
               generalized autoregressive conditional heteroskedasticity models",
  author    = "Engle, Robert",
  abstract  = "Time varying correlations are often estimated with multivariate
               generalized autoregressive conditional heteroskedasticity
               (GARCH) models that are linear in squares and cross products of
               the data. A new class of multivariate models called dynamic
               conditional",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  20,
  number    =  3,
  pages     = "339--350",
  year      =  2002
}

@ARTICLE{Shmueli2010-eu,
  title     = "To Explain or to Predict?",
  author    = "Shmueli, Galit",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Stat. Sci.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  25,
  number    =  3,
  pages     = "289--310",
  month     =  aug,
  year      =  2010,
  keywords  = "Explanatory modeling; causality; predictive modeling; predictive
               power; statistical strategy; data mining; scientific research",
  language  = "en"
}

@BOOK{Lo2010-nl,
  title     = "Hedge Funds: An Analytic Perspective",
  author    = "Lo, Andrew W",
  abstract  = "The hedge fund industry has grown dramatically over the last two
               decades, with more than eight thousand funds now controlling
               close to two trillion dollars. Originally intended for the
               wealthy, these private investments have now attracted a much
               broader following that includes pension funds and retail
               investors. Because hedge funds are largely unregulated and
               shrouded in secrecy, they have developed a mystique and allure
               that can beguile even the most experienced investor. In Hedge
               Funds, Andrew Lo--one of the world's most respected financial
               economists--addresses the pressing need for a systematic
               framework for managing hedge fund investments. Arguing that
               hedge funds have very different risk and return characteristics
               than traditional investments, Lo constructs new tools for
               analyzing their dynamics, including measures of illiquidity
               exposure and performance smoothing, linear and nonlinear risk
               models that capture alternative betas, econometric models of
               hedge fund failure rates, and integrated investment processes
               for alternative investments. In a new chapter, he looks at how
               the strategies for and regulation of hedge funds have changed in
               the aftermath of the financial crisis.",
  publisher = "Princeton University Press",
  month     =  jul,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Afriat1972-uk,
  title     = "Efficiency Estimation of Production Functions",
  author    = "Afriat, S N",
  journal   = "Int. Econ. Rev.",
  publisher = "[Economics Department of the University of Pennsylvania, Wiley,
               Institute of Social and Economic Research, Osaka University]",
  volume    =  13,
  number    =  3,
  pages     = "568--598",
  year      =  1972
}

@ARTICLE{Pessarossi2015-up,
  title    = "Do capital requirements affect cost efficiency? Evidence from
              China",
  author   = "Pessarossi, Pierre and Weill, Laurent",
  abstract = "This paper contributes to the debate on the effect of capital
              requirements on cost efficiency. We study the relation between
              capital ratio and cost efficiency for Chinese banks over the
              period 2004--2009, taking advantage of the profound regulatory
              changes in capital requirements that occurred during this period
              to measure the exogenous impact of an increase in the capital
              ratio on banks' cost efficiency. We find that such an increase
              has a positive effect on cost efficiency, the size of which
              depends to an extent on the bank's ownership type. Our results
              therefore suggest that capital requirements can improve cost
              efficiency.",
  journal  = "Journal of Financial Stability",
  volume   =  19,
  pages    = "119--127",
  month    =  aug,
  year     =  2015,
  keywords = "China; Bank; Capital requirements; Efficiency"
}

@ARTICLE{Marques_Pereira2015-nv,
  title    = "How banks respond to Central Bank supervision: Evidence from
              Brazil",
  author   = "Marques Pereira, Jo{\~a}o Andr{\'e} C and Saito, Richard",
  abstract = "Central Bank supervision is one of the pillars of capital
              regulation. Based on a unique database built using supervision
              data from the Central Bank of Brazil, we evaluate the
              effectiveness of the Central Bank's supervision over banks given
              the Central Bank's proprietary credit rating and signaling
              requests for higher capital buffers. We also examine the main
              determinants of capital buffer management in addition to
              supervision. We find evidence that (i) Brazilian Central Bank
              supervision imposes excess capital buffer needs on banks,
              especially small and midsize banks; (ii) market discipline may
              play no role in driving capital ratios; and (iii) the business
              cycle has a negative influence on bank capital cushions,
              suggesting pro-cyclical capital management. We conclude that
              supervision plays a major role in markets where market discipline
              is weak and for smaller banks which act on pro-cyclical way.",
  journal  = "Journal of Financial Stability",
  volume   =  19,
  pages    = "22--30",
  month    =  aug,
  year     =  2015,
  keywords = "Asset risk; Bank regulation; Regulatory capital"
}

@ARTICLE{Fare2015-fc,
  title     = "Directional Distance Functions Revisited: Selective Overview and
               Update",
  author    = "F{\"a}re, R and Grosskopf, S and Margaritis, D",
  journal   = "Data Envelopment Analysis Journal",
  publisher = "Now Publishers",
  volume    =  1,
  number    =  2,
  pages     = "57--79",
  year      =  2015,
  keywords  = "Theory: Empirical methods of evaluation; Nonparametric methods;
               Optimization; Operations Research"
}

@TECHREPORT{Idzorek2004-pb,
  title       = "A {Step-By-Step} Guide To The {Black-Litterman} Model:
                 Incorporating {User-Specified} Confidence levels",
  author      = "Idzorek, Thomas",
  institution = "Ibbotson Associates",
  year        =  2004
}

@INCOLLECTION{Weston2004-ex,
  title     = "Merger Arbitrage",
  booktitle = "Takeovers, Restructuring and Corporate Governance",
  editor    = "Weston, John and Micthell, Mark and Mulherin, John",
  publisher = "Pearsons",
  year      =  2004
}

@BOOK{Sass2012-gn,
  title     = "Long-term competitive balance under {UEFA} Financial Fair Play
               regulations",
  author    = "Sass, Markus",
  abstract  = "Abstract This paper analyzes the long-term development of
               competitive balance in a professional team sports league with
               win-maximizing clubs facing a strict break-even constraint as
               imposed by UEFA's new Financial Fair Play Regulations. A
               classical model ...",
  publisher = "Univ., Faculty of Economics and Management",
  year      =  2012
}

@TECHREPORT{Karimalis2014-nc,
  title       = "Measuring systemic risk in the European banking sector: A
                 Copula {CoVaR} approach",
  author      = "Karimalis, Emmanouil N and Nomikos, Nikos",
  institution = "Working paper, Cass City College, London",
  year        =  2014
}

@ARTICLE{Lawrence2008-fj,
  title     = "Student managed investment funds: an international perspective",
  author    = "Lawrence, E C",
  abstract  = "Abstract: The most comprehensive survey ever conducted on
               student managed investment funds shows there are now 314
               universities worldwide that offer students the chance to learn
               about portfolio management by investing real money. In
               aggregate, students are directly ...",
  journal   = "Journal of Applied Finance",
  publisher = "papers.ssrn.com",
  year      =  2008,
  keywords  = "Mutual Funds, Portfolio Management, Finance, Business Education,
               College Students, Study \& Teaching Business Students"
}

@MISC{Adams2004-sr,
  title     = "Market Efficiency and Diversification: An Experiential Approach
               Using the Wall Street Journal's Dartboard Portfolio",
  author    = "Adams, John C and Cyree, Ken B",
  journal   = "Journal of Applied Finance",
  publisher = "Financial Management Association International",
  volume    =  14,
  number    =  2,
  pages     = "40--51",
  year      =  2004,
  address   = "Tampa, United States"
}

@ARTICLE{Nahm2013-ko,
  title     = "Measuring scale efficiency from a parametric hyperbolic distance
               function",
  author    = "Nahm, Daehoon and Vu, Ha Thu",
  abstract  = "This paper shows how scale efficiency can be measured from an
               arbitrary parametric hyperbolic distance function with multiple
               outputs and multiple inputs. It extends the methods introduced
               by Ray (J Product Anal 11:183--194, 1998), and Balk (J Product
               Anal 15:159--183, 2001) and Ray (2003) that measure scale
               efficiency from a single-output multi-input distance function
               and from a multi-output and multi-input distance function,
               respectively. The method developed in the present paper is
               different from Ray's and Balk's in that it allows for
               simultaneous contraction of inputs and expansion of outputs.
               Theorems applicable to an arbitrary parametric hyperbolic
               distance function are introduced first, and then their uses in
               measuring scale efficiency are illustrated with the translog
               functional form.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  39,
  number    =  1,
  pages     = "83--88",
  month     =  feb,
  year      =  2013,
  language  = "en"
}

@MISC{noauthor_undated-bj,
  title = "{IPOL\_BRI(2016)574408\_EN.pdf}"
}

@UNPUBLISHED{Annon2017-nd,
  title  = "The impact of sovereign rating changes on the activity of European
            banks",
  author = "{Annon}",
  year   =  2017
}

@ARTICLE{Kane2016-kg,
  title    = "A Theory of How and Why {Central-Bank} Culture Supports Predatory
              {Risk-Taking} at Megabanks",
  author   = "Kane, Edward J",
  abstract = "This paper applies Schein's model of organizational culture to
              financial firms and their prudential regulators. It identifies a
              series of hard-to-change cultural norms and assumptions that
              support go-for-broke risk-taking by megabanks that meets the
              everyday definition of theft. The problem is not to find new ways
              to constrain this behavior, but to change the norms that support
              it by establishing that managers of megabanks owe duties of
              loyalty, competence, and care directly to taxpayers.",
  journal  = "Atl. Econ. J.",
  volume   =  44,
  number   =  1,
  pages    = "51--71",
  month    =  mar,
  year     =  2016
}

@ARTICLE{Song2015-pq,
  title    = "Bank Culture",
  author   = "Song, Fenghua and Thakor, Anjan V",
  abstract = "This paper develops a theoretical model of bank culture. The
              model is based on a multitasking problem within a bank that
              involves the bank designing an optimal",
  month    =  dec,
  year     =  2015,
  keywords = "Bank culture, Multi-tasking problem, Competition, Bank capital,
              Safety nets"
}

@ARTICLE{Stoll2000-ur,
  title     = "Friction",
  author    = "Stoll, Hans R",
  abstract  = "The sources of trading friction are studied, and simple, robust
               empirical measures of friction are provided. Seven distinct
               measures of trading friction are computed from transactions data
               for 1,706 NYSE/AMSE stocks and 2,184 Nasdaq stocks. The measures
               provide insights into the magnitude of trading costs, the
               importance of informational versus real frictions, and the role
               of market structure. The degree to which the various measures
               are associated with each other and with trading characteristics
               of stocks is examined.",
  journal   = "J. Finance",
  publisher = "[American Finance Association, Wiley]",
  volume    =  55,
  number    =  4,
  pages     = "1479--1514",
  year      =  2000
}

@ARTICLE{Rytchkov2014-gz,
  title    = "Asset Pricing with Dynamic Margin Constraints",
  author   = "Rytchkov, Oleg",
  abstract = "This paper provides a novel theoretical analysis of how
              endogenous time-varying margin requirements affect capital market
              equilibrium. I find that margin requirements, when there are no
              other market frictions, reduce the volatility and correlation of
              returns as well as the risk-free rate, but increase the market
              price of risk, the risk premium, and the price of risky assets.
              Furthermore, margin requirements generate a strong
              cross-sectional dispersion of stock return volatilities. The
              results emphasize that a general equilibrium analysis may reverse
              the conclusions of a partial equilibrium analysis often employed
              in the literature.",
  journal  = "J. Finance",
  volume   =  69,
  number   =  1,
  pages    = "405--452",
  month    =  feb,
  year     =  2014
}

@ARTICLE{Lux2015-ok,
  title    = "The State and Fate of Community Banking",
  author   = "Lux, Marshall and Greene, Robert",
  journal  = "M-RCBG Associate Working Paper",
  number   =  37,
  year     =  2015
}

@ARTICLE{Ayadi2016-fq,
  title   = "Bank Business Model Monitor for Europe 2015",
  author  = "Ayadi, Rym and De Groen, Willem Pieter",
  journal = "International Research Center for Cooperative Finance",
  year    =  2016
}

@ARTICLE{Aven2015-oi,
  title    = "On the Need for Rethinking Current Practice that Highlights Goal
              Achievement Risk in an Enterprise Context",
  author   = "Aven, Eyvind and Aven, Terje",
  abstract = "This article addresses the issue of how performance and risk
              management can complement each other in order to enhance the
              management of an enterprise. Often, we see that risk management
              focuses on goal achievements and not the enterprise risk related
              to its activities in the value chain. The statement ``no goal, no
              risk'' is a common misconception. The main aim of the article is
              to present a normative model for describing the links between
              performance and risk, and to use this model to give
              recommendations on how to best structure and plan the management
              of an enterprise in situations involving risk and uncertainties.
              The model, which has several novel features, is based on the
              interaction between different types of risk management
              (enterprise risk management, task risk management, and personal
              risk management) and a structure where the enterprise risk
              management overrules both the task and personal risk management.
              To illustrate the model we use the metaphor of a ship, where the
              ship is loaded with cash-generating activities and has a
              direction over time determined by the overall strategic
              objectives. Compared to the current enterprise risk management
              practice, the model and related analysis are founded on a new
              perspective on risk, highlighting knowledge and uncertainties
              beyond probabilities.",
  journal  = "Risk Anal.",
  volume   =  35,
  number   =  9,
  pages    = "1706--1716",
  month    =  sep,
  year     =  2015,
  keywords = "Enterprise risk management; objectives; task risk management",
  language = "en"
}

@ARTICLE{Leibenstein1966-of,
  title     = "Allocative Efficiency vs. ``{X-Efficiency}''",
  author    = "Leibenstein, Harvey",
  journal   = "Am. Econ. Rev.",
  publisher = "American Economic Association",
  volume    =  56,
  number    =  3,
  pages     = "392--415",
  year      =  1966
}

@ARTICLE{Mester1993-tv,
  title     = "Efficiency in the savings and loan industry",
  author    = "Mester, Loretta J",
  abstract  = "I modify the stochastic econometric cost frontier approach to
               investigate efficiency in mutual and stock S\&L using 1991 data
               on U.S. S\&Ls. My methodology allows both the cost frontier and
               error structures to differ between S\&Ls of these two ownership
               forms. A likelihood ratio test indicates that the data support
               this unrestricted model, which implies efficient mutual and
               stock S\&Ls use different production technologies. Various
               measures of inefficiency show that on average stock S\&Ls are
               less efficient than mutual S\&Ls. The second part of the article
               relates the inefficiency measures to several correlates.(This
               abstract was borrowed from another version of this item.)",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  17,
  number    = "2-3",
  pages     = "267--286",
  year      =  1993
}

@ARTICLE{Stigler_undated-vk,
  title  = "{X-Efficiency}",
  author = "{stigler}"
}

@ARTICLE{Borland1996-ae,
  title     = "Matching and Mobility in the Market for Australian Rules
               Football Coaches",
  author    = "Borland, Jeff and Lye, Jenny",
  abstract  = "This study examines matching effects as a determinant of
               mobility in the market for Australian Rules football coaches
               between 1931 and 1994. Among other results, the authors find
               direct evidence of a coachteam match-specific effect on team
               performance. One implication is that two teams might both
               significantly improve their performance by switching coaches,
               depending on the specific team and coach characteristics.",
  journal   = "Ind. Labor Relat. Rev.",
  publisher = "Sage Publications, Inc.",
  volume    =  50,
  number    =  1,
  pages     = "143--158",
  year      =  1996
}

@ARTICLE{Szymanski2016-ef,
  title     = "Professional Asian Football Leagues and the Global Market",
  author    = "Szymanski, Stefan",
  abstract  = "This paper considers the development potential for professional
               football (soccer) leagues in Asia. This is set in the context of
               a global market where playing talent is easily bought and sold,
               and fans are attracted by the highest quality of play which they
               mostly consume via screens. The paper highlights the relative
               underdevelopment of Asian leagues given the size and growing
               economic power of the Asian markets, and suggests some ways in
               which this might change in the future.",
  journal   = "Asian Economic Policy Review",
  publisher = "Wiley Online Library",
  volume    =  11,
  number    =  1,
  pages     = "16--38",
  month     =  jan,
  year      =  2016,
  keywords  = "football; professional league; soccer; sport; Z21; Z28"
}

@ARTICLE{Anderson2005-zt,
  title     = "The Relationship Between Student Perceptions of Team Dynamics
               and Simulation Game Outcomes: An {Individual-Level} Analysis",
  author    = "Anderson, Jonathan R",
  abstract  = "In many business courses, computer-based simulations are
               becoming a popular choice of pedagogical technique, yet research
               is only beginning to consider how these simulation games impact
               student outcomes. In this study, the author investigated
               individual perceptions of simulation team dynamics and their
               relationship to student affect regarding the simulation as well
               as simulation performance in a sample of 172 responding
               students. The results showed that a student's affect regarding
               the simulation game was influenced by student team cohesion and
               student team independence. Alternatively, student simulation
               performance was influenced by team heterogeneity, opportunistic
               practices, and hypothesis-driven thinking. These findings
               encourage instructors to consider thoughtfully the outcomes they
               want students to experience when structuring student teams that
               will participate in simulation learning games.",
  journal   = "Journal of Education for Business",
  publisher = "Routledge",
  volume    =  81,
  number    =  2,
  pages     = "85--90",
  month     =  nov,
  year      =  2005
}

@ARTICLE{Barros2006-ua,
  title     = "Analyzing the Performance of the English {F.A}. Premier League
               With an Econometric Frontier Model",
  author    = "Barros, Carlos Pestana and Leach, Stephanie",
  abstract  = "This article uses an econometric frontier model to evaluate the
               performance of football clubs present in the English F.A.
               Premier League from 1998-1999 to 2002-2003, combining sport and
               financial variables. A stochastic Cobb-Douglas production
               frontier model is used to generate football club efficiency
               scores. We conclude that the price of labour, the price of
               capital players the price of capital stadium, points gained,
               attendance, and turnover all play a major role in football
               efficiency and find that the efficiency scores are mixed.",
  journal   = "J. Sports Econom.",
  publisher = "SAGE Publications",
  volume    =  7,
  number    =  4,
  pages     = "391--407",
  month     =  nov,
  year      =  2006
}

@BOOK{Gerrard1999-zk,
  title     = "Football, fans and finance: understanding the business of
               professional football",
  author    = "Gerrard, Bill",
  publisher = "Mainstream",
  year      =  1999
}

@INCOLLECTION{Szymanski2010-yl,
  title     = "The English Football Industry: Profit, Performance and
               Industrial Structure",
  booktitle = "Football Economics and Policy",
  author    = "Szymanski, Stefan and Smith, Ron",
  abstract  = "The English (Association) Football League is a long-established
               industrial cartel selling a highly popular product with only
               imperfect substitutes. Despite that, the majority of its member
               clubs lose money and the industry has faced successive financial
               crises over the last decade. This chapter develops an empirical
               model of the financial performance of English League clubs using
               a high-quality data set of 48 clubs over the period 1974---89.
               The underlying model explains how rents are competed away
               through the maximising behaviour of club owners subject to
               production constraints. This model is parameterised by a system
               of equations which describe the behaviour of a maximising owner
               subject to demand and production constraints. The model is then
               used to examine the coordination failure which lies at the heart
               of the English Football League's decline and to assess the
               prospects for the Premier League.",
  publisher = "Palgrave Macmillan, London",
  pages     = "1--26",
  year      =  2010,
  language  = "en"
}

@ARTICLE{Kuypers1999-ya,
  title   = "Winners and Losers, the Business Strategy of Football",
  author  = "Kuypers, Tim and Szymanski, S",
  journal = "London: Viking",
  year    =  1999
}

@ARTICLE{Szymanski1997-yy,
  title     = "The English Football Industry: profit, performance and
               industrial structure",
  author    = "Szymanski, Stefan and Smith, Ron",
  abstract  = "The English (Association) Football League is a long established
               industrial cartel selling a highly popular product with only
               imperfect substitutes. Despite that, the majority of its member
               clubs lose money and the industry has faced successive financial
               crises over the last decade. This paper develops an empirical
               model of the financial performance of English League clubs using
               a high quality dataset of 48 clubs over the period 1974?89. The
               underlying model explains how rents are competed away through
               the maximising behaviour of club owners subject to production
               constraints. This model is parameterised by a system of
               equations which describe the behaviour of a maximising owner
               subject to demand and production constraints. The model is then
               used to examine the coordination failure which lies at the heart
               of the English Football League's decline and to assess the
               prospects for the Premier League.",
  journal   = "International Review of Applied Economics",
  publisher = "Routledge",
  volume    =  11,
  number    =  1,
  pages     = "135--153",
  month     =  jan,
  year      =  1997
}

@ARTICLE{Gerrard2005-wi,
  title     = "A resource-utilization model of organizational efficiency in
               professional sports teams",
  author    = "Gerrard, Bill",
  abstract  = "The resource-based view explains sustainable competitive
               advantage as the consequence of an organization's endowment of
               unique and imperfectly replicable resources. Superior
               organizational performance, however, depends not only on the
               organization's resource endowment but also on the efficiency
               with which the resource endowment is used. In this article a
               resource-utilization model of a professional sports team is
               developed in which teams optimize the stock of athletic
               resources (i.e., playing talent), subject to ownership
               preferences, over sporting and financial performance. The
               resource-utilization model is used to analyze the factors
               influencing the team's current endowment of athletic resources
               and evaluate the efficiency with which teams utilize both their
               athletic and allegiance (i.e., fan base) resources to achieve
               sporting and financial targets. Empirical evidence is presented
               on the sporting and financial performance of English
               professional soccer teams in the FA Premier League over the
               period 1998-2002. It was found that the financial performance of
               teams is significantly affected by their ownership status.",
  journal   = "J. Sport Manage.",
  publisher = "Human Kinetics Publishers",
  volume    =  19,
  number    =  2,
  pages     = "143--169",
  year      =  2005
}

@ARTICLE{Kumbhakar1987-pw,
  title     = "Production Frontiers and Panel Data: An Application to {U.S}.
               Class 1 Railroads",
  author    = "Kumbhakar, Subal C",
  abstract  = "In this article, the appropriateness of inefficiency measures
               obtained directly from the production function as in Schmidt and
               Sickles (1984) is examined relative to those provided by (a) the
               cost function aproach, (b) Klein's approach, and (c) the
               iterative SUR technique. Efficiency rankings yielded by
               different methods are also compared and tested.",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  5,
  number    =  2,
  pages     = "249--255",
  month     =  apr,
  year      =  1987
}

@INCOLLECTION{Mc_Hale2014-vr,
  title     = "Econometric modelling of match results and scores",
  booktitle = "Handbook of the Economics of Professional Football",
  author    = "Mc Hale, Ian and Baker, Rose",
  year      =  2014
}

@ARTICLE{Smith1990-gl,
  title    = "Data envelopment analysis applied to financial statements",
  author   = "Smith, P",
  abstract = "Ratio analysis has been a tool of analysts for as long as
              financial statements have been prepared. Yet its limitation to
              considering only one numerator and one denominator severely
              limits its usefulness. This paper extends the traditional ratio
              analysis to permit the incorporation of any number of dimensions
              of performance, using data envelopment analysis. The method
              produces measures of corporate efficiency, together with a wealth
              of supporting information. The strengths and weaknesses of the
              method applied to financial statements are appraised.",
  journal  = "Omega",
  volume   =  18,
  number   =  2,
  pages    = "131--138",
  month    =  jan,
  year     =  1990,
  keywords = "financial statements; data envelopment analysis; performance
              measurement; ratio analysis"
}

@ARTICLE{Caves1977-gd,
  title     = "From Entry Barriers to Mobility Barriers: Conjectural Decisions
               and Contrived Deterrence to New Competition*",
  author    = "Caves, R E and Porter, M E",
  abstract  = "I. Interdependence and conjecture in entry, 242.--II. Barriers
               to mobility, 249.--III. Diversification by established firms and
               intergroup mobility, 257.--IV. Conclusion, 261.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  91,
  number    =  2,
  pages     = "241--261",
  year      =  1977
}

@ARTICLE{Barney1991-hb,
  title     = "Firm Resources and Sustained Competitive Advantage",
  author    = "Barney, Jay",
  abstract  = "Understanding sources of sustained competitive advantage has
               become a major area of research in strategic management.
               Building on the assumptions that strategic resources are
               heterogeneously distributed acrossfirms and that these
               differences are stable over time, this article examines the link
               betweenfirm resources and sustained competitive advantage. Four
               empirical indicators of the potential of firm resources to
               generate sustained competitive advantage-value, rareness,
               imitability, and substitutability-are discussed. The model is
               applied by analyzing the potential of severalfirm resourcesfor
               generating sustained competitive advantages. The article
               concludes by examining implications of this firm resource model
               of sustained competitive advantage for other business
               disciplines.",
  journal   = "J. Manage.",
  publisher = "jom.sagepub.com",
  volume    =  17,
  number    =  1,
  pages     = "99--120",
  series    = "Strategy Group Working Paper Series",
  month     =  mar,
  year      =  1991
}

@ARTICLE{Rumelt1991-eb,
  title     = "How much does industry matter?",
  author    = "Rumelt, Richard P",
  abstract  = "This study partitions the total variance in rate of return among
               FTC Line of Business reporting units into industry factors
               (whatever their nature), time factors, factors associated with
               the corporate parent, and business-specific factors. Whereas
               Schmalensee (1985) reported that industry factors were the
               strongest, corporate and market share effects being extremely
               weak, this study distinguishes between stable and fluctuating
               effects and reaches markedly different conclusions. The data
               reveal negligible corporate effects, small stable industry
               effects, and very large stable business-unit effects. These
               results imply that the most important sources of economic rents
               are business-specific; industry membership is a much less
               important source and corporate parentage is quite unimportant.",
  journal   = "Strat. Mgmt. J.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  12,
  number    =  3,
  pages     = "167--185",
  month     =  mar,
  year      =  1991
}

@ARTICLE{Warning2004-di,
  title     = "Performance Differences in German Higher Education: Empirical
               Analysis of Strategic Groups",
  author    = "Warning, Susanne",
  abstract  = "Initial investments and different strategic actions of
               universities lead to their different positions in the higher
               education sector. Pursuing similar strategies leads to similar
               positions that influence structure and performance within the
               system. Institutions cannot only choose to focus on research or
               on teaching, but also to focus either on natural sciences or
               social sciences. Using 73 public universities in Germany, this
               paper examines the existence of strategic groups based on
               performance. Common strategic variables only partly determine
               performance in high and low efficiency groups.",
  journal   = "Review of Industrial Organization",
  publisher = "Kluwer Academic Publishers",
  volume    =  24,
  number    =  4,
  pages     = "393--408",
  month     =  jun,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Taymaz2005-os,
  title     = "Are Small Firms Really Less Productive?",
  author    = "Taymaz, Erol",
  abstract  = "Small and medium-sized establishments (SMEs) account for a large
               proportion of industrial employment and production in almost all
               countries. Moreover, the recent literature emphasizes the role
               SMEs play in nurturing entrepreneurship and generating new
               products and processes. Although SMEs could be a source of new
               ideas and innovations, there are substantial productivity
               differences between small and large establishments. In this
               paper, we analyze three sources of productivity differentials:
               technical efficiency, returns to scale, and technical change.
               Our analysis on the creation, survival, and growth of new
               establishments in Turkish manufacturing industries in the period
               1987--1997 shows that all these three factors play a very
               important role in determining the survival probability and
               growth prospects of new establishments.",
  journal   = "Small Bus. Econ.",
  publisher = "Kluwer Academic Publishers",
  volume    =  25,
  number    =  5,
  pages     = "429--445",
  month     =  dec,
  year      =  2005,
  language  = "en"
}

@ARTICLE{Teece1997-id,
  title     = "Dynamic Capabilities and Strategic Management",
  author    = "Teece, David J and Pisano, Gary and Shuen, Amy",
  abstract  = "The dynamic capabilities framework analyzes the sources and
               methods of wealth creation and capture by private enterprise
               firms operating in environments of rapid technological change.
               The competitive advantage of firms is seen as resting on
               distinctive processes (ways of coordinating and combining),
               shaped by the firm's (specific) asset positions (such as the
               firm's portfolio of difficult-to-trade knowledge assets and
               complementary assets), and the evolution path(s) it has adopted
               or inherited. The importance of path dependencies is amplified
               where conditions of increasing returns exist. Whether and how a
               firm's competitive advantage is eroded depends on the stability
               of market demand, and the ease of replicability (expanding
               internally) and imitatability (replication by competitors). If
               correct, the framework suggests that private wealth creation in
               regimes of rapid technological change depends in large measure
               on honing internal technological, organizational, and managerial
               processes inside the firm. In short, identifying new
               opportunities and organizing effectively and efficiently to
               embrace them are generally more fundamental to private wealth
               creation than is strategizing, if by strategizing one means
               engaging in business conduct that keeps competitors off balance,
               raises rival's costs, and excludes new entrants.",
  journal   = "Strategic Manage. J.",
  publisher = "Wiley",
  volume    =  18,
  number    =  7,
  pages     = "509--533",
  year      =  1997
}

@ARTICLE{Des_Rosiers1996-vs,
  title     = "Rental Amenities and the Stability of Hedonic Prices: A
               Comparative Analysis of Five Market Segments",
  author    = "Des Rosiers, Francois and Theriault, Marius",
  abstract  = "The current paper applies the hedonic approach to five rental
               submarkets in the Quebec region, namely Quebec City, Vanier,
               Ste-Foy, Beauport and Charlesbourg. The databank consists of
               information obtained from property owners via a yearly survey;
               some 32,000 rental units and nearly 3,300 buildings are included
               in the study. Data provide detailed information on building and
               apartment size, age, location, services provided, quality of
               premises and type of occupants; vacancy rates can also be
               derived from the bank. In addition, resorting to a regional
               geographic information system permits integration of
               neighborhood effects into the analysis. Findings suggest that
               significant differences in implicit prices do exist across
               market segments. However, while consistent results are obtained
               for major rent determinants, collinearity clearly emerges with
               respect to some rental attributes. Using a regression-based
               paired comparison approach, it is possible to identify stable
               hedonic prices for main rental services; the coefficients thus
               obtained are then forced back as constraints into the
               service-adjusted model, thereby improving its overall
               consistency and practicability.",
  journal   = "Journal of Real Estate Research",
  publisher = "American Real Estate Society",
  volume    =  12,
  number    =  1,
  pages     = "17--36",
  year      =  1996
}

@TECHREPORT{noauthor_2015-rd,
  title       = "Shedding Light on Shadow Banking",
  number      = "IMF Working Paper 1501",
  institution = "International Monetary Fund",
  year        =  2015
}

@ARTICLE{Bartel2004-kx,
  title     = "Using ``Insider Econometrics'' to Study Productivity",
  author    = "Bartel, Ann and Ichniowski, Casey and Shaw, Kathryn",
  journal   = "Am. Econ. Rev.",
  publisher = "American Economic Association",
  volume    =  94,
  number    =  2,
  pages     = "217--223",
  year      =  2004
}

@ARTICLE{Moenninghoff2015-vu,
  title    = "The perennial challenge to counter {Too-Big-to-Fail} in banking:
              Empirical evidence from the new international regulation dealing
              with Global Systemically Important Banks",
  author   = "Moenninghoff, Sebastian C and Ongena, Steven and Wieandt, Axel",
  abstract = "This paper provides evidence on how the new international
              regulation on Global Systemically Important Banks (G-SIBs)
              impacts the market value of large banks. We analyze the stock
              price reactions for the 300 largest banks from 52 countries
              across 12 relevant regulatory announcement and designation
              events. We observe that the new regulation negatively affects the
              value of the newly regulated banks, yet that the official
              designation of banks as ``globally systemically important''
              itself has a partly offsetting positive impact. A cross-sectional
              analysis of the valuation effects with respect to, for example,
              government ownership of banks supports the view that the positive
              reaction to these designations can be attributed to a
              Too-Big-to-Fail (TBTF) perception by investors. The fact that
              these valuation effects emerge from a regulation specifically
              designed to reduce the costs and risks of Too-Big-to-Fail
              demonstrates the inherently paradoxical nature of the new
              regulation. These results further suggest that even though the
              individual components of the regulation have been effective,
              revealing the identities of G-SIBs eliminated ambiguity about the
              presence of government guarantees, and thereby may have run
              counter to the regulators' intent to contain the effects of TBTF.",
  journal  = "Journal of Banking \& Finance",
  volume   =  61,
  pages    = "221--236",
  month    =  dec,
  year     =  2015,
  keywords = "TBTF; Too-Big-to-Fail; G-SIB; Global Systemically Important Bank;
              Bank regulation; Unintended consequences"
}

@ARTICLE{Atkinson1989-hn,
  title     = "Dual Measures of Monopoly and Monopsony Power: An Application to
               Regulated Electric Utilities",
  author    = "Atkinson, Scott E and Kerkvliet, Joe",
  abstract  = "The inefficiency from monopoly pricing, monopsony pricing, and
               other institutional factors should be simultaneously estimated
               to avoid misspecification. Estimation of a behavioral profit
               function, where input and output shadow prices may diverge from
               their market values, allows unbiased simultaneous estimation of
               inefficiencies if its normalized form is employed. In an
               application to electric utilities consuming western coal, we
               cannot reject the hypothesis that utilities act as price-takers
               in output markets and find weak and statistically insignificant
               evidence of fuel-adjustment clause bias. Strong evidence is
               found of monopsony behavior in the market for western coal and
               its transportation.",
  journal   = "Rev. Econ. Stat.",
  publisher = "The MIT Press",
  volume    =  71,
  number    =  2,
  pages     = "250--257",
  year      =  1989
}

@ARTICLE{Huang2015-ff,
  title     = "Applying the New Metafrontier Directional Distance Function to
               Compare Banking Efficiencies in Central and Eastern European
               Countries",
  author    = "Huang, Tai-Hsin and Chiang, Dien-Lin and Tsai, Chao-Min",
  abstract  = "This paper establishes a new metafrontier directional technology
               distance function (MDDF) under a stochastic framework, rather
               than a deterministic setting like the one proposed by Battese et
               al. (2004). The new MDDF allows for calculating comparable
               technical efficiencies for banks under different technologies
               relative to the potential technology available to the industry
               across nations. The inefficiency term of the new MDDF is further
               associated with relevant environmental variables of the form
               proposed by Battese and Coelli (1995). The new MDDF is then
               applied to examine and compare bank efficiencies of 17 Central
               and Eastern European countries. Non-performing loans (NPLs) are
               regarded as an undesirable, jointly produced with various loans,
               and the omission of them tends to underestimate technical
               inefficiency scores. Evidence is found that the estimated
               technology gap dominates technical efficiencies. Bank managers
               are suggested to swiftly adopt new financial innovations with an
               eye to shift the group frontier closer to the metafrontier.",
  journal   = "Econ. Model.",
  publisher = "Elsevier",
  volume    =  44,
  pages     = "188--199",
  month     =  jan,
  year      =  2015,
  keywords  = "metafrontier directional distance function; technical
               efficiencies; environmental variables; undesirable output"
}

@BOOK{Birchall2013-kg,
  title     = "Finance in an Age of Austerity: The Power of {Customer-Owned}
               Banks",
  author    = "Birchall, Johnston",
  abstract  = "This is a book in search of an alternative to the discredited
               investor-owned banks that have brought the rich countries into
               crisis and the world economy into a long period of austerity. It
               finds customer-owned banks \_ credit unions, co-operative banks,
               b",
  publisher = "Edward Elgar Publishing",
  month     =  jan,
  year      =  2013,
  language  = "en"
}

@ARTICLE{McKillop2011-qv,
  title   = "Internet banking and irish credit unions",
  author  = "McKillop, Donal and Quinn, Barry",
  journal = "International Journal of Cooperative Management",
  volume  =  14,
  year    =  2011
}

@ARTICLE{Harvey2000-bz,
  title     = "Subchapter {S--A} new tool for enhancing the value of community
               banks",
  author    = "Harvey, James and Padget, Jane",
  journal   = "Financial Industry Perspectives",
  publisher = "Federal Reserve Bank of Kansas City",
  pages     = "17",
  year      =  2000
}

@ARTICLE{Cherchye2001-em,
  title     = "{FDH} Directional Distance Functions with an Application to
               European Commercial Banks",
  author    = "Cherchye, Laurens and Kuosmanen, Timo and Post, Thierry",
  abstract  = "Weextend Free Disposable Hull (FDH) efficiency analysis
               towardsthe general directional distance function framework. The
               profitinterpretation of directional distance functions is
               extendedto the non-convex FDH technologies. In addition, we
               derive anefficient enumerative algorithm for computing distance
               measuresin Free Disposable Hull (FDH) technologies, which
               applies tothe entire (infinitely large) family of directional
               distancefunctions. A simple numerical example and an application
               to Europeancommercial banks illustrate the algorithm.",
  journal   = "Journal of Productivity Analysis",
  publisher = "Kluwer Academic Publishers",
  volume    =  15,
  number    =  3,
  pages     = "201--215",
  month     =  jan,
  year      =  2001,
  keywords  = "Banks; Data Envelopment Analysis (DEA); Directional distance
               function; Enumeration; Free Disposable Hull (FDH)",
  language  = "en"
}

@ARTICLE{Zardkoohi1986-oj,
  title     = "Homogeneity Restrictions on the Translog Cost Model: A Note",
  author    = "Zardkoohi, Asghar and Rangan, Nanda and Kolari, James",
  abstract  = "The article presents a method of applying a translog cost model
               to the empirical analyses of depository institutions' cost
               characteristics. The authors demonstrate methods for employing
               such a flexible form cost functions, and advise that the
               translog cost model should be linearly homogenous in all input
               prices. The authors also specify several restrictions for
               symmetry and input prices that must be satisfied.",
  journal   = "J. Finance",
  publisher = "[American Finance Association, Wiley]",
  volume    =  41,
  number    =  5,
  pages     = "1153--1155",
  year      =  1986,
  keywords  = "BANKING industry -- Study \& teaching; COST; COST structure;
               ECONOMIES of scale; ECONOMIES of scope; FINANCIAL institutions;
               FINANCIAL instruments; FINANCIAL ratios; MATHEMATICAL models"
}

@ARTICLE{Wang2015-hd,
  title    = "Market volatility and momentum",
  author   = "Wang, Kevin Q and Xu, Jianguo",
  abstract = "We investigate the predictive power of market volatility for
              momentum. We find that (1) market volatility has significant
              power to forecast momentum payoffs, which is robust after
              controlling for market state and business cycle variables; (2)
              market volatility absorbs much of the predictive power of market
              state; (3) after controlling for market volatility and market
              state, other variables do not have incremental predictive power;
              (4) the time-series predictive power of market volatility is
              centered on loser stocks; and (5) default probability helps
              explain the predictive power of market volatility for momentum.
              These findings jointly present a significant challenge to
              existing theories on momentum.",
  journal  = "Journal of Empirical Finance",
  volume   =  30,
  pages    = "79--91",
  month    =  jan,
  year     =  2015,
  keywords = "Default risk; G11; G12; Market volatility; Momentum; Time-series
              predictability of momentum"
}

@ARTICLE{Herlemont2003-pp,
  title   = "Pairs Trading, Convergence Trading, Cointegration",
  author  = "Herlemont, Daniel",
  journal = "YATS Finances and Technology",
  volume  =  33,
  number  =  0,
  pages   = "1--31",
  year    =  2003
}

@ARTICLE{Atkinson2005-zi,
  title   = "Bayesian measurement of productivity and efficiency in the
             presence of undesirable outputs: crediting electric utilities for
             reducing air pollution",
  author  = "{Atkinson} and {Dorfman}",
  journal = "Journal of Econometrics",
  year    =  2005
}

@ARTICLE{Mei2008-mr,
  title     = "Assessing time-varying oligopoly and oligopsony power in the
               {US} paper industry",
  author    = "Mei, Bin and Sun, Changyou",
  journal   = "J. Appl. Agric. Econ.",
  publisher = "Cambridge Univ Press",
  volume    =  40,
  number    =  03,
  pages     = "927--939",
  year      =  2008
}

@ARTICLE{Murray1995-lm,
  title     = "Oligopsony, Vertical Integration, and Output Substitution:
               Welfare Effects in {U.S}. Pulpwood Markets",
  author    = "Murray, Brian C",
  abstract  = "Oligopsonistic commodity markets can induce upstream vertical
               integration by the input demanders. The associated price
               distortion can also induce output substitution by the
               commodity's suppliers. These phenomena are considered in the
               context of pulpwood markets in the U.S. The estimated welfare
               effects of an oligopsony distortion indicate relatively small
               deadweight losses but considerable wealth transfers from open
               market producers to integrated pulpwood processing firms.
               Correcting the distortion would have little effect on aggregate
               output levels from the forest sector, but would alter the
               composition of output between industry and nonindustry sources.",
  journal   = "Land Econ.",
  publisher = "[Board of Regents of the University of Wisconsin System,
               University of Wisconsin Press]",
  volume    =  71,
  number    =  2,
  pages     = "193--206",
  year      =  1995
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Loertscher_undated-do,
  title  = "Market Structure and the Competitive Effects of Vertical
            Integration‚àó",
  author = "Loertscher, Simon and Reisinger, Markus"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kelley_Pace1993-sa,
  title     = "Nonparametric methods with applications to hedonic models",
  author    = "Kelley Pace, R",
  abstract  = "Current real estate statistical valuation involves the
               estimation of parameters within a posited specification.
               Suchparametric estimation requires judgment concerning model (1)
               variables; and (2) functional form. In contrast,nonparametric
               regression estimation requires attention to (1) but permits
               greatly reduced attention to (2). Parametric estimators
               functionally model the parameters and variables affectingE(y¬¶x)
               while nonparametric estimators directly modelpdf(y, x) and
               henceE(y¬¶x).This article applies the kernel nonparametric
               regression estimator to two different data sets and
               specifications. The article shows the nonparametric estimator
               outperforms the standard parametric estimator (OLS) across
               variable transformations and across data subsets differing in
               quality. In addition, the article reviews properties of
               nonparametric estimators, presents the history of nonparametric
               estimators in real estate, and discusses a representation of the
               kernel estimator as a nonparametric grid method.",
  journal   = "J. Real Estate Fin. Econ.",
  publisher = "Kluwer Academic Publishers",
  volume    =  7,
  number    =  3,
  pages     = "185--204",
  month     =  nov,
  year      =  1993,
  language  = "en"
}

@MISC{Wu2014-ah,
  title     = "House Price Index Construction in the Nascent Housing Market:
               The Case of China",
  author    = "Wu, Jing and Deng, Yongheng and Liu, Hongyu",
  journal   = "Journal of Real Estate Finance and Economics",
  publisher = "Springer Science \& Business Media",
  volume    =  48,
  number    =  3,
  pages     = "522--545",
  month     =  apr,
  year      =  2014,
  address   = "Norwell, Netherlands"
}

@MISC{G_Stacy_SirmansLynn_MacDonaldDavid_A_MacphersonEmily_Norman_Zietz2006-ww,
  title     = "The Value of Housing Characteristics: A Meta Analysis",
  author    = "{G. Stacy SirmansLynn MacDonaldDavid A. MacphersonEmily Norman
               Zietz}",
  journal   = "Journal of Real Estate Finance and Economics",
  publisher = "Springer Science \& Business Media",
  volume    =  33,
  number    =  3,
  pages     = "215--240",
  month     =  nov,
  year      =  2006,
  address   = "Norwell, Netherlands"
}

@ARTICLE{Ghysels2012-vo,
  title    = "Forecasting Real Estate Prices",
  author   = "Ghysels, Eric and Plazzi, Alberto and Torous, Walter N and
              Valkanov, Rossen I",
  abstract = "This chapter reviews the evidence of predictability in US
              residential and commercial real estate markets. First, we
              highlight the main methodologies used in the",
  month    =  jun,
  year     =  2012,
  keywords = "real estate, predictability, market efficiency, REIT"
}

@ARTICLE{Ren2012-wu,
  title     = "House price bubbles in China",
  author    = "Ren, Yu and Xiong, Cong and Yuan, Yufei",
  abstract  = "In this paper, we apply the theory of rational expectation
               bubbles proposed by Blanchard and Watson (1983) to the Chinese
               housing market. The theory implies that negative returns on
               house prices are less likely to occur if the bubbles exist.
               Based on data from 35 cities in China, we find no evidence to
               support the existence of such bubbles in the Chinese housing
               market.",
  journal   = "China Econ. Rev.",
  publisher = "Elsevier",
  volume    =  23,
  number    =  4,
  pages     = "786--800",
  month     =  dec,
  year      =  2012,
  keywords  = "China house price; Rational expectation bubble; Hazard rate"
}

@ARTICLE{Schindler2010-de,
  title     = "How efficient is the {UK} housing market?",
  author    = "Schindler, Felix",
  abstract  = "Abstract: Extending the controversial findings from the relevant
               literature, the results from the quarterly transaction-based
               Nationwide indices from 1974 to 2009 provide further empirical
               evidence on the rejection of the weak-form version of efficiency
               in the UK housing market. ...",
  journal   = "ZEW-Centre for European Economic Research Discussion Paper",
  publisher = "papers.ssrn.com",
  number    = "10-030",
  year      =  2010
}

@ARTICLE{Holt2007-bk,
  title     = "The Ownership and Control of Elite Club Competition in European
               Football",
  author    = "Holt, Matthew",
  abstract  = "This essay analyzes the changing nature of governance in
               European football. Looking specifically at the control of elite
               European club competition, it argues that in the context of
               wider environmental transformation, traditional hierarchical
               modes of governance are being replaced by a stakeholder network
               in which the elite clubs are becoming increasingly influential.
               Consequently, the European governing body, UEFA, is under
               pressure to both integrate the most influential stakeholders
               into its decision?making procedures, and relinquish its control
               of elite club competition. The leverage of stakeholders is
               assessed, and it is argued that whilst the professional game
               will continue to pose challenges to the established bodies, the
               integrated nature of football governance means that change is
               likely to be evolutionary, rather than radical, and that the
               football associations of Europe will continue to play an
               important role in the organization of professional football in
               Europe.",
  journal   = "Soccer \& Society",
  publisher = "Routledge",
  volume    =  8,
  number    =  1,
  pages     = "50--67",
  month     =  jan,
  year      =  2007
}

@ARTICLE{Demsetz1985-zp,
  title     = "The Structure of Corporate Ownership: Causes and Consequences",
  author    = "Demsetz, Harold and Lehn, Kenneth",
  abstract  = "... STRUCTURE OF CORPORATE OWNERSHIP ... of shares controlled by
               top 20 shareholders; sources: same as A5 AH Herfindahl index of
               ownership concentration ... Calculated by summing the squared
               percentage of shares controlled by each shareholder ; sources:
               same as A5 F5 ...",
  journal   = "J. Polit. Econ.",
  publisher = "University of Chicago Press",
  volume    =  93,
  number    =  6,
  pages     = "1155--1177",
  year      =  1985
}

@TECHREPORT{Franck2016-ss,
  title       = "A comment on the newly revised ``2015 version'' of the {UEFA}
                 Club Licensing and Financial Fair Play Regulations",
  author      = "Franck, Egon",
  abstract    = "UEFA has revised its Club Licensing and Financial Fair Play
                 Regulations in June 2015 (new FFP). Based on a conceptual
                 analysis of the main components of new FFP -- the Break-Even
                 Requirement (BER), the Fair Market Value Principle (FVP) and
                 the concept of Voluntary Agreements (VA) -- this comment
                 arrives to the following main conclusions: New FFP follows old
                 FFP in creating hard budget constraints for football managers
                 and in discriminating against ``payroll gifts'' and therefore
                 against benefactors that are willing to inject money in
                 exchange for ``pure'' sporting success. But new FFP is now
                 less vulnerable to the allegation that it discriminates
                 against true entrepreneurs wishing to develop mismanaged
                 football clubs into sustainable businesses. The new concept of
                 Voluntary Agreements (VA)gives them more flexibility to
                 invest, particularly in such environments where quality is
                 only slowly remunerated by the football markets.",
  publisher   = "University of Zurich, Department of Business Administration
                 (IBW)",
  number      =  362,
  institution = "University of Zurich, Department of Business Administration
                 (IBW)",
  month       =  feb,
  year        =  2016
}

@ARTICLE{Brav2008-qb,
  title     = "Hedge Fund Activism, Corporate Governance, and Firm Performance",
  author    = "Brav, Alon and Jiang, Wei and Partnoy, Frank and Thomas, Randall",
  abstract  = "Using a large hand-collected data set from 2001 to 2006, we find
               that activist hedge funds in the United States propose
               strategic, operational, and financial remedies and attain
               success or partial success in two-thirds of the cases. Hedge
               funds seldom seek control and in most cases are
               nonconfrontational. The abnormal return around the announcement
               of activism is approximately 7\%, with no reversal during the
               subsequent year. Target firms experience increases in payout,
               operating performance, and higher CEO turnover after activism.
               Our analysis provides important new evidence on the mechanisms
               and effects of informed shareholder monitoring.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  63,
  number    =  4,
  pages     = "1729--1775",
  month     =  aug,
  year      =  2008
}

@ARTICLE{Griffin2009-jd,
  title     = "How Smart Are the Smart Guys? A Unique View from Hedge Fund
               Stock Holdings",
  author    = "Griffin, John M and Xu, Jin",
  abstract  = "Compared to mutual funds, hedge funds prefer smaller, opaque
               value securities, and have higher turnover and more active share
               bets. Decomposing returns into three components, we find that
               hedge funds are better than mutual funds at stock picking by
               only 1.32\% per year on a value-weighted basis, and this result
               is insignificant on an equal-weighted basis or with
               price-to-sales benchmarks. Hedge funds exhibit no ability to
               time sectors or pick better stock styles. Surprisingly, we find
               only weak evidence of differential ability between hedge funds.
               Overall, our study raises serious questions about the perceived
               superior skill of hedge fund managers.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Soc Financial Studies",
  volume    =  22,
  number    =  7,
  pages     = "2531--2570",
  month     =  jul,
  year      =  2009
}

@ARTICLE{Plumley2017-oy,
  title     = "Towards a model for measuring holistic performance of
               professional Football clubs",
  author    = "Plumley, Daniel and Wilson, Rob and Ramchandani, Girish",
  abstract  = "This paper introduces an experimental model to measure the
               holistic performance of professional football clubs. The model
               utilizes a selection of established financial and sporting
               indicators, which are weighted in accordance with their
               perceived relative importance and in relation to components of
               financial management and governing body regulations. The paper
               uses data pertaining to clubs competing in the English Premier
               League to demonstrate the outputs of the model. The authors
               argue that although the model is experimental, it still provides
               a useful platform to analyse performance of football clubs
               through further scientific investigation.",
  journal   = "Soccer \& Society",
  publisher = "Routledge",
  volume    =  18,
  number    =  1,
  pages     = "16--29",
  month     =  jan,
  year      =  2017
}

@PHDTHESIS{Anagnostopoulos2013-ad,
  title  = "Decision-making in English football: the case of corporate social
            responsibility",
  author = "Anagnostopoulos, C",
  year   =  2013,
  school = "Coventry University"
}

@ARTICLE{Demsetz2001-zw,
  title    = "Ownership structure and corporate performance",
  author   = "Demsetz, Harold and Villalonga, Bel{\'e}n",
  abstract = "This paper investigates the relation between the ownership
              structure and the performance of corporations if ownership is
              made multi-dimensional and also is treated as an endogenous
              variable. To our knowledge, no prior study has treated the
              corporate control problem this way. We find no statistically
              significant relation between ownership structure and firm
              performance. This finding is consistent with the view that
              diffuse ownership, while it may exacerbate some agency problems,
              also yields compensating advantages that generally offset such
              problems. Consequently, for data that reflect market-mediated
              ownership structures, no systematic relation between ownership
              structure and firm performance is to be expected.",
  journal  = "Journal of Corporate Finance",
  volume   =  7,
  number   =  3,
  pages    = "209--233",
  month    =  sep,
  year     =  2001,
  keywords = "Ownership structure; Corporate performance; Endogenous variable"
}

@ARTICLE{Environments_undated-lj,
  title  = "Roads to Resilience: Building Dynamic Approaches to Risk",
  author = "Environments, Uncertain Business"
}

@ARTICLE{Klinedinst2016-do,
  title     = "Bank Decapitalization and Credit Union Capitalization",
  author    = "Klinedinst, Mark A",
  abstract  = "This article looks at the theory and empirical findings of
               excessive compensation on the recent financial implosion across
               institutional forms in banking. Compensation levels have gone up
               dramatically over the last 30 years as deregulation and
               concentration have grown. Some banks and quite a few credit
               unions avoided closure by prudent portfolio selection and
               keeping reserves up by maintaining compensation levels closer to
               the median level. Empirical findings here are based on a unique
               panel data set on U.S. commercial banks, thrifts, and credit
               unions from 1994 through 2010 (more than 300,000 observations)
               that provide evidence that the firms with the highest net worth
               typically are smaller institutions, are credit unions, have
               smaller insider loans as a percentage of assets, and have lower
               average pay levels. The favorable results here for credit
               unions, financial cooperatives, should help guide policy when
               deciding which type of financial institutions should be
               encouraged.",
  journal   = "SAGE Open",
  publisher = "SAGE PublicationsSage CA: Los Angeles, CA",
  volume    =  6,
  number    =  1,
  pages     = "2158244016630031",
  month     =  mar,
  year      =  2016,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Rohde2016-yh,
  title    = "The Financial Impact of (Foreign) Private Investors on Team
              Investments and Profits in Professional Football: Empirical
              Evidence from the Premier League",
  author   = "Rohde, Marc and Breuer, Christoph",
  abstract = "US owners at Manchester United, Arsenal London, and Liverpool;
              Arab owners at Manchester City; Russian owners at Chelsea London
              -- the top football clubs from England have been acquired by
              private (mostly foreign) majority investors. Some of these clubs
              have been claimed to distort national competition, and Manchester
              City had to pay ‚Ç¨60m as a punishment for violating UEFA Financial
              Fair Play rules in 2014. Thus, this article addresses the
              controversial financial impact of (foreign) private majority
              investors in the overinvestment environment of European
              professional football. Applying property rights theory to an
              unbalanced panel from the first English division from 2005/06 to
              2011/12, this paper tests theoretical predictions from the `sugar
              daddy' literature and empirically shows that, first, private
              investors increase team investment and decrease profits. And,
              second, the positive influence on team investment can mainly be
              reduced to foreign investors. Implications for utility- and
              profit-maximizing team owners, managers, and regulators are
              derived.",
  journal  = "Decis. Econ. Finance",
  volume   =  3,
  number   =  2,
  pages    = "243--255",
  month    =  mar,
  year     =  2016,
  language = "en"
}

@ARTICLE{Wasserstein2016-pp,
  title     = "The {ASA's} Statement on p-Values: Context, Process, and Purpose",
  author    = "Wasserstein, Ronald L and Lazar, Nicole A",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  70,
  number    =  2,
  pages     = "129--133",
  month     =  apr,
  year      =  2016
}

@ARTICLE{Leclerc1999-it,
  title     = "Estimation des {\'E}conomies D'{\'e}chelle et de gamme dans de
               petites coop{\'e}ratives de services financiers: le cas des
               caisses populaires acadiennes",
  author    = "Leclerc, A and Fortin, M and Thivierge, C",
  abstract  = "This study uses accounting data stemming from 80 credit unions
               affiliated to the ``F{\'e}d{\'e}ration des caisses populaires
               acadiennes'' to estimate a multi-product translog cost function
               with the aim to test for the presence of scale and scope
               economies. The cost model relies on the production approach and
               the financial services are gathered in four categories of
               products. The model is completed by three inputs and one control
               variable, the latter being used to capture the heterogeneity of
               costs arising from the average wealth of membership. Since the
               estimated output elasticity of the total cost, 0.89, is
               statistically less than one, the models detect quite important
               scale economies. As to scope economies, they are present but the
               coefficients are weakly significant.",
  journal   = "Annals of Public and Cooperative Economics",
  publisher = "Blackwell Publishers Ltd.",
  volume    =  70,
  number    =  3,
  pages     = "447--474",
  month     =  sep,
  year      =  1999
}

@ARTICLE{Leclerc2009-st,
  title     = "{{\'E}conomies d'{{\'e}chelle et de gamme dans les
               coop{{\'e}ratives de services financiers: une approche non
               param{{\'e}trique ({DEA})",
  author    = "Leclerc, Andr{\'e} and Fortin, Mario",
  abstract  = "Lorsque des {\'e}conomies d'{\'e}chelle et de gamme sont
               simultan{\'e}ment pr{\'e}sentes, le regroupement des
               {\'e}tablissements tend {\`a} r{\'e}duire les co{\^u}ts. Puisque
               de nombreuses {\'e}tudes empiriques ont d{\'e}j{\`a} {\'e}tabli
               que les co{\^u}ts des banques tendent {\`a} devenir
               proportionnels au ...",
  journal   = "L'Actualit{\'e} {\'e}conomique",
  publisher = "HEC Montr{\'e}al",
  volume    =  85,
  number    =  3,
  pages     = "263--282",
  year      =  2009
}

@ARTICLE{Fortin2000-id,
  title     = "{{\'E}conomies d'{{\'e}chelle et de gamme dans les Caisses
               Desjardins",
  author    = "Fortin, Mario and Leclerc, Andr{\'e} and Thivierge, Claude",
  abstract  = "Nous utilisons les donn{\'e}es comptables de 1 073 caisses
               populaires et d'{\'e}conomies Desjardins du Qu{\'e}bec pour
               estimer une fonction de co{\^u}t translogarithmique
               multiproduits dans le but de tester la pr{\'e}sence
               d'{\'e}conomies d'{\'e}chelle et de gamme. En utilisant l' ...",
  journal   = "L'Actualit{\'e} {\'e}conomique",
  publisher = "HEC Montr{\'e}al",
  volume    =  76,
  number    =  3,
  pages     = "393--421",
  year      =  2000
}

@ARTICLE{Focardi2016-ux,
  title    = "A new approach to statistical arbitrage: Strategies based on
              dynamic factor models of prices and their performance",
  author   = "Focardi, Sergio M and Fabozzi, Frank J and Mitov, Ivan K",
  abstract = "Statistical arbitrage strategies are typically based on models of
              returns. We introduce a new statistical arbitrage strategy based
              on dynamic factor models of prices. Our objective in this paper
              is to exploit the mean-reverting properties of prices reported in
              the literature. We do so because, to capture the same information
              using a return-based factor model, a much larger number of lags
              would be needed, leading to inaccurate parameter estimation. To
              empirically test the relative performance of return-based and
              price-based models, we construct portfolios (long-short,
              long-only, and equally weighted) based on the forecasts generated
              by two dynamic factor models. Using the stock of companies
              included in the S\&P 500 index for constructing portfolios, the
              empirical analysis statistically tests the relative forecasting
              performance using the Diebold--Mariano framework and performing
              the test for statistical arbitrage proposed by Hogan et al.
              (2004). Our results show that prices allow for significantly more
              accurate forecasts than returns and pass the test for statistical
              arbitrage. We attribute this finding to the mean-reverting
              properties of stock prices. The high level of forecasting
              accuracy using price-based factor models has important
              theoretical and practical implications.",
  journal  = "Journal of Banking \& Finance",
  volume   =  65,
  pages    = "134--155",
  month    =  apr,
  year     =  2016,
  keywords = "Statistical arbitrage; Return-based factor models; Price-based
              factor models; Diebold--Mariano framework; Long-short strategies;
              Long-only strategies"
}

@ARTICLE{Moller2008-bo,
  title    = "The evolution of the January effect",
  author   = "Moller, Nicholas and Zilca, Shlomo",
  abstract = "This paper extends recent studies of the January effect by
              investigating the evolution of the daily pattern of the effect
              across size deciles. Our evidence documents a sizable mean
              reverting component beginning in the latter part of January and a
              shorter duration of the seasonal effect. Despite lower abnormal
              returns in the second part of January, higher abnormal returns in
              the first part of January keep the magnitude of the January
              effect unchanged. Further analysis of daily trading volumes
              suggests a stable trading volume intensity in the first part of
              January and a substantial decline in trading volume intensity in
              the second part of January.",
  journal  = "Journal of Banking \& Finance",
  volume   =  32,
  number   =  3,
  pages    = "447--457",
  month    =  mar,
  year     =  2008,
  keywords = "Abnormal returns; Anomalies; G14; January effect"
}

@ARTICLE{Gleasure2015-qx,
  title    = "Resistance to crowdfunding among entrepreneurs: An impression
              management perspective",
  author   = "Gleasure, Rob",
  abstract = "Crowdfunding has been a topic of much excitement for scholars of
              entrepreneurialism. Onlookers and early adopters have noted the
              new strategies afforded when funding bottlenecks can be bypassed
              and members of the public can be engaged early in development.
              Yet if crowdfunding is to prove truly disruptive for
              entrepreneurs then greater efforts must be made to understand
              non-adopters. This study models entrepreneurs' resistance to
              crowdfunding using an impression management perspective. A case
              study of 20 entrepreneurs suggests that resistance is influenced
              by entrepreneurs' fear of disclosure, fear of visible failure,
              and fear of projecting desperation.",
  journal  = "The Journal of Strategic Information Systems",
  volume   =  24,
  number   =  4,
  pages    = "219--233",
  month    =  dec,
  year     =  2015,
  keywords = "Crowdfunding; Crowdsourcing; Entrepreneurialism; Resistance;
              Impression management"
}

@UNPUBLISHED{De_Nardi_undated-gd,
  title  = "{ALL} {STOCKS} {ARE} {EQUAL}, {BUT} {SOME} {STOCKS} {ARE} {MORE}
            {EQUAL} {THAN} {OTHERS}: {ABNORMAL} {RETURNS} {OF} {FOOTBALL}
            {CLUBS}",
  author = "de Nardi, Riccardo"
}

@BOOK{Barth2006-yk,
  title     = "Rethinking Bank Regulation: Till Angels Govern",
  author    = "Barth, James R and Caprio, Gerard and Levine, Ross",
  editor    = "{Govern} and {Till} and {Angels}",
  abstract  = "This volume assembles and presents a database on bank regulation
               in over 150 countries (included also on CD). It offered the
               first comprehensive cross-country assessment of the impact of
               bank regulation on the operation of banks, and assesses the
               validity of the Basel Committee's influential approach to bank
               regulation. The treatment also provides an empirical evaluation
               of the historic debate about the proper role of government in
               the economy by studying bank regulation and analyzes the role of
               politics in determining regulatory approaches to banking. The
               data also indicate that restrictions on the entry of banks,
               government ownership of banks, and restrictions on bank
               activities hurt banking system performance. The authors find
               that domestic political factors shape both regulations and their
               effectiveness.",
  publisher = "Cambridge University Press",
  edition   = "Second edi",
  year      =  2006,
  address   = "New York",
  language  = "en"
}

@ARTICLE{Wilson1995-sp,
  title     = "Detecting influential observations in data envelopment analysis",
  author    = "Wilson, Paul W",
  abstract  = "This paper provides diagnostic tools for examining the role of
               influential observations in Data Envelopment Analysis (DEA)
               applications. Observations may be prioritized for further
               scrutiny to see if they are contaminated by data errors; this
               prioritization is important in situations where data-checking is
               costly and resources are limited. Several empirical examples are
               provided using data from previously published studies.",
  journal   = "J Prod Anal",
  publisher = "Kluwer Academic Publishers",
  volume    =  6,
  number    =  1,
  pages     = "27--45",
  month     =  apr,
  year      =  1995,
  language  = "en"
}

@ARTICLE{Brown1999-zy,
  title     = "Efficiency, Bond of Association and Exit Patterns in Credit
               Unions: Australian Evidence",
  author    = "Brown, Rayna and Brown, Rob and O'Connor, Ian",
  abstract  = "Data envelopment analysis is used in this study to provide
               measures of the efficiency of individual credit unions in the
               Australian state of Victoria in the period 1992--5. The
               resulting measures are consistent with those reported in
               comparable studies. There is no evidence that over the period of
               the study, the `average' credit union moved closer to the
               efficient frontier. Efficiency measures are analysed according
               to the bond of association and the results are consistent with
               the proposition that a tighter bond will tend to reduce
               operating costs. In the period of the study there were a large
               number of exits by merger, including exits by small credit
               unions with high efficiency measures. Possible explanations in
               terms of the expected benefits to the members of acquiring and
               exiting credit unions are suggested and evaluated.",
  journal   = "Annals of Public and Cooperative Economics",
  publisher = "Blackwell Publishers Ltd.",
  volume    =  70,
  number    =  1,
  pages     = "5--23",
  month     =  mar,
  year      =  1999
}

@ARTICLE{Cebenoyan1993-aw,
  title     = "Firm Efficiency and the Regulatory Closure of {S\&Ls}: An
               Empirical Investigation",
  author    = "Cebenoyan, A Sinan and Cooperman, Elizabeth S and Register,
               Charles A",
  abstract  = "This paper uses a two-step methodology to examine the
               relationship between firm inefficiency and the regulatory
               closure of savings and loans (S\&Ls). In the first step, using
               multiproduct, translog stochastic cost frontiers, the authors
               estimate inefficiency scores separately for mutual and stock
               S\&Ls operating in the Southwest in 1988. They use the
               inefficiency scores in second step logit models to identify
               determinants of regulatory closure. For both mutual and stock
               S\&Ls, the authors find a significant positive relationship
               between firm inefficiency and regulatory closure. They also find
               a greater probability of closure for S\&Ls in economically
               depressed states. Copyright 1993 by MIT Press.",
  journal   = "Rev. Econ. Stat.",
  publisher = "MIT Press",
  volume    =  75,
  number    =  3,
  pages     = "540--545",
  year      =  1993
}

@ARTICLE{Berger2007-cf,
  title     = "International Comparisons of Banking Efficiency",
  author    = "Berger, Allen N",
  abstract  = "The banking industry around the globe has been transformed in
               recent years by unprecedented consolidation and cross-border
               activities. However, international consolidation has been
               considerably less than might have been expected in developed
               nations---such as long-term members of the EU---where barriers
               to entry have been significantly lowered. In contrast,
               foreign-owned banks have generally achieved much higher
               penetration in developing nations. We investigate the extent to
               which these differences may be related to bank efficiency
               concerns by reviewing and critiquing over 100 studies that
               compare bank efficiencies across nations. The studies are in
               three distinct categories: (1) comparisons of bank efficiencies
               in different nations based on the use of a common efficient
               frontier, (2) comparisons of bank efficiencies in different
               nations using nation-specific frontiers, and (3) comparisons of
               efficiencies of foreign-owned versus domestically owned banks
               within the same nation using the same nation-specific frontier.
               The research---particularly the findings in the third
               category---is generally consistent with the hypothesis that
               efficiency differences help to explain the consolidation
               patterns. The efficiency disadvantages of foreign-owned banks
               relative to domestically owned banks tend to outweigh the
               efficiency advantages in developed nations on average, and this
               situation is generally reversed in developing nations, with
               notable exceptions to both findings. We also stress the need for
               further research in this area.",
  journal   = "Financial Markets, Institutions \& Instruments",
  publisher = "Blackwell Publishing Inc",
  volume    =  16,
  number    =  3,
  pages     = "119--144",
  month     =  aug,
  year      =  2007
}

@ARTICLE{Simar1998-fi,
  title     = "Sensitivity Analysis of Efficiency Scores: How to Bootstrap in
               Nonparametric Frontier Models",
  author    = "Simar, L{\'e}opold and Wilson, Paul W",
  abstract  = "Efficiency scores of production units are generally measured
               relative to an estimated production frontier. Nonparametric
               estimators (DEA, FDH, ?) are based on a finite sample of
               observed production units. The bootstrap is one easy way to
               analyze the sensitivity of efficiency scores relative to the
               sampling variations of the estimated frontier. The main point in
               order to validate the bootstrap is to define a reasonable
               data-generating process in this complex framework and to propose
               a reasonable estimator of it. This paper provides a general
               methodology of bootstrapping in nonparametric frontier models.
               Some adapted methods are illustrated in analyzing the bootstrap
               sampling variations of input efficiency measures of electricity
               plants.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  44,
  number    =  1,
  pages     = "49--61",
  month     =  jan,
  year      =  1998,
  keywords  = "Bootstrap; Data Envelopment Analysis; Frontier Efficiency
               Models; Resampling Methods"
}

@ARTICLE{Sealey1977-ct,
  title     = "Inputs, Outputs, and a Theory of Production and Cost at
               Depository Financial Institutions",
  author    = "Sealey, C W and Lindley, James T",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  32,
  number    =  4,
  pages     = "1251--1266",
  month     =  sep,
  year      =  1977
}

@ARTICLE{Hughes1993-kk,
  title     = "A quality and risk-adjusted cost function for banks: Evidence on
               the ``too-big-to-fail'' doctrine",
  author    = "Hughes, Joseph P and Mester, Loretta J",
  abstract  = "We estimate a multiproduct cost function model incorporating
               measures of bank output quality and the probability of failure.
               We model a bank's uninsured deposit price as an endogenous
               variable depending on the bank's output level, output quality,
               financial capital level, and risk measures. Accounting for these
               aspects in the cost model significantly affects measures of
               scale and scope economies. We find evidence that the
               ``too-big-to-fail'' doctrine significantly affects the price a
               bank pays for its uninsured deposits. For large banks, an
               increase in size, holding default risk and asset quality
               constant, significantly lowers the uninsured deposit price.",
  journal   = "J Prod Anal",
  publisher = "Kluwer Academic Publishers",
  volume    =  4,
  number    =  3,
  pages     = "293--315",
  month     =  sep,
  year      =  1993,
  language  = "en"
}

@ARTICLE{Hasan2003-of,
  title    = "Development and efficiency of the banking sector in a
              transitional economy: Hungarian experience",
  author   = "Hasan, Iftekhar and Marton, Katherin",
  abstract = "The paper analyzes the experiences and developments of Hungarian
              banking sector during the transitional process from a centralized
              economy to a market-oriented system. The paper identifies that
              early reorganization initiatives, flexible approaches to
              privatization, and liberal policies towards foreign banks'
              involvement with the domestic institutions helped to build a
              relatively stable and increasingly efficient banking system.
              Foreign banks and banks with higher foreign bank ownership
              involvement were associated with lower inefficiency.",
  journal  = "Journal of Banking \& Finance",
  volume   =  27,
  number   =  12,
  pages    = "2249--2271",
  month    =  dec,
  year     =  2003,
  keywords = "Transitional economy; Banking sector; Efficiency; Hungary"
}

@INCOLLECTION{Imbens2007-lm,
  title     = "Nonadditive Models with Endogenous Regressors",
  booktitle = "Advances in Economics and Econometrics",
  author    = "Imbens, Guido W",
  editor    = "Blundell, Richard and Newey, Whitney and Persson, Torsten",
  abstract  = "Conditional on this the of interest is how they relate to the
               specified in Traditionally researchers have focused on of the",
  publisher = "Cambridge University Press",
  volume    =  3,
  pages     = "17--46",
  month     =  aug,
  year      =  2007,
  address   = "New York"
}

@ARTICLE{Hasbrouck1991-hd,
  title     = "Measuring the Information Content of Stock Trades",
  author    = "Hasbrouck, Joel",
  abstract  = "This paper suggests that the interactions of security trades and
               quote revisions be modeled as a vector autoregressive system.
               Within this framework, a trade's information effect may be
               meaningfully measured as the ultimate price impact of the trade
               innovation. Estimates for a sample of NYSE issues suggest: a
               trade's full price impact arrives only with a protracted lag;
               the impact is a positive and concave function of the trade size;
               large trades cause the spread to widen; trades occurring in the
               face of wide spreads have larger price impacts; and, information
               asymmetries are more significant for smaller firms.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  46,
  number    =  1,
  pages     = "179--207",
  month     =  mar,
  year      =  1991
}

@ARTICLE{Feldstein1972-xd,
  title     = "Equity and Efficiency in Public Sector Pricing: The Optimal
               {Two-Part} Tariff",
  author    = "Feldstein, Martin S",
  abstract  = "I. Introduction, 175. --- II. The optimal price, 176. --- III.
               The welfare loss due to incorrect pricing, 182. --- IV. An
               example, 183. --- V. Concluding remarks, 187.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  86,
  number    =  2,
  pages     = "175--187",
  month     =  may,
  year      =  1972
}

@ARTICLE{Das2005-ka,
  title    = "Quality of Financial Policies and Financial System Stress",
  author   = "Das, Udaibir and Iossifov, Plamen K and Podpiera, Richard and
              Rozhkov, Dmitriy",
  abstract = "In this paper, we develop multi-country indices of financial
              system stress and quality of financial policies and use them in
              regression analysis of the determin",
  journal  = "IMF working paper 05/173",
  month    =  aug,
  year     =  2005,
  keywords = "Financial crises, financial stress, financial policies"
}

@ARTICLE{Battese1977-ts,
  title     = "Estimation Of A Production Frontier Model: With Application To
               The Pastoral Zone Of Eastern Australia",
  author    = "Battese, George E and Corra, Greg S",
  abstract  = "This paper considers a statistical model for a production
               frontier that is consistent with the traditional (nonstochastic)
               definition of a production function given in microeconomic
               theory. Limiting cases of the model are the familiar average
               production function and an envelope production function.
               Maximum-likelihood estimators for the parameters of the model
               are defined. The three related models are applied in the
               estimation of a production frontier for the Pastoral Zone of
               Eastern Australia with use of data from the Australian Grazing
               Industry Survey.",
  journal   = "Aust. J. Agric. Econ.",
  publisher = "Australian Agricultural and Resource Economics Society",
  volume    =  21,
  number    =  03,
  year      =  1977,
  keywords  = "Production Economics"
}

@ARTICLE{Fare1978-nm,
  title   = "Measuring the technical efficiency of production",
  author  = "F{\"a}re, Rolf and Knox Lovell, C A",
  journal = "J. Econ. Theory",
  volume  =  19,
  number  =  1,
  pages   = "150--162",
  month   =  oct,
  year    =  1978
}

@ARTICLE{Ray1988-ev,
  title    = "Data envelopment analysis, nondiscretionary inputs and
              efficiency: an alternative interpretation",
  author   = "Ray, Subhash C",
  abstract = "Various methods using econometric or mathematical programming
              techniques are available in the literature for measurement of
              efficiency of firms. None of these, however, investigates the
              sources of differences in efficiency across productive units. In
              this paper we used the method of Data Envelopment Analysis
              introduced by Charnes, Cooper and Rhodes as the basic format to
              show that observed differences in efficiency can be explained in
              terms of differences in nondiscretionary inputs across firms.",
  journal  = "Socioecon. Plann. Sci.",
  volume   =  22,
  number   =  4,
  pages    = "167--176",
  month    =  jan,
  year     =  1988
}

@ARTICLE{Pasiouras2009-mh,
  title    = "The impact of banking regulations on banks' cost and profit
              efficiency: Cross-country evidence",
  author   = "Pasiouras, Fotios and Tanna, Sailesh and Zopounidis, Constantin",
  abstract = "This paper uses stochastic frontier analysis to provide
              international evidence on the impact of the regulatory and
              supervision framework on bank efficiency. Our dataset consists of
              2853 observations from 615 publicly quoted commercial banks
              operating in 74 countries during the period 2000--2004. We
              investigate the impact of regulations related to the three
              pillars of Basel II (i.e. capital adequacy requirements, official
              supervisory power, and market discipline mechanisms), as well as
              restrictions on bank activities, on cost and profit efficiency of
              banks, while controlling for other country-specific
              characteristics. Our results suggest that banking regulations
              that enhance market discipline and empower the supervisory power
              of the authorities increase both cost and profit efficiency of
              banks. In contrast, stricter capital requirements improve cost
              efficiency but reduce profit efficiency, while restrictions on
              bank activities have the opposite effect, reducing cost
              efficiency but improving profit efficiency.",
  journal  = "International Review of Financial Analysis",
  volume   =  18,
  number   =  5,
  pages    = "294--302",
  month    =  dec,
  year     =  2009,
  keywords = "c24; d2; g21; g28; Banking; Efficiency; Regulations; Stochastic
              frontier analysis"
}

@MISC{Sundararajan2001-cu,
  title       = "The Case of Basel Core Principles",
  author      = "Sundararajan, V and Marston, D and Basu, R",
  abstract    = "The relationship between the observance of financial system
                 standards and financial stability is complex owing to the
                 multitude of macroeconomic and structural factors affecting
                 stability. Therefore, assessments of standards in terms of
                 technical criteria for compliance needs to be reinforced with
                 additional information on other factors affecting risks in
                 order to assess financial stability. Preliminary evidence from
                 country data on observance of Basel Core Principles (BCPs)
                 suggests that indicators of credit risk and bank soundness are
                 primarily influenced by macroeconomic and macroprudential
                 factors and that the direct influence of compliance with Basel
                 Core Principles on credit risk and soundness is insignificant.
                 BCP compliance could, however, influence risk and soundness
                 indirectly through its influence on the impact of other macro
                 variables.",
  publisher   = "Washington",
  institution = "International Monetary Fund",
  year        =  2001,
  address     = "Washington"
}

@ARTICLE{McKillop2015-oz,
  title     = "Web Adoption By Irish Credit Unions: Performance Implications",
  author    = "McKillop, D G and Quinn, B",
  abstract  = "ABSTRACT The purpose of this paper is to examine website
               adoption and its resultant effects on credit union performance
               in Ireland over the period 2002 to 2010. While there has been a
               steady increase in web adoption over the period a sizeable
               proportion (53\%) of ...",
  journal   = "Annals of Public and Cooperative",
  publisher = "Wiley Online Library",
  volume    =  86,
  number    =  3,
  year      =  2015,
  keywords  = "Community cooperatives; D21; H44; L33; P13; citizen
               participation; public services"
}

@ARTICLE{Pluemper2004-cr,
  title     = "The Estimation of {Time-Invariant} Variables in Panel Analyses
               with Unit Fixed Effects",
  author    = "Pluemper, Thomas and Troeger, Vera E",
  abstract  = "This paper analyzes the estimation of time-invariant variables
               in panel data models with unit-effects. We compare three
               procedures that have frequently been emp",
  journal   = "Available at SSRN 565904",
  publisher = "papers.ssrn.com",
  month     =  aug,
  year      =  2004,
  keywords  = "Time invariate variable, fixed effects, panel analysis, vector
               decomposition, Hausman-Taylor"
}

@INCOLLECTION{Matzkin1994-aq,
  title     = "Chapter 42 Restrictions of economic theory in nonparametric
               methods",
  booktitle = "Handbook of Econometrics",
  author    = "Matzkin, Rosa L",
  abstract  = "This chapter describes several nonparametric estimation and
               testing methods for econometric models. Instead of using
               parametric assumptions on the functions and distributions in an
               economic model, the methods use the restrictions that can be
               derived from the model. Examples of such restrictions are the
               concavity and monotonicity of functions, equality conditions,
               and exclusion restrictions. The chapter shows, first, how
               economic restrictions can guarantee the identification of
               nonparametric functions in several structural models. It then
               describes how shape restrictions can be used to estimate
               nonparametric functions using popular methods for nonparametric
               estimation. Finally, the chapter describes how to test
               nonparametrically the hypothesis that an economic model is
               correct and the hypothesis that a nonparametric function
               satisfies some specified shape properties.",
  publisher = "Elsevier",
  volume    =  4,
  pages     = "2523--2558",
  series    = "Handbook of Econometrics",
  year      =  1994
}

@ARTICLE{Blundell_undated-st,
  title  = "Evaluating the Impact of Education on Earnings in the {UK}: Models,
            Methods and Results from the {NCDS}",
  author = "Blundell, Richard and Dearden, Lorraine and Sianesi, Barbara"
}

@ARTICLE{Ben-Khedhiri2011-uu,
  title    = "What Drives the Performance of Selected {MENA} Banks? A
              {Meta-Frontier} Analysis",
  author   = "Ben-Khedhiri, Hichem and Casu, Barbara and Ben Naceur, Sami",
  abstract = "This study examines the effect of financial-sector reform on bank
              performance in selected Middle Eastern and North African (MENA)
              countries in the period 1994-2",
  month    =  feb,
  year     =  2011,
  keywords = "DEA; MENA; meta-frontier; performance; Banking sector; Cross
              country analysis; Economic models; Egypt; Jordan; Lebanon; Middle
              East; Morocco; North Africa; Tunisia"
}

@ARTICLE{Becker2007-jx,
  title    = "Mhbounds - Sensitivity Analysis for Average Treatment Effects",
  author   = "Becker, Sascha O and Caliendo, Marco",
  abstract = "Matching has become a popular approach to estimate average
              treatment effects. It is based on the conditional independence or
              unconfoundedness assumption. Checki",
  journal  = "IZA Discussion Paper No. 2542",
  volume   =  7,
  pages    = "71--83",
  month    =  jan,
  year     =  2007,
  keywords = "matching; mhbounds; sensitivity analysis; st0121; treatment
              effects; unobserved; unobserved heterogeneity"
}

@BOOK{Couper2008-zz,
  title     = "Designing effective web surveys",
  author    = "Couper, Mick P",
  publisher = "Cambridge University Press New York",
  volume    =  75,
  year      =  2008
}

@ARTICLE{Trivedi2006-rv,
  title     = "Copula Modeling: An Introduction for Practitioners",
  author    = "Trivedi, Pravin K and Zimmer, David M",
  journal   = "FNT in Econometrics",
  publisher = "Now Publishers",
  volume    =  1,
  number    =  1,
  pages     = "1--111",
  year      =  2006,
  keywords  = "Econometric models"
}

@ARTICLE{Sener2011-ks,
  title     = "A {Copula-Based} Sample Selection Model of Telecommuting Choice
               and Frequency",
  author    = "Sener, Ipek N and Bhat, Chandra R",
  abstract  = "The objective of this study is to contribute to the
               telecommuting literature by jointly examining the propensity and
               frequency of workers to telecommute, using a rich set of
               individual demographics, work-related and industry
               characteristics, household demographics, and
               commute-trip/work-location characteristics. The data are drawn
               from the Chicago Regional Household Travel Inventory, collected
               between 2007 and 2008. From a methodological standpoint, the
               current study adopts a copula approach that allows the testing
               of several types of dependency structures between the
               telecommuting choice and frequency behavioral processes. To our
               knowledge, this is the first formulation and application in the
               econometric literature of a copula approach for the case of a
               binary self-selection mechanism with an ordered-response
               outcome. The results clearly indicate that telecommuting choice
               and the frequency of telecommuting are governed by quite
               different underlying behavioral processes. For instance, women
               are less likel...",
  journal   = "Environment and Planning A",
  publisher = "SAGE PublicationsSage UK: London, England",
  volume    =  43,
  number    =  1,
  pages     = "126--145",
  month     =  jan,
  year      =  2011,
  language  = "en"
}

@TECHREPORT{noauthor_2014-dy,
  title       = "Financial Stability Monitoring",
  institution = "Federal Reserve of New York",
  year        =  2014
}

@ARTICLE{noauthor_undated-tp,
  title = "[{PDF]The} economics of caste and the rat race and other woeful
           tales"
}

@ARTICLE{Carbo-Valverde2015-em,
  title     = "Regulatory response to the financial crisis in Europe: recent
               developments (2010-2013)",
  author    = "Carb{\'o}-Valverde, Santiago and Benink, Harald A and Berglund,
               Tom and Wihlborg, Clas",
  journal   = "Journal of Financial Economic Policy; Bingley",
  publisher = "Emerald Group Publishing Limited",
  volume    =  7,
  number    =  1,
  pages     = "29--50",
  year      =  2015,
  address   = "Greece; Europe, United Kingdom",
  language  = "en"
}

@ARTICLE{Szymanski1998-jn,
  title     = "Why is Manchester United So Successful?",
  author    = "Szymanski, Stefan",
  abstract  = "Football is now a big industry worth studying in its own right:
               but it is also an excellent laboratory for studying success. The
               author argues that this is partly because, in contrast to say,
               American Football, there is less equalisation of resources
               between the clubs and English football is a highly competitive
               market. This article discusses one of the most successful clubs
               ever and argues that the reasons behind its success do not
               characterise football generally. It discusses how the different
               kinds of success in the English football industry are related to
               business success generally.",
  journal   = "Business Strategy Review",
  publisher = "Blackwell Publishers Ltd",
  volume    =  9,
  number    =  4,
  pages     = "47--54",
  month     =  dec,
  year      =  1998
}

@ARTICLE{Pastor2002-mr,
  title     = "Credit risk and efficiency in the European banking system: A
               three-stage analysis",
  author    = "Pastor, Jose",
  abstract  = "Increased competition and the attempts of European banks to
               increase their presence in other markets may have affected the
               efficiency and credit risk in the banking system. The first
               aspect is the incentive in reducing costs in order to gain in
               competitiveness. The second is associated with their lack of
               knowledge of such markets and/ or acceptance of a higher risk in
               order to increase their market share. Despite the importance of
               these aspects, banking literature has usually analysed the
               effects of competition on the efficiency of banking systems
               without considering these aspects. The few studies that attempt
               to obtain risk adjusted efficiency measures do not consider that
               part of the risk is due to exogeneous circumstances. This
               article proposes a new three-stage sequential technique, based
               on the DEA model and on the decomposition of risk into its
               internal and external components, for obtaining efficiency
               measures adjusted for risk and environment. It is seen that the
               technique allows the use of any existing technique of
               incorporation of environmental variables in DEA analysis.",
  journal   = "Applied Financial Economics",
  publisher = "Taylor \& Francis Journals",
  volume    =  12,
  number    =  12,
  pages     = "895--911",
  year      =  2002
}

@ARTICLE{Latruffe2008-ay,
  title     = "Application of a double bootstrap to investigation of
               determinants of technical efficiency of farms in Central Europe",
  author    = "Latruffe, Laure and Davidova, Sophia and Balcombe, Kelvin",
  abstract  = "No abstract is available for this item.",
  journal   = "Journal of Productivity Analysis",
  publisher = "Springer",
  volume    =  29,
  number    =  2,
  pages     = "183--191",
  year      =  2008,
  keywords  = "Corporate farms; Czech Republic; D24; DEA; Double bootstrap;
               Individual farms; Q12; Truncated maximum likelihood"
}

@ARTICLE{Ly2017-py,
  title     = "The Basel {III} net stable funding ratio adjustment speed and
               systemic risk",
  author    = "Ly, Kim Cuong and Chen, Zhizhen and Wang, Senyu and Jiang,
               Yuxiang",
  abstract  = "The theory on the timing of liquidity trades highlights two
               contrasting rational expectations equilibria for the liquidity
               adjustment speed effect, namely an immediate-trading equilibrium
               (trade at the onset of the liquidity shock) and a
               delayed-trading equilibrium (trade at the last resort). Using a
               partial adjustment model and an annual data sample of US bank
               holding companies from 1991 to 2012, we investigate the effect
               of Net Stable Funding Ratio (NSFR) adjustment speeds on systemic
               risk. We find that banks with the immediate-trading equilibrium
               tend to adjust the NSFR quickly in response to the Basel III
               liquidity requirement, thereby, reducing systemic risk. With the
               same level of the NSFR, our findings suggest that only the
               adjustment speed exerts a negative impact on systemic risk. Our
               evidence shows that small banks strengthen the effects of the
               negative impact of the NSFR adjustment speed on systemic risk.
               Our study sheds light on a real-time indicator of the NSFR for
               Basel III revisions before its implementation in 2018.",
  journal   = "Research in International Business and Finance",
  publisher = "Elsevier Ltd",
  volume    =  39,
  pages     = "169--182",
  year      =  2017
}

@TECHREPORT{Alexander2017-kq,
  title       = "The Law of One Bitcoin Price?",
  author      = "Alexander, Kroeger and Asani, Sarkar",
  institution = "Federal Reserve Bank of New York",
  year        =  2017
}

@TECHREPORT{Adrian2008-db,
  title       = "{CoVaR}",
  author      = "Adrian, Tobias and Brunnermeier, Markus K",
  number      = "Staff Report 348",
  institution = "Federal Reserve Bank of New York",
  year        =  2008
}

@ARTICLE{Mainik2014-iu,
  title    = "On dependence consistency of {CoVaR} and some other systemic risk
              measures",
  author   = "Mainik, Georg and Schaanning, Eric",
  abstract = "This paper is dedicated to the consistency of systemic risk
              measures with respect to stochastic dependence. It compares two
              alternative notions of Conditional Value-at-Risk (CoVaR)
              available in the current literature. These notions are both based
              on the conditional distribution of a random variable Y given a
              stress event for a random variable X , but they use different
              types of stress events. We derive representations of these
              alternative CoVaR notions in terms of copulas, study their
              general dependence consistency and compare their performance in
              several stochastic models. Our central finding is that
              conditioning on X $\geq$ VaR $\alpha$ ( X ) gives a much better
              response to dependence between X and Y than conditioning on X =
              VaR $\alpha$ ( X ). We prove general results that relate the
              dependence consistency of CoVaR using conditioning on X $\geq$
              VaR $\alpha$ ( X ) to well established results on concordance
              ordering of multivariate distributions or their copulas. These
              results also apply to some other systemic risk measures, such as
              the Marginal Expected Shortfall (MES) and the Systemic Impact
              Index (SII). We provide counterexamples showing that CoVaR based
              on the stress event X = VaR $\alpha$ ( X ) is not dependence
              consistent. In particular, if ( X , Y ) is bivariate normal, then
              CoVaR based on X = VaR $\alpha$ ( X ) is not an increasing
              function of the correlation parameter. Similar issues arise in
              the bivariate t model and in the model with t margins and a
              Gumbel copula. In all these cases, CoVaR based on X $\geq$ VaR
              $\alpha$ ( X ) is an increasing function of the dependence
              parameter.",
  journal  = "Statistics \& Risk Modeling",
  volume   =  31,
  number   =  1,
  month    =  jan,
  year     =  2014
}

@ARTICLE{Castro2014-ql,
  title    = "Measuring and testing for the systemically important financial
              institutions",
  author   = "Castro, Carlos and Ferrari, Stijn",
  abstract = "This paper analyzes $\Delta$CoVaR proposed by Adrian and
              Brunnermeier (2011) as a tool for identifying/ranking
              systemically important institutions. We develop a test of
              significance of $\Delta$CoVaR that allows determining whether or
              not a financial institution can be classified as being
              systemically important on the basis of the estimated systemic
              risk contribution, as well as a test of dominance aimed at
              testing whether or not, according to $\Delta$CoVaR, one financial
              institution is more systemically important than another. We
              provide an empirical application on a sample of 26 large European
              banks to show the importance of statistical testing when using
              $\Delta$CoVaR, and more generally also other market-based
              systemic risk measures, in this context.",
  journal  = "Journal of Empirical Finance",
  volume   =  25,
  pages    = "1--14",
  month    =  jan,
  year     =  2014,
  keywords = "Systemic risk; SIFIs; Quantile regression; Stochastic dominance
              test"
}

@ARTICLE{Billio2015-au,
  title    = "An {Entropy-Based} Early Warning Indicator for Systemic Risk",
  author   = "Billio, Monica and Casarin, Roberto and Costola, Michele and
              Pasqualini, Andrea",
  abstract = "The purpose of this paper is the construction of an early warning
              indicator for systemic risk using entropy measures. The analysis
              is based on the cross-section",
  month    =  may,
  year     =  2015,
  keywords = "Entropy, systemic risk measures, early warning indicators,
              aggregation"
}

@ARTICLE{Doring2016-qg,
  title    = "Systemic Risk Measures and Their Viability for Banking
              Supervision",
  author   = "D{\"o}ring, Benjamin and Wewel, Claudio Nicolai and
              Hartmann-Wendels, Thomas",
  abstract = "We propose a criteria-based framework to assess the viability of
              systemic risk measures (SRMs) as a monitoring tool for banking
              supervision and investigate the",
  month    =  jan,
  year     =  2016,
  keywords = "banking crises, CoVaR, macro-financial linkages, MES, SRISK,
              supervision, systemic risk"
}

@ARTICLE{Acharya2014-cd,
  title    = "Counterparty risk externality: Centralized versus
              over-the-counter markets",
  author   = "Acharya, Viral and Bisin, Alberto",
  abstract = "We study financial markets where agents share risks, but have
              incentives to default and their financial positions might not be
              transparent, that is, might not be mutually observable. We show
              that a lack of position transparency results in a counterparty
              risk externality, that manifests itself in the form of excess
              ``leverage,'' in that parties take on short positions that lead
              to levels of default risk that are higher than Pareto efficient
              ones. This externality is absent when trading is organized via a
              centralized clearing mechanism that provides transparency of
              trade positions. Collateral requirements and especially
              subordination of non-transparent positions in bankruptcy can
              ameliorate the counterparty risk externality in market settings
              such as over-the-counter (OTC) markets which feature a lack of
              position transparency.",
  journal  = "J. Econ. Theory",
  volume   =  149,
  pages    = "153--182",
  month    =  jan,
  year     =  2014,
  keywords = "Counterparty risk; Leverage; Transparency; Centralized clearing;
              Collateral; OTC markets"
}

@ARTICLE{Acemoglu2015-ad,
  title   = "Networks, Shocks, and Systemic Risk",
  author  = "Acemoglu, D and Ozdaglar, A and Tahbaz-Salehi, A",
  journal = "NBER Working Paper \#20931",
  year    =  2015
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Engle2012-wn,
  title   = "Systemic Risk in Europe",
  author  = "Engle, Robert",
  journal = "Swiss Finance Institute Research Paper Series N¬∞12 - 45",
  month   =  dec,
  year    =  2012
}

@ARTICLE{Mester1996-ug,
  title    = "A study of bank efficiency taking into account risk-preferences",
  author   = "Mester, Loretta J",
  abstract = "I use the stochastic cost frontier approach to investigate
              efficiency of banks operating in the Third Federal Reserve
              District, accounting for the quality and riskiness of bank
              output. In addition to the mean and mode of the conditional
              distribution of the one-sided error term, I calculate confidence
              intervals for the inefficiency measures based on the conditional
              distribution. The results indicate that Third District banks are
              operating at cost-efficient output levels and product mixes, but
              are not efficiently using their inputs. The second part of the
              article relates the inefficiency measures to several correlates.",
  journal  = "Journal of Banking \& Finance",
  volume   =  20,
  number   =  6,
  pages    = "1025--1045",
  month    =  jul,
  year     =  1996,
  keywords = "Banks; Efficiency; Risk"
}

@ARTICLE{Fiordelisi2011-ye,
  title    = "Efficiency and risk in European banking",
  author   = "Fiordelisi, Franco and Marques-Ibanez, David and Molyneux, Phil",
  abstract = "We assess the inter-temporal relationship between bank
              efficiency, capital and risk in a sample of European commercial
              banks employing several definitions of efficiency, risk and
              capital and using the Granger-causality methodology in a panel
              data framework. Our results suggest that lower bank efficiency
              with respect to costs and revenues Granger-causes higher bank
              risk and that increases in bank capital precede cost efficiency
              improvements. We also find that more efficient banks eventually
              become better capitalized and that higher capital levels tend to
              have a positive effect on efficiency levels. These results are
              generally confirmed by a series of robustness tests. The results
              have potentially important implications for bank prudential
              supervision and underline the importance of attaining long-term
              efficiency gains to support financial stability objectives.",
  journal  = "Journal of Banking \& Finance",
  volume   =  35,
  number   =  5,
  pages    = "1315--1326",
  month    =  may,
  year     =  2011,
  keywords = "Banking risk; Capital; Efficiency"
}

@ARTICLE{Gelman2006-dl,
  title     = "Multilevel (Hierarchical) Modeling: What It Can and Cannot Do",
  author    = "Gelman, Andrew",
  abstract  = "Multilevel (hierarchical) modeling is a generalization of linear
               and generalized linear modeling in which regression coefficients
               are themselves given a model, whose parameters are also
               estimated from data. We illustrate the strengths and limitations
               of multilevel modeling through an example of the prediction of
               home radon levels in U.S. counties. The multilevel model is
               highly effective for predictions at both levels of the model,
               but could easily be misinterpreted for causal inference.",
  journal   = "Technometrics",
  publisher = "Taylor \& Francis",
  volume    =  48,
  number    =  3,
  pages     = "432--435",
  month     =  aug,
  year      =  2006
}

@ARTICLE{Government_of_Ontario2015-bb,
  title     = "Credit Unions and Caisses Populaires Act, 1994 - {LEGISLATIVE}
               {REVIEW}",
  author    = "Government of Ontario, Ministry of Finance",
  abstract  = "Credit Unions and Caisses Populaires Act, 1994 - LEGISLATIVE
               REVIEW",
  publisher = "Ministry of Finance: Government of Ontario",
  month     =  nov,
  year      =  2015
}

@ARTICLE{Mayordomo2014-hy,
  title    = "Are All Credit Default Swap Databases Equal?",
  author   = "Mayordomo, Sergio and Pe{\~n}a, Juan Ignacio and Schwartz,
              Eduardo S",
  abstract = "We compare the five major sources of corporate Credit Default
              Swap prices: GFI, Fenics, Reuters, CMA, and Markit, using the
              most liquid single name 5-year CDS in the iTraxx and CDX indexes
              from 2004 to 2010. Deviations from the common trend among prices
              in the different databases are not random but are explained by
              idiosyncratic factors, financing costs, global risk, and other
              trading factors. The CMA quotes lead the price discovery process.
              Moreover, we find that there is not a full agreement among
              databases in the results of the price discovery analysis between
              stock and CDS returns.",
  journal  = "Eur Financial Management",
  volume   =  20,
  number   =  4,
  pages    = "677--713",
  month    =  sep,
  year     =  2014,
  keywords = "credit default swap prices; databases; liquidity"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Mohring_undated-cq,
  title  = "The fixed effects approach as alternative to multilevel models for
            cross‚Äênational analyses",
  author = "M{\"o}hring, Katja"
}

@ARTICLE{Dopico2016-zw,
  title   = "Credit Unions: Financial Sustainability and Scale",
  author  = "Dopico, Luis G",
  journal = "Filene Research Institute",
  year    =  2016
}

@ARTICLE{Hendershott2011-vd,
  title     = "Does Algorithmic Trading Improve Liquidity?",
  author    = "Hendershott, Terrence and Jones, Charles M and Menkveld, Albert
               J",
  abstract  = "Algorithmic trading (AT) has increased sharply over the past
               decade. Does it improve market quality, and should it be
               encouraged? We provide the first analysis of this question. The
               New York Stock Exchange automated quote dissemination in 2003,
               and we use this change in market structure that increases AT as
               an exogenous instrument to measure the causal effect of AT on
               liquidity. For large stocks in particular, AT narrows spreads,
               reduces adverse selection, and reduces trade-related price
               discovery. The findings indicate that AT improves liquidity and
               enhances the informativeness of quotes.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  66,
  number    =  1,
  pages     = "1--33",
  month     =  feb,
  year      =  2011
}

@ARTICLE{Hasbrouck2013-ro,
  title    = "Low-latency trading",
  author   = "Hasbrouck, Joel and Saar, Gideon",
  abstract = "We define low-latency activity as strategies that respond to
              market events in the millisecond environment, the hallmark of
              proprietary trading by high-frequency traders though it could
              include other algorithmic activity as well. We propose a new
              measure of low-latency activity to investigate the impact of
              high-frequency trading on the market environment. Our measure is
              highly correlated with NASDAQ-constructed estimates of
              high-frequency trading, but it can be computed from
              widely-available message data. We use this measure to study how
              low-latency activity affects market quality both during normal
              market conditions and during a period of declining prices and
              heightened economic uncertainty. Our analysis suggests that
              increased low-latency activity improves traditional market
              quality measures---decreasing spreads, increasing displayed depth
              in the limit order book, and lowering short-term volatility. Our
              findings suggest that given the current market structure for U.S.
              equities, increased low-latency activity need not work to the
              detriment of long-term investors.",
  journal  = "Journal of Financial Markets",
  volume   =  16,
  number   =  4,
  pages    = "646--679",
  month    =  nov,
  year     =  2013,
  keywords = "High-frequency trading; Limit order markets; NASDAQ; Order
              placement strategies; Liquidity; Market quality"
}

@ARTICLE{Chaboud2014-qi,
  title    = "Rise of the Machines: Algorithmic Trading in the Foreign Exchange
              Market",
  author   = "Chaboud, Alain P and Chiquoine, Benjamin and Hjalmarsson, Erik
              and Vega, Clara",
  abstract = "We study the impact of algorithmic trading (AT) in the foreign
              exchange market using a long time series of high-frequency data
              that identify computer-generated trading activity. We find that
              AT causes an improvement in two measures of price efficiency: the
              frequency of triangular arbitrage opportunities and the
              autocorrelation of high-frequency returns. We show that the
              reduction in arbitrage opportunities is associated primarily with
              computers taking liquidity. This result is consistent with the
              view that AT improves informational efficiency by speeding up
              price discovery, but that it may also impose higher adverse
              selection costs on slower traders. In contrast, the reduction in
              the autocorrelation of returns owes more to the algorithmic
              provision of liquidity. We also find evidence consistent with the
              strategies of algorithmic traders being highly correlated. This
              correlation, however, does not appear to cause a degradation in
              market quality, at least not on average.",
  journal  = "J. Finance",
  volume   =  69,
  number   =  5,
  pages    = "2045--2084",
  month    =  oct,
  year     =  2014
}

@ARTICLE{Menkveld2013-zm,
  title    = "High frequency trading and the new market makers",
  author   = "Menkveld, Albert J",
  abstract = "This paper characterizes the trading strategy of a large high
              frequency trader (HFT). The HFT incurs a loss on its inventory
              but earns a profit on the bid--ask spread. Sharpe ratio
              calculations show that performance is very sensitive to cost of
              capital assumptions. The HFT employs a cross-market strategy as
              half of its trades materialize on the incumbent market and the
              other half on a small, high-growth entrant market. Its trade
              participation rate in these markets is 8.1\% and 64.4\%,
              respectively. In both markets, four out of five of its trades are
              passive i.e., its price quote was consumed by others.",
  journal  = "Journal of Financial Markets",
  volume   =  16,
  number   =  4,
  pages    = "712--740",
  month    =  nov,
  year     =  2013,
  keywords = "High frequency trading; Market maker; Multiple markets"
}

@ARTICLE{Sarkar1976-ub,
  title    = "Specificity of the vanillin test for flavanols",
  author   = "Sarkar, S K and Howarth, R E",
  abstract = "The potential uses of public resources and powers to improve the
              economic status of economic groups (such as industries and
              occupations) are analyzed to provide a scheme of the demand for
              regulation. The characteristics of the political process which
              allow relatively small groups to obtain such regulation is then
              sketched to provide elements of a theory of supply of regulation.
              A variety of empirical evidence and illustration is also
              presented.",
  journal  = "J. Agric. Food Chem.",
  volume   =  24,
  number   =  2,
  pages    = "317--320",
  month    =  mar,
  year     =  1976,
  language = "en"
}

@ARTICLE{Hasan2009-uv,
  title    = "Bank Efficiency, Financial Depth, and Economic Growth",
  author   = "Hasan, Iftekhar and Koetter, Michael and Lensink, Robert and
              Meesters, Aljar",
  abstract = "The positive relation between financial development and economic
              growth seems to have weakened in recent years and when analyzing
              only developed countries. We s",
  month    =  sep,
  year     =  2009,
  keywords = "Bank performance, economic growth, bank efficiency"
}

@ARTICLE{Koetter2010-kk,
  title    = "Finance and growth in a bank-based economy: Is it quantity or
              quality that matters?",
  author   = "Koetter, Michael and Wedow, Michael",
  abstract = "Most finance--growth studies approximate the size of financial
              systems rather than the quality of intermediation to explain
              economic growth differentials. Furthermore, the neglect of
              systematic differences in cross-country studies could drive the
              result that finance matters. We suggest a measure of bank's
              intermediation quality using bank-specific efficiency estimates
              and focus on the regions of one economy only: Germany. This
              quality measure has a significantly positive effect on growth.
              This result is robust to the exclusion of banks operating in
              multiple regions, controlling for the proximity of financial
              markets, when distinguishing different banking sectors active in
              Germany, and when excluding the structurally weaker East from the
              sample.",
  journal  = "J. Int. Money Finance",
  volume   =  29,
  number   =  8,
  pages    = "1529--1545",
  month    =  dec,
  year     =  2010,
  keywords = "Financial development; Intermediation quality; Regional growth;
              Bank efficiency"
}

@ARTICLE{Laeven2016-hz,
  title     = "Bank size, capital, and systemic risk: Some international
               evidence",
  author    = "Laeven, Luc and Ratnovski, Lev and Tong, Hui",
  abstract  = "This paper studies the significant variation in the
               cross-section of standalone and systemic risk of large banks
               during the recent financial crisis to identify bank specific
               factors that determine risk. We find that systemic risk grows
               with bank size and is inversely related to bank capital, and
               this effect exists above and beyond the effect of bank size and
               capital on standalone bank risk. Our results contribute to the
               ongoing debate on the merits of imposing systemic risk-based
               capital requirements on banks.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  69,
  number    = "S1",
  pages     = "25--34",
  year      =  2016,
  keywords  = "Banking crisis; Bank performance; Bank fragility; Systemic risk;
               Financial regulation"
}

@MISC{Stark2016-la,
  title  = "Student Evaluations of Teaching (Mostly) Do Not Measure Teaching
            Effectiveness",
  author = "Stark, Philip and Ottoboni, Kellie and Boring, Anne and
            Cetinkaya-Rundel, Mine",
  month  =  jan,
  year   =  2016
}

@ARTICLE{Hashem_Pesaran2003-cd,
  title   = "How Costly is it to Ignore Breaks when Forecasting the Direction
             of a Time Series?",
  author  = "Hashem Pesaran, M and Timmermannn, Allan",
  journal = "DAE Working Paper No. 0306",
  year    =  2003
}

@ARTICLE{Saastamoinen2016-ad,
  title    = "Quality frontier of electricity distribution: Supply security,
              best practices, and underground cabling in Finland",
  author   = "Saastamoinen, Antti and Kuosmanen, Timo",
  abstract = "Electricity distribution is a prime example of local monopoly. In
              most countries, the costs of electricity distribution operators
              are regulated by the government. However, the cost regulation may
              create adverse incentives to compromise the quality of service.
              To avoid this, cost regulation is often amended with quality
              incentives. This study applies theory and methods of productivity
              analysis to model the frontier of service quality. A
              semi-nonparametric estimation method is developed, which does not
              assume any particular functional form for the quality frontier,
              but can accommodate stochastic noise and heteroscedasticity. The
              empirical part of our paper examines how underground cabling and
              location affect the interruption costs. As expected, higher
              proportion of underground cabling decreases the level of
              interruption costs. The effects of cabling and location on the
              variance of performance are also considered. Especially the
              location is found to be a significant source of
              heteroscedasticity in the interruption costs. Finally, the
              proposed quality frontier benchmark is compared to the current
              practice of Finnish regulation system. The proposed quality
              frontier is found to provide more meaningful and stable basis for
              setting quality targets than the average practice benchmarks
              currently in use.",
  journal  = "Energy Econ.",
  volume   =  53,
  pages    = "281--292",
  month    =  jan,
  year     =  2016,
  keywords = "Electricity distribution; Productivity and efficiency analysis;
              Regulation; Service quality"
}

@ARTICLE{Santos2006-br,
  title   = "The Log of Gravity",
  author  = "Santos, J M and Tenreyro, Silvana",
  journal = "Review of Economics and Statistics",
  year    =  2006
}

@ARTICLE{noauthor_undated-jp,
  title = "[{PDF]Evaluating} forecast performance - Bank of England"
}

@ARTICLE{Adair1996-br,
  title     = "Hedonic modelling, housing submarkets and residential valuation",
  author    = "Adair, A S and Berry, J N and McGreal, W S",
  abstract  = "Implicit in the comparative method of valuation is the concept
               of housing submarkets yet little attempt has been made to link
               research into housing market areas with the valuation process.
               Hedonic models based on price data from Belfast infer that
               submarkets may be identified over a wider spatial scale than
               previously considered. Implications for the valuation process
               relate to the selection of comparable evidence and the quality
               of variables that the valuer may need to consider.",
  journal   = "Journal of Property Research",
  publisher = "Routledge",
  volume    =  13,
  number    =  1,
  pages     = "67--83",
  month     =  jan,
  year      =  1996
}

@ARTICLE{Gibb2012-rr,
  title     = "Analysing the Belfast housing market: learning lessons from
               extreme volatility",
  author    = "Gibb, Kenneth and O'Sullivan, Tony and Young, Gillian",
  abstract  = "Belfast and Northern Ireland enjoyed a long property-led
               economic boom, in part the result of global economic
               developments that were compounded by specific local
               institutional and other drivers. The market grew more quickly
               and then fell further than in any other UK region. What lessons
               can be learned from this experience in the Belfast housing
               system? This article reports evidence from a rigorous local
               housing systems analysis of the wider Belfast housing market.
               Developing a better understanding of the factors that left the
               Belfast housing system so exposed provides policy lessons to
               help mitigate future volatility in both similar and arguably
               different settings.",
  journal   = "Town Plan. Rev.",
  publisher = "Liverpool University Press",
  volume    =  83,
  number    =  4,
  pages     = "407--430",
  month     =  jan,
  year      =  2012
}

@ARTICLE{McCord2013-ct,
  title     = "Belfast's iron(ic) curtain: ``Peace walls'' and their impact on
               house prices in the Belfast housing market",
  author    = "McCord, John and McCord, Michael J and McCluskey, William and
               Davis, Peadar and McIhatton, David and Haran, Martin",
  abstract  = "Purpose -- Belfast's ``peace walls'' exist to physically
               segregate and provide a measure of security to the communities
               on the religious divide in Northern Ireland. Whilst they do
               ostensibly achieve this aim, it may well be that these
               structures have the capacity to prevent the restoration of
               normal community interactions and market processes and may also
               be providing their benefits at a high price with regard to
               issues such as house price reduction. Indeed, the effect of
               these structures on surrounding residential property values
               remains somewhat of an unknown quantity. This paper therefore
               measures the effect of proximity to locations with social and
               political conflicts. The paper aims to quantify and measure the
               disamenity implications and costs of artificial barriers (peace
               walls) within the Belfast housing market.
               Design/methodology/approach -- This paper attempts to measure
               the disamenity effect of peace walls on house prices, primarily
               focusing on the effect of distance, calculated using a hedonic
               pricing specification and spatially referenced data. The data
               are derived from 3,836 house sales transactions over a one year
               period in 2011. Findings -- The emerging findings demonstrate
               that a greater negative pricing effect is evident with proximity
               to the peace walls, with the exception of the apartment sector.
               The findings also highlight the complex market pricing structure
               of Belfast and offer insight as how to best classify submarkets.
               Practical implications -- The results of the research are of
               particular interest to property valuers and social policy makers
               in regions with contested space. Originality/value -- Tactile
               barriers scar the urban terrain, formalise ethno-segregation
               across Belfast and have implications for spatial planning in the
               urban environment and housing studies and policy. Such an
               externality may have a pervasive and endogenous effect on house
               prices and the identification of submarkets yet there is
               implicit acceptance of peace lines as de facto standard and a
               dearth of empirical evidence relating to direction and magnitude
               of the location-specific effects of peace walls on house prices
               in Belfast. This paper is arguably the first to empirically
               examine the location-specific effects of peace walls on property
               value across the Belfast area.",
  journal   = "Journal of European Real Estate Research",
  publisher = "emeraldinsight.com",
  volume    =  6,
  number    =  3,
  pages     = "333--358",
  year      =  2013
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{McCord2012-oh,
  title     = "Spatial variation as a determinant of house price: Incorporating
               a geographically weighted regression approach within the Belfast
               housing market",
  author    = "McCord, M and Davis, P T and Haran, M and McGreal, S and
               McIlhatton, D",
  abstract  = "Purpose -- Tobler's law of geography states that things that are
               close to one another tend to be more alike than things that are
               far apart. In this regard, the spatial pattern of price
               distribution is defined by the arrangement of individual
               entities in space and the geographic relationships among them.
               The purpose of this paper is to provide emerging findings of
               research analysing the salient factors which impact on the sale
               price of residential properties using a spatial regression
               approach.Design/methodology/approach -- The research develops
               and formulates a geographically weighted regression (GWR) model
               to incorporate residential sales transactions within the Belfast
               Metropolitan Area over the course of 2010. Transaction data were
               sourced from the University of Ulster House Price Index survey
               (2010, Q1‚ÄêQ4). The GWR approach was then evaluated relative to a
               standard hedonic model to determine the spatial heterogeneity of
               residential property price within the Belfast Metropolitan
               Area.Findings -- This investigation finds that the GWR technique
               provides increased accuracy in predicting marginal price
               estimates, in comparison with traditional hedonic modelling,
               within the Belfast housing market.Originality/value -- This
               study is one of only a few investigations of spatial house price
               variation applying the GWR methodology within the confines of a
               UK housing market. In this respect it enhances applied based
               knowledge and understanding of geographically weighted
               regression.",
  journal   = "Journal of Financial Management of Property and Construction",
  publisher = "emeraldinsight.com",
  volume    =  17,
  number    =  1,
  pages     = "49--72",
  year      =  2012
}

@INCOLLECTION{Sopranzetti2010-dy,
  title     = "Hedonic Regression Analysis in Real Estate Markets: A Primer",
  booktitle = "Handbook of Quantitative Finance and Risk Management",
  author    = "Sopranzetti, Ben J",
  abstract  = "This short primer provides an overview of the nature and variety
               of hedonic pricing models that are employed in the market for
               real estate. It explores the history of hedonic modeling and
               summarizes the field's utility-theory-based, microeconomic
               foundations. It also provides empirical examples of each of the
               three principal methodologies and a discussion of and potential
               solutions for common problems associated with hedonic modeling.",
  publisher = "Springer, Boston, MA",
  pages     = "1201--1207",
  year      =  2010,
  language  = "en"
}

@ARTICLE{Rajan2016-mp,
  title    = "Local financial capacity and asset values: Evidence from bank
              failures",
  author   = "Rajan, Raghuram and Ramcharan, Rodney",
  abstract = "Using differences in regulation as a means of identification, we
              find that a reduction in local financial intermediation capacity
              reduces the recovery rates on assets of failing banks. It also
              depresses local land prices and is associated with subsequent
              distress in nearby banks. Fire sales appear to be one channel
              through which lower local intermediation capacity reduces the
              recovery rates on failed banks' assets. The paper provides a
              rationale for why bank failures are contagious, and why the value
              of specialized financial assets could depend on the size of the
              intermediary market that is available to buy it.",
  journal  = "J. financ. econ.",
  volume   =  120,
  number   =  2,
  pages    = "229--251",
  month    =  may,
  year     =  2016,
  keywords = "Fire-sales; Financial crises; Banks"
}

@ARTICLE{Lecocq2006-ae,
  title     = "What determines wine prices: Objective vs. sensory
               characteristics",
  author    = "Lecocq, S{\'e}bastien and Visser, Michael",
  journal   = "Journal of Wine Economics",
  publisher = "Cambridge Univ Press",
  volume    =  1,
  number    =  01,
  pages     = "42--56",
  year      =  2006
}

@ARTICLE{Dimitropoulos2016-bu,
  title     = "Managing the European football industry: {UEFA's} regulatory
               intervention and the impact on accounting quality",
  author    = "Dimitropoulos, Panagiotis and Leventis, Stergios and Dedoulis,
               Emmanouil",
  abstract  = "ABSTRACTResearch question: European football clubs are known for
               an institutionalized management culture which prioritizes
               on-field success over financial performance. This creates an
               extremely competitive context within which most clubs operate,
               producing debts and deficits. However, in order to secure clubs?
               long-term financial viability, Union of European Football
               Association (UEFA) has introduced regulatory and monitoring
               processes tied to accounting data in order to assess clubs?
               financial performance. This study aims to determine whether
               UEFA?s framework has an impact on clubs? management policies
               with regard to accounting quality.Research methods: The study
               employs a sample of 109 European football clubs for a seven-year
               period, 2008?2014 (three years before and four years after the
               regulatory intervention), to investigate the impact of the
               reform upon management practices related to accounting.
               Following prior literature, we employ the three most commonly
               used proxies of accounting quality: earnings management,
               conditional accounting conservatism and auditor
               switching.Results and findings: This study demonstrates that, at
               the expense of accounting quality, club management seeks to
               promote the image of a financially robust organization in order
               to secure licensing and, consequently, much needed funding from
               UEFA. In this manner, the dominance of a management culture
               which impairs financial performance is further
               cemented.Implications: UEFA should take into consideration that,
               in a financially distressed industry focused on achieving
               success on the field of play, the imposition of regulatory
               monitoring tied to accounting data inevitably leads to a loss of
               organizational credibility and transparency. Hence, UEFA?s
               intervention should be accompanied by the imposition of a
               corporate governance framework which would aim to rearrange club
               management priorities by facilitating a change in
               institutionalized mentalities.",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  16,
  number    =  4,
  pages     = "459--486",
  month     =  aug,
  year      =  2016
}

@ARTICLE{noauthor_undated-hs,
  title = "[{PDF]Annex} {L} - Cost Base Efficiency Analysis - Utility Regulator"
}

@TECHREPORT{noauthor_2014-ja,
  title       = "Effective Rates of Corporation Tax in Ireland",
  number      = "Technical Paper",
  institution = "Department of Finance Ireland",
  month       =  apr,
  year        =  2014
}

@MISC{Campbell2016-et,
  title        = "Corporation Tax Response",
  author       = "Campbell, Gareth",
  year         =  2016,
  howpublished = "Working Paper"
}

@TECHREPORT{Mankiw2009-qe,
  title       = "Optimal taxation in theory and practice",
  author      = "Mankiw, N Gregory and Weinzierl, Matthew and Yagan, Danny",
  institution = "National Bureau of Economic Research",
  year        =  2009
}

@TECHREPORT{noauthor_2011-rn,
  title       = "Rebalancing the Northern Ireland economy consultation",
  institution = "UK Treasury Department",
  month       =  mar,
  year        =  2011
}

@BOOK{Cooper2008-at,
  title     = "Designing Effective Web Surveys",
  author    = "Cooper, Mick",
  publisher = "Cambridge University Press",
  year      =  2008
}

@ARTICLE{Szymanski2016-kd,
  title    = "Testing the {O-Ring} theory using data from the English Premier
              League",
  author   = "Szymanski, Stefan and Wilkinson, Guy",
  abstract = "This paper measures the impact of different workers in a
              production process dependent on their expected productivity.
              Using the setting of professional football, expected productivity
              is measured from the transfer fees paid to acquire players. It
              shows that the most expensive players tend to have the largest
              impact on the game whereas the least expensive players have
              little impact. The findings support superstar theories rather
              than O-ring theory. We also find that the optimal spending
              distribution is more skewed than the observed distribution
              suggesting some constraint in the market for superstars.",
  journal  = "Research in Economics",
  volume   =  70,
  number   =  3,
  pages    = "468--481",
  month    =  sep,
  year     =  2016,
  keywords = "O-Ring Theory; Production Function; Optimality"
}

@ARTICLE{noauthor_2001-zx,
  title   = "A Survey of Corporate Finance: Debt is good for you",
  journal = "Economist, The",
  year    =  2001
}

@ARTICLE{Molinos-Senante2015-lt,
  title    = "Assessing the relative efficiency of water companies in the
              English and Welsh water industry: a metafrontier approach",
  author   = "Molinos-Senante, Mar{\'\i}a and Maziotis, Alexandros and
              Sala-Garrido, Ramon",
  abstract = "The assessment of relative efficiency of water companies is
              essential for managers and authorities. This is evident in the UK
              water sector where there are companies with different services
              such as water and sewerage companies (WaSCs) and water-only
              companies (WoCs). Therefore, it is a critical limitation to
              estimate a common production frontier for both types of
              companies, as it might lead to biased efficiency estimates. In
              this paper, a robust and reliable methodology, the metafrontier
              model, is applied to compare the efficiency of water companies
              providing different services. The results illustrate the superior
              performance of WaSCs compared to WoCs. It also confirms the
              presence of economies of scope in the UK water industry. The
              methodology and results of this study are of great interest for
              both regulators and water utility managers to make informed
              decisions.",
  journal  = "Environ. Sci. Pollut. Res. Int.",
  volume   =  22,
  number   =  21,
  pages    = "16987--16996",
  month    =  nov,
  year     =  2015,
  keywords = "Data envelopment analysis (DEA); Economies of scope; Efficiency;
              Metafrontier; Water companies",
  language = "en"
}

@ARTICLE{Gordon_Hughes2009-pn,
  title   = "{NIW} Comparative Efficiency: An Econometric Analysis Using Panel
             Data",
  author  = "Gordon Hughes, William Baker",
  journal = "NERA Economic Consulting",
  year    =  2009
}

@ARTICLE{Elder2001-hs,
  title     = "Testing for Unit Roots: What Should Students Be Taught?",
  author    = "Elder, John and Kennedy, Peter E",
  abstract  = "Unit-root testing strategies are unnecessarily complicated
               because they do not exploit prior knowledge of the growth status
               of the time series, they worry about unrealistic outcomes, and
               they double- or triple-test for unit roots. The authors provide
               a testing strategy that cuts through these complications and so
               facilitates teaching this dimension of the unit-root phenomenon.
               F tests are used as a vehicle for understanding, but t tests are
               recommended in the end, consistent with common practice.",
  journal   = "J. Econ. Educ.",
  publisher = "Taylor \& Francis, Ltd.",
  volume    =  32,
  number    =  2,
  pages     = "137--146",
  year      =  2001
}

@ARTICLE{Scholz1987-ov,
  title     = "{K-Sample} {Anderson-Darling} Tests",
  author    = "Scholz, F W and Stephens, M A",
  abstract  = "Two k-sample versions of an Anderson-Darling rank statistic are
               proposed for testing the homogeneity of samples. Their
               asymptotic null distributions are derived for the continuous as
               well as the discrete case. In the continuous case the asymptotic
               distributions coincide with the (k - 1)-fold convolution of the
               asymptotic distribution for the Anderson-Darling one-sample
               statistic. The quality of this large sample approximation is
               investigated for small samples through Monte Carlo simulation.
               This is done for both versions of the statistic under various
               degrees of data rounding and sample size imbalances. Tables for
               carrying out these tests are provided, and their usage in
               combining independent one- or k-sample Anderson-Darling tests is
               pointed out. The test statistics are essentially based on a
               doubly weighted sum of integrated squared differences between
               the empirical distribution functions of the individual samples
               and that of the pooled sample. One weighting adjusts for the
               possibly different sample sizes, and the other is inside the
               integration placing more weight on tail differences of the
               compared distributions. The two versions differ mainly in the
               definition of the empirical distribution function. These tests
               are consistent against all alternatives. The use of these tests
               is two-fold: (a) in a one-way analysis of variance to establish
               differences in the sampled populations without making any
               restrictive parametric assumptions or (b) to justify the pooling
               of separate samples for increased sample size and power in
               further analyses. Exact finite sample mean and variance formulas
               for one of the two statistics are derived in the continuous
               case. It appears that the asymptotic standardized percentiles
               serve well as approximate critical points of the appropriately
               standardized statistics for individual sample sizes as low as 5.
               The application of the tests is illustrated with an example.
               Because of the convolution nature of the asymptotic
               distribution, a further use of these critical points is possible
               in combining independent Anderson-Darling tests by simply adding
               their test statistics.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  82,
  number    =  399,
  pages     = "918--924",
  year      =  1987
}

@ARTICLE{Stephens1974-bi,
  title     = "{EDF} Statistics for Goodness of Fit and Some Comparisons",
  author    = "Stephens, M A",
  abstract  = "Abstract This article offers a practical guide to
               goodness-of-fit tests using statistics based on the empirical
               distribution function (EDF). Five of the leading statistics are
               examined?those often labelled D, W 2, V, U 2, A 2?and three
               important situations: where the hypothesized distribution F(x)
               is completely specified and where F(x) represents the normal or
               exponential distribution with one or more parameters to be
               estimated from the data. EDF statistics are easily calculated,
               and the tests require only one line of significance points for
               each situation. They are also shown to be competitive in terms
               of power.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  69,
  number    =  347,
  pages     = "730--737",
  month     =  sep,
  year      =  1974
}

@ARTICLE{Kruskal1952-lz,
  title     = "Use of Ranks in {One-Criterion} Variance Analysis",
  author    = "Kruskal, William H and Wallis, W Allen",
  abstract  = "Abstract Given C samples, with n i observations in the ith
               sample, a test of the hypothesis that the samples are from the
               same population may be made by ranking the observations from
               from 1 to $\Sigma$n i (giving each observation in a group of
               ties the mean of the ranks tied for), finding the C sums of
               ranks, and computing a statistic H. Under the stated hypothesis,
               H is distributed approximately as ?2(C ? 1), unless the samples
               are too small, in which case special approximations or exact
               tables are provided. One of the most important applications of
               the test is in detecting differences among the population
               means.* * Based in part on research supported by the Office of
               Naval Research at the Statistical Research Center, University of
               Chicago.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  47,
  number    =  260,
  pages     = "583--621",
  month     =  dec,
  year      =  1952
}

@ARTICLE{Bohm2010-mc,
  title     = "A {Kolmogorov-Smirnov} Test for r Samples",
  author    = "B{\"o}hm, Walter and Hornik, Kurt",
  abstract  = "We consider the problem of testing whether r (>=2) samples are
               drawn from the same continuous distribution F(x). The test
               statistic we will study in some detail is defined as the maximum
               of the circular differences of the empirical distribution
               functions, a generalization of the classical 2-sample
               Kolmogorov-Smirnov test to r (>=2) independent samples. For the
               case of equal sample sizes we derive the exact null distribution
               by counting lattice paths confined to stay in the scaled alcove
               $\mathcal\{A\}_r$ of the affine Weyl group $A_\{r-1\}$. This is
               done using a generalization of the classical reflection
               principle. By a standard diffusion scaling we derive also the
               asymptotic distribution of the test statistic in terms of a
               multivariate Dirichlet series. When the sample sizes are not
               equal the reflection principle no longer works, but we are able
               to establish a weak convergence result even in this case showing
               that by a proper rescaling a test statistic based on a linear
               transformation of the circular differences of the empirical
               distribution functions has the same asymptotic distribution as
               the test statistic in the case of equal sample sizes.",
  journal   = "Research Report Series / Department of Statistics and
               Mathematics",
  publisher = "WU Vienna University of Economics and Business",
  number    =  105,
  month     =  dec,
  year      =  2010,
  address   = "Vienna",
  keywords  = "Kolmogorov-Smirnov test / lattice path counting / reflection
               principle / affine Weyl groups / asymptotics distribution",
  language  = "en"
}

@MISC{Conover_undated-lu,
  title     = "197 1. Practical nonparametric statistics",
  author    = "Conover, W J",
  journal   = "Wiley",
  publisher = "Wiley"
}

@ARTICLE{Casu2016-zi,
  title     = "Post-crisis regulatory reforms and bank performance: lessons
               from Asia",
  author    = "Casu, Barbara and Deng, Bimei and Ferrari, Alessandra",
  abstract  = "Based on a large dataset from eight Asian economies, we test the
               impact of post-crisis regulatory reforms on the performance of
               depository institutions in countries at different levels of
               financial development. We allow for technological heterogeneity
               and estimate a set of country-level stochastic cost frontiers
               followed by a deterministic bootstrapped meta-frontier to
               evaluate cost efficiency and cost technology. Our results
               support the view that liberalization policies have a positive
               impact on bank performance, while the reverse is true for
               prudential regulation policies. The removal of activities
               restrictions, bank privatization and foreign bank entry has a
               positive and significant impact on technological progress and
               cost efficiency. In contrast, prudential policies, which aim to
               protect the banking sector from excessive risk-taking, tend to
               adversely affect banks? cost efficiency but not cost technology.",
  journal   = "The European Journal of Finance",
  publisher = "Routledge",
  volume    =  0,
  number    =  0,
  pages     = "1--28",
  month     =  apr,
  year      =  2016
}

@ARTICLE{Fare2016-oh,
  title     = "Estimating the hyperbolic distance function: A directional
               distance function approach",
  author    = "F{\"a}re, Rolf and Margaritis, Dimitris and Rouse, Paul and
               Roshdi, Israfil",
  abstract  = "F{\"a}re, Grosskopf, and Lovell (1985) merged Farrell's input
               and output oriented technical efficiency measures into a new
               graph-type approach known as hyperbolic distance function (HDF).
               In spite of its appealing special structure in allowing for the
               simultaneous and equiproportionate reduction in inputs and
               increase in outputs, HDF is a non-linear optimization and it is
               hard to solve particularly when dealing with technologies
               operating under variable returns to scale. By connecting HDF to
               the directional distance function, we propose a linear
               programming based procedure for estimating the exact value of
               HDF within the non-parametric framework of data envelopment
               analysis. We illustrate the computational effectiveness of the
               algorithm on several real-world and simulated data sets,
               generating the optimal value of HDF through generally solving at
               most two linear programs. Moreover, our approach has several
               desirable properties such as: (1) introducing a computational
               dual formulation for the HDF and providing an economic
               interpretation in terms of shadow prices; (2) being readily
               adaptable to measure hyperbolic-oriented super-efficiency; and
               (3) being flexible to deal with HDF-based efficiency measures on
               environmental technologies.",
  journal   = "Eur. J. Oper. Res.",
  publisher = "Elsevier",
  volume    =  254,
  number    =  1,
  pages     = "312--319",
  month     =  oct,
  year      =  2016,
  keywords  = "Efficiency measurement; Data envelopment analysis; Hyperbolic
               distance function; Directional distance function"
}

@ARTICLE{noauthor_undated-nl,
  title = "[{PDF]the} financial impact of the fa cup - {TheFA.com}"
}

@ARTICLE{Wijesiri2016-tj,
  title    = "Weathering the storm: ownership structure and performance of
              microfinance institutions in the wake of the global financial
              crisis",
  author   = "Wijesiri, Mahinda",
  abstract = "This study investigates the effects of the 2008 global financial
              crisis on the performance of different microfinance ownership
              types. The analysis in this study relies on a novel
              methodological framework that provides consistent productivity
              measures in the presence of undesirable outputs, while taking
              into account the technological heterogeneity among different
              ownership types. The results show that banks and non-bank
              financial institutions (NBFIs) that performed better immediately
              before the crisis, suffered more during the crisis and early
              post-crisis periods. Cooperatives and non-governmental
              organizations (NGOs), on the other hand, were less affected by
              the crisis. Moreover, results indicate that the pattern of
              productivity growth of all ownership forms three years after the
              eruption of the crisis was remarkably similar to their
              productivity growth pattern in the very early phase of the
              pre-crisis period.",
  journal  = "Econ. Model.",
  volume   =  57,
  pages    = "238--247",
  month    =  sep,
  year     =  2016,
  keywords = "Microfinance; Ownership; Metafrontier; Malmquist--Luenberger;
              Productivity change; Global financial crisis"
}

@ARTICLE{Garvey2016-mm,
  title    = "Why do traders choose dark markets?",
  author   = "Garvey, Ryan and Huang, Tao and Wu, Fei",
  abstract = "We examine U.S. equity trader use of dark and lit markets.
              Marketable orders executed in the dark have lower information
              content and smaller fill rates. Dark orders take longer to
              execute, but they execute at more favorable prices. Traders are
              more likely to go dark when the bid-ask spread is wider and those
              with higher dark participation are more sophisticated. Although
              market regulators have expressed concern over the rise in dark
              trading, our results indicate that dark markets provide important
              benefits to traders that lit markets do not.",
  journal  = "Journal of Banking \& Finance",
  volume   =  68,
  pages    = "12--28",
  month    =  jul,
  year     =  2016,
  keywords = "Trading; Dark pools; Order execution quality"
}

@ARTICLE{Edition_undated-ro,
  title  = "The Big Picture: A Cost Comparison of Futures and {ETFs}",
  author = "Edition, Second"
}

@ARTICLE{Nath2003-fg,
  title    = "High Frequency Pairs Trading with {U.S}. Treasury Securities:
              Risks and Rewards for Hedge Funds",
  author   = "Nath, Purnendu",
  abstract = "This paper examines the implementation of a simple pairs trading
              strategy with automatic extreme risk control using the entire
              universe of securities in the hig",
  month    =  nov,
  year     =  2003,
  keywords = "Arbitrage, Bonds, Extreme Risk, Hedge Funds, Liquidity, Pairs
              Trading, Spread Trading, Statistical Arbitrage, U.S. Treasury
              Securities"
}

@ARTICLE{Jacobs2013-vo,
  title     = "Losing sight of the trees for the forest? Attention allocation
               and anomalies",
  author    = "Jacobs, H and Weber, M",
  abstract  = "Abstract: This paper tests asset pricing implications of the
               investor attention shift hypothesis proposed in recent
               theoretical work. We create a novel proxy for the dynamics of
               inattention towards firm-specific information and explore its
               impact on prominent return anomalies. As ...",
  journal   = "Attention Allocation and Anomalies (June 11,",
  publisher = "papers.ssrn.com",
  year      =  2013
}

@TECHREPORT{Beddington2012-yc,
  title       = "The Future of Computer Trading in Financial Markets An
                 International Perspective Final Project Report",
  author      = "Beddington, John",
  institution = "UK Government Office for Science",
  year        =  2012
}

@BOOK{Eurostat2013-lf,
  title  = "Handbook of Residential Property Price Indices",
  author = "{Eurostat}",
  series = "Methodologies and Working Papers",
  year   =  2013
}

@TECHREPORT{Hill2014-lb,
  title       = "Incorporating Geospatial Data in House Price Indexes: A
                 Hedonic Imputation Approach with Splines",
  author      = "Hill, Robert and Scholz, Michael",
  number      = "GEP 2014--05",
  institution = "University of Graz",
  year        =  2014
}

@ARTICLE{Hyytinen2016-nz,
  title    = "{Olley--Pakes} productivity decomposition: computation and
              inference",
  author   = "Hyytinen, Ari and Ilmakunnas, Pekka and Maliranta, Mika",
  abstract = "We show how a moment-based estimation procedure can be used to
              compute point estimates and standard errors for the two
              components of the widely used Olley--Pakes decomposition of
              aggregate (weighted average) productivity. When applied to
              business level microdata, the procedure allows for autocovariance
              and heteroscedasticity robust inference and hypothesis testing
              about, for example, the coevolution of the productivity
              components in different groups of firms. We provide an
              application to Finnish firm level data and find that formal
              statistical inference casts doubt on the conclusions that one
              might draw on the basis of a visual inspection of the components
              of the decomposition.",
  journal  = "J. R. Stat. Soc. A",
  volume   =  179,
  number   =  3,
  pages    = "749--761",
  month    =  jun,
  year     =  2016,
  keywords = "Generalized method of moments; Inference; Productivity; Weighted
              average"
}

@INCOLLECTION{Moosa2015-hg,
  title     = "The {Meese-Rogoff} Puzzle",
  booktitle = "Demystifying the {Meese-Rogoff} Puzzle",
  author    = "Moosa, Imad A and Burns, Kelly",
  abstract  = "The Meese-Rogoff puzzle refers to the proposition that exchange
               rate models cannot outperform the random walk in out-of-sample
               forecasting of exchange rates. This proposition is regarded as a
               puzzle because it does not make much sense for profit-maximising
               firms to pay for professional forecasts when the `better'
               forecasts generated from the random walk are available for free.
               This proposition is valid only if forecasting accuracy is
               measured in terms of criteria that depend on the magnitude of
               the forecasting error only. The origin of the puzzle, the 1983
               paper of Meese and Rogoff exhibits many flaws.",
  publisher = "Palgrave Pivot, London",
  pages     = "1--13",
  year      =  2015,
  language  = "en"
}

@ARTICLE{Moosa2014-bb,
  title     = "A reappraisal of the {Meese--Rogoff} puzzle",
  author    = "Moosa, Imad and Burns, Kelly",
  abstract  = "Several explanations have been put forward for the Meese?Rogoff
               puzzle that exchange rate models cannot outperform the random
               walk in out-of-sample forecasting. We suggest that a simple
               explanation for the puzzle is the use of the root mean square
               error (RMSE) to measure forecasting accuracy, presenting a
               rationale as to why it is difficult to beat the random walk in
               terms of the RMSE. By using exactly the same exchange rates,
               time periods and estimation methods as those of Meese and
               Rogoff, we find that their results cannot be overturned even if
               the models are estimated with time-varying coefficients.
               However, we also find that the random walk can be outperformed
               by the same models if forecasting accuracy is measured in terms
               of the ability to predict direction, in terms of a measure that
               combines magnitude and direction and in terms of profitability.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  46,
  number    =  1,
  pages     = "30--40",
  month     =  jan,
  year      =  2014
}

@ARTICLE{Ziliak2008-ay,
  title   = "The cult of statistical significance",
  author  = "Ziliak, Stephen T and McCloskey, Deirdre N",
  journal = "Ann Arbor: University of Michigan Press",
  volume  =  27,
  year    =  2008
}

@ARTICLE{Uk_undated-ev,
  title  = "Drivers \& Impacts of Derisking",
  author = "Uk, Data in The and Howell, by John"
}

@ARTICLE{Dieleman2014-iv,
  title    = "Random-effects, fixed-effects and the within-between
              specification for clustered data in observational health studies:
              a simulation study",
  author   = "Dieleman, Joseph L and Templin, Tara",
  abstract = "BACKGROUND: When unaccounted-for group-level characteristics
              affect an outcome variable, traditional linear regression is
              inefficient and can be biased. The random- and fixed-effects
              estimators (RE and FE, respectively) are two competing methods
              that address these problems. While each estimator controls for
              otherwise unaccounted-for effects, the two estimators require
              different assumptions. Health researchers tend to favor RE
              estimation, while researchers from some other disciplines tend to
              favor FE estimation. In addition to RE and FE, an alternative
              method called within-between (WB) was suggested by Mundlak in
              1978, although is utilized infrequently. METHODS: We conduct a
              simulation study to compare RE, FE, and WB estimation across
              16,200 scenarios. The scenarios vary in the number of groups, the
              size of the groups, within-group variation, goodness-of-fit of
              the model, and the degree to which the model is correctly
              specified. Estimator preference is determined by lowest mean
              squared error of the estimated marginal effect and root mean
              squared error of fitted values. RESULTS: Although there are
              scenarios when each estimator is most appropriate, the cases in
              which traditional RE estimation is preferred are less common. In
              finite samples, the WB approach outperforms both traditional
              estimators. The Hausman test guides the practitioner to the
              estimator with the smallest absolute error only 61\% of the time,
              and in many sample sizes simply applying the WB approach produces
              smaller absolute errors than following the suggestion of the
              test. CONCLUSIONS: Specification and estimation should be
              carefully considered and ultimately guided by the objective of
              the analysis and characteristics of the data. The WB approach has
              been underutilized, particularly for inference on marginal
              effects in small samples. Blindly applying any estimator can lead
              to bias, inefficiency, and flawed inference.",
  journal  = "PLoS One",
  volume   =  9,
  number   =  10,
  pages    = "e110257",
  month    =  oct,
  year     =  2014,
  language = "en"
}

@UNPUBLISHED{noauthor_2015-uh,
  title = "Systemic Risk Measure: {CoVaR} and Copula (Masters Thesis Berlin)",
  year  =  2015
}

@ARTICLE{Rodriguez-Moreno2013-hu,
  title    = "Systemic risk measures: The simpler the better?",
  author   = "Rodr{\'\i}guez-Moreno, Mar{\'\i}a and Pe{\~n}a, Juan Ignacio",
  abstract = "This paper estimates and compares two groups of high-frequency
              market-based systemic risk measures using European and US
              interbank rates, stock prices and credit derivatives data from
              2004 to 2009. Measures belonging to the macro group gauge the
              overall tension in the financial sector and micro group measures
              rely on individual institution information to extract joint
              distress. We rank the measures using three criteria: (i) Granger
              causality tests, (ii) Gonzalo and Granger metric, and (iii)
              correlation with an index of systemic events and policy actions.
              We find that the best systemic measure in the macro group is the
              first principal component of a portfolio of Credit Default Swap
              (CDS) spreads whereas the best measure in the micro group is the
              multivariate densities computed from CDS spreads. These results
              suggest that the measures based on CDSs outperform measures based
              on interbank rates or stock market prices.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  6,
  pages    = "1817--1831",
  month    =  jun,
  year     =  2013,
  keywords = "Systemic risk; CDS; Libor spreads; CoVaR"
}

@TECHREPORT{noauthor_2016-ya,
  title       = "Measuring the economic impact of reducing the corporation tax
                 rate in Northern Ireland- Technical Report",
  number      = "DETI",
  institution = "University of Ulster Economic Policy Centre",
  month       =  mar,
  year        =  2016
}

@ARTICLE{Report_undated-lg,
  title  = "Financial Advice Market Review",
  author = "Report, Final"
}

@ARTICLE{Colombo2006-xl,
  title     = "Optimal Corporation Tax: An {I.O}. Approach",
  author    = "Colombo, Luca and Labrecciosa, Paola and Walsh, Patrick Paul",
  abstract  = "Theory predicts that optimal effective corporation tax rates
               will be negatively related to industry specific sunk costs, and
               hence industry concentration. Gov",
  journal   = "LSE STICERD Research",
  publisher = "papers.ssrn.com",
  month     =  feb,
  year      =  2006
}

@ARTICLE{Beaulieu2006-cv,
  title     = "Political uncertainty and stock market returns: evidence from
               the 1995 Quebec referendum",
  author    = "Beaulieu, Marie-Claude and Cosset, Jean-Claude and Essaddam,
               Naceur",
  abstract  = "Abstract. In this study, we investigate the short run effect of
               the 30 October 1995 Quebec referendum on the common stock
               returns of Quebec firms. Our results show that the uncertainty
               surrounding the referendum outcome had an impact on stock
               returns of Quebec firms. We also find that the effect of the
               referendum varied with the political risk exposure of Quebec
               firms, that is, the structure of assets and principally the
               degree of foreign involvement. JEL classification: G14, G15",
  journal   = "Canadian Journal of Economics/Revue canadienne d'{\'e}conomique",
  publisher = "Blackwell Publishing Ltd/Inc.",
  volume    =  39,
  number    =  2,
  pages     = "621--642",
  month     =  may,
  year      =  2006
}

@ARTICLE{Diamonte1996-en,
  title     = "Political Risk in Emerging and Developed Markets",
  author    = "Diamonte, Robin L and Liew, John M and Stevens, Ross L",
  abstract  = "Political risk represents a more important determinant of stock
               returns in emerging than in developed markets. Using analyst
               estimates of political risk, we show that average returns in
               emerging markets experiencing decreased political risk exceed
               those of emerging markets experiencing increased political risk
               by approximately 11 percent a quarter. In contrast, the
               difference is only 2.5 percent a quarter for developed markets.
               Furthermore, the difference between the impact of political risk
               in emerging and developed markets is statistically significant.
               We also document a global convergence in political risk. During
               the past 10 years, political risk has decreased in emerging
               markets and increased in developed markets. If this trend
               continues, the differential impact of political risk on returns
               in emerging and developed markets may narrow.",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  52,
  number    =  3,
  pages     = "71--76",
  month     =  may,
  year      =  1996
}

@ARTICLE{Bailey1995-do,
  title     = "Exchange Rate Fluctuations, Political Risk, and Stock Returns:
               Some Evidence from an Emerging Market",
  author    = "Bailey, Warren and Chung, Y Peter",
  abstract  = "We study the impact of exchange rate fluctuations and political
               risk on the risk premiums reflected in cross-sections of
               individual equity returns from Mexico, a country that has
               experienced significant monetary and political turbulence.
               Indicators from Mexico's currency and sovereign debt markets are
               employed as proxies for exchange rate and political risks. We
               find some evidence of equity market premiums for exposure to
               these risks. The results suggest common factors in emerging
               market equity, currency, and sovereign debt markets, and have
               several implications for corporate and portfolio management and
               for the use of emerging market data by researchers.",
  journal   = "The Journal of Financial and Quantitative Analysis",
  publisher = "Cambridge University Press",
  volume    =  30,
  number    =  4,
  pages     = "541",
  month     =  dec,
  year      =  1995,
  language  = "en"
}

@ARTICLE{Joseph_Davis_Roger_Aliaga-Diaz_Charles_J_Thomas2012-cq,
  title    = "Forecasting stock returns: What signals matter, and what do they
              say now?",
  author   = "{Joseph Davis, Roger Aliaga-D{\'\i}az, Charles J. Thomas}",
  abstract = "Some say the long-run outlook for U.S. stocks is poor (even
              ``dead'') given the backdrop of muted economic growth,
              already-high profit margins, elevated government debt levels, and
              low interest rates. Others take a rosier view, citing attractive
              valuations and a wide spread between stock earnings yields and
              Treasury bond yields as reason to anticipate U.S. stock returns
              of 8\%--10\% annually, close to the historical average, over the
              next decade.",
  journal  = "Vanguard Research",
  year     =  2012
}

@ARTICLE{Wilcockson2012-eg,
  title   = "Modelling and Analysis in the wake of the Global Financial Crisis:
             The Financial Services Perspective",
  author  = "Wilcockson, Steve",
  journal = "MathWorks Report",
  year    =  2012
}

@ARTICLE{Dimson1999-fm,
  title    = "Three centuries of asset pricing",
  author   = "Dimson, Elroy and Mussavian, Massoud",
  abstract = "Theory on the pricing of financial assets can be traced back to
              Bernoulli's famous St Petersburg paper of 1738. Since then,
              research into asset pricing and derivative valuation has been
              influenced by a couple of dozen major contributions published
              during the twentieth century. These seminal works have
              underpinned the key ideas of mean--variance optimisation,
              equilibrium analysis and no-arbitrage arguments. This paper
              presents a historical review of these important contributions to
              finance.",
  journal  = "Journal of Banking \& Finance",
  volume   =  23,
  number   =  12,
  pages    = "1745--1769",
  month    =  dec,
  year     =  1999,
  keywords = "Asset pricing; Option pricing; Arbitrage; Portfolio theory; Risk
              measurement"
}

@ARTICLE{Dimson2000-jr,
  title    = "Three Centuries of Asset Pricing",
  author   = "Dimson, Elroy and Mussavian, Massoud",
  abstract = "Theory on the pricing of financial assets can be traced back to
              Bernoulli's famous St. Petersburg paper of 1738. Since then,
              research into asset pricing and der",
  journal  = "LBS Institute of Finance and Accounting Working Paper No. IFA 385",
  month    =  jan,
  year     =  2000
}

@MISC{noauthor_2015-pe,
  title        = "Evaluating systemic risk proxies empirically | {VOX},
                  {CEPR's} Policy Portal",
  booktitle    = "Stefano Giglio, Bryan Kelly, Seth Pruitt",
  abstract     = "The Global Crisis of 2007-09 has made systemic risk a focal
                  point of research and policy, and has established the
                  financial sector as its centre of analysis. The empirical
                  side of the literature focuses on measuring distress in
                  financial markets. This has produced a staggering variety of
                  systemic risk proxies, many hoping to serve as an early
                  warning signal of market dislocations like those observed
                  during the Crisis. Bisias et al.",
  year         =  2015,
  howpublished = "\url{http://voxeu.org/article/evaluating-systemic-risk-proxies-empirically}",
  note         = "Accessed: 2016-6-22"
}

@ARTICLE{Kopf2016-bu,
  title    = "The Guinness Brewer Who Revolutionized Statistics",
  author   = "Kopf, Dan",
  abstract = "The story of how the early 20th Century brewmaster William T.
              Gosset, the ``Student'' of the Student's t-test, transformed
              statistics and manufacturing.",
  year     =  2016
}

@ARTICLE{Fried1999-co,
  title    = "The impact of mergers on credit union service provision",
  author   = "Fried, Harold O and Lovell, C A Knox and Yaisawarng, Suthathip",
  abstract = "In this paper we conduct an empirical exercise in which we
              attempt to provide answers to three questions concerning credit
              union mergers: (i) do members of acquiring credit unions benefit
              from mergers?; (ii) do members of acquired credit unions benefit
              from mergers?; and (iii) what are the characteristics of
              relatively successful, and relatively unsuccessful, mergers? Our
              empirical exercise is based on annual samples of nearly 6000
              credit unions, including nearly 300 merger participants, during
              the 1988--1995 period. We find member service provision to have
              improved in acquired credit unions, and to have been unchanged in
              acquiring credit unions. We also provide three separate analyses,
              from three different perspectives, of the role of various
              characteristics of merging credit unions in determining the
              success of mergers.",
  journal  = "Journal of Banking \& Finance",
  volume   =  23,
  number   =  2,
  pages    = "367--386",
  month    =  feb,
  year     =  1999,
  keywords = "Mergers; Credit unions"
}

@ARTICLE{Acharya2017-vn,
  title     = "Measuring systemic risk",
  author    = "Acharya, Viral V and Pedersen, Lasse H and Philippon, Thomas and
               Richardson, Matthew",
  abstract  = "We present a simple model of systemic risk and show how each
               financial institution's contribution to systemic risk can be
               measured and priced. An institution's",
  journal   = "Rev. Financ. Stud.",
  publisher = "Soc Financial Studies",
  volume    =  30,
  number    =  1,
  pages     = "2--47",
  year      =  2017,
  keywords  = "systemic risk, risk pricing, systemic expected shortfall, risk
               internalization"
}

@MISC{noauthor_undated-ur,
  title = "2003\_10\_Perron critical values .pdf"
}

@ARTICLE{Bai2005-ve,
  title     = "Tests for Skewness, Kurtosis, and Normality for Time Series Data",
  author    = "Bai, Jushan and Ng, Serena",
  abstract  = "We present the sampling distributions for the coefficient of
               skewness, kurtosis, and a joint test of normality for time
               series observations. We show that when the data are serially
               correlated, consistent estimates of three-dimensional long-run
               covariance matrices are needed for testing symmetry or kurtosis.
               These tests can be used to make inference about any conjectured
               coefficients of skewness and kurtosis. In the special case of
               normality, a joint test for the skewness coefficient of 0 and a
               kurtosis coefficient of 3 can be obtained on construction of a
               four-dimensional long-run covariance matrix. The tests are
               developed for demeaned data, but the statistics have the same
               limiting distributions when applied to regression residuals.
               Monte Carlo simulations show that the test statistics for
               symmetry and normality have good finite-sample size and power.
               However, size distortions render testing for kurtosis almost
               meaningless except for distributions with thin tails, such as
               the normal distribution. Combining skewness and kurtosis is
               still a useful test of normality provided that the limiting
               variance accounts for the serial correlation in the data. The
               tests are applied to 21 macroeconomic time series.",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  23,
  number    =  1,
  pages     = "49--60",
  month     =  jan,
  year      =  2005
}

@ARTICLE{Bai2003-ir,
  title     = "Computation and analysis of multiple structural change models",
  author    = "Bai, Jushan and Perron, Pierre",
  abstract  = "In a recent paper, Bai and Perron (1998) considered theoretical
               issues related to the limiting distribution of estimators and
               test statistics in the linear model with multiple structural
               changes. In this companion paper, we consider practical issues
               for the empirical applications of the procedures. We first
               address the problem of estimation of the break dates and present
               an efficient algorithm to obtain global minimizers of the sum of
               squared residuals. This algorithm is based on the principle of
               dynamic programming and requires at most least-squares
               operations of order O(T2) for any number of breaks. Our method
               can be applied to both pure and partial structural change
               models. Second, we consider the problem of forming confidence
               intervals for the break dates under various hypotheses about the
               structure of the data and the errors across segments. Third, we
               address the issue of testing for structural changes under very
               general conditions on the data and the errors. Fourth, we
               address the issue of estimating the number of breaks. Finally, a
               few empirical applications are presented to illustrate the
               usefulness of the procedures. All methods discussed are
               implemented in a GAUSS program. Copyright \copyright{} 2002 John
               Wiley \& Sons, Ltd.",
  journal   = "J. Appl. Econ.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  18,
  number    =  1,
  pages     = "1--22",
  month     =  jan,
  year      =  2003
}

@ARTICLE{Aissa1998-ga,
  title   = "Jamel {JOUINI} the Bai and Perron's and Spectral Density Methods
             for Structural Change Detection in the {US} Inflation Process",
  author  = "A{\"\i}ssa, Mohamed Safouane Ben and Boutahar, Mohamed",
  journal = "Revised article, forthcoming in'Applied Economics Letters' October
             2003",
  volume  =  66,
  pages   = "47",
  year    =  1998
}

@ARTICLE{Butler2006-hx,
  title     = "Can Managers Successfully Time the Maturity Structure of Their
               Debt Issues?",
  author    = "Butler, Alexander W and Grullon, Gustavo and Weston, James P",
  abstract  = "This paper provides a rational explanation for the apparent
               ability of managers to successfully time the maturity of their
               debt issues. We show that a structural break in excess bond
               returns during the early 1980s generates a spurious correlation
               between the fraction of long-term debt in total debt issues and
               future excess bond returns. Contrary to Baker, Taliaferro, and
               Wurgler (2006), we show that the presence of structural breaks
               can lead to nonsense regressions, whether or not there is any
               small sample bias. Tests using firm-level data further confirm
               that managers are unable to time the debt market successfully.",
  journal   = "J. Finance",
  publisher = "[American Finance Association, Wiley]",
  volume    =  61,
  number    =  4,
  pages     = "1731--1758",
  year      =  2006
}

@TECHREPORT{Cecchetti2006-td,
  title       = "Assessing the Sources of Changes in the Volatility of Real
                 Growth",
  author      = "Cecchetti, Stephen and Flores-Lagunes, Alfonso and Krause,
                 Stefan",
  publisher   = "National Bureau of Economic Research",
  institution = "National Bureau of Economic Research",
  month       =  jan,
  year        =  2006,
  address     = "Cambridge, MA"
}

@MISC{noauthor_undated-ld,
  title = "Chan et al {WP} 2008 {STABILITY} {TESTS} {FOR} {HETEROGENEOUS}
           {PANEL} {DATA} 2008.pdf"
}

@ARTICLE{Forecast_undated-ut,
  title  = "{CUNA's} Economic and Credit Union 2011-2012 Forecast",
  author = "Forecast, Economic"
}

@ARTICLE{Fang2009-tb,
  title    = "Modeling the volatility of real {GDP} growth: The case of Japan
              revisited",
  author   = "Fang, Wenshwo and Miller, Stephen M",
  abstract = "Previous studies [e.g., Hamori, S., 2000. Volatility of real GDP:
              some evidence from the United States, the United Kingdom and
              Japan. Japan and the World Economy 12, 143--152; Ho, K.Y., Tsui,
              A.K.C., 2003. Asymmetric volatility of real GDP: some evidence
              from Canada, Japan, the United Kingdom and the United States.
              Japan and the World Economy 15, 437--445; Fountas, S., Karanasos,
              M., Mendoza, A., 2004. Output variability and economic growth:
              the Japanese case. Bulletin of Economic Research 56, 353--363]
              find high volatility persistence of economic growth rates using
              generalized autoregressive conditional heteroskedasticity (GARCH)
              specifications. This paper reexamines the Japanese case, using
              the same approach and showing that this finding of high
              volatility persistence reflects the Great Moderation, which
              features a sharp decline in the variance as well as two falls in
              the mean of the growth rates identified by Bai and Perron's [Bai,
              J., Perron, P., 1998. Estimating and testing linear models with
              multiple structural changes. Econometrica 66, 47--78; Bai, J.,
              Perron, P., 2003. Computation and analysis of multiple structural
              change models. Journal of Applied Econometrics 18, 1--22]
              multiple structural change test. Our empirical results provide
              new evidence. First, excess kurtosis drops substantially or
              disappears in the GARCH or exponential GARCH model that corrects
              for an additive outlier. Second, using the outlier-corrected
              data, the integrated GARCH effect or high volatility persistence
              remains in the specification once we introduce intercept-shift
              dummies into the mean equation. Third, the time-varying variance
              falls sharply, only when we incorporate the break in the variance
              equation. Fourth, the ARCH in mean model finds no effects of our
              more correct measure of output volatility on output growth or of
              output growth on its volatility.",
  journal  = "Japan World Econ.",
  volume   =  21,
  number   =  3,
  pages    = "312--324",
  month    =  aug,
  year     =  2009,
  keywords = "Japan; Real GDP growth; The Great Moderation; Outlier; Structural
              changes; IGARCH effect"
}

@MISC{noauthor_undated-vz,
  title = "{GAUSS\_UserGuide5.0.pdf}"
}

@MISC{noauthor_undated-qb,
  title = "hansen 2001 {JoEP.pdf}"
}

@ARTICLE{Inclan1994-jt,
  title     = "Use of Cumulative Sums of Squares for Retrospective Detection of
               Changes of Variance",
  author    = "Inclan, Carla and Tiao, George C",
  abstract  = "This article studies the problem of multiple change points in
               the variance of a sequence of independent observations. We
               propose a procedure to detect variance changes based on an
               iterated cumulative sums of squares (ICSS) algorithm. We study
               the properties of the centered cumulative sum of squares
               function and give an intuitive basis for the ICSS algorithm. For
               series of moderate size (i.e., 200 observations and beyond), the
               ICSS algorithm offers results comparable to those obtained by a
               Bayesian approach or by likelihood ratio tests, without the
               heavy computational burden required by these approaches.
               Simulation results comparing the ICSS algorithm to other
               approaches are presented.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  89,
  number    =  427,
  pages     = "913--923",
  year      =  1994
}

@MISC{noauthor_undated-mg,
  title = "lanne lutkepohl 2008 {JMCB} indentifying monetary policy shocks via
           changes in volatility.pdf"
}

@ARTICLE{McConnell1998-dq,
  title    = "Output Fluctuations in the United States: What Has Changed Since
              the Early 1980s?",
  author   = "McConnell, Margaret Mary and Perez-Quiros, Gabriel",
  abstract = "We document a structural break in the volatility of U.S. GDP
              growth in the first quarter of 1984, and provide evidence that
              this break emanates from a reduction",
  journal  = "SSRN Journal",
  year     =  1998
}

@MISC{noauthor_undated-uf,
  title = "Morena {APE} 2002 igarch effect and structural breaks.pdf"
}

@MISC{noauthor_undated-yo,
  title = "{MPRA\_paper\_38037} threshold models in testing herding in asset
           pricing.pdf"
}

@ARTICLE{Perron2009-lc,
  title    = "Let's take a break: Trends and cycles in {US} real {GDP}",
  author   = "Perron, Pierre and Wada, Tatsuma",
  abstract = "Trend--cycle decompositions for US real GDP such as the
              unobserved components models, the Beveridge--Nelson
              decomposition, the Hodrick--Prescott filter and others yield very
              different cycles which bear little resemblance to the NBER
              chronology, ascribes much movements to the trend leaving little
              to the cycle, and some imply a negative correlation between the
              noise to the cycle and the trend. We argue that these features
              are artifacts created by the neglect of a change in the slope of
              the trend function. Once this is accounted for, all methods yield
              the same cycle with a trend that is non-stochastic except for a
              few periods around 1973. The cycle is more important in magnitude
              than previously reported and it accords well with the NBER
              chronology. Our results are corroborated using an alternative
              trend--cycle decomposition based on a generalized unobserved
              components models with errors having a mixture of normals
              distribution for both the slope of the trend function and the
              cyclical component.",
  journal  = "J. Monet. Econ.",
  volume   =  56,
  number   =  6,
  pages    = "749--765",
  month    =  sep,
  year     =  2009,
  keywords = "Trend--cycle decomposition; Structural change; Non-Gaussian
              filtering; Unobserved components model; Beveridge--Nelson
              decomposition"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Perron_undated-zb,
  title  = "Dealing with Structural Breaks‚àó",
  author = "Perron, Pierre"
}

@ARTICLE{Doan_Estima_undated-iw,
  title  = "{RATS} Handbook for Switching Models and Structural Breaks",
  author = "Doan Estima, Thomas A"
}

@ARTICLE{Zeileis2002-xz,
  title    = "strucchange: An {R} Package for Testing for Structural Change in
              Linear Regression Models",
  author   = "Zeileis, Achim and Leisch, Friedrich and Hornik, Kurt and
              Kleiber, Christian",
  abstract = "This paper reviews tests for structural change in linear
              regression models from the generalized fluctuation test framework
              as well as from the F test (Chow test) framework. It introduces a
              unified approach for implementing these tests and presents how
              these ideas have been realized in an R package called
              strucchange. Enhancing the standard significance test approach
              the package contains methods to fit, plot and test empirical
              fluctuation processes (like CUSUM, MOSUM and estimates-based
              processes) and to compute, plot and test sequences of F
              statistics with the supF , aveF and expF test. Thus, it makes
              powerful tools available to display information about structural
              changes in regression relationships and to assess their
              significance. Furthermore, it is described how incoming data can
              be monitored.",
  journal  = "Journal of Statistical Software, Articles",
  volume   =  7,
  number   =  2,
  pages    = "1--38",
  year     =  2002
}

@ARTICLE{Changshuai_Li2011-mx,
  title   = "Structural Break in Persistence of European Stock Markets:
             Evidence from Panel {GARCH} Model",
  author  = "Changshuai Li, - and Qingxian Xiao, -",
  journal = "IJIIP",
  volume  =  2,
  number  =  1,
  pages   = "40--48",
  month   =  mar,
  year    =  2011
}

@ARTICLE{Zeileis2005-kp,
  title     = "Validating multiple structural change models. An extended case
               study",
  author    = "Zeileis, Achim and Kleiber, Christian",
  publisher = "Institut f{\"u}r Statistik und Mathematik, WU Vienna University
               of Economics and Business",
  year      =  2005
}

@ARTICLE{Zeileis2003-ul,
  title    = "Testing and dating of structural changes in practice",
  author   = "Zeileis, Achim and Kleiber, Christian and Kr{\"a}mer, Walter and
              Hornik, Kurt",
  abstract = "An approach to the analysis of data that contains (multiple)
              structural changes in a linear regression setup is presented.
              Various strategies which have been suggested in the literature
              for testing against structural changes as well as a dynamic
              programming algorithm for the dating of the breakpoints are
              implemented in the R statistical software package. Using
              historical data on Nile river discharges, road casualties in
              Great Britain and oil prices in Germany, it is shown that
              statistically detected changes in the mean of a time series as
              well as in the coefficients of a linear regression coincide with
              identifiable historical, political or economic events which might
              have caused these breaks.",
  journal  = "Comput. Stat. Data Anal.",
  volume   =  44,
  number   =  1,
  pages    = "109--123",
  month    =  oct,
  year     =  2003,
  keywords = "Structural change; Changepoint problem; Segmented Regressions; R;
              S; Bellman principle"
}

@ARTICLE{Oka2016-fj,
  title         = "Testing for Common Breaks in a Multiple Equations System",
  author        = "Oka, Tatsushi and Perron, Pierre",
  abstract      = "The issue addressed in this paper is that of testing for
                   common breaks across or within equations of a multivariate
                   system. Our framework is very general and allows integrated
                   regressors and trends as well as stationary regressors. The
                   null hypothesis is that breaks in different parameters
                   (either regression coefficients or elements of the
                   covariance matrix of the errors) occur at a common locations
                   or are separated by some positive fraction of the sample
                   size. Under the alternative hypothesis, the break dates are
                   not the same and also need not be separated by a positive
                   fraction of the sample size across parameters. The test
                   considered is the quasi-likelihood ratio test assuming
                   normal errors, though as usual the limit distribution of the
                   test remains valid with non-normal errors. Also of
                   independent interest, we provide results about the rate of
                   convergence when searching over all possible partitions
                   subject only to the requirement that each regime of
                   different parameters contains at least as many observations
                   as some positive fraction of the sample size. Simulation
                   results show that the test has good finite sample
                   properties. We also provide an application to various
                   measures of inflation to illustrate its usefulness.",
  month         =  jun,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "1606.00092"
}

@ARTICLE{Levich1999-oh,
  title     = "Alternative tests for time series dependence based on
               autocorrelation coefficients",
  author    = "Levich, Richard M and Rizzo, Rosario C",
  journal   = "WORKING PAPER SERIES-NEW YORK UNIVERSITY SALOMON CENTER S",
  publisher = "New York University Salomon Center",
  year      =  1999
}

@TECHREPORT{Randi2006-rg,
  title       = "Is the market microstructure of stock markets Important",
  author      = "Randi, N{\ae}s and Johannes, Skjeltorp",
  number      = "Economic Bulletin",
  institution = "Norges Bank",
  year        =  2006
}

@TECHREPORT{Chui2012-rk,
  title       = "Derivatives markets, products and participants: an overview",
  author      = "Chui, Michael",
  number      = "Economic Bulletin",
  pages       = "3",
  institution = "Bank of International Settlements",
  year        =  2012
}

@TECHREPORT{QSMF_Oversight_Committee2016-un,
  title       = "Queen's Student Managed Fund Overview",
  author      = "{QSMF Oversight Committee}",
  institution = "Queen's University Belfast",
  year        =  2016
}

@ARTICLE{Barron1994-sn,
  title     = "A Time to Grow and a Time to Die: Growth and Mortality of Credit
               Unions in New York City, 1914-1990",
  author    = "Barron, David N and West, Elizabeth and Hannan, Michael T",
  abstract  = "One vision of organizational evolution suggests that old and
               large organizations become increasingly dominant over their
               environment. A second suggests that as organizations age they
               become less able to respond to new challenges. In this article
               the authors investigate which of these visions best
               characterizes the evolution of state-chartered credit unions in
               New York City from 1914 through 1990 by analyzing the effects of
               organizational age, size, and population density on rates of
               organizational failure and growth. The authors find evidence
               that old and small institutions are more likely to fail, while
               young and small organizations have the highest growth rates.",
  journal   = "Am. J. Sociol.",
  publisher = "University of Chicago Press",
  volume    =  100,
  number    =  2,
  pages     = "381--421",
  year      =  1994
}

@ARTICLE{Goddard2009-kf,
  title     = "Which Credit Unions are Acquired?",
  author    = "Goddard, John and McKillop, Donal and Wilson, John O S",
  abstract  = "Recent years have witnessed a wave of consolidation amongst US
               credit unions. Through hazard function estimations, this paper
               identifies the determinants of acquisition for credit unions
               during the period 2001-06. The hazard of acquisition is
               inversely related to both asset size and profitability, and
               positively related to liquidity. Growth-constrained credit
               unions are less attractive acquisition targets. Institutions
               with low capitalization and those with small loans portfolios
               relative to total assets are susceptible to acquisition. The
               investigation presents unique empirical evidence of a link
               between technological capability and the hazard of acquisition.
               During the period 2001-06, when there was sustained growth in
               the use of internet technology, credit unions with no website
               were at the highest risk of acquisition.",
  journal   = "J Financ Serv Res",
  publisher = "Springer US",
  volume    =  36,
  number    = "2-3",
  pages     = "231--252",
  month     =  dec,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Worthington2004-pw,
  title     = "Determinants of merger and acquisition activity in Australian
               cooperative deposit-taking institutions",
  author    = "Worthington, Andrew C",
  abstract  = "A two-stage procedure is employed to evaluate the determinants
               of merger and acquisition (M\&A) activity in Australian credit
               unions over the period 1992/1993 to 1994/1995. In the first
               stage, data envelopment analysis (DEA) is used to calculate
               technical and scale efficiency indices for a sample of credit
               unions. The second stage uses a multinomial logit model to
               relate credit union efficiency scores, along with other
               managerial, regulatory and financial factors, to the probability
               of credit unions acquiring or being acquired by another credit
               union. The results indicate that asset size and quality,
               management ability, earnings and liquidity are a significant
               influence on the level of M\&A. One primary influence on credit
               union acquisitions would appear to be the perceived
               compatibility in associational bond and membership.",
  journal   = "J. Bus. Res.",
  publisher = "Elsevier",
  volume    =  57,
  number    =  1,
  pages     = "47--57",
  year      =  2004,
  keywords  = "Mergers and acquisitions; Credit unions; Data envelopment
               analysis; Technical and scale efficiency"
}

@ARTICLE{Wheelock2000-xh,
  title     = "Why do Banks Disappear? The Determinants of {U.S}. Bank Failures
               and Acquisitions",
  author    = "Wheelock, David C and Wilson, Paul W",
  abstract  = "This paper seeks to identify the characteristics that make
               individual U.S. banks more likely to fail or be acquired. We use
               bank-specific information to estimate competing-risks hazard
               models with time-varying covariates. We use alternative measures
               of productive efficiency to proxy management quality, and find
               that inefficiency increases the risk of failure while reducing
               the probability of a bank's being acquired. Finally, we show
               that the closer to insolvency a bank is (as reflected by a low
               equity-to-assets ratio) the more likely is its acquisition.",
  journal   = "Rev. Econ. Stat.",
  publisher = "MIT Press",
  volume    =  82,
  number    =  1,
  pages     = "127--138",
  month     =  feb,
  year      =  2000
}

@TECHREPORT{Wilcox2005-bg,
  title       = "Failures and Insurance Losses of {Federally-Insured} Credit
                 Unions:1971-2004",
  author      = "Wilcox, James A",
  institution = "Filene Research Institute",
  year        =  2005
}

@MISC{noauthor_2005-ku,
  title        = "Credit Union Failures and Insurance Fund Losses: 1971-2004",
  booktitle    = "Federal Reserve Bank of San Francisco",
  abstract     = "Over the past few decades, assets in the credit union
                  industry have grown considerably and have grown relative to
                  banking. As with banking, the credit union industry has
                  experienced considerable structural change that, in part,
                  involved failures.",
  month        =  aug,
  year         =  2005,
  howpublished = "\url{http://www.frbsf.org/economic-research/publications/economic-letter/2005/august/credit-union-failures-and-insurance-fund-losses-1971-2004/}",
  note         = "Accessed: 2016-6-27"
}

@ARTICLE{noauthor_undated-uv,
  title = "[{PDF]A} primer on asset-backed commercial paper - Wells Fargo Funds"
}

@INBOOK{Llewellyn2013-po,
  title     = "Fifty Years in the Evolution of Bank Business Models",
  author    = "Llewellyn, David",
  publisher = "SUERF - The European Money and Finance Forum",
  pages     = "319--354",
  chapter   =  9,
  year      =  2013,
  keywords  = "Bank business models; ROE strategies; incentive structures;
               financialisation; equity capital; stock-adjustment phase; size
               and cost of the banking industry"
}

@ARTICLE{Borland2006-nj,
  title     = "Production Functions for Sporting Teams",
  author    = "Borland, Jeff",
  abstract  = "This comprehensive Handbook provides a survey of all the major
               research areas in sports economics written by almost all of the
               active researchers in this field. It offers not only an
               accessible insight into the major findings of the literature but
               also presents some of the world's principal researchers' views
               on the unanswered questions that face us today.",
  journal   = "Chapters",
  publisher = "Edward Elgar Publishing",
  year      =  2006,
  keywords  = "Economics and Finance"
}

@ARTICLE{Barros2014-ix,
  title     = "A Bayesian stochastic frontier of Italian football",
  author    = "Barros, C P and Rossi, G",
  abstract  = "This study analyses the technical efficiency of Serie A Italian
               football clubs during the 2004?2012 seasons with a Bayesian
               stochastic frontier model using unique data extracted from
               clubs? annual reports. Specifically, the focus of the study is
               on assessing the impact effects on clubs? efficiency whether
               they are big clubs, they employ several foreign player, they
               compete in European competitions and their involvement with
               football scandals in the Italian football. The empirical results
               reveal that efficiency varies among the clubs analysed and
               policy implications are derived.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  46,
  number    =  20,
  pages     = "2398--2407",
  month     =  jul,
  year      =  2014
}

@UNPUBLISHED{Bauer2012-uf,
  title    = "The Effect of Tarp Funding on Recipient Credit Unions",
  author   = "Bauer, Keldon",
  abstract = "Credit unions have become an important component of the American
              financial system, with more than 10\% of all savings deposits and
              non-revolving consumer loans;",
  month    =  mar,
  year     =  2012
}

@ARTICLE{Engle2001-ll,
  title    = "{GARCH} 101: The Use of {ARCH/GARCH} Models in Applied
              Econometrics",
  author   = "Engle, Robert",
  journal  = "J. Econ. Perspect.",
  volume   =  15,
  number   =  4,
  pages    = "157--168",
  month    =  dec,
  year     =  2001
}

@ARTICLE{Gennaioli2013-sl,
  title     = "A Model of Shadow Banking",
  author    = "Gennaioli, Nicola and Shleifer, Andrei and Vishny, Robert W",
  abstract  = "We present a model of shadow banking in which banks originate
               and trade loans, assemble them into diversified portfolios, and
               finance these portfolios externally with riskless debt. In this
               model: outside investor wealth drives the demand for riskless
               debt and indirectly for securitization, bank assets and leverage
               move together, banks become interconnected through markets, and
               banks increase their exposure to systematic risk as they reduce
               idiosyncratic risk through diversification. The shadow banking
               system is stable and welfare improving under rational
               expectations, but vulnerable to crises and liquidity dry-ups
               when investors neglect tail risks.",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  68,
  number    =  4,
  pages     = "1331--1363",
  month     =  aug,
  year      =  2013
}

@TECHREPORT{Adrian2012-ag,
  title       = "Shadow banking: a review of the literature",
  author      = "Adrian, Tobias and Ashcraft, Adam B",
  editor      = "Jones, Garett",
  abstract    = "Abstract The shadow banking system is a web of specialised
                 financial institutions that channel funding from savers to
                 investors through a range of securitisation and secured
                 funding techniques. Although shadow banks---the institutions
                 that constitute the shadow ...",
  publisher   = "Palgrave Macmillan UK",
  number      =  580,
  pages       = "282--315",
  institution = "Federal Reserve of New York Staff Report",
  year        =  2012,
  language    = "en"
}

@TECHREPORT{Pozsar2010-ah,
  title       = "Shadow Banking",
  author      = "Pozsar, Zoltan and Adrian, Tobias and Ashcraft, Adam B and
                 Boesky, Haley",
  abstract    = "The rapid growth of the market-based financial system since
                 the mid-1980s changed the nature of financial intermediation
                 in the United States profoundly. Within",
  publisher   = "papers.ssrn.com",
  number      =  458,
  institution = "Federal Reserve of New York Staff Report",
  month       =  jul,
  year        =  2010,
  keywords    = "shadow banking, financial intermediation"
}

@ARTICLE{Gorton2010-yb,
  title     = "Regulating the Shadow Banking System",
  author    = "Gorton, Gary and Metrick, Andrew",
  abstract  = "The shadow banking system played a major role in the recent
               financial crisis but remains largely unregulated. We propose
               principles for its regulation and describe a specific proposal
               to implement those principles. We document how the rise of
               shadow banking was helped by regulatory and legal changes that
               gave advantages to three main institutions: money-market mutual
               funds (MMMFs) to capture retail deposits from traditional banks,
               securitization to move assets of traditional banks off their
               balance sheets, and repurchase agreements (repos) that
               facilitated the use of securitized bonds as money. The evolution
               of a bankruptcy safe harbor for repos was crucial to the growth
               and efficiency of shadow banking; regulators can use access to
               this safe harbor as the lever to enforce new rules. History has
               demonstrated two successful methods for regulating privately
               created money: strict guidelines on collateral, and
               government-guaranteed insurance. We propose the use of insurance
               for MMMFs, combined with strict guidelines on collateral for
               both securitization and repos, with regulatory control
               established by chartering new forms of narrow banks for MMMFs
               and securitization, and using the bankruptcy safe harbor to
               incentivize compliance on repos.",
  journal   = "Brookings Pap. Econ. Act.",
  publisher = "Brookings Institution Press",
  volume    =  2010,
  number    =  2,
  pages     = "261--297",
  year      =  2010
}

@TECHREPORT{noauthor_2016-sv,
  title       = "Measuring the economic impact of reducing the corporation tax
                 rate in Northern Ireland- Summary Report",
  number      = "DETI",
  institution = "University of Ulster Economic Policy Centre",
  month       =  mar,
  year        =  2016
}

@ARTICLE{Barth2015-dr,
  title    = "Too Big to Fail and Too Big to Save: Dilemmas for Banking Reform",
  author   = "Barth, James R and Wihlborg, Clas",
  abstract = "``Too big to fail'' traditionally refers to a bank that is
              perceived to generate unacceptable risk to the banking system and
              indirectly to the economy as a whole",
  journal  = "National Institute Economic Review, Journal of the National
              Institute of Economic and Social Research, No. 235",
  month    =  dec,
  year     =  2015,
  keywords = "financial crises, too big to fail, systemically important banks,
              Dodd-Frank Act, Financial Stability Board, regulation and
              supervision"
}

@TECHREPORT{noauthor_2016-wu,
  title       = "Review of Implementation of the Recommendations in the
                 Commission on Credit Unions Report",
  institution = "Credit Union Advisory Committee",
  month       =  jun,
  year        =  2016
}

@ARTICLE{Hens2014-zw,
  title     = "Can utility optimization explain the demand for structured
               investment products?",
  author    = "Hens, Thorsten and Rieger, Marc Oliver",
  abstract  = "In this paper, we first show that for classical rational
               investors with correct beliefs and constant absolute or constant
               relative risk aversion, the utility gains from structured
               products over and above a portfolio consisting of the risk-free
               asset and the market portfolio are typically much smaller than
               their fees. This result holds irrespectively of whether the
               investors can continuously trade the risk-free asset and the
               market portfolio at no costs or whether they can just buy the
               assets and hold them to maturity of the structured product.
               However, when considering behavioural utility functions, such as
               prospect theory, or investors with incorrect beliefs (arising
               from probability weighting or probability misestimation), the
               utility gain can be sizable.",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  volume    =  14,
  number    =  4,
  pages     = "673--681",
  month     =  apr,
  year      =  2014
}

@ARTICLE{Kacperczyk2010-jn,
  title    = "When Safe Proved Risky: Commercial Paper during the Financial
              Crisis of 2007-2009",
  author   = "Kacperczyk, Marcin and Schnabl, Philipp",
  journal  = "J. Econ. Perspect.",
  volume   =  24,
  number   =  1,
  pages    = "29--50",
  month    =  mar,
  year     =  2010
}

@ARTICLE{Peltonen2014-oz,
  title    = "The network structure of the {CDS} market and its determinants",
  author   = "Peltonen, Tuomas A and Scheicher, Martin and Vuillemey, Guillaume",
  abstract = "This paper analyses the network structure of the credit default
              swap (CDS) market and its determinants, using a unique dataset of
              bilateral notional exposures on 642 financial and sovereign
              reference entities. We find that the CDS network is centred
              around 14 major dealers, exhibits a ``small world'' structure and
              a scale-free degree distribution. A large share of investors are
              net CDS buyers, implying that total credit risk exposure is
              fairly concentrated. Consistent with the theoretical literature
              on the use of CDS, the debt volume outstanding and its structure
              (maturity and collateralization), the CDS spread volatility and
              market beta, as well as the type (sovereign/financial) of the
              underlying bond are statistically significantly related---with
              expected signs---to structural characteristics of the CDS market.",
  journal  = "Journal of Financial Stability",
  volume   =  13,
  pages    = "118--133",
  month    =  aug,
  year     =  2014,
  keywords = "Credit default swap (CDS); Financial networks; Network topology;
              Credit risk"
}

@ARTICLE{Getmansky2014-cy,
  title    = "Interconnectedness in the {CDS} Market",
  author   = "Getmansky, Mila and Girardi, Giulio and Lewis, Craig M",
  abstract = "Concentrated risks in markets for credit default swaps (CDS) are
              widely considered to have significantly contributed to the recent
              financial crisis. In this pa",
  journal  = "SSRN Journal",
  year     =  2014,
  keywords = "CDS, dealers, network"
}

@ARTICLE{Treynor2016-qg,
  title     = "{Long-Term} Investing",
  author    = "Treynor, Jack L",
  abstract  = "We are reprinting this article by Jack Treynor, which first
               appeared in the May/June 1976 issue, as a companion to Stephen
               J. Brown?s Editor?s Corner about Treynor?s legacy. Treynor
               distinguishes between ideas whose implications are obvious and
               consequently travel quickly and ideas that require reflection,
               judgment, and special expertise for their evaluation and
               consequently travel slowly. According to Treynor, the second
               kind of idea is the only meaningful basis for ?long-term
               investing.?",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  72,
  number    =  4,
  pages     = "7--10",
  month     =  jul,
  year      =  2016
}

@ARTICLE{Claessens2015-pd,
  title     = "What is shadow banking?",
  author    = "Claessens, Stijn and Ratnovski, Lev",
  abstract  = "... banking focuses on ``hard information'' risks that are easy
               to measure , price and ... it provides for s starting point when
               it comes to measuring the shadow ... FSB ( Financial Stability
               Board, 2012, ``Strengthening Oversight and Regulation of Shadow
               Banking ,'' Consultative Document. ...",
  journal   = "IMF working paper",
  publisher = "IMF Working Paper",
  year      =  2015
}

@ARTICLE{Brave2011-tt,
  title     = "Monitoring Financial Stability: A Financial Conditions Index
               Approach",
  author    = "Brave, Scott A and Butters, R Andrew",
  abstract  = "Monitoring financial stability requires an understanding of both
               how traditional and evolving financial markets relate to each
               other and how they relate to econ",
  journal   = "Economic Perspectives",
  publisher = "papers.ssrn.com",
  month     =  feb,
  year      =  2011,
  keywords  = "Financial Conditions, Financial Crises, Financial Stress,
               Principal Component Analysis, Dynamic Factor Analysis, Financial
               Forecasting And Simulation, Index Numbers And Aggregation"
}

@ARTICLE{Malatesta2016-ur,
  title     = "The Shadow Banking System in the Euro Area: Definitions, Key
               Features and the Funding of Firms",
  author    = "Malatesta, Fabrizio and Masciantonio, Sergio and Zaghini, Andrea",
  abstract  = "We investigate the size and evolution over time of shadow banks
               in the euro area, with a particular focus on their role in the
               funding of non-financial corporations (NFCs). Using an
               institution-based definition of shadow banks, which is
               consistent with available Eurosystem data, we find that,
               notwithstanding a significant heterogeneity across countries,
               the euro-area shadow banking system has grown in importance
               since the outburst of the global financial crisis. In addition,
               also its interconnectedness with the regulated banking system
               has increased over time. An econometric investigation shows that
               macroeconomic variables are the main determinants of the growth
               of loans to NFCs.",
  journal   = "Ital Econ J",
  publisher = "Springer International Publishing",
  volume    =  2,
  number    =  2,
  pages     = "217--237",
  month     =  jul,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Duca2016-ua,
  title    = "How capital regulation and other factors drive the role of shadow
              banking in funding short-term business credit",
  author   = "Duca, John V",
  abstract = "This paper empirically analyzes how capital regulation, risk, and
              other factors altered the relative use of shadow banking-funded,
              short-term business debt since the early 1960s. Results indicate
              that the share was affected over the long run not only by
              changing information and reserve requirement costs, but also by
              shifts in relative regulation of bank versus nonbank credit
              sources---such as Basel I in 1990 and reregulation in 2010. In
              the short-run, the shadow bank share rose when deposit interest
              rate ceilings were binding on traditional banks, the economic
              outlook improved, or risk premia declined, and fell when event
              risks arose.",
  journal  = "Journal of Banking \& Finance",
  volume   =  69,
  pages    = "S10--S24",
  month    =  aug,
  year     =  2016,
  keywords = "Shadow banking; Regulation; Financial frictions; Credit rationing"
}

@UNPUBLISHED{Hansen2012-vv,
  title       = "Challenges in Identifying and Measuring Systemic Risk",
  author      = "Hansen, Lars Peter",
  abstract    = "Sparked by the recent ``great recession'' and the role of
                 financial markets, considerable interest exists among
                 researchers within both the academic community and the public
                 sector in modeling and measuring systemic risk. In this essay
                 I draw on experiences with other measurement agendas to place
                 in perspective the challenge of quantifying systemic risk, or
                 more generally, of providing empirical constructs that can
                 enhance our understanding of linkages between financial
                 markets and the macroeconomy.",
  number      =  18505,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  nov,
  year        =  2012
}

@ARTICLE{Kiyotaki1997-xi,
  title    = "Credit Cycles",
  author   = "Kiyotaki, Nobuhiro and Moore, John",
  abstract = "We construct a model of a dynamic economy in which lenders cannot
              force borrowers to repay their debts unless the debts are
              secured. In such an economy, durable assets play a dual role: not
              only are they factors of production, but they also serve as
              collateral for loans. The dynamic interaction between credit
              limits and asset prices turns out to be a powerful transmission
              mechanism by which the effects of shocks persist, amplify, and
              spill over to other sectors. We show that small, temporary shocks
              to technology or income distribution can generate large,
              persistent fluctuations in output and asset prices.",
  journal  = "J. Polit. Econ.",
  volume   =  105,
  number   =  2,
  pages    = "211--248",
  year     =  1997
}

@ARTICLE{Mundlak1965-pe,
  title     = "Consequences of Alternative Specifications in Estimation of
               {Cobb-Douglas} Production Functions",
  author    = "Mundlak, Yair and Hoch, Irving",
  abstract  = "In estimating parameters of the Cobb-Douglas production
               function, assuming competition and profit maximization, the
               estimator to be employed depends on the specification of the
               behavior of the disturbance term in the production function. If
               this disturbance term is not transmitted to inputs, that is, if
               inputs are independent of this disturbance, then the least
               squares estimator is consistent; if the disturbance is fully
               transmitted to inputs, then a consistent estimator is obtained
               if some restrictions are imposed on the second moments of the
               disturbances in the system. A more general case may be
               specified, however, encompassing the above specifications as
               subcases. In this general case, the disturbance term may be only
               partially transmitted. If this occurs, then neither of the
               estimators noted above are consistent. In fairly general
               situations, these estimators furnish upper and lower bounds for
               the production function elasticity (in a one-input case) or for
               the sum of the elasticities (in the Q input case; Q any number).
               The consequences of each of these specifications, in terms of
               probability limits, are examined in some detail. This is carried
               out, first, for the one input case, and then the Q input case is
               discussed.",
  journal   = "Econometrica",
  publisher = "[Wiley, Econometric Society]",
  volume    =  33,
  number    =  4,
  pages     = "814--828",
  year      =  1965
}

@ARTICLE{Bosworth1985-bn,
  title     = "Taxes and the Investment Recovery",
  author    = "Bosworth, Barry P and Shoven, John B and Summers, Lawrence H",
  journal   = "Brookings Pap. Econ. Act.",
  publisher = "Brookings Institution Press",
  volume    =  1985,
  number    =  1,
  pages     = "1--45",
  year      =  1985
}

@UNPUBLISHED{Perez-Saiz2016-ei,
  title  = "Local vs National: Regulatory and Cultural Entry Barriers in the
            Retail Banking Industry",
  author = "Perez-Saiz, Hector and Xiao, Hongyu",
  year   =  2016
}

@ARTICLE{Pril_undated-xp,
  title  = "{FCA} survey of firms providing financial advice",
  author = "Pril, A"
}

@ARTICLE{Mase2007-tp,
  title     = "The Impact of Changes in the {FTSE} 100 Index",
  author    = "Mase, Bryan",
  abstract  = "This paper investigates FTSE 100 index membership changes, which
               are determined quarterly by market capitalization and should
               have no information content. Return reversal around index
               additions and deletions suggests that buying (selling) pressure
               moves prices temporarily away from equilibrium, consistent with
               short-term downward sloping demand curves. In contrast to widely
               reported results for the S\&P 500, there is no evidence of
               permanent price effects. Further results suggest that investor
               awareness and monitoring due to index membership do not explain
               the price effects. There is statistically significant
               anticipatory trading in stocks that just fail to be promoted to
               the FTSE 100.",
  journal   = "Financial Review",
  publisher = "Blackwell Publishing Inc",
  volume    =  42,
  number    =  3,
  pages     = "461--484",
  month     =  aug,
  year      =  2007,
  keywords  = "FTSE 100 index; index trackers; price pressure; return reversal;
               stock market index changes; stock market liquidity; G14"
}

@ARTICLE{Mayes2013-yn,
  title     = "Achieving plausible separability for the resolution of
               cross-border banks",
  author    = "Mayes, David",
  journal   = "Journal of Financial Economic Policy; Bingley",
  publisher = "Emerald Group Publishing Limited",
  volume    =  5,
  number    =  4,
  pages     = "388--404",
  year      =  2013,
  address   = "Bingley, United Kingdom",
  language  = "en"
}

@ARTICLE{Tsionas2015-vi,
  title    = "Dynamic technical and allocative efficiencies in European banking",
  author   = "Tsionas, Efthymios G and Assaf, A George and Matousek, Roman",
  abstract = "Abstract This paper examines the performance of European banks
              during the pre-crisis and post-crisis periods, both in terms of
              technical and allocative efficiencies. We use an innovative
              Bayesian dynamic frontier model that: (1) distinguishes between
              short-run and long-run performance; and (2) provides impulse
              response functions to examine the dynamic effect of shocks in
              technical and allocative inefficiencies. Based on a rich sample
              of European banks, we show that while there was a drop in
              efficiency for most countries following the crisis, the long-run
              results suggest improvement both in terms of technical and
              allocative efficiencies. The impulse response functions also show
              that in the case of shocks in the system, banks seem to revert
              back to these long-run allocative efficiency scores. We discuss
              the results in terms of the current financial crisis and provide
              interesting implications for the European banking industry. We
              also discuss the determinants of technical and allocative
              efficiencies. (We would like to thank Professor Allen N. Berger
              and Professor Andy Mullineux for their valuable comments on the
              early version of this paper.)",
  journal  = "Journal of Banking \& Finance",
  volume   =  52,
  number   = "Supplement C",
  pages    = "130--139",
  month    =  mar,
  year     =  2015,
  keywords = "European banks; Financial crisis; Technical efficiency;
              Allocative efficiency; Short- and long-run"
}

@ARTICLE{DeYoung2017-xw,
  title    = "Bank Liquidity Management and Bank Capital Shocks",
  author   = "DeYoung, Robert and Distinguin, Isabelle and Tarazi, Amine",
  abstract = "The Basel III Accord imposes minimum liquidity standards on bank
              balance sheets that are already constrained by minimum capital
              standards. It is not clear wheth",
  month    =  jun,
  year     =  2017,
  keywords = "Bank Capital, Bank Liquidity, Basel III, Lending, Net Stable
              Funding Ratio"
}

@ARTICLE{Haar2016-xj,
  title     = "Freedom of Contract and Financial {Stability--Introductory}
               Remarks",
  author    = "Haar, Brigitte",
  journal   = "Eur Bus Org Law Rev",
  publisher = "Springer International Publishing",
  volume    =  17,
  number    = "1-2",
  pages     = "1--13",
  month     =  jun,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Andrade2004-ur,
  title    = "Investigating the economic role of mergers",
  author   = "Andrade, Gregor and Stafford, Erik",
  abstract = "We investigate the economic role of mergers by performing a
              comparative study of mergers and internal corporate investment at
              the industry and firm levels. We find strong evidence that merger
              activity clusters through time by industry, whereas internal
              investment does not. Mergers play both an ``expansionary'' and
              ``contractionary'' role in industry restructuring. During the
              1970s and 1980s, excess capacity drove industry consolidation
              through mergers, while peak capacity utilization triggered
              industry expansion through non-merger investments. In the 1990s,
              this phenomenon is reversed, as industries with strong growth
              prospects, high profitability, and near capacity experience the
              most intense merger activity.",
  journal  = "Journal of Corporate Finance",
  volume   =  10,
  number   =  1,
  pages    = "1--36",
  month    =  jan,
  year     =  2004,
  keywords = "Mergers; Acquisitions; Restructuring; Corporate governance"
}

@ARTICLE{Harford2005-wc,
  title    = "What drives merger waves?",
  author   = "Harford, Jarrad",
  abstract = "Aggregate merger waves could be due to market timing or to
              clustering of industry shocks for which mergers facilitate change
              to the new environment. This study finds that economic,
              regulatory and technological shocks drive industry merger waves.
              Whether the shock leads to a wave of mergers, however, depends on
              whether there is sufficient overall capital liquidity. This
              macro-level liquidity component causes industry merger waves to
              cluster in time even if industry shocks do not. Market-timing
              variables have little explanatory power relative to an economic
              model including this liquidity component. The contemporaneous
              peak in divisional acquisitions for cash also suggests an
              economic motivation for the merger activity.",
  journal  = "J. financ. econ.",
  volume   =  77,
  number   =  3,
  pages    = "529--560",
  month    =  sep,
  year     =  2005,
  keywords = "Mergers and acquisitions; Takeover; Merger waves; Behavioral;
              Capital liquidity"
}

@ARTICLE{Shleifer2003-es,
  title    = "Stock market driven acquisitions",
  author   = "Shleifer, Andrei and Vishny, Robert W",
  abstract = "We present a model of mergers and acquisitions based on stock
              market misvaluations of the combining firms. The key ingredients
              of the model are the relative valuations of the merging firms and
              the market's perception of the synergies from the combination.
              The model explains who acquires whom, the choice of the medium of
              payment, the valuation consequences of mergers, and merger waves.
              The model is consistent with available empirical findings about
              characteristics and returns of merging firms, and yields new
              predictions as well.",
  journal  = "J. financ. econ.",
  volume   =  70,
  number   =  3,
  pages    = "295--311",
  month    =  dec,
  year     =  2003,
  keywords = "Takeover; Synergy; Merger"
}

@INCOLLECTION{Ferri2015-gp,
  title     = "The banking regulatory bubble and how to get out of it",
  booktitle = "The Restructuring of Banks and Financial Systems in the Euro
               Area and the Financing of {SMEs}",
  author    = "Ferri, Giovanni and Neuberger, Doris",
  abstract  = "Abstract We claim that we currently live in a banking regulatory
               bubble. We review how: i) banking intermediation theory hinges
               on dealing with borrower-lender asymmetry of information; ii)
               instead, the presence of complete information is the keystone of
               finance ...",
  publisher = "Springer",
  pages     = "31--61",
  year      =  2015
}

@INCOLLECTION{Tullock1980-zl,
  title     = "Efficient Rent Seeking",
  booktitle = "Toward a theory of the rent-seeking society",
  author    = "Tullock, G",
  editor    = "Buchanan, J M and Tollison, R D and Tullock, G",
  publisher = "Texas A \& M University Press",
  year      =  1980
}

@INCOLLECTION{Tullock2001-wi,
  title     = "Efficient Rent Seeking",
  booktitle = "Efficient {Rent-Seeking}",
  author    = "Tullock, Gordon",
  editor    = "Lockard, Alan A and Tullock, Gordon",
  abstract  = "MOST of the papers in this volume* implicitly or explicitly
               assume that rent-seeking activity discounts the entire rent to
               be derived. Unfortunately, this is not necessarily true; the
               reality is much more complicated. The problem here is that the
               average cost and marginal cost are not necessarily identical.",
  publisher = "Springer, Boston, MA",
  pages     = "3--16",
  year      =  2001,
  language  = "en"
}

@ARTICLE{Giglio2016-iw,
  title    = "Systemic risk and the macroeconomy: An empirical evaluation",
  author   = "Giglio, Stefano and Kelly, Bryan and Pruitt, Seth",
  abstract = "This article studies how systemic risk and financial market
              distress affect the distribution of shocks to real economic
              activity. We analyze how changes in 19 different measures of
              systemic risk skew the distribution of subsequent shocks to
              industrial production and other macroeconomic variables in the US
              and Europe over several decades. We also propose dimension
              reduction estimators for constructing systemic risk indexes from
              the cross section of measures and demonstrate their success in
              predicting future macroeconomic shocks out of sample.",
  journal  = "J. financ. econ.",
  volume   =  119,
  number   =  3,
  pages    = "457--471",
  month    =  mar,
  year     =  2016,
  keywords = "Systemic risk; Quantile regression; Dimension reduction;
              Macroeconomy"
}

@ARTICLE{Bloom2009-lr,
  title     = "The Impact of Uncertainty Shocks",
  author    = "Bloom, Nicholas",
  abstract  = "Uncertainty appears to jump up after major shocks like the Cuban
               Missile crisis, the assassination of JFK, the OPEC I oil-price
               shock, and the 9/11 terrorist attacks. This paper offers a
               structural framework to analyze the impact of these uncertainty
               shocks. I build a model with a time-varying second moment, which
               is numerically solved and estimated using firm-level data. The
               parameterized model is then used to simulate a macro uncertainty
               shock, which produces a rapid drop and rebound in aggregate
               output and employment. This occurs because higher uncertainty
               causes firms to temporarily pause their investment and hiring.
               Productivity growth also falls because this pause in activity
               freezes reallocation across units. In the medium term the
               increased volatility from the shock induces an overshoot in
               output, employment, and productivity. Thus, uncertainty shocks
               generate short sharp recessions and recoveries. This simulated
               impact of an uncertainty shock is compared to vector
               autoregression estimations on actual data, showing a good match
               in both magnitude and timing. The paper also jointly estimates
               labor and capital adjustment costs (both convex and nonconvex).
               Ignoring capital adjustment costs is shown to lead to
               substantial bias, while ignoring labor adjustment costs does
               not.",
  journal   = "Econometrica",
  publisher = "Blackwell Publishing Ltd",
  volume    =  77,
  number    =  3,
  pages     = "623--685",
  month     =  may,
  year      =  2009,
  keywords  = "Adjustment costs; uncertainty; real options; labor and
               investment"
}

@ARTICLE{Estimators1988-bq,
  title   = "Douglas {RIVERS}",
  author  = "Estimators, Limited Information",
  journal = "J. Econom.",
  volume  =  39,
  pages   = "347--366",
  year    =  1988
}

@ARTICLE{Lijesen2013-eb,
  title    = "Solving the Endogeneity Problem in Empirical Cost Functions: An
              Application to {US} Banks",
  author   = "Lijesen, Mark G",
  abstract = "Empirical cost functions tend to ignore that firm differences in
              output levels depend on differences in cost levels and hence
              suffer from an endogeneity problem. We argue that traditional
              solutions for the endogeneity problem are insufficient for
              solving the problem and propose a structural approach to solve
              the problem. We apply both the traditional and the alternative
              models to panel data on large banks and found that a hybrid
              version yields results that are fully compatible with economic
              theory, whereas the traditional model provides theoretically
              incorrect in-sample predictions of scale elasticities.",
  journal  = "B. E. J. Econom. Anal. Policy",
  volume   =  13,
  number   =  2,
  month    =  jan,
  year     =  2013
}

@ARTICLE{Blundell2003-nf,
  title     = "Endogeneity in nonparametric and semiparametric regression
               models",
  author    = "Blundell, R and Powell, J L",
  abstract  = "... G would be the counterfactual conditional expectation of y
               given x if the endogeneity of x ... for`` limited information''
               set- tings in which the structural relations for the endogenous
               regressors in ... estimator of g. First, it is clear that,
               unlike the standard nonparametric regression problem ...",
  journal   = "Econometric Society Monographs",
  publisher = "books.google.com",
  year      =  2003
}

@ARTICLE{Imbens2009-zk,
  title     = "Identification and Estimation of Triangular Simultaneous
               Equations Models Without Additivity",
  author    = "Imbens, Guido W and Newey, Whitney K",
  abstract  = "This paper uses control variables to identify and estimate
               models with nonseparable, multidimensional disturbances.
               Triangular simultaneous equations models are considered, with
               instruments and disturbances that are independent and a reduced
               form that is strictly monotonic in a scalar disturbance. Here it
               is shown that the conditional cumulative distribution function
               of the endogenous variable given the instruments is a control
               variable. Also, for any control variable, identification results
               are given for quantile, average, and policy effects. Bounds are
               given when a common support assumption is not satisfied.
               Estimators of identified objects and bounds are provided, and a
               demand analysis empirical example is given.",
  journal   = "Econometrica",
  publisher = "Blackwell Publishing Ltd",
  volume    =  77,
  number    =  5,
  pages     = "1481--1512",
  month     =  sep,
  year      =  2009,
  keywords  = "Nonseparable models; control variables; quantile effects;
               bounds; average derivative; policy effect; nonparametric
               estimation; demand analysis"
}

@ARTICLE{Hall2002-gw,
  title     = "Testing Causality Between Team Performance and Payroll: The
               Cases of Major League Baseball and English Soccer",
  author    = "Hall, Stephen and Szymanski, Stefan and Zimbalist, Andrew S",
  abstract  = "The link between team payroll and competitive balance plays a
               central role in the theory of team sports but is seldom
               investigated empirically. This paper uses data on team payrolls
               in Major League Baseball between 1980 and 2000 to examine the
               link and implements Granger causality tests to establish whether
               the relationship runs from payroll to performance or vice versa.
               While there is no evidence that causality runs from payroll to
               performance over the entire sample period, the data shows that
               the cross section correlation between payroll and performance
               increased significantly in the 1990s. As a comparison, the paper
               examines the relationship between pay and performance in English
               soccer, and it is shown that Granger causality from higher
               payrolls to better performance cannot be rejected. We argue that
               this difference may be a consequence of the open market for
               player talent that obtains in soccer compared to the significant
               restrictions on trade that exist in Major League Baseball.",
  journal   = "J. Sports Econom.",
  publisher = "jse.sagepub.com",
  volume    =  3,
  number    =  2,
  pages     = "149--168",
  month     =  may,
  year      =  2002
}

@ARTICLE{Scholtens2009-tx,
  title     = "Scoring on the stock exchange? The effect of football matches on
               stock market returns: an event study",
  author    = "Scholtens, Bert and Peenstra, Wijtze",
  abstract  = "We analyse the effect of results of football matches on the
               stock market performance of football teams. We analyse 1274
               matches of eight teams in the national and European competition
               during 2000?2004. We find that the stock market response is
               significant and positive for victories and negative for defeats.
               The response is significantly stronger in the case of defeat.
               The response is stronger for matches in the European competition
               than for those in the national competition. Unexpected results
               have a stronger impact for European matches than expected ones
               but this is not the case in the national competition.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  41,
  number    =  25,
  pages     = "3231--3237",
  month     =  nov,
  year      =  2009
}

@ARTICLE{Torgler2007-tl,
  title     = "What shapes player performance in soccer? Empirical findings
               from a panel analysis",
  author    = "Torgler, Benno and Schmidt, Sascha L",
  abstract  = "In this article, we investigate the pay?performance relationship
               of soccer players using individual data from eight seasons of
               the German soccer league Bundesliga. We find a nonlinear
               pay?performance relationship, indicating that salary does indeed
               affect individual performance. The results further show that
               player performance is affected not only by absolute income level
               but also by relative income position. An additional analysis of
               the performance impact of team effects provides evidence of a
               direct impact of team-mate attributes on individual player
               performance.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  39,
  number    =  18,
  pages     = "2355--2369",
  month     =  oct,
  year      =  2007
}

@TECHREPORT{noauthor_2016-hg,
  title       = "{EBA} Report on Leverage Ratio Requirement under Article 511
                 of the {CRR}",
  number      = "EBA-Op-2016-13",
  institution = "European Banking Authority",
  month       =  aug,
  year        =  2016
}

@ARTICLE{Hyndman2015-pv,
  title     = "Transparency in Reporting on Charities' Efficiency",
  author    = "Hyndman, Noel and McConville, Danielle",
  abstract  = "In recent difficult economic times, the efficiency with which a
               charity spends the funds entrusted to it has become an
               increasingly important aspect of charitable performance.
               Transparency on efficiency, including the reporting of relevant
               measures and information to understand, contextualize, and
               evaluate such measures, is suggested as important to a range of
               stakeholders. However, using a novel framework for the analysis
               of efficiency reporting in the context of transparency and
               stakeholder theory, this research provides evidence that
               reporting on efficiency in U.K. charities lacks transparency,
               both in terms of the extent and manner of disclosure. It is
               argued that efficiency reporting in U.K. charities is more
               concerned with legitimizing these organizations rather than
               providing ethically driven accounts of their efficiency.",
  journal   = "Nonprofit and Voluntary Sector Quarterly",
  publisher = "SAGE PublicationsSage CA: Los Angeles, CA",
  volume    =  45,
  number    =  4,
  pages     = "844--865",
  month     =  sep,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Callen1994-ri,
  title     = "Money donations, volunteering and organizational efficiency",
  author    = "Callen, Jeffrey L",
  abstract  = "The purpose of this article is to explain the cross-sectional
               variation in money donations to charities at the organizational
               level. Using a unique data base which includes volunteer labor
               data, this article tests the hypotheses that money donations are
               positively related to volunteering and the technical efficiency
               of the firm. Technical efficiency is measured by a number of
               non-parametric indices. The empirical results indicateinter alia
               that the more technically efficient the charity, the more money
               donations it is able to raise. Moreoer, at least for one model,
               money donations and volunteering are found to be complementary
               at the organizational level. In addition, the results in this
               article are not consistent with the well-known hypothesis that
               government financing crowds out private donations.",
  journal   = "J Prod Anal",
  publisher = "Kluwer Academic Publishers",
  volume    =  5,
  number    =  3,
  pages     = "215--228",
  month     =  oct,
  year      =  1994,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Van_der_Heijden2013-jo,
  title    = "Small is beautiful? Financial efficiency of small fundraising
              charities",
  author   = "van der Heijden, Hans",
  abstract = "The study analyses potential scale efficiencies of 1196 Dutch
              fundraising charities for 2005--2009. The data set includes a
              unique group of small charities (reporting an income of less than
              ‚Ç¨1 million). The study articulates and tests differences in
              financial efficiency between smaller and larger charities,
              specifically concerning program expenditure, administrative
              expenditure, and fundraising expenditure. The study finds that
              reported levels of program-spending efficiency and administrative
              efficiency are similar across small and large charities, with no
              economies of scale. In addition, the study finds that smaller
              charities report considerably better fundraising efficiency
              ratios, with the smallest charities reporting an average spend of
              ‚Ç¨8 to raise ‚Ç¨100 and the largest charities reporting an average
              spend of ‚Ç¨15. The paper discusses why larger charities appear to
              experience scale inefficiencies in fundraising and provides
              directions for further research.",
  journal  = "The British Accounting Review",
  volume   =  45,
  number   =  1,
  pages    = "50--57",
  month    =  mar,
  year     =  2013,
  keywords = "Charities; Charity size; Fundraising ratio; Program ratio"
}

@ARTICLE{Connolly2013-cz,
  title    = "{UK} charity accounting: An exercise in widening stakeholder
              engagement",
  author   = "Connolly, Ciaran and Hyndman, Noel and McConville, Danielle",
  abstract = "Given the economic and social impact of the charity sector in the
              United Kingdom (UK), the importance of good governance has been
              recognised as a basis for underpinning effective and efficient
              performance, and for ensuring that charities meet the legitimate
              aspirations of key stakeholders. A major aspect of this is
              high-quality accounting and reporting. Over the past 25 years
              attempts have been made to improve this through the medium of
              successive, evolving versions of a Statement of Recommended
              Practice (SORP) for charities. As a foundation for the future
              review of the SORP (expected to be published in 2015), the SORP
              Committee undertook its largest ever consultation on an
              accounting pronouncement. This paper presents the findings of
              that consultation and, analysing them using stakeholder theory,
              concludes that this ambitious exercise facilitated much wider
              stakeholder engagement than had been experienced before and has
              the potential to legitimise further the SORP.",
  journal  = "The British Accounting Review",
  volume   =  45,
  number   =  1,
  pages    = "58--69",
  month    =  mar,
  year     =  2013,
  keywords = "Accounting; Charities; Reporting; Stakeholders"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Nunamaker1985-ua,
  title     = "Using data envelopment analysis to measure the efficiency of
               non‚Äêprofit organizations: A critical evaluation",
  author    = "Nunamaker, T R",
  abstract  = "Abstract This study examines the potential effects of variable
               set expansion and data variations upon the efficiency scores
               generated using the Data Envelopment Analysis (DEA) model. It
               was found that variable set expansion (either through
               disaggregation of existing ...",
  journal   = "Manage. Decis. Econ.",
  publisher = "Wiley Online Library",
  year      =  1985
}

@ARTICLE{Charnes1980-yk,
  title     = "Auditing and accounting for program efficiency and management
               efficiency in not-for-profit entities",
  author    = "Charnes, A and Cooper, W W",
  abstract  = "A measure of efficiency for not-for-profit entities - developed
               by the authors in association with Edward Rhodes - is explained
               and illustrated by data from Program Follow Through, a large
               scale social experiment in U.S. public school education. A
               division into Follow Through and Non-Follow Through participants
               facilitates a distinction between ``program efficiency'' and
               ``managerial efficiency'' which is also illustrated and examined
               for its use in evaluating such programs. Relations to
               comprehensive audits and other possible uses are explored.",
  journal   = "Accounting, Organizations and Society",
  publisher = "Elsevier",
  volume    =  5,
  number    =  1,
  pages     = "87--107",
  month     =  jan,
  year      =  1980
}

@BOOK{Ganley1992-ri,
  title     = "Public sector efficiency measurement: applications of data
               envelopment analysis",
  author    = "Ganley, Joseph Augustine and Cubbin, John",
  publisher = "Elsevier Science Publishers",
  year      =  1992,
  address   = "New York, NY, USA",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ahn1988-ap,
  title     = "Using data envelopment analysis to measure the efficiency of
               not‚Äêfor‚Äêprofit organizations: A critical evaluation---comment",
  author    = "Ahn, T and Charnes, A and Cooper, W W",
  abstract  = "Abstract This note responds to Nunamaker (1985) who supposedly
               deals with deficiencies in Data Envelopment Analysis (DEA) as an
               approach for (1) measuring efficiencies of not -for- profit
               entities identified as Decision Making Units (DMUs) and (2)
               locating sources and ...",
  journal   = "Manage. Decis. Econ.",
  publisher = "Wiley Online Library",
  year      =  1988
}

@ARTICLE{Robert_D_Hayes1990-hs,
  title     = "Measuring Production Efficiency in a {Not-for-Profit} Setting",
  author    = "{Robert D. Hayes} and {James A. Millar}",
  abstract  = "Productivity measurement has not generally been considered part
               of the information that managers use in planning and control
               decisions. Kaplan (1983) criticizes accounting research for the
               lack of studies on production efficiency stating that the
               effects of output volume and substitution possibilities among
               key production inputs on productivity measures have not been the
               subject of any accounting research. Hasseldine (1967), Mensah
               (1982), and Marcinko and Petri (1984) have addressed the
               short-comings of traditional financial measures of productive
               efficiency. Barlev and Callen (1986) show that input standards
               should not be defined independent of input prices, the state of
               technology, and the level of output. These authors, however,
               provide no empirical applications. This study provides empirical
               evidence related to performance measures of efficiency of
               production. Traditional budgeting methods and measures used for
               analysis may provide inadequate information for effective
               performance evaluation and control monitoring. This is
               particularly true if the budget model assumes that input cost
               shares are fixed. Analytical methods including budget analysis
               that fail to consider available input substitution possibilities
               in response to changes in relative input prices, and methods
               that fail to consider changes in operating conditions, may
               result in lost opportunities for cost savings. For example,
               traditional methods of assigning responsibility for accounting
               variances tend to focus attention on meeting the budget and may
               divert attention from production input-mix decisions when
               relative input prices change. For this study, we obtained data
               on the output produced and the input consumed via on-site visits
               to 33 county correctional institutions (jails) in Tennessee. A
               multivariate regression system of simultaneous equations and a
               translog cost function specification were employed to analyze
               these data. Our empirical model specification required input
               prices, output levels, and the state of technology as
               independent variables (not summary financial measures) to
               explain total operating expenditures for the budget period.
               Agency theory provides a means for inferring managerial
               behavior. The translog cost function coefficients provide
               essential information about variability in input cost shares for
               the sample data. Based on the hypotheses tested, we rejected the
               reasonableness of the conventional budget model assumption of
               fixed cost shares. We provide empirical evidence that managerial
               decisions based on matching expenditures and appropriations in
               line item budgets may not be cost-minimizing. The evidence
               suggests that a translog budget model may produce additional
               useful performance evaluation and control monitoring information
               that is not available from budget models which assume that
               cost-minimizing input cost (budget) shares are fixed.",
  journal   = "The Accounting Review",
  publisher = "American Accounting Association",
  volume    =  65,
  number    =  3,
  pages     = "505--519",
  year      =  1990
}

@INCOLLECTION{Hansen2016-qp,
  title     = "Revenue Sharing in European Football Leagues: A Theoretical
               Analysis",
  booktitle = "Trends in Mathematical Economics",
  author    = "Hansen, Bodil Olai and Tvede, Mich",
  editor    = "Pinto, Alberto A and Gamba, Elvio Accinelli and Yannacopoulos,
               Athanasios N and Herv{\'e}s-Beloso, Carlos",
  abstract  = "In the present chapter, a general model of competition between
               clubs in sports leagues with flexible supply of inputs is
               studied. There are externalities between clubs because it takes
               more than one club to produce games and tournaments. It is
               assumed that the externalities take the form of
               complementarities. Firstly, it is shown that revenue sharing
               leads to lower overall quality of sports leagues. Secondly, it
               is shown that the optimal quality for the league is lower
               (higher) than the quality in a league without revenue sharing in
               case of negative (positive) externalities between clubs. Thirdly
               an example is used to illustrate the findings.",
  publisher = "Springer, Cham",
  pages     = "245--262",
  year      =  2016,
  language  = "en"
}

@UNPUBLISHED{Atanasova2016-wq,
  title  = "Deposit Insurance Design and Credit Union Risk",
  author = "Atanasova, Christina and Li, Mingxin",
  year   =  2016
}

@ARTICLE{Banulescu2015-pz,
  title    = "Which are the {SIFIs}? A Component Expected Shortfall approach to
              systemic risk",
  author   = "Banulescu, Georgiana-Denisa and Dumitrescu, Elena-Ivona",
  abstract = "Abstract This paper proposes a component approach to systemic
              risk which allows to decompose the risk of the aggregate
              financial system (measured by Expected Shortfall) while
              accounting for the firm characteristics. Developed by analogy
              with the Component Value-at-Risk concept, our new systemic risk
              measure, called Component ES, presents several advantages. It is
              a hybrid measure, which combines the Too Interconnected To Fail
              and the Too Big To Fail logics. CES relies only on publicly
              available daily data and encompasses the popular Marginal ES
              measure. CES can be used to assess the contribution of a firm to
              systemic risk at a precise date but also to forecast its
              contribution over a certain period. The empirical application
              verifies the ability of CES to identify the most systemically
              risky firms during the 2007--2009 financial crisis. We show that
              our measure identifies the institutions labeled as SIFIs by the
              Financial Stability Board.",
  journal  = "Journal of Banking \& Finance",
  volume   =  50,
  pages    = "575--588",
  year     =  2015,
  keywords = "Systemic risk; Component Expected Shortfall; Marginal Expected
              Shortfall; Forecasting"
}

@ARTICLE{Elliott2006-jg,
  title   = "Efficient Tests for General Persistent Time Variation in
             Regression Coefficients1",
  author  = "Elliott, Graham",
  journal = "Rev. Econ. Stud.",
  volume  =  73,
  pages   = "907--940",
  year    =  2006
}

@ARTICLE{Cummins1994-jr,
  title     = "A Reconsideration of Investment Behavior Using Tax Reforms as
               Natural Experiments",
  author    = "Cummins, Jason G and Hassett, Kevin A and Hubbard, R Glenn and
               Hall, Robert E and Caballero, Ricardo J",
  journal   = "Brookings Pap. Econ. Act.",
  publisher = "Brookings Institution Press",
  volume    =  1994,
  number    =  2,
  pages     = "1--74",
  year      =  1994
}

@ARTICLE{Cummins1996-ia,
  title    = "Tax reforms and investment: A cross-country comparison",
  author   = "Cummins, Jason G and Hassett, Kevin A and Hubbard, R Glenn",
  abstract = "We use firm-level panel data to explore the extent to which fixed
              investment responds to tax reforms in 14 OECD countries. Previous
              studies have often found that investment does not respond to
              changes in the marginal cost of investment. We identify some of
              the factors responsible for this finding, and employ an
              estimation procedure that sidesteps the most important of them.
              In so doing, we find evidence of statistically and economically
              significant investment responses to tax changes in 12 of the 14
              countries.",
  journal  = "J. Public Econ.",
  volume   =  62,
  number   =  1,
  pages    = "237--273",
  month    =  oct,
  year     =  1996,
  keywords = "Tax reform; Investment; q theory; theory"
}

@ARTICLE{Bond1993-wl,
  title     = "Capital allowances and the impact of corporation tax on
               investment in the {UK}",
  author    = "Bond, S and Denny, K and Devereux, M",
  abstract  = "Since 1986, Corporation Tax in the UK has had the effect of
               reducing business investment. This is a clear and direct
               consequence of the reform to Corporation Tax introduced in 1984.
               The post-1986 system gives a roughly appropriate relief for the
               cost of depreciation, but ...",
  journal   = "Fisc. Stud.",
  publisher = "Wiley Online Library",
  year      =  1993
}

@ARTICLE{Ellis2004-ox,
  title     = "{UK} Business Investment and the User Cost of Capital",
  author    = "Ellis, Colin and Price, Simon",
  abstract  = "Theory tells us that output, the capital stock and the user cost
               of capital are related. From the capital accumulation identity,
               it also follows that the capital stock and investment have a
               long-run proportional relationship. The dynamic structure thus
               implies a multicointegrating framework, in which separate
               cointegrating relationships are identifiable. This has been used
               to justify the estimation of investment equations embodying a
               reduced-form long-run relationship between investment and output
               (rather than between the capital stock and output). In this
               paper, a new investment equation is estimated in the full
               structural framework, exploiting a measure of the capital stock
               constructed by the Bank, and a long series for the cost of
               capital. A constant elasticity of substitution production
               function is assumed, and a well-determined estimate of the
               elasticity of substitution is obtained by a variety of measures.
               The robust result is that the elasticity of substitution is
               significantly different from unity (the Cobb--Douglas case), at
               about 0.45. Overidentifying restrictions on the long-run
               relationship are all accepted. Thus there is strong evidence for
               a significant effect from the user cost of capital.",
  journal   = "Manchester Sch. Econ. Soc. Stud.",
  publisher = "Blackwell Publishing Ltd.",
  volume    =  72,
  pages     = "72--93",
  month     =  sep,
  year      =  2004
}

@ARTICLE{Adrian2011-ot,
  title   = "Systemic Risk Measurement: {CoVar}",
  author  = "Adrian, Tobias and Brunnermeier, Markus K",
  journal = "Federal Reserve Bank of New York",
  year    =  2011
}

@ARTICLE{Emekter2015-mt,
  title     = "Evaluating credit risk and loan performance in online
               {Peer-to-Peer} ({P2P}) lending",
  author    = "Emekter, Riza and Tu, Yanbin and Jirasakuldech, Benjamas and Lu,
               Min",
  abstract  = "Online Peer-to-Peer (P2P) lending has emerged recently. This
               micro loan market could offer certain benefits to both borrowers
               and lenders. Using data from the Lending Club, which is one of
               the popular online P2P lending houses, this article explores the
               P2P loan characteristics, evaluates their credit risk and
               measures loan performances. We find that credit grade,
               debt-to-income ratio, FICO score and revolving line utilization
               play an important role in loan defaults. Loans with lower credit
               grade and longer duration are associated with high mortality
               rate. The result is consistent with the Cox Proportional Hazard
               test which suggests that the hazard rate or the likelihood of
               the loan default increases with the credit risk of the
               borrowers. Finally, we find that higher interest rates charged
               on the high-risk borrowers are not enough to compensate for
               higher probability of the loan default. The Lending Club must
               find ways to attract high FICO score and high-income borrowers
               in order to sustain their businesses.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  47,
  number    =  1,
  pages     = "54--70",
  month     =  jan,
  year      =  2015
}

@ARTICLE{Zhang_undated-my,
  title  = "{THE} 2015 {UK} {ALTERNATIVE} {FINANCE} {INDUSTRY} {REPORT}",
  author = "Zhang, Bryan and Baeck, Peter and Ziegler, Tania and Bone, Jonathan
            and Garvey, Kieran"
}

@ARTICLE{Birnie2017-nz,
  title     = "Should the fiscal powers of the Northern Ireland Assembly be
               enhanced?",
  author    = "Birnie, Esmond and Brownlow, Graham",
  abstract  = "ABSTRACTShould the fiscal powers of the Northern Ireland
               Assembly be enhanced? Regional Studies. Northern Ireland has
               been characterized by an inability to narrow the persistent
               economic gap relative to Britain. Some commentators have
               suggested that regional corporation tax variation may be the
               ?game changer? in closing this gap. This paper draws on a range
               of studies that help one better understand the historical and
               institutional context. However, the analysis of tax variation is
               broader than this. Consideration is given as to which taxes
               might be the most suitable candidates for devolution. While
               greater tax variations could certainly complement an emphasis on
               increased competitiveness aimed at improving economic outcomes,
               they are no substitute for such a focus. As is often the case in
               institutional and economic development, issues of sequencing and
               policy capacity are salient.",
  journal   = "Reg. Stud.",
  publisher = "Routledge",
  volume    =  51,
  number    =  9,
  pages     = "1429--1439",
  month     =  sep,
  year      =  2017
}

@TECHREPORT{noauthor_2012-aa,
  title       = "Core principles for effective banking supervision",
  abstract    = "Core principles for effective banking supervision - September
                 2012",
  institution = "BIS",
  year        =  2012
}

@INCOLLECTION{Kringstad2007-le,
  title     = "Chapter 8 - Beyond Competitive Balance",
  booktitle = "International Perspectives on the Management of Sport",
  author    = "Kringstad, Morten and Gerrard, Bill",
  editor    = "Parent, Milena M and Slack, Trevor",
  publisher = "Butterworth-Heinemann",
  pages     = "149--172",
  month     =  jan,
  year      =  2007,
  address   = "Boston"
}

@MISC{Lin2012-oz,
  title   = "Comment on Article by Lum and Gelfand",
  author  = "Lin, Nan and Chang, Chao",
  journal = "Bayesian Analysis",
  volume  =  7,
  number  =  2,
  pages   = "263--270",
  year    =  2012
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Magrini2019-tz,
  title     = "Distributed-lag linear structural equation models in R: the
               dlsem package",
  author    = "Magrini, A",
  abstract  = "In this paper, an extension of linear Markovian structural
               causal models is introduced, called distributed-lag linear
               structural equation models (DLSEMs), where each factor of the
               joint probability distribution is a distributed-lag linear
               regression with constrained lag shapes ‚Ä¶",
  journal   = "Austrian Journal of Statistics",
  publisher = "ajs.or.at",
  year      =  2019
}

@ARTICLE{Tsionas2016-mq,
  title    = "Parameters measuring bank risk and their estimation",
  author   = "Tsionas, Mike G",
  abstract = "The paper develops estimation of three parameters of banking risk
              based on an explicit model of expected utility maximization by
              financial institutions subject to the classical technology
              restrictions of neoclassical production theory. The parameters
              are risk aversion, prudence or downside risk aversion and
              generalized risk resulting from a factor model of loan prices.
              The model can be estimated using standard econometric techniques,
              like GMM for dynamic panel data and latent factor analysis for
              the estimation of covariance matrices. An explicit functional
              form for the utility function is not needed and we show how
              measures of risk aversion and prudence (downside risk aversion)
              can be derived and estimated from the model. The model is
              estimated using data for Eurozone countries and we focus
              particularly on (i) the use of the modeling approach as a device
              close to an ``early warning mechanism'', (ii) the bank- and
              country-specific estimates of risk aversion and prudence
              (downside risk aversion), and (iii) the derivation of a
              generalized measure of risk that relies on loan-price
              uncertainty. Moreover, the model provides estimates of loan price
              distortions and thus, allocative efficiency.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  250,
  number   =  1,
  pages    = "291--304",
  month    =  apr,
  year     =  2016,
  keywords = "Financial stability; Banking; Expected utility maximization;
              Sub-prime crisis; Financial crisis"
}

@ARTICLE{noauthor_undated-ht,
  title = "[{PDF]2013} Open Markets Index - {ICC}"
}

@INCOLLECTION{Mulbert2015-of,
  title     = "Managing Risk in the Financial System",
  booktitle = "The Oxford Handbook of Financial Regulation",
  author    = "M{\"u}lbert, Peter O",
  editor    = "Moloney, Niamh and Ferran, Eil{\'\i}s and Payne, Jennifer",
  abstract  = "This chapter discusses the micro- and macro-prudential
               regulation of banks and investment firms as a means of
               protecting depositors and investors as well as minimizing
               systemic risk. After providing an overview of the objectives of
               micro- and macro-prudential regulation and the causes of
               systemic risks, it looks at the taxonomies of micro-prudential,
               macro-prudential, and dual-purpose tools of financial
               regulation. It also considers risk management, corporate
               governance, and risk culture at the firm level. The discussion
               then shifts to some policy implications for further regulation
               of financial institutions, with particular reference to tools
               that promote the soundness of individual firms and financial
               stability (e.g. disclosure and deposit insurance). Finally, the
               chapter examines the conflicts that arise between traditional
               micro- and macro-economic policies owing to their interaction on
               financial markets.",
  publisher = "Oxford University Press",
  month     =  aug,
  year      =  2015,
  keywords  = "micro-prudential regulation; banks; investment firms;
               macro-prudential regulation; systemic risks; risk management;
               financial stability; micro-economic policy; macro-economic
               policy; financial markets"
}

@TECHREPORT{Ayadi2016-hq,
  title       = "Banking Business Model Monitor Europe 2015",
  author      = "Ayadi, Rym and De Groen, Willem Pieter and Sassi, Ibtihel and
                 Mathlouthi, Walid and Rey, Harol and Aubry, Olivier",
  institution = "International Research Centre for Cooperative Finance",
  year        =  2016
}

@TECHREPORT{Ayadi2016-mf,
  title       = "Banking Business Models Monitor 2015 {EUROPE}",
  author      = "Ayadi, Rym and Pieter De Groen, Willem",
  institution = "Centre for European Policy Studies",
  year        =  2016
}

@TECHREPORT{Canada2015-zy,
  title       = "Candian Credit Unions: Different by Design",
  author      = "Canada, Conference Board of",
  institution = "The Conference Board of Canada",
  year        =  2015
}

@ARTICLE{Vives2014-zc,
  title     = "Strategic Complementarity, Fragility, and Regulation",
  author    = "Vives, Xavier",
  abstract  = "Fragility is affected by how the balance sheet composition of
               financial intermediaries, the precision of information signals,
               and market stress parameters all influence the extent of
               strategic complementarity among investors' strategies. A
               solvency and a liquidity ratio are required to control the
               likelihood of insolvency and illiquidity. The solvency
               requirement must be strengthened in the face of increased
               competition, whereas the liquidity requirement must be
               strengthened under more conservative fund managers and higher
               penalties for fire sales. Greater disclosure may aggravate
               fragility and require an increase in the liquidity ratio, so
               regulators should establish prudential and disclosure policies
               in tandem.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  27,
  number    =  12,
  pages     = "3547--3592",
  month     =  dec,
  year      =  2014
}

@TECHREPORT{noauthor_2017-rr,
  title       = "Artificial intelligence and machine learning in financial
                 services",
  institution = "Financial Stability Board",
  year        =  2017
}

@ARTICLE{Serrano-Cinca2015-lv,
  title    = "Determinants of Default in {P2P} Lending",
  author   = "Serrano-Cinca, Carlos and Guti{\'e}rrez-Nieto, Bego{\~n}a and
              L{\'o}pez-Palacios, Luz",
  abstract = "This paper studies P2P lending and the factors explaining loan
              default. This is an important issue because in P2P lending
              individual investors bear the credit risk, instead of financial
              institutions, which are experts in dealing with this risk. P2P
              lenders suffer a severe problem of information asymmetry, because
              they are at a disadvantage facing the borrower. For this reason,
              P2P lending sites provide potential lenders with information
              about borrowers and their loan purpose. They also assign a grade
              to each loan. The empirical study is based on loans' data
              collected from Lending Club (N = 24,449) from 2008 to 2014 that
              are first analyzed by using univariate means tests and survival
              analysis. Factors explaining default are loan purpose, annual
              income, current housing situation, credit history and
              indebtedness. Secondly, a logistic regression model is developed
              to predict defaults. The grade assigned by the P2P lending site
              is the most predictive factor of default, but the accuracy of the
              model is improved by adding other information, especially the
              borrower's debt level.",
  journal  = "PLoS One",
  volume   =  10,
  number   =  10,
  pages    = "e0139427",
  month    =  oct,
  year     =  2015,
  language = "en"
}

@ARTICLE{Acemoglu2015-fp,
  title    = "Systemic Risk and Stability in Financial Networks",
  author   = "Acemoglu, Daron and Ozdaglar, Asuman and Tahbaz-Salehi, Alireza",
  journal  = "Am. Econ. Rev.",
  volume   =  105,
  number   =  2,
  pages    = "564--608",
  month    =  feb,
  year     =  2015
}

@ARTICLE{Bhattacharya2012-ci,
  title     = "Is Unbiased Financial Advice to Retail Investors Sufficient?
               Answers from a Large Field Study",
  author    = "Bhattacharya, Utpal and Hackethal, Andreas and Kaesler, Simon
               and Loos, Benjamin and Meyer, Steffen",
  abstract  = "Working with one of the largest brokerages in Germany, we record
               what happens when unbiased investment advice is offered to a
               random set of approximately 8,000 active retail customers out of
               the brokerage's several hundred thousand retail customers. We
               find that investors who most need the financial advice are least
               likely to obtain it. The investors who do obtain the advice
               (about 5\%), however, hardly follow the advice and do not
               improve their portfolio efficiency by much. Overall, our results
               imply that the mere availability of unbiased financial advice is
               a necessary but not sufficient condition for benefiting retail
               investors.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  25,
  number    =  4,
  pages     = "975--1032",
  month     =  apr,
  year      =  2012
}

@ARTICLE{Monti2014-ca,
  title    = "Retail investors and financial advisors: New evidence on trust
              and advice taking heuristics",
  author   = "Monti, Marco and Pelligra, Vittorio and Martignon, Laura and
              Berg, Nathan",
  abstract = "This paper investigates factors that influence trust and advice
              taking among retail investors when consulting with financial
              advisors and making real-world portfolio decisions. The data
              reveal that non-expert retail investors trust their advisors a
              lot. Trust formation appears to be well described by a simple
              heuristic that relies substantially on the advisor's
              communication style when deciding how much to trust and delegate
              investment decisions. Portfolio decisions appear to depend more
              on investors' perceptions about the investor--advisor
              relationship than on the risk and return characteristics of
              investments comprising the portfolio choice set. This evidence
              supports Pentland's (2008) ``honest signals'' as a more powerful
              mechanism underlying investor trust than standard metrics based
              on past performance. Trust and advice-taking heuristics can be
              interpreted as well adapted to the environment of the non-profit
              bank cooperatives in which they are observed, implying that
              trusting based on simple honest signals, although vulnerable to
              exploitation, can be interpreted as ecologically rational.
              Features of the investor's environment typical of non-profit
              cooperative banks imply that the heuristics investors use can
              perform rather well without requiring investment experience or
              financial sophistication, which most investors in our sample are
              well aware they lack.",
  journal  = "J. Bus. Res.",
  volume   =  67,
  number   =  8,
  pages    = "1749--1757",
  month    =  aug,
  year     =  2014,
  keywords = "Trust; Delegation; Advice-taking; Heuristic; Non-expert;
              Financial literacy"
}

@ARTICLE{Calcagno2015-zr,
  title    = "Financial literacy and the demand for financial advice",
  author   = "Calcagno, Riccardo and Monticone, Chiara",
  abstract = "The low level of financial literacy across households suggests
              that they are at risk of making suboptimal financial decisions.
              In this paper, we analyze the effect of investors' financial
              literacy on their decision to demand professional,
              non-independent advice. We find that non-independent advisors are
              not sufficient to alleviate the problem of low financial
              literacy. The investors with a low level of financial literacy
              are less likely to consult an advisor, but they delegate their
              portfolio choice more often or do not invest in risky assets at
              all. We explain this evidence with a highly stylized model of
              strategic interaction between investors and better informed
              advisors with conflicts of interests. The advisors provide more
              information to knowledgeable investors, who anticipating this are
              more likely to consult them.",
  journal  = "Journal of Banking \& Finance",
  volume   =  50,
  pages    = "363--380",
  month    =  jan,
  year     =  2015,
  keywords = "Financial literacy; Financial advice; Household finance"
}

@ARTICLE{Gennaioli2015-jd,
  title    = "Money Doctors",
  author   = "Gennaioli, Nicola and Shleifer, Andrei and Vishny, Robert",
  abstract = "We present a new model of investors delegating portfolio
              management to professionals based on trust. Trust in the manager
              reduces an investor's perception of the riskiness of a given
              investment, and allows managers to charge fees. Money managers
              compete for investor funds by setting fees, but because of trust,
              fees do not fall to costs. In equilibrium, fees are higher for
              assets with higher expected return, managers on average
              underperform the market net of fees, but investors nevertheless
              prefer to hire managers to investing on their own. When investors
              hold biased expectations, trust causes managers to pander to
              investor beliefs.",
  journal  = "J. Finance",
  volume   =  70,
  number   =  1,
  pages    = "91--114",
  month    =  feb,
  year     =  2015
}

@UNPUBLISHED{Chalmers2012-vo,
  title       = "Is Conflicted Investment Advice Better than No Advice?",
  author      = "Chalmers, John and Reuter, Jonathan",
  abstract    = "The answer depends on how broker clients would have invested
                 in the absence of broker recommendations. To identify
                 counterfactual retirement portfolios, we exploit time-series
                 variation in access to brokers by new plan participants. When
                 brokers are available, they are chosen by new participants who
                 value recommendations on asset allocation and fund selection
                 because they are less financially experienced. When brokers
                 are no longer available, demand for target-date funds (TDFs)
                 increases differentially among participants with the highest
                 predicted demand for brokers. Broker client portfolios earn
                 significantly lower risk-adjusted returns and Sharpe ratios
                 than matched portfolios based on TDFs---due in part to broker
                 fees that average 0.90\% per year---but offer similar levels
                 of risk. More generally, the portfolios of participants with
                 high predicted demand for brokers who lack access to brokers
                 comparable favorably to the portfolios of similar participants
                 who had access to brokers when they joined. Exploiting
                 across-fund variation in the level of broker fees, we find
                 that broker clients allocate more dollars to higher fee funds.
                 This finding increases our confidence that actual broker
                 client portfolios reflect broker recommendations, and it
                 highlights an agency conflict that can be eliminated when TDFs
                 replace brokers.",
  number      =  18158,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  jun,
  year        =  2012
}

@ARTICLE{Cronqvist2014-um,
  title    = "The genetics of investment biases",
  author   = "Cronqvist, Henrik and Siegel, Stephan",
  abstract = "For a long list of investment ``biases,'' including lack of
              diversification, excessive trading, and the disposition effect,
              we find that genetic differences explain up to 45\% of the
              remaining variation across individual investors, after
              controlling for observable individual characteristics. The
              evidence is consistent with a view that investment biases are
              manifestations of innate and evolutionary ancient features of
              human behavior. We find that work experience with finance reduces
              genetic predispositions to investment biases. Finally, we find
              that even genetically identical investors, who grew up in the
              same family environment, often differ substantially in their
              investment behaviors due to individual-specific experiences or
              events.",
  journal  = "J. financ. econ.",
  volume   =  113,
  number   =  2,
  pages    = "215--234",
  month    =  aug,
  year     =  2014,
  keywords = "Investment biases; Behavioral genetics; Portfolio choice"
}

@MISC{noauthor_undated-nj,
  title = "{The-impact-of-RDR-Cass-version.pdf}"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ennew1992-mg,
  title    = "Consumer Attitudes to Independent Financial Advice",
  author   = "Ennew, Christine T",
  abstract = "Consumer protection was an important motivating factor behind the
              introduction of polarization in the Financial Services Act.
              Despite the potential benefits to the consumer of using
              independent financial advice as a source of information and a
              medium for the purchase of financial services, the majority of
              consumers appear to attach little value to the status of a
              financial adviser per se and instead attach importance to the
              image and reputation of particular suppliers. Reports a survey by
              in‚Äêdepth interviews of 140 consumers in the East Midlands, UK,
              that confirms the relatively low level of interest in independent
              financial advice, with the groups most likely to use such
              advisers being identified as the younger consumers from higher
              social class groupings who do not regularly collect product
              information from alternative sources such as newspapers and
              television.",
  journal  = "International Journal of Bank Marketing",
  volume   =  10,
  number   =  5,
  pages    = "4--12",
  year     =  1992
}

@UNPUBLISHED{Kelly2015-vz,
  title      = "{Too-Systemic-To-Fail}: What Option Markets Imply About
                Sector-wide Government Guarantees",
  author     = "Kelly, Bryan and Lustig, Hanno and Van Nieuwerburgh, Stijn",
  year       =  2015,
  conference = "Chicago Booth Business School Finance Workshop"
}

@ARTICLE{Draft--_undated-jn,
  title  = "Does Basel {III-Compliant} Bank Efficiency Enhance Industry Growth
            in Developing Countries?",
  author = "Draft--, --Manuscript"
}

@ARTICLE{Avgouleas2010-ea,
  title     = "Living wills as a catalyst for action",
  author    = "Avgouleas, E and Goodhart, C and Schoenmaker, D",
  abstract  = "... lawyers. Such Living Wills should be drawn up by banks and
               authorities. Banks would be the prime actors for developing
               their own recovery plans. Supervisors will challenge banks on
               the credibility of the recovery plan. This ...",
  journal   = "DSF Policy Paper",
  publisher = "researchgate.net",
  year      =  2010
}

@TECHREPORT{Bright2016-da,
  title       = "What Can We Learn from Publicly Available Data in Banks'
                 Living Wills?",
  author      = "Bright, S and Glasserman, P and Gregg, C and Hamandi, H",
  abstract    = "OFR researchers analyzed the public portions of resolution
                 plans, or `` living wills ,'' in which large US banks describe
                 how they would manage their own potential failure. This
                 analysis supports the OFR's mission to conduct research to
                 ensure a transparent, ...",
  number      = "Brief Series",
  institution = "Office of Financial Research",
  month       =  may,
  year        =  2016
}

@ARTICLE{Goodhart2015-vx,
  title     = "Optimal Bank Recovery",
  author    = "Goodhart, Charles and Segoviano Basurto, Miguel",
  abstract  = "Banks' living wills involve both recovery and resolution. Since
               it may not always be clear when recovery plans or actions should
               be triggered, there is a role f",
  publisher = "papers.ssrn.com",
  month     =  sep,
  year      =  2015,
  keywords  = "Bank Recovery, Metrics for Triggers, Loss Absorption,
               Probability of Distress, Loan Default, banks, bank, risk, value,
               systemic risk, Government Policy and Regulation, All Countries,;"
}

@ARTICLE{Hendershott2013-zh,
  title     = "Algorithmic trading and the market for liquidity",
  author    = "Hendershott, Terrence and Riordan, Ryan",
  abstract  = "Abstract We examine the role of algorithmic traders (ATs) in
               liquidity supply and demand in the 30 Deutscher Aktien Index
               stocks on the Deutsche Boerse in Jan. 2008. ATs represent 52\%
               of market order volume and 64\% of nonmarketable limit order
               volume. ATs more ...",
  journal   = "Journal of Financial and Quantitative Analysis",
  publisher = "Cambridge Univ Press",
  volume    =  48,
  number    =  04,
  pages     = "1001--1024",
  year      =  2013
}

@ARTICLE{Mundlak1961-io,
  title     = "Empirical Production Function Free of Management Bias",
  author    = "Mundlak, Yair",
  journal   = "Journal of Farm Economics",
  publisher = "[Oxford University Press, Agricultural \& Applied Economics
               Association]",
  volume    =  43,
  number    =  1,
  pages     = "44--56",
  year      =  1961
}

@ARTICLE{De_Bandt2000-qd,
  title    = "Systemic Risk: A Survey",
  author   = "de Bandt, Olivier and Hartmann, Philipp",
  abstract = "This paper develops a broad concept of systemic risk, the basic
              economic concept for the understanding of financial crises. It is
              claimed that any such concept",
  journal  = "European Central Bank Working Paper",
  volume   = "No. 35",
  month    =  nov,
  year     =  2000,
  keywords = "Systemic risk, financial stability, banking crises, contagion,
              financial markets, payment and settlement systems,;"
}

@ARTICLE{noauthor_undated-xr,
  title    = "Macroeconomics of bank capital and liquidity regulations",
  abstract = "BIS Working Papers No 596"
}

@MISC{noauthor_undated-po,
  title = "oxfordhb-9780199559084-e-7.pdf"
}

@ARTICLE{Casu2016-fl,
  title    = "Integration, productivity and technological spillovers: Evidence
              for eurozone banking industries",
  author   = "Casu, Barbara and Ferrari, Alessandra and Girardone, Claudia and
              Wilson, John O S",
  abstract = "In the context of the current debate on increased integration of
              eurozone banking markets following the global financial and
              sovereign debt crises, this paper evaluates the impact of
              regulatory reform, starting from the inception of the Single
              Market in 1992, on bank productivity and assesses the
              cross-border benefits of integration in terms of technological
              spillovers. We utilise a parametric meta-frontier Divisia index
              to estimate productivity change and identify technological gaps.
              We then assess the extent to which productivity converges within
              and across banking industries as a result of technological
              spillovers. Our results suggest that productivity growth has
              occurred for eurozone countries, driven by technological
              progress, both at the country and the supra-country level,
              although the latter slows or in some cases reverses since the
              onset of the crisis. Technological spillovers do exist, and have
              led to progression toward the best technology. However,
              convergence is not complete and significant long run differences
              in productivity persist. Improvements in technology are
              increasingly concentrated in fewer banking industries.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  255,
  number   =  3,
  pages    = "971--983",
  month    =  dec,
  year     =  2016,
  keywords = "Productivity growth; Divisia index; Spillovers; Convergence;
              European banking"
}

@ARTICLE{Larson_undated-ol,
  title    = "{LINKING} {SOCIAL} {EQUITY} {AND} {PERFORMANCE} {MEASUREMENT}",
  author   = "Larson, S J",
  abstract = "Abstract This paper explores the link between social equity and
              performance measurement, a priority that is increasingly promoted
              by leading social equity scholars today. Calls for improved
              assessment and performance management have become ...",
  journal  = "patheory.net"
}

@ARTICLE{Afonso2010-pg,
  title     = "Public sector efficiency: evidence for new {EU} member states
               and emerging markets",
  author    = "Afonso, Ant{\'o}nio and Schuknecht, Ludger and Tanzi, Vito",
  abstract  = "In this article, we analyse public sector efficiency in the new
               member states of the EU compared to that in emerging markets.
               After a conceptual discussion of expenditure efficiency
               measurement, we compute efficiency scores and rankings by
               applying a range of measurement techniques. The study finds that
               expenditure efficiency across new EU member states is rather
               diverse especially as compared to the group of top performing
               emerging markets in Asia. Econometric analysis shows that higher
               income, civil service competence and education levels as well as
               the security of property rights seem to facilitate the
               prevention of inefficiencies in the public sector.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  42,
  number    =  17,
  pages     = "2147--2164",
  month     =  jul,
  year      =  2010
}

@ARTICLE{Afonso2005-cl,
  title     = "Public sector efficiency: An international comparison",
  author    = "Afonso, Ant{\'o}nio and Schuknecht, Ludger and Tanzi, Vito",
  abstract  = "We compute public sector performance (PSP) and public sector
               efficiency (PSE) indicators, comprising a composite and seven
               sub-indicators, for 23 industrialised countries. The first four
               sub-indicators are ``opportunity'' indicators that take into
               account administrative, education and health outcomes and the
               quality of public infrastructure and that support the rule of
               law and a level playing-field in a market economy. Three other
               indicators reflect the standard ``Musgravian'' tasks for
               government: allocation, distribution and stabilisation. The
               input and output efficiency of public sectors across countries
               is then measured via a non-parametric production frontier
               technique. The study finds significant differences in PSP and
               PSE, which suggests a large potential for expenditure savings in
               many countries.",
  journal   = "Public Choice",
  publisher = "Kluwer Academic Publishers",
  volume    =  123,
  number    = "3-4",
  pages     = "321--347",
  month     =  jun,
  year      =  2005,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{DemirgucKunt2013-lk,
  title     = "Bank capital: Lessons from the financial crisis",
  author    = "Demirguc‚ÄêKunt, A and Detragiache, E and {others}",
  abstract  = "Using a multicountry panel of banks, we study whether better
               capitalized banks experienced higher stock returns during the
               financial crisis. We differentiate among various types of
               capital ratios: the Basel risk-adjusted ratio, the leverage
               ratio, the Tier 1 and Tier 2 ratios, ...",
  journal   = "J. Money Credit Bank.",
  publisher = "Wiley Online Library",
  year      =  2013
}

@ARTICLE{Huang2012-nr,
  title     = "Systemic Risk Contributions",
  author    = "Huang, Xin and Zhou, Hao and Zhu, Haibin",
  abstract  = "We adopt a systemic risk indicator measured by the price of
               insurance against systemic financial distress and assess
               individual banks' marginal contributions to the systemic risk.
               The methodology is applied using publicly available data to the
               19 bank holding companies covered by the U.S. Supervisory
               Capital Assessment Program (SCAP), with the systemic risk
               indicator peaking around \$1.1 trillion in March 2009. Our
               systemic risk contribution measure shows interesting similarity
               to and divergence from the SCAP loss estimates under stress test
               scenarios. In general, we find that a bank's contribution to the
               systemic risk is roughly linear in its default probability but
               highly nonlinear with respect to institution size and asset
               correlation.",
  journal   = "J Financ Serv Res",
  publisher = "Springer US",
  volume    =  42,
  number    = "1-2",
  pages     = "55--83",
  month     =  oct,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Drehmann2013-up,
  title    = "Measuring the systemic importance of interconnected banks",
  author   = "Drehmann, Mathias and Tarashev, Nikola",
  abstract = "We propose a method for measuring the systemic importance of
              interconnected banks. In order to capture contributions to
              system-wide risk, our measure accounts fully for the extent to
              which a bank (i) propagates shocks across the system and (ii) is
              vulnerable to propagated shocks. An empirical implementation of
              this measure and a popular alternative reveals that
              interconnectedness is a key driver of systemic importance.
              However, since the two measures reflect the impact of interbank
              borrowing and lending on system-wide risk differently, they can
              disagree substantially about the systemic importance of
              individual banks.",
  journal  = "Journal of Financial Intermediation",
  volume   =  22,
  number   =  4,
  pages    = "586--607",
  month    =  oct,
  year     =  2013,
  keywords = "Systemic risk; Shapley value; Interbank network"
}

@ARTICLE{Fahlenbrach2012-nt,
  title     = "This Time Is the Same: Using Bank Performance in 1998 to Explain
               Bank Performance during the Recent Financial Crisis",
  author    = "Fahlenbrach, R{\"u}diger and Prilmeier, Robert and Stulz,
               Ren{\'e} M",
  abstract  = "Are some banks prone to perform poorly during crises? If yes,
               why? In this paper, we show that a bank's stock return
               performance during the 1998 crisis predicts its stock return
               performance and probability of failure during the recent
               financial crisis. This effect is economically large. Our
               findings are consistent with persistence in a bank's risk
               culture and/or aspects of its business model that make its
               performance sensitive to crises. Banks that relied more on
               short-term funding, had more leverage, and grew more are more
               likely to be banks that performed poorly in both crises.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  67,
  number    =  6,
  pages     = "2139--2185",
  month     =  dec,
  year      =  2012
}

@ARTICLE{Acharya2013-xb,
  title     = "Aggregate Risk and the Choice between Cash and Lines of Credit",
  author    = "Acharya, Viral V and Almeida, Heitor and Campello, Murillo",
  abstract  = "Banks can create liquidity for firms by pooling their
               idiosyncratic risks. As a result, bank lines of credit to firms
               with greater aggregate risk should be costlier and such firms
               opt for cash in spite of the incurred liquidity premium. We find
               empirical support for this novel theoretical insight. Firms with
               higher beta have a higher ratio of cash to credit lines and face
               greater costs on their lines. In times of heightened aggregate
               volatility, banks exposed to undrawn credit lines become
               riskier; bank credit lines feature fewer initiations, higher
               spreads, and shorter maturity; and, firms' cash reserves rise.",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  68,
  number    =  5,
  pages     = "2059--2116",
  month     =  oct,
  year      =  2013
}

@ARTICLE{Hendershott2009-ij,
  title     = "Algorithmic trading and information",
  author    = "Hendershott, Terrence and Riordan, Ryan and {Others}",
  abstract  = "We examine algorithmic trades (AT) and their role in the price
               discovery process in the 30 DAX stocks on the Deutsche Boerse in
               January 2008. AT liquidity demand represents 52\% of volume and
               AT supplies liquidity on 50\% of volume. AT act strategically by
               monitoring the market ...",
  journal   = "Manuscript, University of California, Berkeley",
  publisher = "academia.edu",
  year      =  2009
}

@ARTICLE{Hasan2009-ki,
  title    = "Regional growth and finance in Europe: Is there a quality effect
              of bank efficiency?",
  author   = "Hasan, Iftekhar and Koetter, Michael and Wedow, Michael",
  abstract = "In this study, we test whether regional growth in 11 European
              countries depends on financial development and suggest the use of
              cost- and profit-efficiency estimates as quality measures of
              financial institutions. Contrary to the usual quantitative
              proxies of financial development, the quality of financial
              institutions is measured in this study as the relative ability of
              banks to intermediate funds. An improvement in bank efficiency
              spurs five times more regional growth then an identical increase
              in credit does. More credit provided by efficient banks exerts an
              independent growth effect in addition to direct quantity and
              quality channel effects.",
  journal  = "Journal of Banking \& Finance",
  volume   =  33,
  number   =  8,
  pages    = "1446--1453",
  month    =  aug,
  year     =  2009,
  keywords = "Bank performance; Regional growth; Bank efficiency; Europe"
}

@ARTICLE{Acharya2008-ex,
  title     = "Information Contagion and Bank Herding",
  author    = "Acharya, Viral V and Yorulmazer, Tanju",
  abstract  = "We show that the likelihood of information contagion induces
               profit-maximizing bank owners to herd with other banks. When
               bank loan returns have a common systematic factor, the cost of
               borrowing for a bank increases when there is adverse news on
               other banks since such news conveys adverse information about
               the common factor. The increase in a bank's cost of borrowing
               relative to the situation of good news about other banks is
               greater when bank loan returns have less commonality (in
               addition to the systematic risk factor). Hence, banks herd and
               undertake correlated investments so as to minimize the impact of
               such information contagion on the expected cost of borrowing.
               Competitive effects such as superior margins from lending in
               different industries mitigate herding incentives.",
  journal   = "J. Money Credit Bank.",
  publisher = "Blackwell Publishing Inc",
  volume    =  40,
  number    =  1,
  pages     = "215--231",
  month     =  feb,
  year      =  2008,
  keywords  = "G21; G28; G38; E58; D62; systemic risk; information spillover;
               inter-bank correlation"
}

@ARTICLE{Londono2014-dr,
  title    = "Bank Interventions and {Options-Based} Systemic Risk: Evidence
              from the Global and {Euro-Area} Crisis",
  author   = "Londono, Juan M and Tian, Mary H",
  abstract = "Using a novel dataset on central bank interventions to financial
              institutions, we examine the impact of capital injection
              announcements on systemic risk for the",
  month    =  sep,
  year     =  2014,
  keywords = "Systemic Risk, Downside Correlation Risk Premium, Bank
              Interventions, Variance Risk Premium, European Banking Union"
}

@ARTICLE{Balcaen2006-jv,
  title    = "35 years of studies on business failure: an overview of the
              classic statistical methodologies and their related problems",
  author   = "Balcaen, Sofie and Ooghe, Hubert",
  abstract = "Over the last 35 years, business failure prediction has become a
              major research domain within corporate finance. Numerous
              corporate failure prediction models have been developed, based on
              various modelling techniques. The most popular are the classic
              cross-sectional statistical methods, which have resulted in
              various `single-period' or static models, especially multivariate
              discriminant models and logit models. To date, there has been no
              clear overview and discussion of the application of classic
              statistical methods to business failure prediction. Therefore,
              this paper extensively elaborates on the application of (1)
              univariate analysis, (2) risk index models, (3) multivariate
              discriminant analysis, and (4) conditional probability models in
              corporate failure prediction. In addition, because there is no
              clear and comprehensive analysis in the existing literature of
              the diverse problems related to the application of these methods
              to the topic of corporate failure prediction, this paper brings
              together all problem issues and enlarges upon each of them. It
              discusses all problems related to: (1) the classical paradigm
              (i.e. the arbitrary definition of failure, non-stationarity and
              data instability, sampling selectivity, and the choice of the
              optimisation criteria); (2) the neglect of the time dimension of
              failure; and (3) the application focus in failure prediction
              modelling. Further, the paper elaborates on a number of other
              problems related to the use of a linear classification rule, the
              use of annual account information, and neglect of the
              multidimensional nature of failure. This paper contributes
              towards a thorough understanding of the features of the classic
              statistical business failure prediction models and their related
              problems.",
  journal  = "The British Accounting Review",
  volume   =  38,
  number   =  1,
  pages    = "63--93",
  month    =  mar,
  year     =  2006,
  keywords = "Business failure; Failure prediction models; Overview; Problem
              topics"
}

@ARTICLE{Bos2005-ww,
  title    = "Inefficient or Just Different? Effects of Heterogeneity on Bank
              Efficiency Scores",
  author   = "Bos, Jaap W B and Heid, Frank and Koetter, Michael and Kolari,
              James W and Kool, Clemens J M",
  abstract = "In this paper, we show the importance of accounting for
              heterogeneity among sample firms in stochastic frontier analysis.
              For a fairly homogenous sample of Germ",
  journal  = "Deutsche Bundesbank Discussion Paper",
  year     =  2005,
  keywords = "Heterogeneity, X-efficiency, benchmarking, bank production"
}

@ARTICLE{Battese1995-mb,
  title     = "A model for technical inefficiency effects in a stochastic
               frontier production function for panel data",
  author    = "Battese, G E and Coelli, T J",
  abstract  = "A stochastic frontier production function is defined for panel
               data on firms, in which the non-negative technical inefficiency
               effects are assumed to be a function of firm-specific variables
               and time. The inefficiency effects are assumed to be
               independently distributed as truncations of normal distributions
               with constant variance, but with means which are a linear
               function of observable variables. This panel data model is an
               extension of recently proposed models for inefficiency effects
               in stochastic frontiers for cross-sectional data. An empirical
               application of the model is obtained using up to ten years of
               data on paddy farmers from an Indian village. The null
               hypotheses, that the inefficiency effects are not stochastic or
               do not depend on the farmer-specific variables and time of
               observation, are rejected for these data.",
  journal   = "Empir. Econ.",
  publisher = "Physica-Verlag",
  volume    =  20,
  number    =  2,
  pages     = "325--332",
  month     =  jun,
  year      =  1995,
  language  = "en"
}

@INCOLLECTION{Battese1992-yu,
  title     = "Frontier Production Functions, Technical Efficiency and Panel
               Data: With Application to Paddy Farmers in India",
  booktitle = "International Applications of Productivity and Efficiency
               Analysis",
  author    = "Battese, G E and Coelli, T J",
  editor    = "Gulledge, Jr., Thomas R and Knox Lovell, C A",
  abstract  = "Frontier production functions are important for the prediction
               of technical efficiencies of individual firms in an industry. A
               stochastic frontier production function model for panel data is
               presented, for which the firm effects are an exponential
               function of time. The best predictor for the technical
               efficiency of an individual firm at a particular time period is
               presented for this time-varying model. An empirical example is
               presented using agricultural data for paddy farmers in a village
               in India.",
  publisher = "Springer, Dordrecht",
  pages     = "149--165",
  year      =  1992,
  language  = "en"
}

@MISC{Koetter2006-fl,
  title     = "Measurement {Matters--Alternative} Input Price Proxies for Bank
               Efficiency Analyses",
  author    = "Koetter, Michael",
  journal   = "Journal of Financial Services Research",
  publisher = "Springer Science \& Business Media",
  volume    =  30,
  number    =  2,
  pages     = "199",
  month     =  oct,
  year      =  2006,
  address   = "Dordrecht, Netherlands"
}

@MISC{Beccalli2013-zc,
  title     = "The Determinants of Mergers and Acquisitions in Banking",
  author    = "Beccalli, Elena and Frantz, Pascal",
  journal   = "Journal of Financial Services Research",
  publisher = "Springer Science \& Business Media",
  volume    =  43,
  number    =  3,
  pages     = "265--291",
  month     =  jun,
  year      =  2013,
  address   = "Dordrecht, Netherlands"
}

@ARTICLE{Li2016-so,
  title     = "Bank regulation, financial crisis, and the announcement effects
               of seasoned equity offerings of {US} commercial banks",
  author    = "Li, Hui and Liu, Hong and Siganos, Antonios and Zhou, Mingming",
  abstract  = "This paper studies the differences in the announcement effects
               of seasoned equity offerings (SEOs) of commercial banks and
               non-banks, and explores the influence of bank regulation and the
               financial crisis on such differences. We find that abnormal
               stock returns on SEO announcements for US commercial banks are
               significantly higher than those of non-banks, consistent with
               the hypothesis that bank regulations reduce the likelihood that
               bank SEOs signal overpriced equity. The propensity score
               matching-based difference-in-difference analysis indicates that
               the differences in stock returns between banks and non-banks
               decreased during the 2007-09 financial crisis period and
               increased after the passage of the Dodd-Frank Act in 2010.",
  journal   = "Journal of Financial Stability",
  publisher = "Elsevier",
  month     =  jun,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Chan-Lau2009-dm,
  title     = "Assessing the Systemic Implications of Financial Linkages",
  author    = "Chan-Lau, Jorge A and Espinosa, Marco and Giesecke, Kay and
               Sol{\'e}, Juan A",
  abstract  = "The rise in the complexity and globalization of financial
               services has contributed to stronger interconnections or
               linkages. While more extensive linkages contr",
  journal   = "IMF global financial",
  publisher = "papers.ssrn.com",
  month     =  apr,
  year      =  2009,
  keywords  = "financial crisis, interconnectedness, network analysis, corisk,
               default intensity, perimeter of regulation, information gaps"
}

@ARTICLE{Koenker2001-qx,
  title    = "Quantile Regresssion",
  author   = "Koenker, Roger",
  abstract = "Classical least squares regression may be viewed as a natural way
              of extending the idea of estimating an unconditional mean
              parameter to the problem of estimating conditional mean
              functions; the crucial link is the formulation of an optimization
              problem that encompasses both problems. Likewise, quantile
              regression offers an extension of univariate quantile estimation
              to estimation of conditional quantile functions via an
              optimization of a piecewise linear objective function in the
              residuals. Median regression minimizes the sum of absolute
              residuals, an idea introduced by Boscovich in the eighteenth
              century, and developed by Edgeworth in the nineteenth century.
              The asymptotic theory of quantile regression closely parallels
              the theory of the univariate sample quantiles. Computation of
              quantile regression estimators may be formulated as a linear
              programming problem and efficiently solved by simplex or barrier
              methods. A close link to rank-based inference has been forged
              from the theory of the dual regression quantile process, or
              regression rankscore process. Recent work has extended quantile
              regression into time-series, spatial models, survival analysis,
              and nonparametric estimation.",
  journal  = "Journal of Econometric Perspectives",
  volume   =  14,
  number   =  4,
  pages    = "143--156",
  year     =  2001
}

@ARTICLE{Patro2013-vo,
  title     = "A simple indicator of systemic risk",
  author    = "Patro, Dilip K and Qi, Min and Sun, Xian",
  abstract  = "We examine the relevance and effectiveness of stock return
               correlations among financial institutions as an indicator of
               systemic risk. By analyzing the trends and fluctuations of daily
               stock return correlations and default correlations among the 22
               largest bank holding companies and investment banks from 1988 to
               2008, we find that daily stock return correlation is a simple,
               robust, forward-looking, and timely systemic risk indicator.
               There is an increasing trend in stock return correlation among
               banks, whereas there is no obvious correlation trend among
               non-banks. We also disaggregate the stock returns into
               systematic and idiosyncratic components and find that the
               correlation increases are largely driven by the increases in
               correlations between banks' idiosyncratic risks, which give rise
               to increasing systemic risk. Correlation spikes tend to predict
               or coincide with significant economic or market events,
               especially during the 2007--2008 financial crisis. Furthermore,
               we show that stock return correlations offer a perspective on
               the level of systemic risk in the financial sector that is not
               already captured by default correlations. Stock return
               correlations are not subject to data limitations or model
               specification errors that other potential systemic risk measures
               may face. Therefore, we recommend that regulators and businesses
               monitor daily stock return correlations among those large and
               highly leveraged financial institutions to track the level of
               systemic risk.",
  journal   = "Journal of Financial Stability",
  publisher = "Elsevier",
  volume    =  9,
  number    =  1,
  pages     = "105--116",
  month     =  apr,
  year      =  2013,
  keywords  = "Systemic risk; Correlation; Stock returns"
}

@ARTICLE{Athanassopoulos1997-qd,
  title    = "Service quality and operating efficiency synergies for management
              control in the provision of financial services: Evidence from
              Greek bank branches",
  author   = "Athanassopoulos, Antreas D",
  abstract = "In this paper we concentrate on the assessment of the productive
              efficiency of bank branches. Bank branch operations are
              characterised by the effort made by management to pursue the
              banks' corporate objectives. The tangible part of this effort can
              be assessed by the operating efficiency of the branch while the
              intangible part is encapsulated by the quality of the provided
              services. The assessment of branch efficiency is pursued using
              data envelopment analysis methods enhanced by the value
              judgements of individual branch managers. This development gives
              insights on issues related to the appropriateness of branch input
              mix. The effort effectiveness is estimated by embodying three
              quality dimensions on the operating efficiency of bank branches.
              Empirical results are discussed from a sample of sixty eight
              commercial bank branches in Greece.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  98,
  number   =  2,
  pages    = "300--313",
  month    =  apr,
  year     =  1997,
  keywords = "Bank branches; Efficiency; Service quality; Data envelopment
              analysis; Value judgments"
}

@ARTICLE{Grifell-Tatje1999-ra,
  title     = "Profits and Productivity",
  author    = "Grifell-Tatj{\'e}, E and Lovell, C A K",
  abstract  = "In this study we consider the linkage between productivity
               change and profit change. We develop an analytical framework in
               which profit change between one period and the next is
               decomposed into three sources: (i) a productivity change effect
               (which includes a technical change effect and an operating
               efficiency effect), (ii) an activity effect (which includes a
               product mix effect, a resource mix effect, and a scale effect),
               and (iii) a price effect. We then show how to quantify the
               contribution of each effect, using only observed prices and
               quantities of products and resources in the two periods. We
               illustrate our analytical decomposition of profit change with an
               empirical application to Spanish banking during the period
               1987?1994.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  45,
  number    =  9,
  pages     = "1177--1193",
  month     =  sep,
  year      =  1999
}

@ARTICLE{Zmijewski1984-ff,
  title     = "Methodological Issues Related to the Estimation of Financial
               Distress Prediction Models",
  author    = "Zmijewski, Mark E",
  journal   = "Journal of Accounting Research",
  publisher = "[Accounting Research Center, Booth School of Business,
               University of Chicago, Wiley]",
  volume    =  22,
  pages     = "59--82",
  year      =  1984
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Iyer2016-uh,
  title    = "A Tale of Two Runs: Depositor Responses to Bank Solvency Risk",
  author   = "Iyer, Rajkamal and Puri, Manju and Ryan, Nicholas",
  abstract = "We examine heterogeneity in depositor responses to solvency risk
              using depositor‚Äêlevel data for a bank that faced two different
              runs. We find that depositors with loans and bank staff are less
              likely to run than others during a low‚Äêsolvency‚Äêrisk shock, but
              are more likely to run during a high‚Äêsolvency‚Äêrisk shock.
              Uninsured depositors are also sensitive to bank solvency. In
              contrast, depositors with older accounts run less, and those with
              frequent past transactions run more, irrespective of the
              underlying risk. Our results show that the fragility of a bank
              depends on the composition of its deposit base.",
  journal  = "J. Finance",
  volume   =  71,
  number   =  6,
  pages    = "2687--2726",
  month    =  dec,
  year     =  2016
}

@ARTICLE{Glode2016-fp,
  title    = "Compensating Financial Experts",
  author   = "Glode, Vincent and Lowery, Richard",
  abstract = "We propose a labor market model in which financial firms compete
              for a scarce supply of workers who can be employed as either
              bankers or traders. While hiring bankers helps create a surplus
              that can be split between a firm and its trading counterparties,
              hiring traders helps the firm appropriate a greater share of that
              surplus away from its counterparties. Firms bid defensively for
              workers bound to become traders, who then earn more than bankers.
              As counterparties employ more traders, the benefit of employing
              bankers decreases. The model sheds light on the historical
              evolution of compensation in finance.",
  journal  = "J. Finance",
  volume   =  71,
  number   =  6,
  pages    = "2781--2808",
  month    =  dec,
  year     =  2016
}

@ARTICLE{Karabiyik2016-kz,
  title    = "On the estimation and testing of predictive panel regressions",
  author   = "Karabiyik, Hande and Westerlund, Joakim and Narayan, Paresh",
  abstract = "Hjalmarsson (2010) considers an OLS-based estimator of predictive
              panel regressions that is argued to be mixed normal under very
              general conditions. In a recent paper, Westerlund et al. (2016)
              show that while consistent, the estimator is generally not mixed
              normal, which invalidates standard normal and chi-squared
              inference. The purpose of the present paper is to study the
              consequences of this theoretical result in small samples, which
              is done using both simulated and real data.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  45,
  pages    = "115--125",
  month    =  nov,
  year     =  2016,
  keywords = "Panel data; Predictive regression; Common factors; Mixed
              normality"
}

@ARTICLE{Brunnermeier2009-bw,
  title     = "Market Liquidity and Funding Liquidity",
  author    = "Brunnermeier, Markus K and Pedersen, Lasse Heje",
  abstract  = "We provide a model that links an asset's market liquidity (i.e.,
               the ease with which it is traded) and traders' funding liquidity
               (i.e., the ease with which they can obtain funding). Traders
               provide market liquidity, and their ability to do so depends on
               their availability of funding. Conversely, traders' funding,
               i.e., their capital and margin requirements, depends on the
               assets' market liquidity. We show that, under certain
               conditions, margins are destabilizing and market liquidity and
               funding liquidity are mutually reinforcing, leading to liquidity
               spirals. The model explains the empirically documented features
               that market liquidity (i) can suddenly dry up, (ii) has
               commonality across securities, (iii) is related to volatility,
               (iv) is subject to ``flight to quality,'' and (v) co-moves with
               the market. The model provides new testable predictions,
               including that speculators' capital is a driver of market
               liquidity and risk premiums.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  22,
  number    =  6,
  pages     = "2201--2238",
  month     =  jun,
  year      =  2009
}

@ARTICLE{noauthor_undated-yz,
  title    = "Systemic implications of the European bail-in tool: a",
  abstract = "Financial Stability Review, May 2016"
}

@ARTICLE{Bertrand2004-ol,
  title     = "How Much Should We Trust {Differences-In-Differences} Estimates?",
  author    = "Bertrand, Marianne and Duflo, Esther and Mullainathan, Sendhil",
  abstract  = "Most papers that employ Differences-in-Differences estimation
               (DD) use many years of data and focus on serially correlated
               outcomes but ignore that the resulting standard errors are
               inconsistent. To illustrate the severity of this issue, we
               randomly generate placebo laws in state-level data on female
               wages from the Current Population Survey. For each law, we use
               OLS to compute the DD estimate of its ``effect'' as well as the
               standard error of this estimate. These conventional DD standard
               errors severely understate the standard deviation of the
               estimators: we find an ``effect'' significant at the 5 percent
               level for up to 45 percent of the placebo interventions. We use
               Monte Carlo simulations to investigate how well existing methods
               help solve this problem. Econometric corrections that place a
               specific parametric form on the time-series process do not
               perform well. Bootstrap (taking into account the autocorrelation
               of the data) works well when the number of states is large
               enough. Two corrections based on asymptotic approximation of the
               variance-covariance matrix work well for moderate numbers of
               states and one correction that collapses the time series
               information into a ``pre''- and ``post''-period and explicitly
               takes into account the effective sample size works well even for
               small numbers of states.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  119,
  number    =  1,
  pages     = "249--275",
  month     =  feb,
  year      =  2004
}

@UNPUBLISHED{Schwert2014-lt,
  title    = "Capital Structure and Systematic Risk",
  author   = "Schwert, Michael and Strebulaev, Ilya A",
  abstract = "Systematic risk is an important determinant of corporate capital
              structure. A one standard deviation increase in asset beta
              corresponds to a decrease in leverag",
  month    =  apr,
  year     =  2014,
  keywords = "capital structure, systematic risk"
}

@INCOLLECTION{Gertler2010-ua,
  title     = "Financial Intermediation and Credit Policy in Business Cycle
               Analysis",
  booktitle = "Handbook of Monetary Economics",
  author    = "Gertler, Mark and Kiyotaki, Nobuhiro",
  editor    = "{Benjamin M. Friedman and Michael Woodford}",
  abstract  = "Abstract We develop a canonical framework to think about credit
               market frictions and aggregate economic activity in the context
               of the current crisis. We use the framework to address two
               issues in particular: first, how disruptions in financial
               intermediation can induce a crisis that affects real activity;
               and second, how various credit market interventions by the
               central bank and the Treasury of the type we have seen recently,
               might work to mitigate the crisis. We make use of earlier
               literature to develop our framework and characterize how very
               recent literature is incorporating insights from the crisis.",
  publisher = "Elsevier",
  volume    =  3,
  pages     = "547--599",
  series    = "Handbook of Monetary Economics",
  year      =  2010,
  keywords  = "Asset Prices; Credit Policy; Financial Intermediation; Moral
               Hazard; Net Worth; Spreads"
}

@TECHREPORT{Ecb2011-qc,
  title       = "Financial Networks and Financial Stability",
  author      = "{ECB}",
  number      = "Financial Stability Review",
  institution = "ECB",
  year        =  2011
}

@TECHREPORT{noauthor_2009-fy,
  title       = "Global Financial Stability Report: Responding to the Financial
                 Crisis and Measuring Systemic Risk",
  abstract    = "Responding to the Financial Crisis and Measuring Systemic Risk",
  institution = "IMF",
  year        =  2009
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hendershott2015-ft,
  title     = "Click or Call? Auction versus Search in the {Over‚Äêthe‚ÄêCounter}
               Market",
  author    = "Hendershott, T and Madhavan, A",
  abstract  = "ABSTRACT Over-the-counter (OTC) markets dominate trading in many
               asset classes. Will electronic trading displace traditional OTC
               ``voice'' trading? Can electronic and voice systems coexist?
               What types of securities and trades are best suited for
               electronic trading?",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  year      =  2015
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Joyce2011-ud,
  title     = "The financial market impact of quantitative easing in the United
               Kingdom",
  author    = "Joyce, Michael and Lasaosa, Ana and Stevens, Ibrahim and Tong,
               Matthew and {Others}",
  abstract  = "‚Ä¶ supply imbalances), changes in gilt-OIS spreads will tend to
               underestimate the effects of portfolio ‚Ä¶ One implication of our
               approach is that QE can potentially affect the term premium ‚Ä¶
               suggest that the term premium effect has broadly coincided with
               the portfolio balance effect ‚Ä¶",
  journal   = "Int. J. Cent. Bank",
  publisher = "International Journal of Central Banking",
  volume    =  7,
  number    =  3,
  pages     = "113--161",
  year      =  2011
}

@ARTICLE{Fare2018-oj,
  title    = "On technical inefficiency indicators at the industry level",
  author   = "F{\"a}re, Rolf and Grosskopf, Shawna and Karagiannis, Giannis",
  abstract = "Abstract In this note, we show how alternative measures of
              technical inefficiency at the industry level--evaluated along
              different direction vectors--are related to one another. We
              exploit the homogeneity property of the directional distance
              function with respect to the direction vector, to verify that
              aggregate (industry) efficiency evaluated relative to the
              industry aggregate input-output vector is equal to the average of
              the individual efficiency scores in the industry evaluated
              relative to the average input-output vector.",
  journal  = "Int. J. Prod. Econ.",
  volume   =  196,
  pages    = "333--334",
  year     =  2018,
  keywords = "Industry efficiency indicators; Directional distance function;
              Direction vector; Aggregation"
}

@MISC{noauthor_undated-mt,
  title = "health-income-ms-2011119-revision.pdf"
}

@ARTICLE{Gehrig2013-zm,
  title    = "Capital, Trust and Competitiveness in the Banking Sector",
  author   = "Gehrig, Thomas",
  abstract = "This note critically assesses the Basel reform process of capital
              regulation. It highlights the political nature of this process
              and argues that the absence of",
  journal  = "CEPR Discussion Paper No. DP9348",
  month    =  feb,
  year     =  2013,
  keywords = "bank capital, Basel process of capital regulation, trust"
}

@ARTICLE{Avgouleas2013-ck,
  title    = "Bank Resolution Plans as a catalyst for global financial reform",
  author   = "Avgouleas, Emilios and Goodhart, Charles and Schoenmaker, Dirk",
  abstract = "Bank Resolution Plans (Living Wills) should help with the
              resolution of systemically important financial institutions
              (SIFIs) in distress. They should be used to clarify and simplify
              the legal structure and make it commensurate with the functional
              business lines of the institution. Living Wills could also prove
              the right regulatory instrument to achieve two further
              innovations in the resolution of SIFIs with cross-border
              presence. First, they could incorporate burden sharing
              arrangements between countries enabling burden sharing on an
              institution by institution basis. However, there would remain
              problems arising from the incompatibility of the laws governing
              cross-border bank insolvencies. Many countries are currently
              introducing special laws covering the resolution of SIFIs. This
              creates a window of opportunity to use Living Wills to introduce
              a second innovation: a consistent legal regime for the resolution
              of SIFIs across the G20 countries.",
  journal  = "Journal of Financial Stability",
  volume   =  9,
  number   =  2,
  pages    = "210--218",
  month    =  jun,
  year     =  2013,
  keywords = "Bank resolution; Living wills; SIFIs; Burden sharing;
              Cross-border insolvency"
}

@ARTICLE{Basurto2006-oa,
  title     = "Consistent Information Multivariate Density Optimizing
               Methodology",
  author    = "Basurto, Miguel Angel Segoviano",
  journal   = "LSE FINANCIAL MARKETS GROUP DISCUSSION PAPER SERIES",
  publisher = "LONDON SCHOOL OF ECONOMICS",
  volume    =  557,
  year      =  2006
}

@TECHREPORT{Miguel_A_Segoviano2009-cl,
  title       = "Banking Stability Measures",
  author      = "Miguel A. Segoviano, Charles Goodhart",
  institution = "IMF Working Paper",
  year        =  2009
}

@UNPUBLISHED{Glaeser2014-bi,
  title       = "Housing Bubbles",
  author      = "Glaeser, Edward L and Nathanson, Charles G",
  abstract    = "Housing markets experience substantial price volatility, short
                 term price change momentum and mean reversion of prices over
                 the long run. Together these features, particularly at their
                 most extreme, produce the classic shape of an asset bubble. In
                 this paper, we review the stylized facts of housing bubbles
                 and discuss theories that can potentially explain events like
                 the boom-bust cycles of the 2000s. One set of theories assumes
                 rationality and uses idiosyncratic features of the housing
                 market, such as intensive search and short selling
                 constraints, to explain the stylized facts. Cheap credit
                 provides a particularly common rationalization for price
                 booms, but temporary periods of low interest rates will not
                 explain massive price swings in simple rational models. An
                 incorrectly under-priced default option is needed to explain
                 the formation of rational bubbles. Many non-rational
                 explanations for real estate bubbles exist, but the most
                 promising theories emphasize some form of trend-chasing, which
                 in turn reflects boundedly rational learning.",
  number      =  20426,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  aug,
  year        =  2014
}

@TECHREPORT{Bruhl2016-bp,
  title       = "How to Define a Systemically Important Financial Institution
                 ({SIFI}) -- A New Perspective",
  author      = "Br{\"u}hl, Volker",
  abstract    = "The recent financial crisis has demonstrated that a failure of
                 Systemically Important Financial Institutions (SIFIs) could
                 seriously damage the stability of the",
  institution = "Center for Financial Studies",
  month       =  oct,
  year        =  2016
}

@ARTICLE{Adrian2016-cu,
  title    = "{CoVaR}",
  author   = "Adrian, Tobias and Brunnermeier, Markus",
  journal  = "Am. Econ. Rev.",
  volume   =  106,
  number   =  7,
  pages    = "1705--1741",
  month    =  jul,
  year     =  2016
}

@ARTICLE{Brunnermeier2009-ls,
  title     = "Deciphering the Liquidity and Credit Crunch 2007--2008",
  author    = "Brunnermeier, Markus K",
  abstract  = "... These structured investment vehicles raise funds by selling
               short-term asset-backed commercial paper with an ... suddenly
               stop buying asset-backed commercial paper, preventing these
               vehicles from rolling ... To ensure funding liquidity for the
               vehicle , the sponsoring bank grants a ...",
  journal   = "J. Econ. Perspect.",
  publisher = "ingentaconnect.com",
  volume    =  23,
  number    =  1,
  pages     = "77--100",
  month     =  jan,
  year      =  2009
}

@ARTICLE{Rochet1996-ya,
  title     = "Interbank lending and systemic risk",
  author    = "Rochet, Jean-Charles and Tirole, Jean",
  abstract  = "SYSTEMIC RISK refers to the propagation of an agent's economic
               distress to other agents linked to that agent through financial
               transactions. Systemic risk is a serious concern in
               manufacturing, where trade credit links producers through a
               chain of obligations,'and in ...",
  journal   = "Journal of Money, Credit, and Banking",
  publisher = "Ohio State University Press",
  volume    =  28,
  number    =  4,
  pages     = "733--762",
  month     =  nov,
  year      =  1996,
  address   = "Columbus, United States",
  language  = "en"
}

@ARTICLE{Brunnermeier2014-rd,
  title    = "A Macroeconomic Model with a Financial Sector",
  author   = "Brunnermeier, Markus K and Sannikov, Yuliy",
  abstract = "This article studies the full equilibrium dynamics of an economy
              with financial frictions. Due to highly nonlinear amplification
              effects, the economy is prone to instability and occasionally
              enters volatile crisis episodes. Endogenous risk, driven by asset
              illiquidity, persists in crisis even for very low levels of
              exogenous risk. This phenomenon, which we call the volatility
              paradox, resolves the Kocherlakota (2000) critique. Endogenous
              leverage determines the distance to crisis. Securitization and
              derivatives contracts that improve risk sharing may lead to
              higher leverage and more frequent crises.",
  journal  = "Am. Econ. Rev.",
  volume   =  104,
  number   =  2,
  pages    = "379--421",
  month    =  feb,
  year     =  2014
}

@ARTICLE{Acharya_undated-bq,
  title  = "Financial Crisis of 2007-09",
  author = "Acharya, Viral V and Schnabl, Philipp"
}

@ARTICLE{Cerutti2017-ba,
  title    = "The use and effectiveness of macroprudential policies: New
              evidence",
  author   = "Cerutti, Eugenio and Claessens, Stijn and Laeven, Luc",
  abstract = "Abstract Using a recent IMF survey and expanding on previous
              studies, we document the use of macroprudential policies for 119
              countries over the 2000--2013 period, covering many instruments.
              Emerging economies use macroprudential policies most frequently;
              especially foreign exchange related ones while advanced countries
              use borrower-based policies more. Usage is generally associated
              with lower growth in credit, notably in household credit. Effects
              are less in financially more developed and open economies,
              however, and usage comes with greater cross-border borrowing,
              suggesting some avoidance. And while macroprudential policies can
              help manage financial cycles, they work less well in busts.",
  journal  = "Journal of Financial Stability",
  volume   =  28,
  pages    = "203--224",
  month    =  feb,
  year     =  2017,
  keywords = "Macroprudential policies; Effectiveness; Procyclicality;
              Financial cycles; Boom and busts"
}

@ARTICLE{Tillmann2015-me,
  title    = "Estimating the effects of macroprudential policy shocks: A Qual
              {VAR} approach",
  author   = "Tillmann, Peter",
  abstract = "This paper proposes a Qual VAR, i.e. a VAR augmented by
              qualitative variables, to estimate the effects of lowering
              maximum loan-to-value (LTV) ratios, a key macroprudential policy
              tool. We use Korea as a case study, where LTV ratios have been
              used frequently as a policy instrument. The Qual VAR has several
              advantages over competing methods. We conclude that a
              macroprudential tightening is effective in dampening credit
              growth and reducing the appreciation of house prices.",
  journal  = "Econ. Lett.",
  volume   =  135,
  pages    = "1--4",
  month    =  oct,
  year     =  2015,
  keywords = "Macroprudential policy; Qual VAR; LTV; House prices"
}

@ARTICLE{Zhang2016-wi,
  title    = "Leaning against the wind: Macroprudential policy in Asia",
  author   = "Zhang, Longmei and Zoli, Edda",
  abstract = "In recent years, many countries have adopted macroprudential
              measures to safeguard financial stability, in particular to deal
              with the credit and asset price cycles driven by global capital
              flows. Using a newly constructed database on macroprudential
              instruments and capital flow measures in 13 Asian economies and
              33 economies in other regions for the period 2000--2013, the
              paper formulates various macroprudential policy indices,
              aggregating sub-indices on key instruments. Asian economies
              appear to have made greater use of macroprudential tools,
              especially housing-related measures, than their counterparts in
              other regions. The effects of macroprudential policy are assessed
              through an event study, cross-country macro panel regressions,
              and bank-level micro panel regressions. The analysis suggests
              that housing-related macroprudential instruments-particularly
              loan-to-value ratio caps and housing tax measures---have helped
              curb housing price growth, credit growth, and bank leverage in
              Asia.",
  journal  = "J. Asian Econ.",
  volume   =  42,
  pages    = "33--52",
  month    =  feb,
  year     =  2016,
  keywords = "Macroprudential policy; Capital flow measures; Credit growth;
              Housing price"
}

@ARTICLE{Bailliu2015-go,
  title    = "Macroprudential rules and monetary policy when financial
              frictions matter",
  author   = "Bailliu, Jeannine and Meh, Cesaire and Zhang, Yahong",
  abstract = "This paper examines the interaction between monetary policy and
              macroprudential rules and whether policy makers should respond to
              financial imbalances. To address this issue, we build a dynamic
              general equilibrium model that features financial market
              frictions and financial shocks as well as standard macroeconomic
              shocks. We estimate the model using Canadian data. Based on these
              estimates, we show that it is beneficial to react to financial
              imbalances. The size of these benefits depends on the nature of
              the shock where the benefits are larger in the presence of
              financial shocks that have broader effects on the macroeconomy.",
  journal  = "Econ. Model.",
  volume   =  50,
  pages    = "148--161",
  month    =  nov,
  year     =  2015,
  keywords = "Monetary policy; Price stability; Macroprudential rule; Financial
              stability; Financial market imperfections"
}

@ARTICLE{Galati2013-hc,
  title    = "{MACROPRUDENTIAL} {POLICY} -- A {LITERATURE} {REVIEW}",
  author   = "Galati, Gabriele and Moessner, Richhild",
  abstract = "The recent financial crisis has highlighted the need to go beyond
              a purely micro approach to financial regulation and supervision.
              As a consequence, the number of policy speeches, research papers
              and conferences that discuss a macro perspective on financial
              regulation has grown considerably. The policy debate is focusing
              in particular on macroprudential tools and their usage, their
              relationship with monetary policy, their implementation and their
              effectiveness. Macroprudential policy has recently also attracted
              considerable attention among researchers. This paper provides an
              overview of research on this topic. We also identify important
              future research questions that emerge from both the literature
              and the current policy debate.",
  journal  = "J. Econ. Surv.",
  volume   =  27,
  number   =  5,
  pages    = "846--878",
  month    =  dec,
  year     =  2013,
  keywords = "Macroprudential policy"
}

@ARTICLE{Xiong2016-lp,
  title   = "Review on Financial Innovations in Big Data Era",
  author  = "Xiong, Xiong and Zhang, Jin and Xi, Jin and Xu, Feng",
  journal = "Int. J. Inf. Syst. Sci.",
  volume  =  4,
  number  =  6,
  pages   = "489--504",
  year    =  2016
}

@MISC{noauthor_undated-xc,
  title        = "[No title]",
  howpublished = "\url{https://www.researchgate.net/profile/Benjamin_Kaefer/publication/302531700_Peer-to-Peer_Lending_-_A_Financial_Stability_Risk_Perspective/links/5730eccb08ae6cca19a1f7a8.pdf}",
  note         = "Accessed: 2017-8-16"
}

@BOOK{Kafer2016-wj,
  title     = "The Interaction between Financial Stability and Financial
               Institutions: Some Reflections",
  author    = "{K{\"a}fer} and {Benjamin}",
  abstract  = "The contribution of this dissertation is to investigate
               financial stability issues from three different perspectives,
               illustrating that financial instability shows different
               characteristics over time, among financial institutions, and
               across financial activities. Chapter 1 reviews the normative and
               positive monetary policy literature on Taylor rules which have
               been augmented with exchange rates, asset prices, credit or
               leverage, and spreads. In addition, the chapter compares the
               development of these indicators for the core and the periphery
               of the Eurozone from 1999 (with the introduction of the euro)
               until 2013. Chapter 2 goes on to investigate the funding
               advantage that is provided to German Landesbanken by the joint
               liability scheme of the German Savings Banks Finance Group.
               Chapter 3 investigates peer-to-peer (P2P) lending and shows that
               the changing role of soft information, online platform default
               risk, liquidity risk and underdeveloped online secondary
               markets, and the institutionalization of P2P markets implies
               larger risks than traditional banking. Moreover, P2P lending can
               be considered part of the shadow banking sector.",
  publisher = "kassel university press GmbH",
  month     =  dec,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Lin2012-ve,
  title     = "Judging Borrowers by the Company They Keep: Friendship Networks
               and Information Asymmetry in Online {Peer-to-Peer} Lending",
  author    = "Lin, Mingfeng and Prabhala, Nagpurnanand R and Viswanathan, Siva",
  abstract  = "We study the online market for peer-to-peer (P2P) lending, in
               which individuals bid on unsecured microloans sought by other
               individual borrowers. Using a large sample of consummated and
               failed listings from the largest online P2P lending marketplace,
               Prosper.com, we find that the online friendships of borrowers
               act as signals of credit quality. Friendships increase the
               probability of successful funding, lower interest rates on
               funded loans, and are associated with lower ex post default
               rates. The economic effects of friendships show a striking
               gradation based on the roles and identities of the friends. We
               discuss the implications of our findings for the
               disintermediation of financial markets and the design of
               decentralized electronic markets. This paper was accepted by
               Sandra Slaughter, information systems.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  59,
  number    =  1,
  pages     = "17--35",
  month     =  sep,
  year      =  2012
}

@ARTICLE{Calluzzo2015-et,
  title    = "Has the financial system become safer after the crisis? The
              changing nature of financial institution risk",
  author   = "Calluzzo, Paul and Dong, Gang Nathan",
  abstract = "Six years after the collapse of Lehman Brothers, the question of
              whether the U.S. financial system has become less risky remains
              unanswered. On the one side, new regulations including Dodd-Frank
              and Basel III have made improvements by requiring higher bank
              capital, and financial institutions themselves have reduced
              risk-taking activities. On the other side, it has been argued
              that ``the fundamental risks remained and the efforts of
              regulators and politicians were simply rearranging the deckchairs
              on the Titanic.'' (Baily and Elliott, 2013) This paper highlights
              the changing nature of financial institution risk from 2005 to
              2011. It finds that while these institutions have become less
              risky individually after the crisis, the financial market has
              become more vulnerable to systemic contagion. The causal
              inference that the crisis and the post-crisis legislation have
              gradually changed the nature of financial institution risk is
              drawn from a quasi-experimental design. This finding suggests
              that the ever more integrated financial system might experience
              more synchronized contractions in future crises, providing
              empirical support for the proposals of the inter-bank collective
              regulation of banks by Acharya (2009) in addition to the
              intra-bank collective regulations as in Froot and Stein (1998)
              and BIS (1996, 1999).",
  journal  = "Journal of Banking \& Finance",
  volume   =  53,
  pages    = "233--248",
  month    =  apr,
  year     =  2015,
  keywords = "Banking risk; Financial institution; Systemic risk; Catastrophic
              risk"
}

@ARTICLE{Reongpitya2014-ew,
  title    = "Bank business models",
  author   = "Reongpitya, R and Tarashev, N and Tsataronis, K",
  abstract = "Part 5 of ``International banking and financial market
              developments'' (BIS Quarterly Review), December 2014",
  journal  = "Bank for International Settlements Quarterly Review",
  month    =  dec,
  year     =  2014
}

@ARTICLE{Tashman2000-js,
  title    = "Out-of-sample tests of forecasting accuracy: an analysis and
              review",
  author   = "Tashman, Leonard J",
  abstract = "In evaluations of forecasting accuracy, including forecasting
              competitions, researchers have paid attention to the selection of
              time series and to the appropriateness of forecast-error
              measures. However, they have not formally analyzed choices in the
              implementation of out-of-sample tests, making it difficult to
              replicate and compare forecasting accuracy studies. In this
              paper, I (1) explain the structure of out-of-sample tests, (2)
              provide guidelines for implementing these tests, and (3) evaluate
              the adequacy of out-of-sample tests in forecasting software. The
              issues examined include series-splitting rules, fixed versus
              rolling origins, updating versus recalibration of model
              coefficients, fixed versus rolling windows, single versus
              multiple test periods, diversification through multiple time
              series, and design characteristics of forecasting competitions.
              For individual time series, the efficiency and reliability of
              out-of-sample tests can be improved by employing rolling-origin
              evaluations, recalibrating coefficients, and using multiple test
              periods. The results of forecasting competitions would be more
              generalizable if based upon precisely described groups of time
              series, in which the series are homogeneous within group and
              heterogeneous between groups. Few forecasting software programs
              adequately implement out-of-sample evaluations, especially
              general statistical packages and spreadsheet add-ins.",
  journal  = "Int. J. Forecast.",
  volume   =  16,
  number   =  4,
  pages    = "437--450",
  month    =  oct,
  year     =  2000,
  keywords = "Out-of-sample; Fit period; Test period; Fixed origin; Rolling
              origin; Updating; Recalibration; Rolling window; Sliding
              simulation; Forecasting competitions"
}

@ARTICLE{Campbell2008-xi,
  title     = "Predicting Excess Stock Returns Out of Sample: Can Anything Beat
               the Historical Average?",
  author    = "Campbell, John Y and Thompson, Samuel B",
  abstract  = "Goyal and Welch (2007) argue that the historical average excess
               stock return forecasts future excess stock returns better than
               regressions of excess returns on predictor variables. In this
               article, we show that many predictive regressions beat the
               historical average return, once weak restrictions are imposed on
               the signs of coefficients and return forecasts. The
               out-of-sample explanatory power is small, but nonetheless is
               economically meaningful for mean-variance investors. Even better
               results can be obtained by imposing the restrictions of
               steady-state valuation models, thereby removing the need to
               estimate the average from a short sample of volatile stock
               returns.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  4,
  pages     = "1509--1531",
  month     =  jul,
  year      =  2008
}

@ARTICLE{Varian2014-ow,
  title    = "Big Data: New Tricks for Econometrics",
  author   = "Varian, Hal R",
  journal  = "J. Econ. Perspect.",
  volume   =  28,
  number   =  2,
  pages    = "3--28",
  month    =  may,
  year     =  2014
}

@ARTICLE{Treasury2010-kp,
  title   = "Corporate Tax Reform: delivering a more competitive system",
  author  = "Treasury, H M",
  journal = "HM Treasury and HM Revenue and Customs",
  year    =  2010
}

@ARTICLE{Altunbas2016-gl,
  title    = "Does Monetary Policy Affect Bank {Risk-Taking}?",
  author   = "Altunbas, Yener and Gambacorta, Leonardo and Marques-Ibanez,
              David",
  abstract = "This paper investigates the relationship between short-term
              interest rates and bank risk. Using a unique database that
              includes quarterly balance sheet informat",
  journal  = "International Journal of Central Banking",
  year     =  2016,
  keywords = "bank risk, monetary policy, credit crisis"
}

@INCOLLECTION{Franck1995-pw,
  title     = "Rationalit{\"a}tssurrogate in der Teamsportindustrie",
  booktitle = "Die {\"o}konomischen Institutionen der Teamsportindustrie",
  author    = "Franck, Egon",
  abstract  = "ZusammenfassungIn Abschnitt II.7.4.3 wurde ausgef{\"u}hrt,
               da{\ss} die neoinstitutionalistische Analyse eines
               Gegenstandsbereichs die existierenden Transaktionsebenen als
               Bezugspunkt w{\"a}hlen sollte. Auf ihnen stellen sich die
               origin{\"a}ren Probleme der Institutionenwahl, deren L{\"o}sung
               je nach Betrachtunsperspektive als Au{\ss}enorganisation der
               gestaltenden Akteure oder als Innenorganisation eines
               m{\"o}glicherweise im Rahmen der Gestaltung entstehenden Akteurs
               interpretierbar ist.",
  publisher = "Deutscher Universit{\"a}tsverlag, Wiesbaden",
  pages     = "124--222",
  series    = "Markt- und Unternehmensentwicklung",
  year      =  1995,
  language  = "de"
}

@ARTICLE{Karagiannis2012-ig,
  title    = "More on the Fox paradox",
  author   = "Karagiannis, Giannis",
  abstract = "This paper shows that the compatibility between efficiency
              measures and the aggregation procedure is not enough to resolve
              the Fox paradox when different inputs are employed in each
              activity. We explicitly illustrate this point by considering the
              additive aggregation of cost efficiency indicators for production
              and advertising activities, which use different inputs. Then, the
              overall cost efficiency indicator is given by the weighted
              (rather than the simple) sum of the production and the
              advertising cost efficiency indicators. The reason is that the
              value of the directional vector used to normalize the difference
              between the minimum and the observed cost in each activity-based
              efficiency indicator will differ.",
  journal  = "Econ. Lett.",
  volume   =  116,
  number   =  3,
  pages    = "333--334",
  month    =  sep,
  year     =  2012,
  keywords = "Efficiency measurement; Aggregation; Cost efficiency indicator"
}

@ARTICLE{Fox1999-yj,
  title    = "Efficiency at different levels of aggregation: public vs. private
              sector firms",
  author   = "Fox, Kevin J",
  abstract = "Using a standard definition of cost efficiency, a multi-product
              firm may be more efficient at producing each product than any
              other firm, yet it may not be the most efficient firm overall.
              The explanation for this seeming paradox leads to important
              implications for the reporting of efficiency scores at different
              levels of aggregation for the public sector versus the private
              sector. Studies which use aggregate data to evaluate public
              sector performance may be seriously flawed.",
  journal  = "Econ. Lett.",
  volume   =  65,
  number   =  2,
  pages    = "173--176",
  month    =  nov,
  year     =  1999,
  keywords = "Efficiency measurement; Public sector"
}

@INCOLLECTION{Billingsley2006-wm,
  title     = "Why Is Arbitrage So Important?",
  booktitle = "Arbitrage, Hedging, and the Law of One Price",
  author    = "Billingsley, Randall",
  abstract  = "Arbitrage is the process of buying assets in one market and
               selling them in another to profit from unjustifiable price
               differences. This violates the expectation that the same product
               should sell for the same price. Arbitrage offers guaranteed
               profit with no risk, and therefore undermines the stability and
               functionality of markets. This chapter explains how these
               expectations and forces interact, and why arbitrage is such a
               dangerous thing.",
  publisher = "FT Press",
  year      =  2006
}

@ARTICLE{Malkiel2003-pj,
  title     = "Passive Investment Strategies and Efficient Markets",
  author    = "Malkiel, Burton G",
  abstract  = "This paper presents the case for and the evidence in favour of
               passive investment strategies and examines the major criticisms
               of the technique. I conclude that the evidence strongly supports
               passive investment management in all
               markets---small--capitalisation stocks as well as
               large--capitalisation equities, US markets as well as
               international markets, and bonds as well as stocks. Recent
               attacks on the efficient market hypothesis do not weaken the
               case for indexing.",
  journal   = "European Financial Management",
  publisher = "Blackwell Publishing Ltd",
  volume    =  9,
  number    =  1,
  pages     = "1--10",
  month     =  mar,
  year      =  2003,
  keywords  = "passive investment strategies; efficient markets"
}

@ARTICLE{Barber2000-zb,
  title     = "Trading Is Hazardous to Your Wealth: The Common Stock Investment
               Performance of Individual Investors",
  author    = "Barber, Brad M and Odean, Terrance",
  abstract  = "Individual investors who hold common stocks directly pay a
               tremendous performance penalty for active trading. Of 66,465
               households with accounts at a large discount broker during 1991
               to 1996, those that trade most earn an annual return of 11.4
               percent, while the market returns 17.9 percent. The average
               household earns an annual return of 16.4 percent, tilts its
               common stock investment toward high-beta, small, value stocks,
               and turns over 75 percent of its portfolio annually.
               Overconfidence can explain high trading levels and the resulting
               poor performance of individual investors. Our central message is
               that trading is hazardous to your wealth.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishers, Inc.",
  volume    =  55,
  number    =  2,
  pages     = "773--806",
  month     =  apr,
  year      =  2000
}

@ARTICLE{French2008-qe,
  title     = "Presidential Address: The Cost of Active Investing",
  author    = "French, Kenneth R",
  abstract  = "I compare the fees, expenses, and trading costs society pays to
               invest in the U.S. stock market with an estimate of what would
               be paid if everyone invested passively. Averaging over
               1980--2006, I find investors spend 0.67\% of the aggregate value
               of the market each year searching for superior returns.
               Society's capitalized cost of price discovery is at least 10\%
               of the current market cap. Under reasonable assumptions, the
               typical investor would increase his average annual return by 67
               basis points over the 1980--2006 period if he switched to a
               passive market portfolio.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  63,
  number    =  4,
  pages     = "1537--1573",
  month     =  aug,
  year      =  2008
}

@ARTICLE{noauthor_undated-qp,
  title = "{CIEPR2015toWeb.pdf}"
}

@TECHREPORT{noauthor_2016-ew,
  title       = "Algorithmic Trading Brief Note",
  institution = "Federal Reserve Bank of New York",
  month       =  apr,
  year        =  2016
}

@ARTICLE{Dandapani2016-cm,
  title     = "Determinants of Transactional Internet Banking",
  author    = "Dandapani, Krishnan and Lawrence, Edward R and Rodriguez,
               Jodonnis",
  abstract  = "The decision of credit unions in the United States to adopt
               transactional web-based services is consistent with
               profit-maximization behavior. Credit unions adopt transactional
               internet banking services when they provide a higher proportion
               of consumer loans and when there is increased competition from
               other financial institutions. They adopt transactional internet
               banking to attract new customers. The larger the credit union
               the higher the probability of adoption of transactional internet
               banking. The probability of adoption of transactional banking is
               directly related to credit unions' efficiency and indirectly
               related to loan delinquencies. We also find that the probability
               of credit unions offering transactional internet banking is
               positively related to the percentage of the young population in
               the counties where credit unions are located.",
  journal   = "J Financ Serv Res",
  publisher = "Springer US",
  pages     = "1--25",
  month     =  dec,
  year      =  2016,
  language  = "en"
}

@TECHREPORT{Parmeter2016-co,
  title     = "Model Averaging Estimators for the Stochastic Frontier Model",
  author    = "Parmeter, Christopher and Wan, Alan and Zhang, Xinyu and
               {Others}",
  abstract  = "Abstract. Given the concern over the impact of distributional
               assumptions for the stochastic frontier model, the present
               research proposes two distinct model averaging estimators, one
               which can average over distinct distributions for firm specific
               inefficiency and",
  publisher = "bus.miami.edu",
  year      =  2016
}

@ARTICLE{Orea_undated-eh,
  title    = "Using a spatial econometric approach to mitigate omitted
              variables in stochastic frontier models: An application to
              Norwegian electricity distribution networks",
  author   = "Orea, Luis and {\'A}lvarez, Inmaculada C and Jamasb, Tooraj",
  abstract = "Abstract An important methodological issue for the use of
              efficiency analysis in incentive regulation of regulated
              utilities is how to account for the effect of unobserved cost
              drivers such as environmental factors. This study combines the
              spatial econometric",
  journal  = "eprg.group.cam.ac.uk"
}

@ARTICLE{Fukuyama2017-sg,
  title     = "Modelling bank performance: A network {DEA} approach",
  author    = "Fukuyama, Hirofumi and Matousek, Roman",
  abstract  = "In this paper, we develop a bank network revenue function to
               evaluate banks' network revenue performance. The bank network
               revenue function, which extends the environmental revenue
               function and the two-stage network cost function, is constructed
               as the difference between total revenue and the reserves for
               possible loan losses to incorporate the roles played by
               non-performing loans in bank production. The second part of the
               paper then applies Nerlove's revenue inefficiency model. We
               consider revenue maximization in two stages. We apply this
               function to Japanese banks operating from September 2000 to
               March 2013.",
  journal   = "Eur. J. Oper. Res.",
  publisher = "Elsevier",
  volume    =  259,
  number    =  2,
  pages     = "721--732",
  month     =  jun,
  year      =  2017,
  keywords  = "Data envelopment analysis; Bank revenue function; Japanese
               banks; Non-performing loans; Two stage network DEA"
}

@ARTICLE{Baum2001-jj,
  title   = "Stata: The language of choice for time series analysis",
  author  = "Baum, Christopher F and {Others}",
  journal = "Stata J.",
  volume  =  1,
  number  =  1,
  pages   = "1--16",
  year    =  2001
}

@BOOK{Beck2016-ya,
  title     = "The Palgrave Handbook of European Banking",
  author    = "Beck, Thorsten and Casu, Barbara",
  editor    = "Beck, Thorsten and Casu, Barbara",
  abstract  = "This handbook presents a timely collection of original studies
               on relevant themes, policies and developments in European
               banking. The contributors analyse how the crisis years have had
               a long lasting impact on the structure of European banking and
               explore the regulatory architecture that has started to take
               form in their wake. Academic experts and senior policy makers
               have contributed to this volume, which is organized in five main
               parts. The first part presents an overview of European banking
               through the crisis and beyond. The second part analyses
               performance and innovation in EU banking markets. The third part
               discusses the key regulatory changes aimed at fostering
               financial stability. Part four looks at the relevance of
               cross-border banking and part five presents a detailed analysis
               of the main EU banking markets. This is a highly informative and
               carefully presented handbook, which provides thought-provoking
               insights into the past, present and future landscapes of
               European banking. It will appeal to a wide readership, from
               scholars and students, through to researchers, practitioners and
               policy-makers.",
  publisher = "Palgrave Macmillan UK",
  month     =  dec,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Brownlees2017-mc,
  title     = "{SRISK}: A Conditional Capital Shortfall Measure of Systemic
               Risk",
  author    = "Brownlees, Christian and Engle, Robert F",
  abstract  = "We introduce SRISK to measure the systemic risk contribution of
               a financial firm. SRISK measures the capital shortfall of a firm
               conditional on a severe market decline, and is a function of its
               size, leverage and risk. We use the measure to study top
               financial institutions in the recent financial crisis. SRISK
               delivers useful rankings of systemic institutions at various
               stages of the crisis and identifies Fannie Mae, Freddie Mac,
               Morgan Stanley, Bear Stearns, and Lehman Brothers as top
               contributors as early as 2005-Q1. Moreover, aggregate SRISK
               provides early warning signals of distress in indicators of real
               activity.Received June 7, 2011; accepted April 18, 2016 by
               Editor Geert Bekaert.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  30,
  number    =  1,
  pages     = "48--79",
  month     =  jan,
  year      =  2017
}

@ARTICLE{Pildes1991-nt,
  title     = "The Unintended Cultural Consequences of Public Policy: A Comment
               on the Symposium",
  author    = "Pildes, Richard H",
  journal   = "Mich. Law Rev.",
  publisher = "The Michigan Law Review Association",
  volume    =  89,
  number    =  4,
  pages     = "936--978",
  year      =  1991
}

@ARTICLE{Pigou1932-lf,
  title   = "The economics of welfare, 1920",
  author  = "Pigou, Arthur C",
  journal = "McMillan\&Co. , London",
  year    =  1932
}

@ARTICLE{Burde2017-vr,
  title     = "Prediction of financial vulnerability to funding instability",
  author    = "Burde, Gila and Rosenfeld, Ahron and Sheaffer, Zachary",
  journal   = "Nonprofit and Voluntary Sector Quarterly",
  publisher = "SAGE Publications Sage CA: Los Angeles, CA",
  volume    =  46,
  number    =  2,
  pages     = "280--304",
  year      =  2017
}

@ARTICLE{Taffler1983-aq,
  title     = "The Assessment of Company Solvency and Performance Using a
               Statistical Model",
  author    = "Taffler, R J",
  journal   = "Accounting and Business Research",
  publisher = "Routledge",
  volume    =  13,
  number    =  52,
  pages     = "295--308",
  month     =  sep,
  year      =  1983
}

@ARTICLE{Fama1998-lo,
  title     = "Market efficiency, long-term returns, and behavioral finance1",
  author    = "Fama, Eugene F",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  49,
  number    =  3,
  pages     = "283--306",
  year      =  1998
}

@ARTICLE{Edwards1977-zy,
  title     = "Managerial Objectives in Regulated Industries:
               {Expense-Preference} Behavior in Banking",
  author    = "Edwards, Franklin R",
  abstract  = "Recent work on the theory of the firm under regulation suggests
               that managers of regulated firms may be utility maximizers
               rather than profit maximizers. There is, however, very little
               empirical evidence on managerial behavior in regulated
               industries. This article examines one kind of utility-maximizing
               behavior that seems particularly applicable to regulated firms:
               expense-preference behavior. Specifically, I develop a test
               capable of discriminating between expense-preference and
               profit-maximizing behavior and apply it to the banking industry,
               a highly regulated industry. My findings indicate that an
               expense-preference theoretical framework better explains the
               behavior of regulated firms than does a profit-maximization
               framework.",
  journal   = "J. Polit. Econ.",
  publisher = "journals.uchicago.edu",
  volume    =  85,
  number    =  1,
  pages     = "147--162",
  year      =  1977
}

@ARTICLE{Purroy2000-mg,
  title     = "Strategic competition in retail banking under expense preference
               behavior",
  author    = "Purroy, Pedro and Salas, Vicente",
  abstract  = "This paper presents a theoretical analysis of strategic
               competition in retail banking when some of the firms show
               expense preference behavior. The literature on expense
               preference behavior by banking firms is fairly large, but to our
               knowledge, so far, the strategic interaction between profit
               maximizing banks and banks with expense preference behavior has
               not been investigated. The paper has also an empirical interest
               since it is motivated by the current situation in Spanish retail
               banking where commercial banks, profit maximizers, compete with
               savings banks, non profit maximizers.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  24,
  number    =  5,
  pages     = "809--824",
  month     =  may,
  year      =  2000,
  keywords  = "Expense preferences; Strategic competition; Spanish banking"
}

@ARTICLE{Cook2014-qn,
  title    = "Data envelopment analysis: Prior to choosing a model",
  author   = "Cook, Wade D and Tone, Kaoru and Zhu, Joe",
  abstract = "In this paper, we address several issues related to the use of
              data envelopment analysis (DEA). These issues include model
              orientation, input and output selection/definition, the use of
              mixed and raw data, and the number of inputs and outputs to use
              versus the number of decision making units (DMUs). We believe
              that within the DEA community, researchers, practitioners, and
              reviewers may have concerns and, in many cases, incorrect views
              about these issues. Some of the concerns stem from what is
              perceived as being the purpose of the DEA exercise. While the DEA
              frontier can rightly be viewed as a production frontier, it must
              be remembered that ultimately DEA is a method for performance
              evaluation and benchmarking against best-practice. DEA can be
              viewed as a tool for multiple-criteria evaluation problems where
              DMUs are alternatives and each DMU is represented by its
              performance in multiple criteria which are coined/classified as
              DEA inputs and outputs. The purpose of this paper is to offer
              some clarification and direction on these matters.",
  journal  = "Omega",
  volume   =  44,
  pages    = "1--4",
  month    =  apr,
  year     =  2014,
  keywords = "Data envelopment analysis (DEA); Efficiency; Input; Output; Ratio"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Boyd1993-sk,
  title     = "Bank holding company mergers with nonbank financial firms:
               Effects on the risk of failure",
  author    = "Boyd, John H and Graham, Stanley L and Hewitt, R Shawn",
  abstract  = "An important issue in the debate over whether bank holding
               companies (BHCs) should be permitted to enter nonbanking
               activities is the effect of expand‚Ä¶",
  journal   = "Journal of Banking \& Finance",
  publisher = "North-Holland",
  volume    =  17,
  number    =  1,
  pages     = "43--63",
  month     =  feb,
  year      =  1993
}

@ARTICLE{noauthor_undated-zh,
  title = "[{PDF]History} of {Value-at-Risk}: 1922-1998 - Wharton Statistics"
}

@ARTICLE{Roy1952-jk,
  title     = "Safety First and the Holding of Assets",
  author    = "Roy, A D",
  journal   = "Econometrica",
  publisher = "[Wiley, Econometric Society]",
  volume    =  20,
  number    =  3,
  pages     = "431--449",
  year      =  1952
}

@ARTICLE{Boyd1986-vt,
  title     = "Risk, regulation, and bank holding company expansion into
               nonbanking",
  author    = "Boyd, John H and Graham, Stanley L",
  abstract  = "When banking institutions can expand into other lines of
               business, some think they will diversify to reduce their total
               risk. Others think just the opposite. In this article, John H.
               Boyd and Stanley L. Graham explain the reasoning behind these
               two views and then test to see which one best describes the
               behavior of U.S. bank holding companies since 1970. They find
               that in 1971-77, when these companies were relatively free to
               invest in some new lines of business, diversification was
               associated with greater risk of failure. But in 1977-83, when
               the companies were more tightly regulated, that association
               disappeared. These findings suggest to Boyd and Graham that,
               left to their own devices, bank holding companies will expand
               into new lines of business to increase their risk, but that
               regulation can control this risk-taking -- at least for the
               lines of business bank holding companies are currently allowed.",
  journal   = "Q. Rev. DC Nurses. Assoc.",
  publisher = "Federal Reserve Bank of Minneapolis",
  volume    =  10,
  number    = "Spr",
  pages     = "2--17",
  year      =  1986,
  keywords  = "Bank holding companies ; Bank investments ; Risk"
}

@ARTICLE{Yeyati2007-sd,
  title    = "Concentration and foreign penetration in Latin American banking
              sectors: Impact on competition and risk",
  author   = "Yeyati, Eduardo Levy and Micco, Alejandro",
  abstract = "In the 1990s, Latin American banking sectors experienced an
              accelerated process of concentration and foreign penetration that
              prompted diverse views regarding its implications for the
              competitive behavior of banks and the financial stability of the
              system. In this paper, we examine these issues exploiting a rich
              bank-level database for eight Latin American countries. We find
              that, while increased concentration did not weaken banking
              competition within the region, foreign penetration appears to
              have led to a less competitive industry. Moreover, we find that
              bank risk has been negatively associated with competition which,
              coupled with the previous finding, explains the positive link
              between banking sector stability and foreign penetration revealed
              by the data.",
  journal  = "Journal of Banking \& Finance",
  volume   =  31,
  number   =  6,
  pages    = "1633--1647",
  month    =  jun,
  year     =  2007,
  keywords = "Banking; Finance; Concentration; Competition"
}

@TECHREPORT{Hughes2017-xg,
  title  = "Capital Regulation: Less Really Can Be More When Incentives Are
            Socially Aligned",
  author = "Hughes, Joseph P and {Others}",
  year   =  2017
}

@ARTICLE{Herring2017-uu,
  title   = "The Evolving Complexity of Capital Regulation",
  author  = "{Herring}",
  journal = "Slides",
  year    =  2017
}

@ARTICLE{Eisenbeis2015-nc,
  title     = "Playing for time: the Fed's attempt to mange the crisis as a
               liquidity problem",
  author    = "Eisenbeis, Robert A and Herring, Richard J",
  journal   = "Journal of Financial Economic Policy; Bingley",
  publisher = "Emerald Group Publishing Limited",
  volume    =  7,
  number    =  1,
  pages     = "68--88",
  year      =  2015,
  address   = "Bingley, United Kingdom",
  language  = "en"
}

@ARTICLE{Hong2000-hz,
  title     = "Bad News Travels Slowly: Size, Analyst Coverage, and the
               Profitability of Momentum Strategies",
  author    = "Hong, Harrison and Lim, Terence and Stein, Jeremy C",
  abstract  = "Various theories have been proposed to explain momentum in stock
               returns. We test the gradual-information-diffusion model of Hong
               and Stein (1999) and establish three key results. First, once
               one moves past the very smallest stocks, the profitability of
               momentum strategies declines sharply with firm size. Second,
               holding size fixed, momentum strategies work better among stocks
               with low analyst coverage. Finally, the effect of analyst
               coverage is greater for stocks that are past losers than for
               past winners. These findings are consistent with the hypothesis
               that firm-specific information, especially negative information,
               diffuses only gradually across the investing public.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishers, Inc.",
  volume    =  55,
  number    =  1,
  pages     = "265--295",
  month     =  feb,
  year      =  2000
}

@ARTICLE{Gencay1998-ia,
  title     = "The predictability of security returns with simple technical
               trading rules",
  author    = "Gen{\c c}ay, Ramazan",
  abstract  = "Abstract Technical traders base their analysis on the premise
               that the patterns in market prices are assumed to recur in the
               future, and thus, these patterns can be used for predictive
               purposes. This paper uses the daily Dow Jones Industrial Average
               Index from 1897 to 1988 to examine the linear and nonlinear
               predictability of stock market returns with simple technical
               trading rules. The nonlinear specification of returns are
               modelled by single layer feedforward networks. The results
               indicate strong evidence of nonlinear predictability in the
               stock market returns by using the past buy and sell signals of
               the moving average rules.",
  journal   = "Journal of Empirical Finance",
  publisher = "Elsevier",
  volume    =  5,
  number    =  4,
  pages     = "347--359",
  month     =  oct,
  year      =  1998,
  keywords  = "Market efficiency; Technical trading rules; Feedforward networks"
}

@ARTICLE{Cox2006-nm,
  title     = "Speaking Stata: In praise of trigonometric predictors",
  author    = "Cox, Nicholas J",
  abstract  = "Using sine and cosine terms as predictors in modeling periodic
               time series and other kinds of periodic responses is a
               long-established technique, but it is often overlooked in many
               courses or textbooks. Such trigonometric regression is
               straightforward in Stata through applications of existing
               commands. I give various examples using classic periodic
               datasets on the motion of the asteroid Pallas and the daily
               rhythm of birth numbers. I make a brief connection to
               polynomial-trigonometric regression.",
  journal   = "Stata J.",
  publisher = "StataCorp LP",
  volume    =  6,
  number    =  4,
  pages     = "561--579",
  year      =  2006,
  keywords  = "circular regression; Fourier regression; harmonic regression;
               periodic regression; polynomial-trigonometric regression;
               trigonometric regression; sine; cosine"
}

@ARTICLE{noauthor_2017-xd,
  title    = "European banks' technical efficiency and performance: do business
              models matter? The case of European co-operatives banks",
  abstract = "European banks' technical efficiency and performance: do business
              models matter? The case of European co-operatives banks",
  journal  = "Bank of France Working Paper",
  year     =  2017
}

@ARTICLE{McKillop2017-ac,
  title    = "Irish credit unions: Differential regulation based on business
              model complexity",
  author   = "McKillop, Donal G and Quinn, Barry",
  abstract = "This study examines the business model complexity of Irish credit
              unions using a latent class approach to measure structural
              performance over the period 2002 to 2013. The latent class
              approach allows the endogenous identification of a multi-class
              framework for business models based on credit union specific
              characteristics. The analysis finds a three class system to be
              appropriate with the multi-class model dependent on three
              financial viability characteristics. This finding is consistent
              with the deliberations of the Irish Commission on Credit Unions
              (2012) which identified complexity and diversity in the business
              models of Irish credit unions and recommended that such
              complexity and diversity could not be accommodated within a one
              size fits all regulatory framework. The analysis also highlights
              that two of the classes are subject to diseconomies of scale.
              This may suggest credit unions would benefit from a reduction in
              scale or perhaps that there is an imbalance in the present change
              process. Finally, relative performance differences are identified
              for each class in terms of technical efficiency. This suggests
              that there is an opportunity for credit unions to improve their
              performance by using within-class best practice or alternatively
              by switching to another class.",
  journal  = "The British Accounting Review",
  volume   =  49,
  number   =  2,
  pages    = "230--241",
  month    =  mar,
  year     =  2017,
  keywords = "Credit union; Regulation; Business model; Structural performance;
              Frontier efficiency; Latent class"
}

@ARTICLE{Jain2005-fl,
  title     = "Financial Market Design and the Equity Premium: Electronic
               versus Floor Trading",
  author    = "Jain, Pankaj K",
  abstract  = "We assemble the announcement and actual introduction dates of
               electronic trading by the leading exchanges of 120 countries to
               examine the impact of automation, controlling for risk factors
               and economic conditions. Dividend growth models and
               international CAPM suggest a significant decline in the equity
               premium, especially in emerging markets. Consistent with this
               reduction in the equity premium in the long run, there is a
               positive short-term price reaction to the switch. Further
               analysis of trading turnover supports the notion that electronic
               trading enhances the liquidity and informativeness of stock
               markets, leading to a reduction in the cost of capital.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing, Inc.",
  volume    =  60,
  number    =  6,
  pages     = "2955--2985",
  month     =  dec,
  year      =  2005
}

@ARTICLE{Hendershott2011-ek,
  title     = "Automation, speed, and stock market quality: The {NYSE's} Hybrid",
  author    = "Hendershott, Terrence and Moulton, Pamela C",
  abstract  = "Automation and trading speed are increasingly important aspects
               of competition among financial markets. Yet we know little about
               how changing a market's automation and speed affects the cost of
               immediacy and price discovery, two key dimensions of market
               quality. At the end of 2006 the New York Stock Exchange
               introduced its Hybrid Market, increasing automation and reducing
               the execution time for market orders from 10 seconds to less
               than one second. We find that the change raises the cost of
               immediacy (bid-ask spreads) because of increased adverse
               selection and reduces the noise in prices, making prices more
               efficient.",
  journal   = "Journal of Financial Markets",
  publisher = "Elsevier",
  volume    =  14,
  number    =  4,
  pages     = "568--604",
  month     =  nov,
  year      =  2011,
  keywords  = "Automation; Liquidity; Speed; Transparency; Latency; Adverse
               selection"
}

@MISC{Jeffries2012-kw,
  title        = "{High-Frequency} Trading Approaches the Speed of Light",
  booktitle    = "Observer",
  author       = "Jeffries, Adrianne",
  abstract     = "The finance industry is in a race against science being waged
                  with transatlantic cables and ever-tinier chips. A new paper
                  authored by a team of physicists, engineers and industry data
                  experts explores what Physics of Finance blogger Mark
                  Buchanan calls the ``approaching singularity.",
  month        =  feb,
  year         =  2012,
  howpublished = "\url{http://observer.com/2012/02/high-frequency-trading-approaches-the-speed-of-light/}",
  note         = "Accessed: 2017-3-19"
}

@MISC{Pesaran2004-il,
  title   = "How costly is it to ignore breaks when forecasting the direction
             of a time series?",
  author  = "Pesaran, M Hashem and Hashem Pesaran, M and Timmermann, Allan",
  journal = "International Journal of Forecasting",
  volume  =  20,
  number  =  3,
  pages   = "411--425",
  year    =  2004
}

@ARTICLE{Shleifer1990-if,
  title     = "The Noise Trader Approach to Finance",
  author    = "Shleifer, Andrei and Summers, Lawrence H",
  abstract  = "I f the efficient markets hypothesis was a publicly traded
               security, its price would be enormously volatile. Following
               Samuelson's (1965) proof that stock prices should follow a
               random walk if rational competitive investors require a fixed
               rate of return and Fama's (1965)",
  journal   = "J. Econ. Perspect.",
  publisher = "American Economic Association",
  volume    =  4,
  number    =  2,
  pages     = "19--33",
  year      =  1990
}

@ARTICLE{Johnson2012-ia,
  title         = "Financial black swans driven by ultrafast machine ecology",
  author        = "Johnson, Neil and Zhao, Guannan and Hunsader, Eric and Meng,
                   Jing and Ravindar, Amith and Carran, Spencer and Tivnan,
                   Brian",
  abstract      = "Society's drive toward ever faster socio-technical systems,
                   means that there is an urgent need to understand the threat
                   from 'black swan' extreme events that might emerge. On 6 May
                   2010, it took just five minutes for a spontaneous mix of
                   human and machine interactions in the global trading
                   cyberspace to generate an unprecedented system-wide Flash
                   Crash. However, little is known about what lies ahead in the
                   crucial sub-second regime where humans become unable to
                   respond or intervene sufficiently quickly. Here we analyze a
                   set of 18,520 ultrafast black swan events that we have
                   uncovered in stock-price movements between 2006 and 2011. We
                   provide empirical evidence for, and an accompanying theory
                   of, an abrupt system-wide transition from a mixed
                   human-machine phase to a new all-machine phase characterized
                   by frequent black swan events with ultrafast durations
                   (<650ms for crashes, <950ms for spikes). Our theory
                   quantifies the systemic fluctuations in these two distinct
                   phases in terms of the diversity of the system's internal
                   ecology and the amount of global information being
                   processed. Our finding that the ten most susceptible
                   entities are major international banks, hints at a hidden
                   relationship between these ultrafast 'fractures' and the
                   slow 'breaking' of the global financial system post-2006.
                   More generally, our work provides tools to help predict and
                   mitigate the systemic risk developing in any complex
                   socio-technical system that attempts to operate at, or
                   beyond, the limits of human response times.",
  month         =  feb,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "physics.soc-ph",
  eprint        = "1202.1448"
}

@ARTICLE{Black1971-wu,
  title     = "Toward a Fully Automated Stock Exchange, Part {II}",
  author    = "Black, Fischer",
  abstract  = "... Toward a Fully Automated Stock Exchange , Part II. Fischer
               Black . DOI: http://dx.doi.org/10.2469/ faj.v27.n6.24. First
               Page; PDF; Cited by. First Page Image. Your Access Options. CFA
               Institute members. Access the content you requested. (May
               require logging in if you have not already done",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  27,
  number    =  6,
  pages     = "24--28",
  month     =  nov,
  year      =  1971
}

@ARTICLE{Black1971-kl,
  title     = "Toward a Fully Automated Stock Exchange, Part {I}",
  author    = "Black, Fischer",
  abstract  = "... July/August 1971, Volume 27 Issue 4 Previous Next. Toward a
               Fully Automated Stock Exchange , Part I. Fischer Black . DOI:
               http://dx.doi.org/10.2469/faj.v27.n4.28. First Page; PDF; Cited
               by. First Page Image. Users who read this article also read.
               Toward a Fully Automated Stock Exchange ,",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  27,
  number    =  4,
  pages     = "28--35",
  month     =  jul,
  year      =  1971
}

@ARTICLE{Fujii2017-cr,
  title     = "Bank efficiency, productivity, and convergence in {EU}
               countries: a weighted Russell directional distance model",
  author    = "Fujii, Hidemichi and Managi, Shunsuke and Matousek, Roman and
               Rughoo, Aarti",
  abstract  = "The objective of this study is three-fold. First we estimate and
               analyse bank efficiency and productivity changes in the EU28
               countries with the application of a novel approach, a weighted
               Russell directional distance model. Second, we take a
               disaggregated approach and analyse the contribution of the
               individual bank inputs on bank efficiency and productivity
               growth. Third, we test for convergence in EU28 bank productivity
               as well as in the inefficiency of individual bank inputs. We
               find that bank efficiency has been undermined by the financial
               crisis in banks notably from the EU15 countries. We also argue
               that bank efficiency and productivity in EU countries vary
               across the banking sector with banks from the ?old? EU showing
               higher efficiency levels. Nonetheless, a noticeable catching up
               process is observed for banks from the ?new? EU countries.
               Consequently, we do not find evidence of group convergence for
               bank productivity but there is evidence of convergence in bank
               efficiency change and technical change among the EU28 countries
               throughout the period 2005?2014. The driving force seems to be
               convergent technical change from the old EU member states?
               banks. On the other hand, almost no convergence is detected for
               the banks? individual inputs while the transition paths show
               heightened diversity during the crisis years.",
  journal   = "The European Journal of Finance",
  publisher = "Routledge",
  pages     = "1--25",
  month     =  mar,
  year      =  2017
}

@TECHREPORT{Coyne2014-ag,
  title       = "Unintended Consequences: How regulations affects behaviour",
  author      = "Coyne, Christopher",
  institution = "Fraser Institute",
  year        =  2014
}

@ARTICLE{Buehler2019-pi,
  title     = "Deep hedging",
  author    = "Buehler, H and Gonon, L and Teichmann, J and Wood, B",
  abstract  = "We present a framework for hedging a portfolio of derivatives in
               the presence of market frictions such as transaction costs,
               liquidity constraints or risk limits using modern deep
               reinforcement machine learning methods. We discuss how standard
               reinforcement learning methods can be applied to non-linear
               reward structures, i.e. in our case convex risk measures. As a
               general contribution to the use of deep learning for stochastic
               processes, we also show in Section 4 that the set of constrained
               trading strategies used by our algorithm is large enough to
               $\epsilon$-approximate any optimal solution. Our algorithm can
               be implemented efficiently even in high-dimensional situations
               using modern machine learning tools. Its structure does not
               depend on specific market dynamics, and generalizes across
               hedging instruments including the use of liquid derivatives. Its
               computational performance is largely invariant in the size of
               the portfolio as it depends mainly on the number of hedging
               instruments available. We illustrate our approach by an
               experiment on the S\&P500 index and by showing the effect on
               hedging under transaction costs in a synthetic market driven by
               the Heston model, where we outperform the standard
               ?complete-market? solution.",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  pages     = "1--21",
  month     =  feb,
  year      =  2019
}

@ARTICLE{Gofman2017-aw,
  title    = "Efficiency and stability of a financial architecture with
              too-interconnected-to-fail institutions",
  author   = "Gofman, Michael",
  abstract = "The regulation of large interconnected financial institutions has
              become a key policy issue. To improve financial stability,
              regulators have proposed limiting banks' size and
              interconnectedness. I estimate a network-based model of the
              over-the-counter interbank lending market in the US and quantify
              the efficiency-stability implications of this policy. Trading
              efficiency decreases with limits on interconnectedness because
              the intermediation chains become longer. While restricting the
              interconnectedness of banks improves stability, the effect is
              non-monotonic. Stability also improves with higher liquidity
              requirements, when banks have access to liquidity during the
              crisis, and when failed banks' depositors maintain confidence in
              the banking system.",
  journal  = "J. financ. econ.",
  volume   =  124,
  number   =  1,
  pages    = "113--146",
  month    =  apr,
  year     =  2017,
  keywords = "Financial regulation; Networks; Trading efficiency; Contagion
              risk; Federal funds market"
}

@ARTICLE{Graefe2015-cd,
  title    = "Limitations of Ensemble Bayesian Model Averaging for forecasting
              social science problems",
  author   = "Graefe, Andreas and K{\"u}chenhoff, Helmut and Stierle, Veronika
              and Riedl, Bernhard",
  abstract = "We compare the accuracies of simple unweighted averages and
              Ensemble Bayesian Model Averaging (EBMA) for combining forecasts
              in the social sciences. A review of prior studies from the domain
              of economic forecasting finds that the simple average was more
              accurate than EBMA in four studies out of five. On average, the
              error of EBMA was 5\% higher than that of the simple average. A
              reanalysis and extension of a published study provides further
              evidence for US presidential election forecasting. The error of
              EBMA was 33\% higher than the corresponding error of the simple
              average. Simple averages are easy both to describe and to
              understand, and thus are easy to use. In addition, simple
              averages provide accurate forecasts in many settings. Researchers
              who are developing new approaches to combining forecasts need to
              compare the accuracy of their method to this widely established
              benchmark. Forecasting practitioners should favor simple averages
              over more complex methods unless there is strong evidence in
              support of differential weights.",
  journal  = "Int. J. Forecast.",
  volume   =  31,
  number   =  3,
  pages    = "943--951",
  month    =  jul,
  year     =  2015,
  keywords = "Bayesian analysis; Combining forecasts; Economic forecasting;
              Election forecasting; Equal weights"
}

@ARTICLE{Ferri2016-qk,
  title     = "Bank regulatory arbitrage via risk weighted assets dispersion",
  author    = "Ferri, Giovanni and Pesic, Valerio",
  abstract  = "Increased dispersion of Risk Weighted Assets (RWA) troubles
               regulators as potentially undermining prudential supervision. We
               study the determinants of RWA/EAD (Exposure-At-Default) on data
               painstakingly compiled from Basel Pillar-Three for 239 European
               banks over 2007--2013. We improve on most previous studies,
               which consider instead RWA/TA (Total Assets). Indeed,
               Internal-Rating-Based (IRB) models allow lawful capital-saving
               Roll-Out effects which RWA/TA analyses disregard and likely
               misidentify as regulatory arbitrage. Instead, encapsulating
               Roll-Out effects, RWA/EAD avoids false positive identification.
               We find that regulatory arbitrage: (i) was present; (ii) likely
               materialized via risk weights manipulation with IRB models;
               (iii) was stronger at Advanced-IRB vs Foundation-IRB banks.",
  journal   = "Journal of Financial Stability",
  publisher = "Elsevier",
  month     =  oct,
  year      =  2016,
  keywords  = "Regulatory arbitrage; Internal rating based models; Risk
               weighted assets dispersion"
}

@ARTICLE{De_Bandt2017-ty,
  title     = "Optimal capital, regulatory requirements and bank performance in
               times of crisis: Evidence from France",
  author    = "de Bandt, Olivier and Camara, Boubacar and Maitre, Alexis and
               Pessarossi, Pierre",
  abstract  = "The recent implementation of the Basel III framework has
               re-ignited the debate around the link between actual capital
               levels, performance and capital requirements in the banking
               sector. There is a dominant view in the earlier empirical
               literature in favor of a positive effect of capital on
               performance. Using panel data gathered by the French supervisor,
               we also find evidence of this beneficial effect of capital, but
               try to go one step further by distinguishing between regulatory
               and voluntary capital. Using a two-step estimation procedure,
               and controlling for many factors (risk, asset composition,
               etc.), we show that voluntary capital, i.e. capital held by
               banks irrespective of their regulatory requirements, turns out
               to be the sole component of capital that affects performance
               positively. In contrast, the effect of regulatory capital on
               profitability appears to be insignificant, indicating that so
               far the increase in capital requirements has not been
               detrimental to bank profitability in France.",
  journal   = "Journal of Financial Stability",
  publisher = "Elsevier",
  month     =  mar,
  year      =  2017,
  keywords  = "Bank capital; Performance; ROA; Capital requirements; Financial
               crisis"
}

@ARTICLE{Hannan1988-oi,
  title     = "Bank Insolvency Risk and the Market for Large Certificates of
               Deposit",
  author    = "Hannan, Timothy H and Hanweck, Gerald A",
  journal   = "J. Money Credit Bank.",
  publisher = "[Wiley, Ohio State University Press]",
  volume    =  20,
  number    =  2,
  pages     = "203--211",
  year      =  1988
}

@ARTICLE{Mendoza2010-pl,
  title    = "Sudden Stops, Financial Crises, and Leverage",
  author   = "Mendoza, Enrique G",
  journal  = "Am. Econ. Rev.",
  volume   =  100,
  number   =  5,
  pages    = "1941--1966",
  month    =  dec,
  year     =  2010
}

@ARTICLE{Jarque2015-mx,
  title     = "Understanding Living Wills",
  author    = "Jarque, Arantxa and Athreya, Kartik",
  abstract  = "Abstract [...] regulators need to exercise care that (i) the
               company does invest the necessary resources to produce a
               truthful and useful living will, and (ii) that the costs of the
               changes and the resources necessary to crafta good living will
               do not wipe out the ...",
  journal   = "Economic Quarterly - Federal Reserve Bank of Richmond; Richmond",
  publisher = "Federal Reserve Bank of Richmond",
  volume    =  101,
  number    =  3,
  pages     = "193--223",
  year      =  2015,
  address   = "United States--US, United States",
  language  = "en"
}

@INCOLLECTION{Brunnermeier2016-ns,
  title     = "bubbles",
  booktitle = "Banking Crises",
  author    = "Brunnermeier, Markus K",
  editor    = "Jones, Garett",
  abstract  = "Bubbles are typically associated with dramatic asset price
               increases followed by a collapse. Bubbles arise if the price
               exceeds the asset's fundamental value. This can occur if
               investors hold the asset because they believe that they can sell
               it at a higher price than some other investor even though the
               asset's price exceeds its fundamental value. Famous historical
               examples are the Dutch tulip mania (1634--7), the Mississippi
               Bubble (1719--20), the South Sea Bubble (1720), and the `Roaring
               `20s' that preceded the 1929 crash. More recently, up to March
               2000 Internet share prices (CBOE Internet Index) surged to
               astronomical heights before plummeting by more than 75 per cent
               by the end of 2000.",
  publisher = "Palgrave Macmillan, London",
  pages     = "28--36",
  year      =  2016,
  language  = "en"
}

@INCOLLECTION{Gordon2015-tf,
  title     = "Unfinished Agenda of Structural Reform",
  booktitle = "Bank Resolution in Europe",
  author    = "Gordon, J N and Ringe, G",
  abstract  = "... Although structural reform in the EU could be achieved by
               supervisory implementation of the `` living wills '' requirement
               for effective resolution or ... Columbia University, New York
               Language: English Published in: European Banking Union, 2015, p.
               500-523 Editor: ...",
  publisher = "forskningsdatabasen.dk",
  year      =  2015
}

@ARTICLE{Agarwal2014-ek,
  title    = "Inconsistent Regulators: Evidence from Banking",
  author   = "Agarwal, Sumit and Lucca, David and Seru, Amit and Trebbi,
              Francesco",
  abstract = "We find that regulators can implement identical rules
              inconsistently due to differences in their institutional design
              and incentives, and this behavior may adversely impact the
              effectiveness with which regulation is implemented. We study
              supervisory decisions of U.S. banking regulators and exploit a
              legally determined rotation policy that assigns federal and state
              supervisors to the same bank at exogenously set time intervals.
              Comparing federal and state regulator supervisory ratings within
              the same bank, we find that federal regulators are systematically
              tougher, downgrading supervisory ratings almost twice as
              frequently as do state supervisors. State regulators counteract
              these downgrades to some degree by upgrading more frequently.
              Under federal regulators, banks report worse asset quality,
              higher regulatory capital ratios, and lower return on assets.
              Leniency of state regulators relative to their federal
              counterparts is related to costly outcomes, such as higher
              failure rates and lower repayment rates of government assistance
              funds. The discrepancy in regulator behavior is related to
              different weights given by regulators to local economic
              conditions and, to some extent, differences in regulatory
              resources. We find no support for regulator self-interest, which
              includes ``revolving doors'' as a reason for leniency of state
              regulators.",
  journal  = "Q. J. Econ.",
  volume   =  129,
  number   =  2,
  pages    = "889--938",
  month    =  may,
  year     =  2014
}

@ARTICLE{Bernal2014-pm,
  title    = "Assessing the contribution of banks, insurance and other
              financial services to systemic risk",
  author   = "Bernal, Oscar and Gnabo, Jean-Yves and Guilmin, Gr{\'e}gory",
  abstract = "The aim of this paper is to contribute to the debate on systemic
              risk by assessing the extent to which distress within the main
              different financial sectors, namely, the banking, insurance and
              other financial services industries contribute to systemic risk.
              To this end, we rely on the $\Delta$CoVaR systemic risk measure
              introduced by Adrian and Brunnermeier (2011). In order to provide
              a formal ranking of the financial sectors with respect to their
              contribution to systemic risk, the original $\Delta$CoVaR
              approach is extended here to include the Kolmogorov--Smirnov test
              developed by Abadie (2002), based on bootstrapping. Our empirical
              results reveal that in the Eurozone, for the period ranging from
              2004 to 2012, the other financial services sector contributes
              relatively the most to systemic risk at times of distress
              affecting this sector. In turn, the banking sector appears to
              contribute more to systemic risk than the insurance sector. By
              contrast, the insurance industry is the systemically riskiest
              financial sector in the United States for the same period, while
              the banking sector contributes the least to systemic risk in this
              area. Beyond this ranking, the three financial sectors of
              interest are found to contribute significantly to systemic risk,
              both in the Eurozone and in the United States.",
  journal  = "Journal of Banking \& Finance",
  volume   =  47,
  pages    = "270--287",
  month    =  oct,
  year     =  2014,
  keywords = "Systemic risk; CoVaR; Quantile regression; Stochastic dominance
              test"
}

@ARTICLE{Oet2013-zo,
  title    = "{SAFE}: An early warning system for systemic banking risk",
  author   = "Oet, Mikhail V and Bianco, Timothy and Gramlich, Dieter and Ong,
              Stephen J",
  abstract = "This paper builds on existing microprudential and macroprudential
              early warning systems (EWSs) to develop a new, hybrid class of
              models for systemic risk that incorporates the structural
              characteristics of the financial system and a feedback
              amplification mechanism. The models explain financial stress
              using both public and proprietary supervisory data from
              systemically important institutions, regressing institutional
              imbalances using an optimal lag method. The Systemic Assessment
              of Financial Environment (SAFE) EWS monitors microprudential
              information from the largest bank holding companies to anticipate
              the buildup of macroeconomic stresses in the financial markets.
              To mitigate inherent uncertainty, SAFE develops a set of
              medium-term forecasting specifications that gives policymakers
              enough time to take ex-ante policy action and a set of short-term
              forecasting specifications for verification and adjustment of
              supervisory actions. This paper highlights the application of
              these models to stress testing and policy.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  11,
  pages    = "4510--4533",
  month    =  nov,
  year     =  2013,
  keywords = "Systemic risk; Early warning system; Financial stress index;
              Microprudential; Macroprudential; Liquidity feedback"
}

@ARTICLE{Mollick2014-hr,
  title    = "The dynamics of crowdfunding: An exploratory study",
  author   = "Mollick, Ethan",
  abstract = "Crowdfunding allows founders of for-profit, artistic, and
              cultural ventures to fund their efforts by drawing on relatively
              small contributions from a relatively large number of individuals
              using the internet, without standard financial intermediaries.
              Drawing on a dataset of over 48,500 projects with combined
              funding over \$237M, this paper offers a description of the
              underlying dynamics of success and failure among crowdfunded
              ventures. It suggests that personal networks and underlying
              project quality are associated with the success of crowdfunding
              efforts, and that geography is related to both the type of
              projects proposed and successful fundraising. Finally, I find
              that the vast majority of founders seem to fulfill their
              obligations to funders, but that over 75\% deliver products later
              than expected, with the degree of delay predicted by the level
              and amount of funding a project receives. These results offer
              insight into the emerging phenomenon of crowdfunding, and also
              shed light more generally on the ways that the actions of
              founders may affect their ability to receive entrepreneurial
              financing.",
  journal  = "J. Bus. Venturing",
  volume   =  29,
  number   =  1,
  pages    = "1--16",
  month    =  jan,
  year     =  2014,
  keywords = "Crowdfunding; New ventures; Entrepreneurial finance; Startups;
              Geography"
}

@BOOK{Malkiel1999-ar,
  title     = "A random walk down Wall Street: including a life-cycle guide to
               personal investing",
  author    = "Malkiel, Burton Gordon",
  publisher = "WW Norton \& Company",
  year      =  1999
}

@UNPUBLISHED{Goncharenko2017-az,
  title  = "The Dark Side of Stress {Test:Negative} Effects of Information
            Disclosure",
  author = "{Goncharenko} and {Hledik} and {Pinto}",
  year   =  2017
}

@BOOK{Friedman2009-po,
  title     = "The elements of statistical learning",
  author    = "Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert",
  publisher = "Springer series in statistics New York, NY, USA:",
  volume    =  2,
  year      =  2009
}

@ARTICLE{Bell2015-am,
  title     = "Explaining Fixed Effects: Random Effects Modeling of
               {Time-Series} {Cross-Sectional} and Panel Data*",
  author    = "Bell, Andrew and Jones, Kelvyn",
  abstract  = "Explaining Fixed Effects: Random Effects Modeling of Time-Series
               Cross-Sectional and Panel Data* - Volume 3 Issue 1 - Andrew
               Bell, Kelvyn Jones",
  journal   = "Political Science Research and Methods",
  publisher = "Cambridge University Press",
  volume    =  3,
  number    =  1,
  pages     = "133--153",
  month     =  jan,
  year      =  2015
}

@ARTICLE{Strebulaev2012-lx,
  title     = "Dynamic Models and Structural Estimation in Corporate Finance",
  author    = "Strebulaev, Ilya A and Whited, Toni M",
  abstract  = "Abstract We review the last two decades of research in dynamic
               corporate finance, focusing on capital structure and the
               financing of investment. We first cover continuous time
               contingent claims models, starting with real options models, and
               working through static and",
  journal   = "Foundations and Trends\textregistered{} in Finance",
  publisher = "Now Publishers",
  volume    =  6,
  number    = "1--2",
  pages     = "1--163",
  year      =  2012,
  keywords  = "Corporate finance"
}

@ARTICLE{Han2014-qg,
  title     = "X-differencing and dynamic panel model estimation",
  author    = "Han, Chirok and Phillips, Peter C B and Sul, Donggyu",
  abstract  = "Abstract This paper introduces a new estimation method for
               dynamic panel models with fixed effects and AR (p) idiosyncratic
               errors. The proposed estimator uses a novel form of systematic
               differencing, called X-differencing, that eliminates fixed
               effects and retains",
  journal   = "Econometric Theory",
  publisher = "Cambridge Univ Press",
  volume    =  30,
  number    =  01,
  pages     = "201--251",
  year      =  2014
}

@ARTICLE{Westerlund2013-qq,
  title     = "Lessons from a Decade of {IPS} and {LLC}",
  author    = "Westerlund, Joakim and Breitung, J{\"o}rg",
  abstract  = "This paper points to some of the facts that have emerged from 20
               years of research into the analysis of unit roots in panel data,
               an area that has been heavily influenced by two studies, IPS
               (Im, Pesaran, and Shin, 2003) and LLC (Levin et al., 2002). Some
               of these facts are known, others are not. But they all have in
               common that, if ignored, the effects can be very serious. This
               is demonstrated using both simulations and theoretical
               arguments.",
  journal   = "Economet. Rev.",
  publisher = "Taylor \& Francis",
  volume    =  32,
  number    = "5-6",
  pages     = "547--591",
  month     =  may,
  year      =  2013
}

@ARTICLE{Erickson2011-le,
  title     = "Treating measurement error in Tobin's q",
  author    = "Erickson, Timothy and Whited, Toni M",
  abstract  = "Abstract We compare the ability of three measurement error
               remedies to deliver unbiased estimates of coefficients in
               investment regressions. We examine high-order moment estimators,
               dynamic panel estimators, and simple instrumental variables
               estimators that use",
  journal   = "Rev. Financ. Stud.",
  publisher = "Soc Financial Studies",
  pages     = "hhr120",
  year      =  2011
}

@ARTICLE{Huang2015-ay,
  title     = "The Sovereign Effect on Bank Credit Ratings",
  author    = "Huang, Yu-Li and Shen, Chung-Hua",
  abstract  = "We investigate the effect of sovereign credit ratings on bank
               credit ratings, known as the sovereign effect. Our study differs
               from the literature in three respects. First, we examine whether
               bank ratings below, at, or above the sovereign ceiling impact
               the sovereign effect. We find that the sovereign effect holds in
               all cases. Next, we consider the asymmetric impact of the
               sovereign effect on bank ratings with or without the ceiling
               effect. We find that a downgrade exhibits a stronger sovereign
               effect than an upgrade. Third, we examine whether the asset
               deterioration or the stable foreign fund hypotheses are possible
               explanations for the sovereign effect. Our results support both
               hypotheses.",
  journal   = "J Financ Serv Res",
  publisher = "Springer US",
  volume    =  47,
  number    =  3,
  pages     = "341--379",
  month     =  jun,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Han2006-pr,
  title     = "{GMM} with Many Moment Conditions",
  author    = "Han, Chirok and Phillips, Peter C B",
  abstract  = "This paper provides a first order asymptotic theory for
               generalized method of moments (GMM) estimators when the number
               of moment conditions is allowed to increase with the sample size
               and the moment conditions may be weak. Examples in which these
               asymptotics are relevant include instrumental variable (IV)
               estimation with many (possibly weak or uninformed) instruments
               and some panel data models that cover moderate time spans and
               have correspondingly large numbers of instruments. Under certain
               regularity conditions, the GMM estimators are shown to converge
               in probability but not necessarily to the true parameter, and
               conditions for consistent GMM estimation are given. A general
               framework for the GMM limit distribution theory is developed
               based on epiconvergence methods. Some illustrations are
               provided, including consistent GMM estimation of a panel model
               with time varying individual effects, consistent limited
               information maximum likelihood estimation as a continuously
               updated GMM estimator, and consistent IV structural estimation
               using large numbers of weak or irrelevant instruments. Some
               simulations are reported.",
  journal   = "Econometrica",
  publisher = "Blackwell Publishing Ltd",
  volume    =  74,
  number    =  1,
  pages     = "147--192",
  month     =  jan,
  year      =  2006,
  keywords  = "Epiconvergence; GMM; irrelevant instruments; IV; large numbers
               of instruments; LIML estimation; panel models; pseudo true
               value; signal; signal variability; weak instrumentation"
}

@ARTICLE{Han2010-bn,
  title     = "{GMM} estimation for dynamic panels with fixed effects and
               strong instruments at unity",
  author    = "Han, Chirok and Phillips, Peter C B",
  journal   = "Econometric Theory",
  publisher = "Cambridge Univ Press",
  volume    =  26,
  number    =  01,
  pages     = "119--151",
  year      =  2010
}

@ARTICLE{Longworth2014-qh,
  title     = "Strengths and weaknesses of Canadian financial regulation before
               and after the global financial crisis",
  author    = "Longworth, David",
  abstract  = "The Canadian financial system came through the global financial
               crisis of 2007--2009 relatively unscathed. Important elements in
               this success were Canadian financial regulation and supervision.
               This case study argues that Canadian strengths in regulation and
               supervision before the crisis fell into four categories: capital
               regulation, a learning culture, the use of principles in
               addition to rules and the regulation of mortgage insurance.
               Nonetheless, the crisis in Canada and globally also pointed out
               some weaknesses in Canadian regulation, particularly in its
               neglect of certain types of systemic risk. Many of these
               weaknesses have been dealt with in terms of new macro-prudential
               regulations. However, some concerns remain in that area, as well
               as in securities regulation and competition policy.",
  journal   = "J. Bank. Regul.",
  publisher = "Palgrave Macmillan UK",
  volume    =  15,
  number    = "3-4",
  pages     = "277--287",
  month     =  sep,
  year      =  2014,
  language  = "en"
}

@TECHREPORT{Cunningham2016-gu,
  title       = "The Role of Central Banks in Promoting Financial Stability: An
                 International Perspective",
  author      = "Cunningham, Rose and Friedrich, Christian and {Others}",
  number      = "Discussion paper",
  institution = "Bank of Canada",
  year        =  2016
}

@TECHREPORT{Chen2012-wj,
  title       = "Canadian bank balance-sheet management: Breakdown by types of
                 Canadian financial institutions",
  author      = "Chen, David Xiao and Damar, H Evren and Soubra, Hani and
                 Terajima, Yaz",
  number      = "Discussion Paper",
  institution = "Bank of Canada Discussion Paper",
  year        =  2012
}

@ARTICLE{Goldstein2005-ka,
  title     = "{Demand--Deposit} Contracts and the Probability of Bank Runs",
  author    = "Goldstein, Itay and Pauzner, Ady",
  abstract  = "Diamond and Dybvig (1983) show that while demand--deposit
               contracts let banks provide liquidity, they expose them to
               panic-based bank runs. However, their model does not provide
               tools to derive the probability of the bank-run equilibrium, and
               thus cannot determine whether banks increase welfare overall. We
               study a modified model in which the fundamentals determine which
               equilibrium occurs. This lets us compute the ex ante probability
               of panic-based bank runs and relate it to the contract. We find
               conditions under which banks increase welfare overall and
               construct a demand--deposit contract that trades off the
               benefits from liquidity against the costs of runs.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing, Inc.",
  volume    =  60,
  number    =  3,
  pages     = "1293--1327",
  month     =  jun,
  year      =  2005
}

@ARTICLE{Gelman2019-cb,
  title     = "R-squared for Bayesian Regression Models",
  author    = "Gelman, Andrew and Goodrich, Ben and Gabry, Jonah and Vehtari,
               Aki",
  abstract  = "AbstractThe usual definition of R2 (variance of the predicted
               values divided by the variance of the data) has a problem for
               Bayesian fits, as the numerator can be larger than the
               denominator. We propose an alternative definition similar to one
               that has appeared in the survival analysis literature: the
               variance of the predicted values divided by the variance of
               predicted values plus the expected variance of the errors.",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  73,
  number    =  3,
  pages     = "307--309",
  month     =  jul,
  year      =  2019
}

@ARTICLE{Hocquard2013-oa,
  title     = "A {Constant-Volatility} Framework for Managing Tail Risk",
  author    = "Hocquard, Alexandre and Ng, Sunny and Papageorgiou, Nicolas",
  journal   = "Journal of Portfolio Management; New York",
  publisher = "Euromoney Institutional Investor PLC",
  volume    =  39,
  number    =  2,
  pages     = "28--40,6,8",
  year      =  2013,
  address   = "United States--US, United Kingdom, New York",
  language  = "en"
}

@TECHREPORT{Vilani2018-lf,
  title       = "For a meaningful artificial intelligence: Towards a French and
                 European strategy",
  author      = "Vilani, Cedric",
  institution = "French Parliament",
  year        =  2018
}

@TECHREPORT{Coen2017-ij,
  title       = "The Determinants of {UK} Credit Union Failure",
  author      = "Coen, Jamie and Francis, William and Rostom, May",
  abstract    = "This paper examines the determinants of credit union failure
                 in the United Kingdom. Using regulatory data on credit unions,
                 we estimate several discrete-time lo",
  institution = "Bank of England",
  month       =  apr,
  year        =  2017,
  keywords    = "Credit Unions; Failure; Early Warning; Logit; Policymaker Loss
                 Function"
}

@ARTICLE{Powell2015-is,
  title    = "Takeover Prediction Models and Portfolio Strategies: A
              Multinomial Approach",
  author   = "Powell, Ronan",
  abstract = "This paper uses a multinomial framework to develop several
              takeover prediction models. The motivation for this approach lies
              with Morck, Shleifer and Vishny (19",
  month    =  jun,
  year     =  2015,
  keywords = "multinomial logit; takeover prediction; abnormal returns; size
              effect"
}

@ARTICLE{Nam2008-an,
  title     = "Bankruptcy prediction using a discrete-time duration model
               incorporating temporal and macroeconomic dependencies",
  author    = "Nam, Chae Woo and Kim, Tong Suk and Park, Nam Jung and Lee, Hoe
               Kyung",
  abstract  = "The purpose of this paper is to build an alternative method of
               bankruptcy prediction that accounts for some deficiencies in
               previous approaches that resulted in poor out-of-sample
               performances. Most of the traditional approaches suffer from
               restrictive presumptions and structural limitations and fail to
               reflect the panel properties of financial statements and/or the
               common macroeconomic influence. Extending the work of Shumway
               (2001), we present a duration model with time-varying covariates
               and a baseline hazard function incorporating macroeconomic
               dependencies. Using the proposed model, we investigate how the
               hazard rates of listed companies in the Korea Stock Exchange
               (KSE) are affected by changes in the macroeconomic environment
               and by time-varying covariate vectors that show unique financial
               characteristics of each company. We also investigate
               out-of-sample forecasting performances of the suggested model
               and demonstrate improvements produced by allowing temporal and
               macroeconomic dependencies. Copyright \copyright{} 2008 John
               Wiley \& Sons, Ltd.",
  journal   = "J. Forecast.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  27,
  number    =  6,
  pages     = "493--506",
  month     =  sep,
  year      =  2008,
  keywords  = "bankruptcy prediction; discrete-time hazard model; time-varying
               covariate; duration-dependent hazard rate; default correlation"
}

@ARTICLE{Bapna2015-bj,
  title     = "Do Your Online Friends Make You Pay? A Randomized Field
               Experiment on Peer Influence in Online Social Networks",
  author    = "Bapna, Ravi and Umyarov, Akhmed",
  abstract  = "Demonstrating compelling causal evidence of the existence and
               strength of peer-to-peer influence has become the holy grail of
               modern research in online social networks. In these networks, it
               has been consistently demonstrated that user characteristics and
               behavior tend to cluster both in space and in time. There are
               multiple well-known rival mechanisms that compete to be the
               explanation for this observed clustering. These range from peer
               influence to homophily to other unobservable external stimuli.
               These multiple mechanisms lead to similar observational data,
               yet have vastly different policy implications. In this paper, we
               present a novel randomized experiment that tests the existence
               of causal peer influence in the general population?one that did
               not involve subject recruitment for experimentation?of a
               particular large-scale online social network. We utilize a
               unique social feature to exogenously induce adoption of a paid
               service among a group of randomly selected users, and in the
               process develop a clean exogenous randomization of treatment and
               control groups. A variety of nonparametric, semiparametric, and
               parametric approaches, ranging from resampling-based inference
               to ego-level random effects to logistic regression to survival
               models, yield close to identical, statistically and economically
               significant estimates of peer influence in the general
               population of a freemium social network. Our estimates show that
               peer influence causes more than a 60\% increase in odds of
               buying the service due to the influence coming from an adopting
               friend. In addition, we find that users with a smaller number of
               friends experience stronger relative increase in the adoption
               likelihood due to influence from their peers as compared to the
               users with a larger number of friends. Our nonparametric
               resampling procedure-based estimates are helpful in situations
               of networked data that violate independence assumptions. We
               establish that peer influence is a powerful force in getting
               users from free to premium levels, a known challenge in freemium
               communities. This paper was accepted by Sandra Slaughter,
               information systems.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  61,
  number    =  8,
  pages     = "1902--1920",
  month     =  apr,
  year      =  2015
}

@ARTICLE{Luoma1991-uk,
  title     = "Survival analysis as a tool for company failure prediction",
  author    = "Luoma, M and Laitinen, E K",
  abstract  = "1. INTRODUCTION failure and nonfailure, but they do not give any
               estimate for the time to failure. Furthermore, the THE STUDIES
               of company failure prediction are predictions given by the
               cross-sectional models, largely based on the original work of
               Beaver [2] based on",
  journal   = "Omega",
  publisher = "Elsevier",
  volume    =  19,
  number    =  6,
  pages     = "673--678",
  month     =  jan,
  year      =  1991
}

@ARTICLE{Cole2009-yn,
  title     = "Is hazard or probit more accurate in predicting financial
               distress? Evidence from {U.S}. bank failures",
  author    = "Cole, Rebel A and Wu, Qiongbing",
  abstract  = "We compare the out-of-sample forecasting accuracy of the
               time-varying hazard model developed by Shumway (2001) and the
               one-period probit model used by Cole and Gunther (1998). Using
               data on U.S. bank failures from 1985 -- 1992, we find that, from
               an econometric perspective, the hazard model is more accurate
               than the probit model in predicting bank failures, but this
               improvement in accuracy results from incorporating more recent
               information in the hazard, but not the probit, model. When we
               limit both models to the same information set, we find that the
               one-period probit model is slightly more accurate than the
               time-varying hazard model. We also find that a parsimonious
               specification of the one-period probit model fit to data from
               the 1980s performs surprisingly well in forecasting bank
               failures during 2009 -- 2010.",
  publisher = "mpra.ub.uni-muenchen.de",
  month     =  feb,
  year      =  2009,
  keywords  = "bank; bank failure; failure prediction; financial crisis;
               forecasting; hazard model; probit model; static model;
               time-varying covariates",
  language  = "en"
}

@ARTICLE{Betz2014-qb,
  title     = "Predicting distress in European banks",
  author    = "Betz, Frank and Opric{\u a}, Silviu and Peltonen, Tuomas A and
               Sarlin, Peter",
  abstract  = "The paper develops an early-warning model for predicting
               vulnerabilities leading to distress in European banks using both
               bank and country-level data. As outright bank failures have been
               rare in Europe, the paper introduces a novel dataset that
               complements bankruptcies and defaults with state interventions
               and mergers in distress. The signals of the early-warning model
               are calibrated not only according to the policymaker's
               preferences between type I and II errors, but also to take into
               account the potential systemic relevance of each individual
               financial institution. The key findings of the paper are that
               complementing bank-specific vulnerabilities with indicators for
               macro-financial imbalances and banking sector vulnerabilities
               improves model performance and yields useful out-of-sample
               predictions of bank distress during the current financial
               crisis.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  45,
  pages     = "225--241",
  month     =  aug,
  year      =  2014,
  keywords  = "Bank distress; Early-warning model; Prudential policy; Signal
               evaluation"
}

@ARTICLE{Bologna2015-oe,
  title     = "Structural Funding and Bank Failures",
  author    = "Bologna, Pierluigi",
  abstract  = "This paper investigates bank structural funding vis-{\`a}-vis
               bank failures. An empirical analysis is conducted on the
               defaults of commercial banks occurred in the United States
               between 2007 and 2009. The results highlight that structural
               funding position indeed plays a significant role in explaining
               the probability of bank defaults. A balanced funding position,
               with more deposits and a lower loans-deposits gap to be filled
               with wholesale funding, positively contributes to the resilience
               of the banks. Not all deposits are, however, the same: within
               deposit funding, the higher the reliance on the less-stable
               components the more likely a bank is to face distress. Results
               are robust to the sampling and modelling choices as well as to
               the variable specification used. While the empirical analysis
               focuses on the US and the loan-to-deposit ratio, the findings
               can be easily generalized to any banking system, and the
               relevant policy messages can be extended to the banking
               stability impacts of the Basel III structural funding
               regulation.",
  journal   = "J Financ Serv Res",
  publisher = "Springer US",
  volume    =  47,
  number    =  1,
  pages     = "81--113",
  month     =  feb,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Chiaramonte2015-ax,
  title     = "Should we trust the Z-score? Evidence from the European Banking
               Industry",
  author    = "Chiaramonte, Laura and Croci, Ettore and Poli, Federica",
  abstract  = "We investigate the accuracy of the Z-score, a widely used proxy
               of bank soundness, on a sample of European banks from 12
               countries over the period 2001--2011. Specifically, we run a
               horse race analysis between the Z-score and the CAMELS related
               covariates. Using probit and complementary log--log models, we
               find that the Z-score's ability to identify distress events,
               both in the whole period and during the crisis years
               (2008--2011), is at least as good as the CAMELS variables, but
               with the advantage of being less data demanding. Finally, the
               Z-score proves to be more effective when bank business models
               may be more sophisticated as it is the case for large and
               commercial banks.",
  journal   = "Global Finance Journal",
  publisher = "Elsevier",
  volume    =  28,
  pages     = "111--131",
  month     =  oct,
  year      =  2015,
  keywords  = "Bank distress; Z-score; CAMELS; Financial crisis"
}

@ARTICLE{Bruno2017-bz,
  title    = "Comparative assessment of macroprudential policies",
  author   = "Bruno, Valentina and Shim, Ilhyock and Shin, Hyun Song",
  abstract = "This paper provides a comparative assessment of the effectiveness
              of macroprudential policies in 12 Asia-Pacific economies over
              2004--2013, using databases of domestic macroprudential policies
              and capital flow management (CFM) policies. We find that banking
              sector CFM policies and bond market CFM policies are effective in
              slowing down banking inflows and bond inflows, respectively. We
              also find some evidence of spillover effects of these policies.
              Finally, regarding the interaction of monetary policy and
              macroprudential policies, our empirical findings suggest that
              macroprudential policies are more successful when they complement
              monetary policy by reinforcing monetary tightening, than when
              they act in opposite directions.",
  journal  = "Journal of Financial Stability",
  volume   =  28,
  pages    = "183--202",
  month    =  feb,
  year     =  2017,
  keywords = "Macroprudential policy; Capital flow management policy; Interest
              rate policy; Complementarity; Asia-Pacific"
}

@ARTICLE{Buch2016-um,
  title    = "Banks and sovereign risk: A granular view",
  author   = "Buch, Claudia M and Koetter, Michael and Ohls, Jana",
  abstract = "We investigate the determinants of sovereign bond holdings of
              German banks and the implications of such holdings for bank risk.
              We use granular information on all German banks and all sovereign
              debt exposures in the years 2005--2013. As regards the
              determinants of sovereign bond holdings of banks, we find that
              these are larger for weakly capitalized banks, banks that are
              active on capital markets, and for large banks. Yet, only around
              two thirds of all German banks hold sovereign bonds.
              Macroeconomic fundamentals were significant drivers of sovereign
              bond holdings only after the collapse of Lehman Brothers. With
              the outbreak of the sovereign debt crisis, German banks
              reallocated their portfolios toward sovereigns with lower debt
              ratios and bonds with lower yields. With regard to the
              implications for bank risk, we find that low-risk government
              bonds decreased the risk of German banks, especially for savings
              and cooperative banks. Holdings of high-risk government bonds, in
              turn, increased the risk of commercial banks during the sovereign
              debt crisis.",
  journal  = "Journal of Financial Stability",
  volume   =  25,
  pages    = "1--15",
  month    =  aug,
  year     =  2016,
  keywords = "Sovereign debt; Bank-level heterogeneity; Bank risk"
}

@ARTICLE{Galati2013-tz,
  title     = "{MACROPRUDENTIAL} {POLICY} -- A {LITERATURE} {REVIEW}",
  author    = "Galati, Gabriele and Moessner, Richhild",
  abstract  = "The recent financial crisis has highlighted the need to go
               beyond a purely micro approach to financial regulation and
               supervision. As a consequence, the number of policy speeches,
               research papers and conferences that discuss a macro perspective
               on financial regulation has grown considerably. The policy
               debate is focusing in particular on macroprudential tools and
               their usage, their relationship with monetary policy, their
               implementation and their effectiveness. Macroprudential policy
               has recently also attracted considerable attention among
               researchers. This paper provides an overview of research on this
               topic. We also identify important future research questions that
               emerge from both the literature and the current policy debate.",
  journal   = "J. Econ. Surv.",
  publisher = "Wiley Online Library",
  volume    =  27,
  number    =  5,
  pages     = "846--878",
  month     =  dec,
  year      =  2013,
  keywords  = "Macroprudential policy"
}

@ARTICLE{Ebrahimi_Kahou2017-vu,
  title     = "Macroprudential policy: A review",
  author    = "Ebrahimi Kahou, Mahdi and Lehar, Alfred",
  abstract  = "The severity and longevity of the recession caused by the 2007
               financial crisis has highlighted the lack of a reliable
               macro-based financial regulation framework. As a consequence,
               addressing the link between the stability of the financial
               system as a whole and the performance of the overall economy has
               become a mandate for policymakers and scholars. Many countries
               have adopted macroprudential tools as policy responses for
               safeguarding the financial system. This paper provides a
               literature review of macroprudential policies, its objectives
               and the challenges that a macro-based framework needs to
               overcome, such as financial stability, procyclicality, and
               systemic risk.",
  journal   = "Journal of Financial Stability",
  publisher = "Elsevier",
  volume    =  29,
  pages     = "92--105",
  month     =  apr,
  year      =  2017,
  keywords  = "Bank regulation; Macroprudential; Financial crisis"
}

@ARTICLE{Bluhm2014-tj,
  title    = "Systemic risk in an interconnected banking system with endogenous
              asset markets",
  author   = "Bluhm, Marcel and Krahnen, Jan Pieter",
  abstract = "We analyze the emergence of systemic risk in a network model of
              interconnected bank balance sheets. The model incorporates
              multiple sources of systemic risk, including size of financial
              institutions, direct exposure from interbank lendings, and asset
              fire sales. We suggest a new macroprudential risk management
              approach building on a system wide value at risk (SVaR). Under
              the SVaR metric, the contribution of individual banks to systemic
              risk is well defined and can be approximated by a Shapley
              value-type measure. We show that, in a SVaR regime, a fair
              systemic risk charge which is proportional to a bank's individual
              contribution to systemic risk diverges from the optimal
              macroprudential capitalization of the banks from a planner's
              perspective. The results have implications for the design of
              macroprudential capital surcharges.",
  journal  = "Journal of Financial Stability",
  volume   =  13,
  pages    = "75--94",
  month    =  aug,
  year     =  2014,
  keywords = "Systemic risk; Systemic risk charge; Macroprudential supervision;
              Shapley value; Financial network"
}

@ARTICLE{noauthor_undated-of,
  title    = "[{PDF]Implementing} a macroprudential framework - Bank for
              International ..",
  abstract = "Speech given at the HKIMR-BIS conference on ``Financial
              Stability: Towards a Macroprudential Approach'', 5-6 July 2010"
}

@ARTICLE{Claessens2013-cf,
  title    = "Macro-prudential policies to mitigate financial system
              vulnerabilities",
  author   = "Claessens, Stijn and Ghosh, Swati R and Mihet, Roxana",
  abstract = "Macro-prudential policies aimed at mitigating systemic financial
              risks have become part of the policy toolkit in many emerging
              markets and some advanced countries. Their effectiveness and
              efficacy are not well-known, however. Using panel data
              regressions, we analyze how changes in balance sheets of some
              2800 banks in 48 countries over 2000--2010 respond to specific
              policies. Controlling for endogeneity, we find that measures
              aimed at borrowers -- caps on debt-to-income and loan-to-value
              ratios, and limits on credit growth and foreign currency lending
              -- are effective in reducing leverage, asset and noncore to core
              liabilities growth during boom times. While countercyclical
              buffers (such as reserve requirements, limits on profit
              distribution, and dynamic provisioning) also help mitigate
              increases in bank leverage and assets, few policies help stop
              declines in adverse times, consistent with the ex-ante nature of
              macro-prudential tools.",
  journal  = "J. Int. Money Finance",
  volume   =  39,
  pages    = "153--185",
  month    =  dec,
  year     =  2013,
  keywords = "Systemic risk; Macro-prudential policies; Effectiveness; Banking
              vulnerabilities"
}

@ARTICLE{Mirzaei_undated-sn,
  title  = "Does Basel {III-Compliant} Bank Efficiency Enhance Industry Growth
            in Developing Countries?",
  author = "Mirzaei, Ali and Mooreb, Tomoe"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Borio2003-ai,
  title     = "Towards a Macroprudential Framework for Financial Supervision
               and Regulation?",
  author    = "Borio, Claudio",
  abstract  = "Over the last decade or so, addressing financial instability has
               risen to the top of the policy agenda. This essay argues that in
               order to improve the safeguards against financial instability,
               it may be desirable to strengthen further the macroprudential
               orientation of current prudential frameworks, a process that is
               already under way. The essay defines, compares and contrasts the
               macro- and microprudential dimensions that inevitably coexist in
               financial regulatory and supervisory arrangements, examines the
               nature of financial instability against this background and
               draws conclusions about the broad outline of desirable policy
               efforts.(JEL G 2) ``Words, like nature, half reveal and half
               conceal the soul within''Alfred Lord Tennyson``When I use a
               word‚Ä¶ it just means what I choose it to mean -- neither more,
               nor less''Humpty Dumpty",
  journal   = "CESifo Econ Stud",
  publisher = "Oxford University Press",
  volume    =  49,
  number    =  2,
  pages     = "181--215",
  month     =  jan,
  year      =  2003
}

@TECHREPORT{Biggar2011-pj,
  title       = "The fifty most important papers in the economics of regulation",
  author      = "Biggar, Darryl",
  institution = "Working Paper",
  year        =  2011
}

@ARTICLE{Peltzman1976-su,
  title   = "Toward a More General Theory of Regulation",
  author  = "Peltzman, Sam",
  journal = "The Journal of Law and Economics",
  volume  =  19,
  number  =  2,
  pages   = "211--240",
  year    =  1976
}

@UNPUBLISHED{Sauter2017-kd,
  title  = "On the persistence of relationship banking: Evidence from the
            corporate perspective",
  author = "Sauter, Katharina and Mietzner, Mark",
  year   =  2017
}

@BOOK{Hsiao2014-td,
  title     = "Analysis of Panel Data",
  author    = "Hsiao, Cheng",
  abstract  = "This book provides a comprehensive, coherent, and intuitive
               review of panel data methodologies that are useful for empirical
               analysis. Substantially revised from the second edition, it
               includes two new chapters on modeling cross-sectionally
               dependent data and dynamic systems of equations. Some of the
               more complicated concepts have been further streamlined. Other
               new material includes correlated random coefficient models,
               pseudo-panels, duration and count data models, quantile
               analysis, and alternative approaches for controlling the impact
               of unobserved heterogeneity in nonlinear panel data models.",
  publisher = "Cambridge University Press",
  month     =  dec,
  year      =  2014,
  language  = "en"
}

@INCOLLECTION{Blundell2001-px,
  title     = "Estimation in dynamic panel data models: Improving on the
               performance of the standard {GMM} estimator",
  booktitle = "Nonstationary Panels, Panel Cointegration, and Dynamic Panels",
  author    = "Blundell, Richard and Bond, Stephen and Windmeijer, Frank",
  abstract  = "This chapter reviews developments to improve on the poor
               performance of the standard GMM estimator for highly
               autoregressive panel series. It considers the use of the
               `system' GMM estimator that relies on relatively mild
               restrictions on the initial condition process. This system GMM
               estimator encompasses the GMM estimator based on the non-linear
               moment conditions available in the dynamic error components
               model and has substantial asymptotic efficiency gains.
               Simulations, that include weakly exogenous covariates, find
               large finite sample biases and very low precision for the
               standard first differenced estimator. The use of the system GMM
               estimator not only greatly improves the precision but also
               greatly reduces the finite sample bias. An application to panel
               production function data for the U.S. is provided and confirms
               these theoretical and experimental findings.",
  publisher = "emeraldinsight.com",
  pages     = "53--91",
  year      =  2001
}

@ARTICLE{Besstremyannaya2017-tm,
  title     = "Heterogeneous effect of the global financial crisis and the
               Great East Japan Earthquake on costs of Japanese banks",
  author    = "Besstremyannaya, Galina",
  abstract  = "The effect of financial and economic ctarises depends on bank
               technology, which includes risk attitude and business model. The
               paper focuses on Japanese banking and examines how technology
               distinctions determined impact of the 2007--2009 global
               financial crisis and the economic recession that followed the
               Great East Japan Earthquake of 2011. Assuming that different
               types of technology correspond to different cost quantiles, we
               use panel data quantile regressions to establish a link between
               efficiency, economies of scale/scope and the effects of the two
               crises. The analysis reveals technological heterogeneity and
               shows that the impact of profitability, non-traditional
               activities and non-performing loans in the two crises differs
               between high-cost and low-cost banks. Finally, we contrast the
               business models and risk-taking behavior of Japanese and
               European banks.",
  journal   = "Journal of Empirical Finance",
  publisher = "Elsevier",
  volume    =  42,
  pages     = "66--89",
  month     =  jun,
  year      =  2017,
  keywords  = "Financial crisis; Banking; Economies of scale; Quantile
               regressions"
}

@PHDTHESIS{Vu2017-zp,
  title     = "Bank competition, efficiency, productivity, and the impact of
               quantitative easing in Japan",
  author    = "Vu, Anh Nguyet",
  abstract  = "The Japanese banking system provides a distinctive platform for
               the examination of the long-lasting effect of problem loans on
               bank performance. Japan is also known for an extended
               quantitative easing programme of unprecedented scale. Yet the
               links between risk-taking activities, quantitative easing, and
               bank competition are largely unexplored. This thesis employs a
               unique database, which allows us to distinguish between bankrupt
               and restructured loans. The aim of the thesis is to investigate
               the impact of these loans on Japanese bank efficiency and
               productivity growth, as well as their relationship with bank
               competition and quantitative easing policy. We measure technical
               efficiency by modifying a translog enhanced hyperbolic distance
               function with two undesirable outputs, identified as problem
               loans and problem other earning assets. Further analyses reveal
               that bankrupt loans affect efficiency in a manner related to the
               ``moral hazard, skimping'' hypothesis, with the causality
               originating from bankrupt loans. In contrast, the relationship
               between restructured loans and efficiency supports the ``bad
               luck'' hypothesis. We also follow the parametric approach to
               quantify the impact of bankrupt and restructured loans on
               productivity growth of the Japanese banking system. We further
               perform convergence cluster analysis to examine convergence in
               productivity growth between regions, where limited convergence
               is reported. Additionally, this thesis employs, for the first
               time, the bank-level Boone indicator to measure bank competition
               in Japan to examine the underlying linkages between quantitative
               easing, competition, and risk. Given the scale of problem loans,
               we measure bank risk-taking based on bankrupt and restructured
               loans. Our analyses show that enhancing quantitative easing and
               competition would reduce bankrupt and restructured loans, but it
               would negatively affect financial stability. In light of the
               ongoing negative interest rates and quantitative and qualitative
               easing policy to enhance economic growth in Japan, this thesis
               would provide insightful implications for policymakers and
               regulators.",
  publisher = "sro.sussex.ac.uk",
  month     =  apr,
  year      =  2017,
  school    = "University of Sussex",
  language  = "en"
}

@ARTICLE{Goncharenko2015-cm,
  title    = "The Dark Side of Stress Test: Negative Effects of Information
              Disclosure",
  author   = "Goncharenko, Roman and Hledik, Juraj and Pinto, Roberto",
  abstract = "This paper studies the effect of information disclosure on banks'
              portfolio risk. We cast a simple banking system into a general
              equilibrium model with trading",
  month    =  nov,
  year     =  2015,
  keywords = "Network, stress tests, general equilibrium, mean-variance, banks"
}

@UNPUBLISHED{Alvarez2015-vt,
  title       = "Mandatory Disclosure and Financial Contagion",
  author      = "Alvarez, Fernando and Barlevy, Gadi",
  abstract    = "This paper explores whether mandatory disclosure of bank
                 balance sheet information can improve welfare. In our
                 benchmark model, mandatory disclosure can raise welfare only
                 when markets are frozen, i.e. when investors refuse to fund
                 banks in the absence of balance sheet information. Even then,
                 intervention is only warranted if there is sufficient
                 contagion across banks, in a sense we make precise within our
                 model. In the same benchmark model, if in the absence of
                 balance sheet information investors would fund banks,
                 mandatory disclosure cannot raise welfare and it will be
                 desirable to forbid banks to disclose their financial
                 positions. When we modify the model to allow banks to engage
                 in moral hazard, mandatory disclosure can increase welfare in
                 normal times. But the case for intervention still hinges on
                 there being sufficient contagion. Finally, we argue disclosure
                 represents a substitute to other financial reforms rather than
                 complement them as some have argued.",
  number      =  21328,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  jul,
  year        =  2015
}

@ARTICLE{Shapiro2015-sc,
  title     = "Information Management in Banking Crises",
  author    = "Shapiro, Joel and Skeie, David",
  abstract  = "A regulator resolving a bank faces two audiences: depositors,
               who may run if they believe the regulator will not provide
               capital, and banks, which may take excess risk if they believe
               the regulator will provide capital. When the regulator's cost of
               injecting capital is private information, it manages
               expectations by using costly signals: (1) a regulator with a low
               cost of injecting capital may forbear on bad banks to signal
               toughness and reduce risk taking, and (2) a regulator with a
               high cost of injecting capital may bail out bad banks to
               increase confidence and prevent runs.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  28,
  number    =  8,
  pages     = "2322--2363",
  month     =  aug,
  year      =  2015
}

@ARTICLE{Bouvard2015-ct,
  title     = "Transparency in the Financial System: Rollover Risk and Crises",
  author    = "Bouvard, Matthieu and Chaigneau, Pierre and Motta, Adolfo D E",
  abstract  = "We present a theory of optimal transparency when banks are
               exposed to rollover risk. Disclosing bank-specific information
               enhances the stability of the financial system during crises,
               but has a destabilizing effect in normal economic times. Thus,
               the regulator optimally increases transparency during crises.
               Under this policy, however, information disclosure signals a
               deterioration of economic fundamentals, which gives the
               regulator ex post incentives to withhold information. This
               commitment problem precludes a disclosure policy that provides
               ex ante optimal insurance against aggregate shocks, and can
               result in excess opacity that increases the likelihood of a
               systemic crisis.",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  70,
  number    =  4,
  pages     = "1805--1837",
  month     =  aug,
  year      =  2015
}

@ARTICLE{Goldstein2014-vg,
  title     = "Should Banks' Stress Test Results be Disclosed? An Analysis of
               the Costs and Benefits",
  author    = "Goldstein, Itay and Sapra, Haresh",
  abstract  = "Abstract Stress tests have become an important component of the
               supervisory toolkit. However, the extent of disclosure of
               stress-test results remains controversial. We argue that while
               stress tests uncover unique information to outsiders---because
               banks operate in",
  journal   = "Foundations and Trends\textregistered{} in Finance",
  publisher = "Now Publishers",
  volume    =  8,
  number    =  1,
  pages     = "1--54",
  year      =  2014,
  keywords  = "Disclosure; Financial reporting; Corporate finance; Financial
               markets; Financial markets; Economic Theory; Economic Theory;
               Regulation"
}

@ARTICLE{Schuermann2013-nw,
  title    = "Stress Testing Banks",
  author   = "Schuermann, Til",
  abstract = "How much capital and liquidity does a bank need -- to support its
              risk taking activities? During the recent (and still ongoing)
              financial crisis, answers to thi",
  journal  = "Int. J. Forecast.",
  volume   = "30(3)",
  month    =  feb,
  year     =  2013,
  keywords = "capital requirements, leverage, systemic risk"
}

@ARTICLE{Goldstein2015-tn,
  title    = "Stress Tests and Information Disclosure",
  author   = "Goldstein, Itay and Leitner, Yaron",
  abstract = "Supersedes WP13-26. We study an optimal disclosure policy of a
              regulator that has information about banks' ability to overcome
              future liquidity shocks. We focu",
  month    =  nov,
  year     =  2015,
  keywords = "Bayesian Persuasion, Optimal Disclosure, Stress Tests"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Beatty2014-lx,
  title    = "Financial accounting in the banking industry: A review of the
              empirical literature",
  author   = "Beatty, Anne and Liao, Scott",
  abstract = "We survey research on banks◊≥ financial accounting. After
              providing a brief background of the theoretical models and
              accounting and regulatory institutions underlying the bank
              accounting literature, we review three streams of empirical
              research. Specifically we review studies associating bank
              financial reporting with the valuation and risk assessments,
              associating bank financial reporting discretion with regulatory
              capital and earnings management, and examining banks' economic
              decisions under differing accounting regimes. We discuss what we
              have already learned and about what else we would like to know.
              We also discuss methodological challenges associated with
              predicting the effects of alternative accounting and regulatory
              capital regimes.",
  journal  = "J. Account. Econ.",
  volume   =  58,
  number   =  2,
  pages    = "339--383",
  month    =  nov,
  year     =  2014,
  keywords = "Financial accounting; Bank regulatory capital; Information
              asymmetry"
}

@ARTICLE{Acharya2016-gx,
  title     = "Banks' Financial Reporting and Financial System Stability",
  author    = "Acharya, Viral V and Ryan, Stephen G",
  abstract  = "The use of accounting measures and disclosures in banks'
               contracts and regulation suggests that the quality of banks'
               financial reporting is central to the efficacy of market
               discipline and nonmarket mechanisms in limiting banks'
               development of debt and risk overhangs in economic good times
               and in mitigating the adverse consequences of those overhangs
               for the stability of the financial system in downturns. This
               essay examines how research on banks' financial reporting,
               informed by the financial economics literature on banking, can
               generate insights about how to enhance the stability of the
               financial system. We begin with a foundational discussion of how
               aspects of banks' accounting and disclosures may affect
               stability. We then evaluate representative papers in the
               empirical literature on banks' financial reporting and
               stability, pointing out the research design issues that
               empirical accounting researchers need to confront to develop
               well-specified tests able to generate reliably interpretable
               findings. To this end, we provide examples of settings amenable
               to addressing these issues. We conclude with considerations for
               accounting standard setters and financial system policy makers.",
  journal   = "Journal of Accounting Research",
  publisher = "Wiley Online Library",
  volume    =  54,
  number    =  2,
  pages     = "277--340",
  month     =  may,
  year      =  2016,
  keywords  = "G01; G21; G28; M41; M48; banks; financial reporting; financial
               system; stability; opacity; provision for loan losses; fair
               value accounting; amortized cost accounting"
}

@ARTICLE{Illing2006-ta,
  title    = "Measuring financial stress in a developed country: An application
              to Canada",
  author   = "Illing, Mark and Liu, Ying",
  abstract = "This paper develops an index of financial stress for the Canadian
              financial system. It is a continuous variable with a spectrum of
              values, where extreme values are called financial crises. An
              internal Bank of Canada survey is used to condition the choice of
              variables. The authors show that alternative measures of
              financial crisis suggested by the literature do not accurately
              reflect the Canadian experience, while several measures developed
              in this paper are more representative and are thus likely better
              suited to a developed financial system. An accurate
              characterization of stress is a prerequisite for any researcher
              attempting to forecast financial crises.",
  journal  = "Journal of Financial Stability",
  volume   =  2,
  number   =  3,
  pages    = "243--265",
  month    =  oct,
  year     =  2006,
  keywords = "Financial crisis; Financial stress; Early warning indicators;
              Financial institutions; Financial markets"
}

@ARTICLE{Roukny2016-ea,
  title    = "Interconnectedness as a source of uncertainty in systemic risk",
  author   = "Roukny, Tarik and Battiston, Stefano and Stiglitz, Joseph E",
  abstract = "Financial networks have shown to be important in understanding
              systemic events in credit markets. In this paper, we investigate
              how the structure of those networks can affect the capacity of
              regulators to assess the level of systemic risk. We introduce a
              model to compute the individual and systemic probability of
              default in a system of banks connected in a generic network of
              credit contracts and exposed to external shocks with a generic
              correlation structure. Even in the presence of complete
              knowledge, we identify conditions on the network for the
              emergence of multiple equilibria. Multiple equilibria give rise
              to uncertainty in the determination of the default probability.
              We show how this uncertainty can affect the estimation of
              systemic risk in terms of expected losses. We further quantify
              the effects of cyclicality, leverage, volatility and
              correlations. Our results are relevant to the current policy
              discussions on new regulatory framework to deal with systemic
              events of distress as well as on the desirable level of
              regulatory data disclosure.",
  journal  = "Journal of Financial Stability",
  month    =  dec,
  year     =  2016,
  keywords = "Financial networks; Systemic risk; Uncertainty; Regulatory
              framework; Contagion"
}

@ARTICLE{Szymanski_undated-nz,
  title    = "Entry into exit: insolvency in English professional football",
  author   = "Szymanski, Stefan",
  abstract = "This study uses a unique database of financial accounts for
              English football clubs between 1974 and 2010 to examine the
              process by which firms fail, which in this context means entering
              insolvency proceedings. From the data it is possible to estimate
              shocks to demand and productivity and to show that failing firms
              typically experience a series of negative shocks. This is
              consistent with the standard IO theory models of exit.",
  journal  = "Scott. J. Polit. Econ."
}

@UNPUBLISHED{Hansen2010-ct,
  title  = "A Winner's Curse for Econometric Models: On the Joint Distribution
            of {In-Sample} Fit and {Out-of-Sample} Fit and its Implications for
            Model Selection",
  author = "Hansen, Peter Reinhard",
  year   =  2010
}

@ARTICLE{Ahmed2016-kq,
  title    = "Can currency-based risk factors help forecast exchange rates?",
  author   = "Ahmed, Shamim and Liu, Xiaoquan and Valente, Giorgio",
  abstract = "This paper examines the time series predictability of bilateral
              exchange rates from linear factor models that utilize the
              unconditional and conditional expectations of three
              currency-based risk factors. Exploiting a comprehensive set of
              statistical criteria, we find that all versions of the linear
              factor models largely fail to outperform the benchmark random
              walk with drift model for the out-of-sample forecasting of
              monthly exchange rate returns. This holds true for both
              individual currencies and currency portfolios formed on forward
              discounts. We also show that the information embedded in the
              currency-based risk factors does not generate systematic economic
              value for investors.",
  journal  = "Int. J. Forecast.",
  volume   =  32,
  number   =  1,
  pages    = "75--97",
  month    =  jan,
  year     =  2016,
  keywords = "Exchange rates; Out-of-sample predictability; Economic value;
              Time series; Econometric models"
}

@ARTICLE{Levich2015-ig,
  title    = "Predictability and `good deals' in currency markets",
  author   = "Levich, Richard M and Pot{\`\i}, Valerio",
  abstract = "In this paper, we study predictability in currency markets over
              the period 1972--2012. To assess the economic significance of
              this predictability, we construct an upper bound on the
              explanatory power of predictive regressions of currency returns.
              The bound is motivated by ``no good-deal'' restrictions that, in
              efficient markets, rule out unduly attractive investment
              opportunities. We find that currency predictability exceeds this
              bound during recurring albeit short-lived episodes.
              Excess-predictability is highest in the 1970s and tends to
              decrease over time, but is still present in the final part of the
              sample period. Moreover, periods of high and low predictability
              tend to alternate. These stylized facts pose a challenge to
              Fama's (1970) Efficient Market Hypothesis but are consistent with
              Lo's (2004) Adaptive Market Hypothesis, coupled with a slow
              convergence towards efficient markets. Transaction costs can
              explain much of the daily excess-predictability, but not the
              monthly excess-predictability.",
  journal  = "Int. J. Forecast.",
  volume   =  31,
  number   =  2,
  pages    = "454--472",
  month    =  apr,
  year     =  2015,
  keywords = "Foreign exchange; Predictability; Filter rules; Market efficiency"
}

@ARTICLE{Campbell2016-mw,
  title    = "This time is different: Causes and consequences of British
              banking instability over the long run",
  author   = "Campbell, Gareth and Coyle, Christopher and Turner, John D",
  abstract = "This paper addresses three questions: (1) How severe were the
              episodes of banking instability experienced by the UK over the
              past two centuries? (2) What have been the macroeconomic
              indicators of UK banking instability? and (3) What have been the
              consequences of UK banking instability for the cost of credit?
              Using a unique dataset of bank share prices from 1830 to 2010 to
              assess the stability of the UK banking system, we find that
              banking instability has grown more severe since the 1970s. We
              also find that interest rates, inflation, lending growth, and
              equity prices are consistent macroeconomic indicators of UK
              banking instability over the long run. Furthermore, utilising a
              unique dataset of corporate-bond yields for the period 1860 to
              2010, we find that there is a significant long-run relationship
              between banking instability and the credit-risk premium faced by
              businesses.",
  journal  = "Journal of Financial Stability",
  volume   =  27,
  pages    = "74--94",
  month    =  dec,
  year     =  2016,
  keywords = "Banking crises; Cost of credit; Financial instability"
}

@ARTICLE{Tonzer2015-mm,
  title    = "Cross-border interbank networks, banking risk and contagion",
  author   = "Tonzer, Lena",
  abstract = "Recent events have highlighted the role of cross-border linkages
              between banking systems in transmitting local developments across
              national borders. This paper analyzes whether international
              linkages in interbank markets affect the stability of
              interconnected banking systems and channel financial distress
              within a network consisting of banking systems of the main
              advanced countries for the period 1994--2012. Methodologically, I
              use a spatial modeling approach to test for spillovers in
              cross-border interbank markets. The results suggest that foreign
              exposures in banking play a significant role in channeling
              banking risk: I find that countries that are linked through
              foreign borrowing or lending positions to more stable banking
              systems abroad are significantly affected by positive spillover
              effects. From a policy point of view, this implies that in stable
              times, linkages in the banking system can be beneficial, while
              they have to be taken with caution in times of financial turmoil
              affecting the whole system.",
  journal  = "Journal of Financial Stability",
  volume   =  18,
  pages    = "19--32",
  month    =  jun,
  year     =  2015,
  keywords = "Financial contagion; Financial integration; Banking networks"
}

@ARTICLE{Berger2013-kp,
  title    = "How does capital affect bank performance during financial crises?",
  author   = "Berger, Allen N and Bouwman, Christa H S",
  abstract = "This paper empirically examines how capital affects a bank's
              performance (survival and market share) and how this effect
              varies across banking crises, market crises, and normal times
              that occurred in the US over the past quarter century. We have
              two main results. First, capital helps small banks to increase
              their probability of survival and market share at all times
              (during banking crises, market crises, and normal times). Second,
              capital enhances the performance of medium and large banks
              primarily during banking crises. Additional tests explore
              channels through which capital generates these effects. Numerous
              robustness checks and additional tests are performed.",
  journal  = "J. financ. econ.",
  volume   =  109,
  number   =  1,
  pages    = "146--176",
  month    =  jul,
  year     =  2013,
  keywords = "Financial crises; Survival; Market share; Profitability; Banking"
}

@ARTICLE{Anginer2014-gg,
  title    = "How does competition affect bank systemic risk?",
  author   = "Anginer, Deniz and Demirguc-Kunt, Asli and Zhu, Min",
  abstract = "Using bank level measures of competition and co-dependence, we
              show a robust negative relationship between bank competition and
              systemic risk. Whereas much of the extant literature has focused
              on the relationship between competition and the absolute level of
              risk of individual banks, in this paper we examine the
              correlation in the risk taking behavior of banks. We find that
              greater competition encourages banks to take on more diversified
              risks, making the banking system less fragile to shocks.
              Examining the impact of the institutional and regulatory
              environment on bank systemic risk shows that banking systems are
              more fragile in countries with weak supervision and private
              monitoring, greater government ownership of banks, and with
              public policies that restrict competition. We also find that the
              negative effect of lack of competition can be mitigated by a
              strong institutional environment that allows for efficient public
              and private monitoring of financial institutions.",
  journal  = "Journal of Financial Intermediation",
  volume   =  23,
  number   =  1,
  pages    = "1--26",
  month    =  jan,
  year     =  2014,
  keywords = "Systemic risk; Bank competition; Credit risk; Merton model;
              Distance to default; Default risk; Lerner index; Bank
              concentration"
}

@ARTICLE{Dong2010-zw,
  title  = "Jumpy or kinky? regression discontinuity without the discontinuity",
  author = "Dong, Yingying",
  year   =  2010
}

@ARTICLE{Li2014-mk,
  title   = "Jump regressions",
  author  = "Li, Jia and Todorov, Viktor and Tauchen, George",
  journal = "Econometrica",
  year    =  2014
}

@ARTICLE{Lee2010-sf,
  title     = "Regression Discontinuity Designs in Economics",
  author    = "Lee, David S and Lemieux, Thomas",
  abstract  = "This paper provides an introduction and ``user guide'' to
               Regression Discontinuity (RD) designs for empirical researchers.
               It presents the basic theory behind the research design, details
               when RD is likely to be valid or invalid given economic
               incentives, explains why it is considered a
               ``quasi-experimental'' design, and summarizes different ways
               (with their advantages and disadvantages) of estimating RD
               designs and the limitations of interpreting these estimates.
               Concepts are discussed using examples drawn from the growing
               body of empirical research using RD.",
  journal   = "J. Econ. Lit.",
  publisher = "American Economic Association",
  volume    =  48,
  number    =  2,
  pages     = "281--355",
  year      =  2010
}

@UNPUBLISHED{Lengwiler2017-lm,
  title  = "Credit from the Monopoly Bank",
  author = "Lengwiler, Yvan and Rishabh, Kumar",
  year   =  2017
}

@ARTICLE{Guzman2010-cd,
  title    = "An Inflation Expectations Horserace",
  author   = "Guzman, Giselle",
  abstract = "For decades, the academic literature has focused on three survey
              measures of expected inflation: the Livingston Survey, the Survey
              of Professional Forecasters,",
  month    =  jan,
  year     =  2010,
  keywords = "Inflation, expectations, surveys, households, economists,
              rationality, efficiency, unbiasedness, forecast accuracy,
              out-of-sample forecasts, Granger Causality, high-frequency data,
              price level, money and prices, CPI, PPI, PCE"
}

@ARTICLE{Kolm2017-av,
  title     = "Bank Regulation, {CEO} Compensation, and Boards",
  author    = "Kolm, Julian and Laux, Christian and L{\'o}r{\'a}nth,
               Gy{\"o}ngyi",
  abstract  = "We analyze the limits of regulating bank CEO compensation to
               reduce risk shifting in the presence of an active board that
               retains the right to approve new investment strategies.
               Compensation regulation prevents overinvestment in strategies
               that increase risk, but it is ineffective in preventing
               underinvestment in strategies that reduce risk. The regulator
               optimally combines compensation and capital regulations. In
               contrast, if the board delegates the choice of strategy to the
               CEO, compensation regulation is sufficient to prevent both types
               of risk shifting. Compensation regulation increases
               shareholders' incentives to implement an active board, which
               reduces the effectiveness of compensation regulation.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  5,
  pages     = "1901--1932",
  month     =  aug,
  year      =  2017
}

@ARTICLE{Daniel2016-oq,
  title     = "Momentum crashes",
  author    = "Daniel, Kent and Moskowitz, Tobias J",
  abstract  = "Abstract Despite their strong positive average returns across
               numerous asset classes, momentum strategies can experience
               infrequent and persistent strings of negative returns. These
               momentum crashes are partly forecastable. They occur in panic
               states, following market declines and when market volatility is
               high, and are contemporaneous with market rebounds. The low ex
               ante expected returns in panic states are consistent with a
               conditionally high premium attached to the option like payoffs
               of past losers. An implementable dynamic momentum strategy based
               on forecasts of momentum's mean and variance approximately
               doubles the alpha and Sharpe ratio of a static momentum strategy
               and is not explained by other factors. These results are robust
               across multiple time periods, international equity markets, and
               other asset classes.",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  122,
  number    =  2,
  pages     = "221--247",
  month     =  nov,
  year      =  2016,
  keywords  = "Asset pricing; Market anomalies; Market efficiency; Momentum"
}

@ARTICLE{Crawford2003-ps,
  title     = "Assessing the forecasting performance of regime-switching,
               {ARIMA} and {GARCH} models of house prices",
  author    = "Crawford, Gordon W and Fratantoni, Michael C",
  journal   = "Real Estate Econ.",
  publisher = "Wiley Online Library",
  volume    =  31,
  number    =  2,
  pages     = "223--243",
  year      =  2003
}

@ARTICLE{Gompers2003-ge,
  title     = "Corporate Governance and Equity Prices",
  author    = "Gompers, Paul and Ishii, Joy and Metrick, Andrew",
  abstract  = "Shareholder rights vary across firms. Using the incidence of 24
               governance rules, we construct a ``Governance Index'' to proxy
               for the level of shareholder rights at about 1500 large firms
               during the 1990s. An investment strategy that bought firms in
               the lowest decile of the index (strongest rights) and sold firms
               in the highest decile of the index (weakest rights) would have
               earned abnormal returns of 8.5 percent per year during the
               sample period. We find that firms with stronger shareholder
               rights had higher firm value, higher profits, higher sales
               growth, lower capital expenditures, and made fewer corporate
               acquisitions.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  118,
  number    =  1,
  pages     = "107--156",
  month     =  feb,
  year      =  2003
}

@ARTICLE{Brown2004-fo,
  title    = "Corporate Governance and Firm Performance",
  author   = "Brown, Lawrence D and Caylor, Marcus L",
  abstract = "We create a broad measure of corporate governance, Gov-Score,
              based on a new dataset provided by Institutional Shareholder
              Services. Gov-Score is a composite m",
  month    =  dec,
  year     =  2004,
  keywords = "Corporate governance, firm performance, gov score, nominating
              committee, governance committee, option burn rate"
}

@ARTICLE{Bebchuk2009-xf,
  title     = "What Matters in Corporate Governance?",
  author    = "Bebchuk, Lucian and Cohen, Alma and Ferrell, Allen",
  abstract  = "We investigate the relative importance of the twenty-four
               provisions followed by the Investor Responsibility Research
               Center (IRRC) and included in the Gompers, Ishii, and Metrick
               governance index (Gompers, Ishii, and Metrick 2003). We put
               forward an entrenchment index based on six provisions: staggered
               boards, limits to shareholder bylaw amendments, poison pills,
               golden parachutes, and supermajority requirements for mergers
               and charter amendments. We find that increases in the index
               level are monotonically associated with economically significant
               reductions in firm valuation as well as large negative abnormal
               returns during the 1990-2003 period. The other eighteen IRRC
               provisions not in our entrenchment index were uncorrelated with
               either reduced firm valuation or negative abnormal returns.",
  journal   = "Rev. Financ. Stud.",
  publisher = "[Oxford University Press, Society for Financial Studies]",
  volume    =  22,
  number    =  2,
  pages     = "783--827",
  year      =  2009
}

@INCOLLECTION{Hu2013-eo,
  title     = "Systemic Risk and Financial Innovation: Toward a ``Unified''
               Approach",
  booktitle = "Quantifying Systemic Risk",
  author    = "Hu, Henry T C",
  publisher = "University of Chicago Press",
  pages     = "11--28",
  year      =  2013
}

@INCOLLECTION{Haubrich2013-ah,
  title     = "Introduction to ``Quantifying Systemic Risk''",
  booktitle = "Quantifying Systemic Risk",
  author    = "Haubrich, Joseph G and Lo, Andrew W",
  publisher = "University of Chicago Press",
  pages     = "1--10",
  month     =  apr,
  year      =  2013
}

@ARTICLE{Malikov2014-jp,
  title    = "Are All {U.S}. Credit Unions Alike? A Generalized Model of
              Heterogeneous Technologies with Endogenous Switching and
              Correlated Effects",
  author   = "Malikov, Emir and Restrepo-Tobon, Diego A and Kumbhakar, Subal C",
  abstract = "Credit unions differ in the types of financial services they
              offer to their members. This paper explicitly models this
              observed heterogeneity using a generalize",
  month    =  mar,
  year     =  2014,
  keywords = "Credit Union, Correlated Effects, Panel Data, Returns to Scale,
              Selection, Switching Regression"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kirilenko2017-xu,
  title    = "The Flash Crash: {High‚ÄêFrequency} Trading in an Electronic Market",
  author   = "Kirilenko, Andrei and Kyle, Albert S and Samadi, Mehrdad and
              Tuzun, Tugkan",
  abstract = "We study intraday market intermediation in an electronic market
              before and during a period of large and temporary selling
              pressure. On May 6, 2010, U.S. financial markets experienced a
              systemic intraday event---the Flash Crash---where a large
              automated selling program was rapidly executed in the E‚Äêmini S\&P
              500 stock index futures market. Using audit trail
              transaction‚Äêlevel data for the E‚Äêmini on May 6 and the previous
              three days, we find that the trading pattern of the most active
              nondesignated intraday intermediaries (classified as
              High‚ÄêFrequency Traders) did not change when prices fell during
              the Flash Crash.",
  journal  = "J. Finance",
  volume   =  72,
  number   =  3,
  pages    = "967--998",
  month    =  jun,
  year     =  2017
}

@ARTICLE{Stein2010-qt,
  title     = "Securitization, shadow banking \& financial fragility",
  author    = "Stein, Jeremy C",
  journal   = "Daedalus",
  publisher = "MIT Press",
  volume    =  139,
  number    =  4,
  pages     = "41--51",
  month     =  oct,
  year      =  2010
}

@UNPUBLISHED{Ma2017-hp,
  title  = "Financial Stability, Growth and Macroprudential Policies",
  author = "Ma, Chang",
  year   =  2017
}

@ARTICLE{Neuberger2009-ek,
  title     = "Microenterprises and multiple bank relationships: The case of
               professionals",
  author    = "Neuberger, Doris and R{\"a}thke, Solvig",
  abstract  = "The present paper focuses on professionals as a special group of
               microenterprises. It explains their characteristics and
               financial relationships, using data from a survey conducted in
               Germany in 2002. Consistent with the theory of asymmetric
               information and relationship lending, we find that these firms
               maintain a small number of bank relationships, which increases
               in firm size and age. They tend to choose multiple banking
               relationships to overcome credit rationing and finance larger
               loans. Credit risk and the structure of the banking market do
               not seem to matter.",
  journal   = "Small Bus. Econ.",
  publisher = "Springer US",
  volume    =  32,
  number    =  2,
  pages     = "207--229",
  month     =  feb,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Ashraf2017-qw,
  title    = "Political institutions and bank risk-taking behavior",
  author   = "Ashraf, Badar Nadeem",
  abstract = "This paper examines the impact of political institutions on bank
              risk-taking behavior. Using an international sample of banks from
              98 countries over the period 1998--2007, I document that sound
              political institutions stimulate higher bank risk-taking. This is
              consistent with the hypotheses that better political institutions
              increase banks' risk by boosting the credit market competition
              from alternative sources of finance and generating the moral
              hazard problems due to the expectation of government bailouts in
              worst economic conditions. While it is contrary to the hypotheses
              that better political institutions decrease banks' risk by
              lowering the government expropriation risk and the information
              asymmetries between banks and borrowers. The results are robust
              to a number of sensitivity tests, including alternative proxies
              of bank risk-taking and political institutions, cross-sectional
              bank- and country-level regressions, endogeneity concerns of
              political institutions, country income levels, explicit deposit
              insurance schemes and sample extension from 1998 to 2014. I also
              examine the interdependence between political and legal
              institutions and find that political and legal institutions
              complement each other to influence bank risk-taking behavior.",
  journal  = "Journal of Financial Stability",
  volume   =  29,
  pages    = "13--35",
  month    =  apr,
  year     =  2017,
  keywords = "Political institutions; Political constraints; Bank risk-taking;
              Legal institutions; Moral hazard problems"
}

@ARTICLE{Diaz2017-qg,
  title    = "The role of governance on bank liquidity creation",
  author   = "D{\'\i}az, Violeta and Huang, Ying",
  abstract = "This paper examines the impact of internal bank governance on
              bank liquidity creation in the U.S. before, during and after the
              2007--2009 financial crisis. Using bank holding company level
              data, we analyze whether better-governed banks create higher
              levels of liquidity. We find that this effect is positive and
              significant but only for large bank holding companies. Further
              analysis reveals that specific internal governance categories:
              CEO education, compensation structure, progressive practices, and
              ownership have a significant effect on bank liquidity. However,
              this positive effect occurs mostly during the crisis period, and
              for large banks that are also high liquidity creators. Finally,
              we find that the effect of governance on liquidity creation
              increases during the crisis period. These findings are robust
              even while controlling for liquidity measures, bank size, and
              endogeneity problems between governance and liquidity creation.",
  journal  = "Journal of Banking \& Finance",
  volume   =  77,
  pages    = "137--156",
  month    =  apr,
  year     =  2017,
  keywords = "Liquidity creation; Governance; Crisis; Bank holding companies;
              Compensation"
}

@ARTICLE{Lambert2017-qz,
  title    = "How do insured deposits affect bank risk? Evidence from the 2008
              Emergency Economic Stabilization Act",
  author   = "Lambert, Claudia and Noth, Felix and Sch{\"u}wer, Ulrich",
  abstract = "This paper tests whether an increase in insured deposits causes
              banks to become more risky. We use variation introduced by the
              U.S. Emergency Economic Stabilization Act in October 2008, which
              increased the deposit insurance coverage from $100,000 to
              $250,000 per depositor and bank. For some banks, the amount of
              insured deposits increased significantly; for others, it was a
              minor change. Our analysis shows that the more affected banks
              increase their investments in risky commercial real estate loans
              and become more risky relative to unaffected banks following the
              change. This effect is most distinct for affected banks that are
              low capitalized.",
  journal  = "Journal of Financial Intermediation",
  volume   =  29,
  pages    = "81--102",
  month    =  jan,
  year     =  2017,
  keywords = "Financial crisis; Deposit insurance; Bank regulation"
}

@ARTICLE{Chen2017-oq,
  title    = "Monetary policy and bank risk-taking: Evidence from emerging
              economies",
  author   = "Chen, Minghua and Wu, Ji and Jeon, Bang Nam and Wang, Rui",
  abstract = "This paper addresses the impact of monetary policy on banks'
              risk-taking by using bank-level panel data from more than 1000
              banks in 29 emerging economies during 2000--2012. We find that,
              consistent with the proposition of the ``bank risk-taking
              channel'' of monetary policy transmission, banks' riskiness
              increases when monetary policy is eased. This result is robust
              when we adopt alternative measures of monetary policy and bank
              risk, and use different econometric methodologies. In addition,
              we find that bank risk-taking amid expansionary monetary policy
              is less conspicuous in a more consolidated banking sector and
              when monetary policy is more transparent.",
  journal  = "Emerging Markets Review",
  volume   =  31,
  pages    = "116--140",
  month    =  jun,
  year     =  2017,
  keywords = "Monetary policy; Bank risk-taking; Emerging economies"
}

@ARTICLE{Carboni2017-pm,
  title    = "Surprised or not surprised? The investors' reaction to the
              comprehensive assessment preceding the launch of the banking
              union",
  author   = "Carboni, Marika and Fiordelisi, Franco and Ricci, Ornella and
              Lopes, Francesco Saverio Stentella",
  abstract = "Did the Comprehensive Assessment (CA), preceding the Single
              Supervisory Mechanism (SSM) launch in Europe, achieve its aims of
              producing new valuable information for the market? We show that
              the CA achieved the goal of increasing transparency: investors
              were able to detect weak banks at the announcement of the
              procedure (23rd October 2013), but gained full information on the
              amount of the capital shortfall only at the disclosure of the
              results (26th October 2014). Furthermore, at the official launch
              of the SSM (4th November 2014), banks under direct European
              Central Bank (ECB) supervision registered a more negative market
              reaction with respect to banks maintaining their national
              supervisors. Using a regression model including possible
              confounders and allowing for treatment effect heterogeneity, this
              negative reaction is confirmed. These findings suggest that, at
              least in the short run, investors penalized banks subject to
              direct ECB supervision, probably because of the fear of
              regulatory inconsistencies.",
  journal  = "Journal of Banking \& Finance",
  volume   =  74,
  pages    = "122--132",
  month    =  jan,
  year     =  2017,
  keywords = "Banking; Supervision; Regulation; Lending; Risk-taking"
}

@ARTICLE{Cao2017-lc,
  title    = "Monetary policy and financial stability in the long run: A simple
              game-theoretic approach",
  author   = "Cao, Jin and Chollete, Lor{\'a}n",
  abstract = "Many theoretical central bank models use short horizons and focus
              on a single tradeoff. However, in reality, central banks play
              complex, long-horizon games and face more than one tradeoff. We
              account for these strategic interactions in a simple
              infinite-horizon game with a novel tradeoff: tighter monetary
              policy deters financial imbalances, but looser monetary policy
              reduces the likelihood of insolvency. We term these factors
              discipline and stability effects, respectively. The central
              bank's welfare decreases with dependence between real and
              financial shocks, so it may reduce costs with correlation-indexed
              securities. An independent central bank cannot in general attain
              both low inflation and financial stability.",
  journal  = "Journal of Financial Stability",
  volume   =  28,
  pages    = "125--142",
  month    =  feb,
  year     =  2017,
  keywords = "Central banking; Correlation-indexed security; Discipline effect;
              Stability effect; Strategic interaction"
}

@ARTICLE{Sarmiento2017-ac,
  title    = "The influence of risk-taking on bank efficiency: Evidence from
              Colombia",
  author   = "Sarmiento, Miguel and Gal{\'a}n, Jorge E",
  abstract = "This paper shows evidence on the influence of risk-taking on bank
              efficiency in emerging markets and identifies heterogeneity in
              the way risk affects banks with different characteristics. We fit
              a stochastic frontier model with random inefficiency parameters
              to a sample of Colombian banks. The model provides accurate cost
              and profit efficiency estimates. The effects of risk-taking on
              efficiency vary with size and affiliation. Large and foreign
              banks benefit more from higher exposure to credit and market
              risk, while domestic and small banks from being more capitalised.
              We identify some channels explaining these differences and
              provide insights for prudential regulation.",
  journal  = "Emerging Markets Review",
  month    =  may,
  year     =  2017,
  keywords = "Bank efficiency; Bayesian inference; Heterogeneity; Random
              parameters; Risk-taking; Stochastic frontier models"
}

@ARTICLE{Garel2017-at,
  title    = "Bank capital in the crisis: It's not just how much you have but
              who provides it",
  author   = "Garel, Alexandre and Petit-Romec, Arthur",
  abstract = "Bank capital is the cornerstone of bank regulation and is
              considered a key determinant of a bank's ability to withstand
              economic shocks. In the area of bank capital regulation, the
              general view is that more bank capital is better, irrespective of
              who provides it. In this paper, we investigate whether the
              investment horizon of bank capital providers matters for bank
              performance during the recent financial crisis. We observe that
              banks with more short-term investor ownership have worse stock
              returns during the crisis. Further exploration suggests that this
              is partially because banks with higher short-term investor
              ownership took more risk prior to the crisis but mainly because
              they experienced higher selling pressure during the crisis. Our
              results confirm the economic benefit of bank capital in helping
              banks to perform better during crises. However, when we decompose
              bank capital by the nature of its providers, we show that more
              capital is associated with worse performance when it is provided
              by short-term institutional investors.",
  journal  = "Journal of Banking \& Finance",
  volume   =  75,
  pages    = "152--166",
  month    =  feb,
  year     =  2017,
  keywords = "Financial crisis; Investor horizons; Institutional investors;
              Bank capital"
}

@ARTICLE{Akhter2017-yc,
  title    = "Contagion risk for Australian banks from global systemically
              important banks: Evidence from extreme events",
  author   = "Akhter, Selim and Daly, Kevin",
  abstract = "This paper presents evidence that extreme negative shocks for the
              global systemically important banks (GSIBs) are contagious to
              Australian banks. Our logit regression models predict
              transmission of adverse extreme shocks in the distance to default
              (DD) of GSIBs to the Australian banks. While most previous
              studies consider contagion across national stock markets, we
              investigate the degree of contagion risk for Australian banks
              spreading from GSIBs. Our results point to the critical
              importance for the Australian Prudential Regulation Authority
              (APRA) (2015) to closely observe and monitor developments across
              the major GSIBs and direct appropriate local policy measures
              accordingly.",
  journal  = "Econ. Model.",
  volume   =  63,
  pages    = "191--205",
  month    =  jun,
  year     =  2017,
  keywords = "Global systemically important banks (GSIBs); Australian banks;
              Extreme value theory (EVT); Extreme events; Distance to default
              (DD); GARCH; Logistic regression model"
}

@ARTICLE{Cubillas2017-jy,
  title    = "How credible is a too-big-to-fail policy? International evidence
              from market discipline",
  author   = "Cubillas, Elena and Fern{\'a}ndez, Ana I and Gonz{\'a}lez,
              Francisco",
  abstract = "This paper analyzes in an international sample of banks from 104
              countries if the sensitivity of the cost of deposits to bank risk
              varies across banks depending on their systemic and absolute
              size. We analyze a period before the 2007 financial crisis and
              control for endogeneity of bank size, intervention policies in
              past banking crises, and soundness of countries' public finances.
              Our results are consistent with the predominance of the
              too-big-to-fail hypothesis, although this effect is stronger in
              countries that did not impose losses on depositors in past
              banking crises and in countries with sounder public finances.",
  journal  = "Journal of Financial Intermediation",
  volume   =  29,
  pages    = "46--67",
  month    =  jan,
  year     =  2017,
  keywords = "Market discipline; Too-big-to-fail; Too-big-to-save; Banking
              crisis; Public deficit"
}

@ARTICLE{Blau2017-uv,
  title    = "Bank opacity and the efficiency of stock prices",
  author   = "Blau, Benjamin M and Brough, Tyler J and Griffith, Todd G",
  abstract = "Prior research argues that the process of intermediation is
              opaque and produces uncertainty about the riskiness of banks,
              which may adversely affect the efficiency of bank stock prices.
              Using the Hou and Moskowitz (2005) measure of price delay, which
              captures the inefficiency of stock prices, we test for, and find
              evidence supporting the idea that opacity is positively
              associated with price delay. Bank stocks have markedly higher
              delay than similar non-bank stocks. This higher level of delay is
              driven, in part, by market-based measures of informational
              opacity as well as the asset composition of the bank's balance
              sheet. Combined, our findings suggest that bank opacity reduces
              the efficiency of financial markets.",
  journal  = "Journal of Banking \& Finance",
  volume   =  76,
  pages    = "32--47",
  month    =  mar,
  year     =  2017,
  keywords = "Opacity; Friction; Price delay; Market efficiency; Intermediation
              risk; Impulse response functions"
}

@ARTICLE{Thanassoulis2017-iu,
  title    = "Optimal pay regulation for too-big-to-fail banks",
  author   = "Thanassoulis, John and Tanaka, Misa",
  abstract = "This paper considers optimal executive pay regulations for banks
              that are too-big-to-fail. Theoretically, we map the consequences
              of a series of commonly-used pay schemes, describing their
              relative optimality and ultimate societal consequences. We argue
              that in a world of too-big-to-fail policy, simple equity-linked
              remuneration schemes maximise shareholder value by incentivising
              executives to choose excessively risky projects at the expense of
              the taxpayer. We find that paying the executive partly in debt
              fails to mitigate the project choice distortion when debt markets
              are informed. By contrast, both clawback rules and linking pay to
              interest rates can incentivise the executive to make socially
              optimal risk choices, but only if they are accompanied by
              appropriate restrictions on the curvature of pay with respect to
              the bank's market value. Pay curvature can be generated by tools
              such as equity options and promotion policy. The policy
              implication is that unless regulators can enforce restrictions on
              pay curvature, bank shareholders can undermine the effectiveness
              of these pay regulations.",
  journal  = "Journal of Financial Intermediation",
  month    =  apr,
  year     =  2017,
  keywords = "Clawback; Executive compensation; Bankers' bonuses;
              Too-big-to-fail; Risk taking; Financial regulation"
}

@ARTICLE{Piccotti2017-wr,
  title    = "Financial contagion risk and the stochastic discount factor",
  author   = "Piccotti, Louis R",
  abstract = "I provide evidence that financial contagion risk is an important
              source of the equity risk premium. Banks' contributions to
              aggregate financial contagion are estimated in a state space
              framework and linked to systemic risk. Greater bank connectedness
              today leads to increased systemic risk 3--12 months later. More
              contagious banks earn significantly greater risk-adjusted returns
              than less contagious ones and the tradable high
              contagion-minus-low contagion bank portfolio is priced in the
              cross-section of stock returns. Stocks that co-move more strongly
              with contagious banks have greater expected returns. These
              results are robust to factor model specification, test assets,
              and time period considered.",
  journal  = "Journal of Banking \& Finance",
  volume   =  77,
  pages    = "230--248",
  month    =  apr,
  year     =  2017,
  keywords = "Asset pricing; Equity risk premium; Financial contagion;
              State-space modeling; Systemic risk"
}

@ARTICLE{Koster2017-hu,
  title    = "Financial penalties and bank performance",
  author   = "K{\"o}ster, Hannes and Pelster, Matthias",
  abstract = "This paper investigates the impact of financial penalties on the
              profitability and stock performance of banks. Using a unique
              dataset of 671 financial penalties imposed on 68 international
              listed banks over the period 2007 to 2014, we find a negative
              relation between financial penalties and pre-tax profitability
              but no relation with after-tax profitability. This result is
              explained by tax savings, as banks are allowed to deduct specific
              financial penalties from their taxable income. Moreover, our
              empirical analysis of the stock performance shows a positive
              relation between financial penalties and buy-and-hold returns,
              indicating that investors are pleased that cases are closed, that
              the banks successfully manage the consequences of misconduct, and
              that the financial penalties imposed are smaller than the accrued
              economic gains from the banks' misconduct. This argument is
              supported by the positive abnormal returns accompanying on the
              announcement of a financial penalty.",
  journal  = "Journal of Banking \& Finance",
  volume   =  79,
  pages    = "57--73",
  month    =  jun,
  year     =  2017,
  keywords = "Financial penalty; Misconduct; Bank fines; Bank profitability;
              Bank performance"
}

@ARTICLE{Peng2017-pj,
  title    = "The impact of bancassurance on efficiency and profitability of
              banks: Evidence from the banking industry in Taiwan",
  author   = "Peng, Jin-Lung and Jeng, Vivian and Wang, Jennifer L and Chen,
              Yen-Chih",
  abstract = "We set out in this study to investigate whether bancassurance
              business leads to improvements in the efficiency and
              profitability of banks. We examine the positive impacts on the
              system using actual data provided by a unique database on banks
              engaging in bancassurance business in Taiwan between 2004 and
              2012. Our results reveal that banks with greater involvement in
              bancassurance business generally tend to experience improvements
              in their efficiency, and thus also accrue greater profits. Our
              empirical results provide evidence to support that bancassurance
              business offers substantial benefits for banks, ultimately
              leading to an increase in shareholder value. Finally, our results
              also reveal that the adoption of a diversification strategy in
              bancassurance can impact bank performance.",
  journal  = "Journal of Banking \& Finance",
  volume   =  80,
  pages    = "1--13",
  month    =  jul,
  year     =  2017,
  keywords = "Bancassurance; Banking; Efficiency; Profitability"
}

@ARTICLE{Gluzmann2017-bk,
  title    = "Assessing the robustness of the relationship between financial
              reforms and banking crises",
  author   = "Gluzmann, Pablo and Guzman, Martin",
  abstract = "This paper provides a novel approach for assessing the robustness
              of the relationship between different types of financial reforms
              and banking crises for the period 1973--2005. We document the
              following facts for emerging economies: (i) liberalizations of
              capital accounts, securities markets, interest rates, removal of
              credit controls, barriers to entry, and reduction of state
              ownership in the banking sector, all are positively associated
              with a higher frequency of banking crises; (ii) the increase in
              financial turbulence is mainly concentrated within a time-window
              of five years after the reforms: If a country does not experience
              a banking crisis within that period, the probability of
              experiencing a crisis afterwards becomes insignificant; and (iii)
              the results are robust to the inclusion of all control variables
              that have been found in the literature as significant
              determinants of banking crises.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  49,
  pages    = "32--47",
  month    =  jul,
  year     =  2017,
  keywords = "Financial reforms; Banking crises; Robustness"
}

@ARTICLE{Kreis2017-ot,
  title    = "Systemic risk in a structural model of bank default linkages",
  author   = "Kreis, Yvonne and Leisen, Dietmar P J",
  abstract = "We study a structural model of individual bank defaults across
              the banking sector; banks are interconnected through their
              exposure to a common risk factor. The paper introduces a systemic
              risk measure based on the default frequency in the banking
              sector; this measure depends non-linearly on the factor's
              loadings, in contrast to previous systemic risk measures that
              depend linearly on loadings. We estimate loadings in the U.S.
              banking system over the course of the last 36 years; we find that
              they have considerably increased over time and identify four
              major regimes. Our measure shows that systemic risk became
              critical in the last of our four regimes, covering the most
              recent time period from 05/2007 to 09/2016. The empirical
              findings highlight that our measure complements existing systemic
              risk measures.",
  journal  = "Journal of Financial Stability",
  month    =  may,
  year     =  2017,
  keywords = "Default; Structural model; Systemic risk; Macro-prudential
              regulation"
}

@ARTICLE{Boubakri2017-en,
  title    = "National culture and bank performance: Evidence from the recent
              financial crisis",
  author   = "Boubakri, Narjess and Mirzaei, Ali and Samet, Anis",
  abstract = "We examine whether the prevailing national culture has been
              material in determining bank performance during the recent
              financial crisis. In this paper, we focus on three particular
              national culture dimensions: uncertainty avoidance,
              individualism/collectivism, and power distance. We expect banks
              from high uncertainty avoidance and power distance societies to
              perform relatively better during the recent financial crisis. On
              the other hand, banks in individualistic (collectivist) societies
              are likely to perform worse (better) during the crisis. Using an
              international sample of 3438 banks from 48 countries, we find
              support for our main conjectures. Specifically, we establish that
              uncertainty avoidance, collectivism, and power distance have a
              first order impact on bank performance during the crisis. Our
              results are robust to a battery of additional checks, including
              additional variables, alternative samples, and correcting for
              potential endogeneity.",
  journal  = "Journal of Financial Stability",
  volume   =  29,
  pages    = "36--56",
  month    =  apr,
  year     =  2017,
  keywords = "Culture; Bank performance; Bank diversification; Risk; Crisis"
}

@ARTICLE{Kim2017-pc,
  title    = "The effect of bank capital on lending: Does liquidity matter?",
  author   = "Kim, Dohan and Sohn, Wook",
  abstract = "This paper uses a sample of quarterly observations of insured US
              commercial banks to examine whether the effect of bank capital on
              lending differs depending upon the level of bank liquidity. We
              find that the effect of an increase in bank capital on credit
              growth, defined as growth rate of net loans and unused
              commitments, is positively associated with the level of bank
              liquidity only for large banks and that this positive
              relationship has been more substantial during the recent
              financial crisis period. This result suggests that bank capital
              exerts a significantly positive effect on lending only after
              large banks retain sufficient liquid assets.",
  journal  = "Journal of Banking \& Finance",
  volume   =  77,
  pages    = "95--107",
  month    =  apr,
  year     =  2017,
  keywords = "Bank capital; Bank liquidity; Lending behavior; Financial crisis;
              Basel III"
}

@ARTICLE{Cole2017-qz,
  title    = "When time is not on our side: The costs of regulatory forbearance
              in the closure of insolvent banks",
  author   = "Cole, Rebel A and White, Lawrence J",
  abstract = "In this paper, we empirically estimate the costs of delay in the
              FDIC's closures of 433 commercial banks between 2007 and 2014
              based upon a counterfactual closure regime. We find that the
              costs of delay could have been as high as $18.5 billion, or 37\%
              of the FDIC's estimated costs of closure of $49.8 billion. We
              think that these findings call for a more aggressive stance by
              bank regulators with respect to the provisions for loan losses
              and write-downs of banks' non-performing assets. More aggressive
              (and earlier) provisions and write-downs, or adoption of a
              capital ratio that penalizes nonperforming loans, would allow the
              concept of ``prompt corrective action'' (PCA) to play the role
              that it was meant to play in reducing FDIC losses from insolvent
              banks.",
  journal  = "Journal of Banking \& Finance",
  volume   =  80,
  pages    = "235--249",
  month    =  jul,
  year     =  2017,
  keywords = "Bank; Bank failure; CAMELS; Commercial real estate; Failure cost;
              FDIC; Financial crisis; Forbearance; Insolvent"
}

@ARTICLE{Lewellen2011-mv,
  title    = "Institutional investors and the limits of arbitrage",
  author   = "Lewellen, Jonathan",
  abstract = "The returns and stock holdings of institutional investors from
              1980 to 2007 provide little evidence of stock-picking skill.
              Institutions as a whole closely mimic the market portfolio, with
              pre-cost returns that have nearly perfect correlation with the
              value-weighted index and an insignificant CAPM alpha of 0.08\%
              quarterly. Institutions also show little tendency to bet on any
              of the main characteristics known to predict stock returns, such
              as book-to-market, momentum, or accruals. While particular groups
              of institutions have modest stock-picking skill relative to the
              CAPM, their performance is almost entirely explained by the
              book-to-market and momentum effects in returns. Further, no group
              holds a portfolio that deviates efficiently from the market
              portfolio.",
  journal  = "J. financ. econ.",
  volume   =  102,
  number   =  1,
  pages    = "62--80",
  month    =  oct,
  year     =  2011,
  keywords = "Institutional investors; Arbitrage; Trading strategies"
}

@ARTICLE{Li2017-ll,
  title    = "The impact of the liquidity coverage ratio on money creation: A
              stock-flow based dynamic approach",
  author   = "Li, Boyao and Xiong, Wanting and Chen, Liujun and Wang, Yougui",
  abstract = "This paper examines money creation process of the banking system
              when it is complying with the Liquidity Coverage Ratio (LCR). A
              stock-flow based dynamic model of credit creation process is
              developed in which the commercial bank supplies loans to the
              firm. The change of credit is governed by the bank lending and
              the repayment of the existing loans, where the equilibrium stock
              of credit could be attained once the lending is exactly equal to
              the repayment. However, the supply of bank loans is restricted by
              both the reserve requirement set by the central bank and the LCR
              prescribed by the banking authority; and, as a result, money
              creation must be affected by all these regulations. The bank loan
              supply under the constraint of the required liquidity buffer
              might have different prescriptions under different economic
              scenarios, and would eventually result in an equilibrium monetary
              stock correspondingly. The final formula of money multiplier is
              derived respectively as the rational response of the bank to the
              corresponding regulation. When the reserve requirement is tighter
              than the LCR, the money multiplier has the same expression of
              that in the prevailing fractional reserve regime. Yet when the
              situation departs from this regime, the determinants of the money
              multiplier are found to be associated with the parameters that
              characterize the behavior of banks subject to the regulation and
              of the private sector rather than those monetary structural
              factors. It is noteworthy that there may be a credit contraction
              and even a significant reduction in money multiplier when the
              bank is regulated by the LCR. This novel perspective on credit
              creation of the banking system also offers us an insightful
              understanding on the impacts of banking regulations on the
              stability of the banking system and suggests a new guide tool for
              designing them.",
  journal  = "Econ. Model.",
  month    =  jan,
  year     =  2017,
  keywords = "Money multiplier; Banking regulation; Liquidity coverage ratio;
              Credit creation; Stock-flow based dynamic model"
}

@TECHREPORT{Foucher2014-ug,
  title       = "Exchange Traded Funds: Evolution of Benefits, Vulnerabilities
                 and Risks",
  author      = "Foucher, Ian and Gray, Kyle",
  number      = "Financial System Review",
  institution = "Bank of Canada",
  year        =  2014
}

@ARTICLE{Leung2016-lv,
  title    = "{LEVERAGED} {ETF} {IMPLIED} {VOLATILITIES} {FROM} {ETF}
              {DYNAMICS}",
  author   = "Leung, Tim and Lorig, Matthew and Pascucci, Andrea",
  abstract = "The growth of the exchange-traded fund (ETF) industry has given
              rise to the trading of options written on ETFs and their
              leveraged counterparts (LETFs). We study the relationship between
              the ETF and LETF implied volatility surfaces when the underlying
              ETF is modeled by a general class of local-stochastic volatility
              models. A closed-form approximation for prices is derived for
              European-style options whose payoffs depend on the terminal value
              of the ETF and/or LETF. Rigorous error bounds for this pricing
              approximation are established. A closed-form approximation for
              implied volatilities is also derived. We also discuss a scaling
              procedure for comparing implied volatilities across leverage
              ratios. The implied volatility expansions and scalings are tested
              in three settings: Heston, limited constant elasticity of
              variance (CEV), and limited SABR; the last two are regularized
              versions of the well-known CEV and SABR models.",
  journal  = "Math. Finance",
  month    =  may,
  year     =  2016,
  keywords = "implied volatility; local-stochastic volatility; leveraged
              exchange-traded fund; implied volatility scaling"
}

@TECHREPORT{Lopez2016-ql,
  title       = "{Dodd-Frank}: Washington, We Have a Problem",
  author      = "Lopez, Claude and Saeidinezhad, Elham",
  abstract    = "The Dodd-Frank Act was the most far-reaching financial
                 regulatory reform in the U.S. since the nation emerged from
                 the Great Depression in the 1930s. The act ai",
  institution = "Milken Institute",
  month       =  jun,
  year        =  2016,
  keywords    = "Dodd-Frank, macroprudential policy, regulation"
}

@BOOK{Cerutti2016-hw,
  title     = "Changes in Prudential Policy Instruments --- A New
               {Cross-Country} Database",
  author    = "Cerutti, Mr Eugenio M and Correa, Mr Ricardo and Fiorentino,
               Elisabetta and Segalla, Esther",
  abstract  = "This paper documents the features of a new database that focuses
               on changes in the intensity in the usage of several widely used
               prudential tools, taking into account both macro-prudential and
               micro-prudential objectives. The database coverage is broad,
               spanning 64 countries, and with quarterly data for the period
               2000Q1 through 2014Q4. The five types of prudential instruments
               in the database are: capital buffers, interbank exposure limits,
               concentration limits, loan to value (LTV) ratio limits, and
               reserve requirements. A total of nine prudential tools are
               constructed since some useful further decompositions are
               presented, with capital buffers divided into four subindices:
               general capital requirements, real state credit specific capital
               buffers, consumer credit specific capital buffers, and other
               specific capital buffers; and with reserve requirements divided
               into two sub-indices: domestic currency capital requirements and
               foreign currency capital requirements. While general capital
               requirements have the most changes from the cross-country
               perspective, LTV ratio limits and reserve requirements have the
               largest number of tightening and loosening episodes. We also
               analyze the instruments' usage in relation to the evolution of
               key variables such as credit, policy rates, and house prices,
               finding substantial differences in the patterns of loosening or
               tightening of instruments in relation to business and financial
               cycles.",
  publisher = "International Monetary Fund",
  month     =  sep,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Faleye2017-rf,
  title    = "Risky lending: Does bank corporate governance matter?",
  author   = "Faleye, Olubunmi and Krishnan, Karthik",
  abstract = "We study the effect of bank governance on risk-taking in
              commercial lending. Banks with more effective boards are less
              likely to lend to riskier borrowers. This effect is restricted to
              periods of distress in the banking industry and is stronger at
              banks with board-level credit committees. Banks with more
              effective boards are less likely to lend to riskier borrowers
              right after the Russian default, which exogenously imposed
              distress conditions on U.S. banks. Thus, value-maximizing banks
              appear to ration credit to riskier borrowers precisely when such
              firms might be credit-constrained, suggesting that bank
              governance regulations may have potential unintended
              consequences.",
  journal  = "Journal of Banking \& Finance",
  volume   =  83,
  pages    = "57--69",
  month    =  oct,
  year     =  2017
}

@ARTICLE{Akinci2017-dd,
  title    = "How effective are macroprudential policies? An empirical
              investigation",
  author   = "Akinci, Ozge and Olmstead-Rumsey, Jane",
  abstract = "In recent years, policymakers have generally relied on
              macroprudential policies to address financial stability concerns.
              However, our understanding of these policies and their efficacy
              is limited. In this paper, we construct a novel index of
              macroprudential policies in 57 advanced and emerging economies
              covering the period from 2000:Q1 to 2013:Q4, with tightenings and
              easings recorded separately. The effectiveness of these policies
              in curbing credit growth and house price appreciation is then
              assessed using a dynamic panel data model. The main findings of
              the paper are: (1) Macroprudential policies have been used far
              more actively after the global financial crisis in both advanced
              and emerging economies. (2) These policies have primarily
              targeted the housing sector, especially in the advanced
              economies. (3) Macroprudential policies are usually changed in
              tandem with bank reserve requirements, capital flow restrictions,
              and monetary policy. (4) Our analysis suggests that
              macroprudential tightening is associated with lower bank credit
              growth, housing credit growth, and house price appreciation. (5)
              Targeted policies -- for example, those specifically intended to
              limit house price appreciation -- seem to be more effective,
              especially in economies where bank finance is important.",
  journal  = "Journal of Financial Intermediation",
  month    =  apr,
  year     =  2017,
  keywords = "Bank credit; House prices; Macroprudential policy; Dynamic panel
              data model"
}

@ARTICLE{Barros2010-uf,
  title     = "A note on productivity change in European cooperative banks: the
               Luenberger indicator approach",
  author    = "Barros, Carlos Pestana and Peypoch, Nicolas and Williams,
               Jonathan",
  abstract  = "The Luenberger productivity indicator is employed to estimate
               and decompose productivity change in a sample of cooperative
               banks operating in 10 EU member states. An average annualised
               productivity growth of 2.59\% is reported between 1996 and 2003,
               though there is heterogeneity in growth rates across countries.
               Generally speaking, productivity growth is driven by
               technological change. However, cooperative banks in southern
               European banking markets benefit as much from efficiency growth
               or catching-up with industry best practice. The results suggest
               that technology sharing arrangements and greater competition
               arising from deregulation are positive contributors towards
               productivity change. [ABSTRACT FROM AUTHOR]",
  journal   = "International Review of Applied Economics",
  publisher = "Routledge",
  volume    =  24,
  number    =  2,
  pages     = "137--147",
  month     =  mar,
  year      =  2010,
  keywords  = "COOPERATIVE banking industry; PRODUCTIVITY accounting; GROWTH
               rate; FINANCIAL performance; cooperative banks; Europe;
               Luenberger productivity indicator; productivity change; EUROPEAN
               Union"
}

@ARTICLE{Glass2006-yb,
  title    = "A `technically level playing-field' profit efficiency analysis of
              enforced competition between publicly funded institutions",
  author   = "Glass, J C and McCallion, G and McKillop, D G and Stringer, K",
  abstract = "The study demonstrates how recent advances in the data
              envelopment analysis (DEA) of profit efficiency, and in combining
              DEA and stochastic frontier analysis (SFA) to measure producer
              performance, can be used to obtain a 'technically level
              playing-field' profit efficiency performance measure. A
              three-stage analysis is used to provide an empirical
              investigation of public-sector universities facing regulatory
              pressures to attain both profit efficiency and output quality.
              Stage one employs directional distance function analysis to
              decompose DEA profit efficiency measures into technical and
              allocative components. As these performance measures incorporate
              the impacts of managerial inefficiency, the operating environment
              and luck (statistical noise), stage two applies SFA to total
              (radial and non-radial) slacks to obtain a degree of
              disentanglement from the latter two impacts. The stage-two
              results are used to define a common environment and common luck
              scenario, thus permitting university producers' inputs to be
              adjusted so as to reflect operation within this more level
              playing-field scenario. Stage three recomputes the first-stage
              DEA for this scenario, to yield an (input-focused) 'technically
              level playing-field' measure of profit efficiency that unifies
              the regulatory financial audit and output-quality indicators in a
              single, best-practice-benchmarked, performance measure.",
  journal  = "Eur. Econ. Rev.",
  volume   =  50,
  number   =  6,
  pages    = "1601--1626",
  month    =  aug,
  year     =  2006,
  keywords = "Production; Cost; Capital; Capital, Total Factor, and Multifactor
              Productivity; Capacity D24; Firm Performance: Size,
              Diversification, and Scope L25"
}

@ARTICLE{Klinedinst2010-vv,
  title    = "Bad loans in the meltdown: micro analysis of credit union
              performance versus banks, an initial investigation",
  author   = "Klinedinst, Mark",
  abstract = "The current economic crisis has had a devastating impact in the
              credit markets as evidenced by bank failures, large bailouts, and
              foreclosures. Trillions of dollars have been spent to prop up the
              financial sector in the U.S. alone. Credit unions, commercial
              banks and thrifts are where Americans go for home loans, but
              credit unions have a very different track record when it has come
              to bailouts from the government. Credit unions instead of taking
              trillions may ultimately not take a dime from the taxpayer. This
              paper will try to discern this advantage that credit unions have
              by focusing on the direct impact felt by financial institutions
              in the United States through net charge-offs from 1994 through
              2009 using an exceptional data set that combines information on
              credit unions and banks in the U.S. from 1994 through 2009.",
  journal  = "Journal of Money, Investment and Banking",
  number   =  19,
  pages    = "55--60",
  year     =  2010,
  keywords = "Financial Crises G01; Banks; Depository Institutions; Micro
              Finance Institutions; Mortgages G21; Financial Institutions and
              Services: Government Policy and Regulation G28; Firm Performance:
              Size, Diversification, and Scope L25"
}

@ARTICLE{Akiyoshi2010-rh,
  title    = "Banking crisis and productivity of borrowing firms: Evidence from
              Japan",
  author   = "Akiyoshi, Fumio and Kobayashi, Keiichiro",
  abstract = "Abstract: We investigate the effects of bank distress on the
              productivity of borrowing firms by using data on listed companies
              in the Japanese manufacturing industry during the 1990s. We find
              that deterioration in the financial health of banks, which is
              measured by a decline in the capital-asset ratio, decreased the
              productivity of their borrowers during the period of the severe
              financial crisis (FY1997--1998). Our finding empirically confirms
              the theoretical view that an increase in financial friction
              negatively affects the productivity of the corporate sector.
              [Copyright \&y\& Elsevier]",
  journal  = "Japan World Econ.",
  volume   =  22,
  number   =  3,
  pages    = "141--150",
  month    =  aug,
  year     =  2010,
  keywords = "BANKING industry; FINANCIAL crises; INDUSTRIAL productivity;
              MANUFACTURING industries; CAPITAL; LOANS; BUSINESS enterprises;
              JAPAN; Banking crisis; Credit crunch; Total factor productivity"
}

@ARTICLE{Hu2008-yd,
  title     = "Decomposition of mutual fund underperformance",
  author    = "Hu, Jin-Li and Chang, Tzu-Pu",
  abstract  = "This article follows a three-stage data envelopment analysis
               (DEA) approach proposed by Fried et al. (2002) to decompose
               mutual fund underperformance, in order to obtain pure managerial
               performance. In the first stage, DEA is used to compute each
               fund's performance. In the second stage, a stochastic frontier
               regression decomposes fund underperformance into characteristics
               (including fund and management attributes), managerial
               inefficiency, and statistical noise. In the third stage, DEA
               with slack-adjusted data is used to find out the pure
               performance. It is found that a fund's performance significantly
               increases with its size, previous performance, manager's tenure
               and education, while it decreases with the age of the fund and
               number of managed funds.",
  journal   = "Applied Financial Economics Letters",
  publisher = "Routledge",
  volume    =  4,
  number    =  5,
  pages     = "363--367",
  month     =  sep,
  year      =  2008,
  keywords  = "Pension Funds; Non-bank Financial Institutions; Financial
               Instruments; Institutional Investors G23"
}

@ARTICLE{Delgado2007-gv,
  title     = "Do European Primarily Internet Banks Show Scale and Experience
               Efficiencies?",
  author    = "Delgado, Javier and Hernando, Ignacio and Nieto, Mar{\'\i}a J",
  abstract  = "Empirical evidence shows that Internet banks worldwide have
               underperformed newly chartered traditional banks mainly because
               of their higher overhead costs. European banks have not been an
               exception in this regard. This paper analyses, for the first
               time in Europe, whether this is a temporary phenomenon and
               whether Internet banks may generate scale economies in excess of
               those available to traditional banks. Also do they (and their
               customers) accumulate experience with this new business model,
               allowing them to perform as well or even better than their
               peers, the traditional banks? To this end, we have generally
               followed the same analytical framework and methodology used by
               DeYoung (2001, 2002, 2005) for Internet banks in the USA
               although the limitations in the availability of data, as well as
               the existence of different regulatory frameworks and market
               conditions, particularly in the retail segment, in the 15
               European Union countries have required some modifications to the
               methodology. The empirical analysis confirms that, as is the
               case for US banks, European Internet banks show technologically
               based scale economies, while no conclusive evidence exists of
               technology based learning economies. As Internet banks get
               larger, the profitability gap with traditional banks shrinks. To
               the extent that Internet banks are profitable, European
               authorities may encourage a larger number of consumers to use
               this delivery channel, by tackling consumers security concerns.
               This would allow Internet banks to capture more of the potential
               scale efficiencies implied in our estimations. [ABSTRACT FROM
               AUTHOR]",
  journal   = "Eur Financial Management",
  publisher = "Wiley-Blackwell",
  volume    =  13,
  number    =  4,
  pages     = "643--671",
  month     =  sep,
  year      =  2007,
  keywords  = "INTERNET banking; ECONOMIES of scale; PROFITABILITY; BUSINESS
               models; CONSUMER behavior; BUSINESS planning; COMPUTER network
               resources; LEAST squares; EUROPE; cost and income structure;
               experience economies; G21; internet banks; O32; O33;
               profitability; scale economies"
}

@ARTICLE{Servin2012-gw,
  title    = "Ownership and technical efficiency of microfinance institutions:
              Empirical evidence from Latin America",
  author   = "Servin, Roselia and Lensink, Robert and van den Berg, Marrit",
  abstract = "By using stochastic frontier analysis, this article examines the
              technical efficiency of different types of microfinance
              institutions in Latin America. In particular, it tests whether
              differences in technical efficiency, both intra- and interfirm,
              can be explained by differences in ownership. With a focus on
              non-governmental organizations, cooperatives and credit unions,
              non-bank financial intermediaries, and banks, the data set
              contains 1681 observations from a panel of 315 institutions
              operating in 18 Latin American countries. The results show that
              non-governmental organizations and cooperatives have much lower
              interfirm and intrafirm technical efficiencies than non-bank
              financial intermediaries and banks, which indicates the
              importance of ownership type for technical efficiency.",
  journal  = "Journal of Banking \& Finance",
  volume   =  36,
  number   =  7,
  pages    = "2136--2144",
  month    =  jul,
  year     =  2012,
  keywords = "Banks; Depository Institutions; Micro Finance Institutions;
              Mortgages G21; Firm Performance: Size, Diversification, and Scope
              L25; Economic Development: Financial Markets; Saving and Capital
              Investment; Corporate Finance and Governance O16"
}

@ARTICLE{Olgu2008-yr,
  title    = "Parametric and nonparametric Malmquist productivity
              decomposition: a case study of European commercial banks",
  author   = "Olgu, Ozlem and Jones, Thomas G Weyman",
  abstract = "An abstract of the article ``Parametric and nonparametric
              Malmquist productivity decomposition: a case study of European
              commercial banks,'' by Ozlem Olgu and Thomas G. Weyman-Jones is
              presented.",
  journal  = "IJBPM",
  volume   =  10,
  number   =  4,
  pages    = "344",
  year     =  2008,
  keywords = "BANKING industry; ABSTRACTS; data envelopment analysis; DEA;
              European commercial banks; Malmquist index; nonparametric
              techniques; output-distance functions; parametric techniques;
              productivity decomposition; small banks"
}

@ARTICLE{Bernad2010-hn,
  title    = "The effect of mergers and acquisitions on productivity: An
              empirical application to Spanish banking",
  author   = "Bernad, Cristina and Fuentelsaz, Lucio and G{\'o}mez, Jaime",
  abstract = "Abstract: Mergers and acquisitions are frequently justified in
              terms of value creation or efficiency improvements. Nevertheless,
              the evidence is not consistent with the existence of benefits in
              terms of the costs, productivity, profitability or market value
              of the firms involved. A distinguishing feature of extant
              research is that it focuses on the assessment of the consequences
              of mergers around the time in which the operation takes place,
              limiting the possibility of observing a complete integration
              between the merged firms. In this context, the objective of this
              paper is to evaluate the effects of mergers and acquisitions on
              the long-run productivity of Spanish savings banks. Our results
              show that productivity improvements can be found in only half of
              the mergers that take place during the period analyzed.
              [Copyright \&y\& Elsevier]",
  journal  = "Omega",
  volume   =  38,
  number   =  5,
  pages    = "283--293",
  month    =  oct,
  year     =  2010,
  keywords = "SAVINGS banks; BANKING industry; EMPIRICAL research; PRODUCTION
              (Economic theory); PROFITABILITY; CONSOLIDATION \& merger of
              corporations; VALUATION; SPAIN; Acquisitions; Mergers;
              Productivity; Savings banks; Spain"
}

@ARTICLE{DeYoung2005-jq,
  title     = "The Performance of {Internet-Based} Business Models: Evidence
               from the Banking Industry",
  author    = "DeYoung, Robert",
  abstract  = "The initial Internet bank startups tended to underperform
               branching bank startups. This suggested that Internet-only
               business models were not economically viable for banks. However,
               firms that pioneer new business models may benefit substantially
               from experience as they grow older, and firms that use automated
               production technologies may benefit from scale effects as they
               grow larger. Econometric analysis of Internet-only bank startups
               finds strong evidence of the latter, but not the former, effect.
               The results suggest that Internet-only banking success depends
               on attaining sufficient scale and strong management practices.
               [ABSTRACT FROM AUTHOR]",
  journal   = "J. Bus.",
  publisher = "University of Chicago Press",
  volume    =  78,
  number    =  3,
  pages     = "893--947",
  month     =  may,
  year      =  2005,
  keywords  = "ELECTRONIC banking; INTERNET; BANKING industry; ELECTRONIC
               commerce; ECONOMETRICS"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Fiordelisi2010-qr,
  title    = "Total factor productivity and shareholder returns in banking",
  author   = "Fiordelisi, Franco and Molyneux, Phil",
  abstract = "Abstract: This paper examines shareholder value drivers in
              European banking focusing on the efficiency and productivity
              features of individual banks. In particular, we analyse the value
              relevance of bank cost efficiency and total factor productivity
              (TFP) (in all its components, including technological change,
              pure technical efficiency change and scale efficiency change) to
              see how these influence shareholder value creation in European
              banking. The paper focuses on the French, German, Italian and
              U.K. banking systems over the period 1995--2002 and includes both
              listed and non-listed banks. We find that TFP changes best
              explain variations in shareholder value (measured by
              market-adjusted returns, MAR, for listed banks and by the ratio
              of EVAbkg to invested capital at time t‚àí1 for non-listed banks).
              In both samples, we also find that technological change seems to
              be the most important component of TFP influencing shareholder
              value creation in European banking. [Copyright \&y\& Elsevier]",
  journal  = "Omega",
  volume   =  38,
  number   =  5,
  pages    = "241--253",
  month    =  oct,
  year     =  2010,
  keywords = "ECONOMIC impact; PRODUCTION (Economic theory); BANKING industry;
              STOCKHOLDER wealth; COST effectiveness; CAPITAL; DATA envelopment
              analysis; EUROPE; Banking; Data envelopment analysis (DEA);
              Malmquist productivity index; Shareholder value; Value-relevance"
}

@ARTICLE{Kneller2006-as,
  title     = "Frontier Technology and Absorptive Capacity: Evidence from
               {OECD} Manufacturing Industries*",
  author    = "Kneller, Richard and Stevens, Philip Andrew",
  abstract  = "In this paper, we examine the three facets of technology: its
               creation, dispersion and absorption. We investigate whether
               differences in absorptive capacity help to explain cross-country
               differences in the level of productivity. We utilize stochastic
               frontier analysis to investigate two potential sources of this
               inefficiency -- differences in human capital and R\&D -- for
               nine industries in 12 Organization for Economic Co-operation and
               Development (OECD) countries over the period 1973--91. We find
               that inefficiency in production does indeed exist and it depends
               upon the level of human capital of the country's workforce.
               Evidence that the amount of R\&D an industry undertakes is also
               important is less robust. [ABSTRACT FROM AUTHOR]",
  journal   = "Oxford Bull Econ \& Stats",
  publisher = "Wiley-Blackwell",
  volume    =  68,
  number    =  1,
  pages     = "1--21",
  month     =  feb,
  year      =  2006,
  keywords  = "TECHNOLOGY; INDUSTRIAL productivity; HUMAN capital; INDUSTRIES;
               LABOR supply; LABOR economics"
}

@ARTICLE{Kneller2005-yy,
  title     = "Frontier Technology, Absorptive Capacity and Distance*",
  author    = "Kneller, Richard",
  abstract  = "Recent literature on international technology diffusion has
               demonstrated the positive effect in foreign country productivity
               on the domestic economy. Using a sample of Organization for
               Economic Co-operation and Development (OECD) manufacturing
               industries we search for evidence that the effect of this
               foreign technology varies according to the level of absorptive
               capacity and physical distance. We find evidence that both help
               to explain differences in the level of productivity across
               countries, but that absorptive capacity is quantitatively more
               important. Physical distance had a greater effect at the start
               of the time period and in industries in which trade is local and
               technology is high-tech. [ABSTRACT FROM AUTHOR]",
  journal   = "Oxford Bull Econ \& Stats",
  publisher = "Wiley-Blackwell",
  volume    =  67,
  number    =  1,
  pages     = "1--23",
  month     =  feb,
  year      =  2005,
  keywords  = "TECHNOLOGY transfer; TECHNOLOGY; SEPARATION (Technology);
               DIFFUSION"
}

@ARTICLE{Green2016-ff,
  title     = "Play Hard, Shirk Hard? The Effect of Bar Hours Regulation on
               Worker Absence",
  author    = "Green, Colin P and Navarro Paniagua, Maria",
  abstract  = "The regulation of alcohol availability has the potential to
               influence worker productivity. This paper uses legislative
               changes in bar opening hours to provide a potential
               quasi-natural experiment of the effect of alcohol availability
               on working effort, focusing on worker absenteeism. We examine
               two recent policy changes, one in England/Wales and one in Spain
               that increased and decreased opening hours respectively. We
               demonstrate a robust positive causal link between opening hours
               and absenteeism, although short-lived for Spain. The effect is
               long lasting for the UK where we provide evidence which suggests
               that increased alcohol consumption is a key mechanism. [ABSTRACT
               FROM AUTHOR]",
  journal   = "Oxf. Bull. Econ. Stat.",
  publisher = "Wiley-Blackwell",
  volume    =  78,
  number    =  2,
  pages     = "248--264",
  month     =  apr,
  year      =  2016,
  keywords  = "BARS (Drinking establishments) -- Employees; ABSENTEEISM
               (Labor); BUSINESS hours; LABOR productivity; DRINKING of
               alcoholic beverages"
}

@ARTICLE{Gallo2013-iq,
  title     = "Punishing the Foreigner: Implicit Discrimination in the Premier
               League Based on Oppositional Identity*: Implicit discrimination
               in the Premier League based on oppositional identity",
  author    = "Gallo, Edoardo and Grund, Thomas and James Reade, J",
  abstract  = "We present the first empirical study to reveal the presence of
               implicit discrimination in a non-experimental setting. By using
               a large dataset of in-match data in the English Premier League,
               we show that white referees award significantly more yellow
               cards against non-white players of oppositional identity. We
               argue that this is the result of implicit discrimination by
               showing that this discriminatory behaviour: (i) increases in how
               rushed the referee is before making a decision, and (ii) it
               increases in the level of ambiguity of the decision. The
               variation in (i) and (ii) cannot be explained by any form of
               conscious discrimination such as taste-based or statistical
               discrimination. Moreover, we show that oppositional identity
               players do not differ in their behaviour from other players
               along several dimensions related to aggressiveness and style of
               play providing further evidence that this is not statistical
               discrimination. [ABSTRACT FROM AUTHOR]",
  journal   = "Oxf. Bull. Econ. Stat.",
  publisher = "Wiley-Blackwell",
  volume    =  75,
  number    =  1,
  pages     = "136--156",
  month     =  feb,
  year      =  2013,
  keywords  = "DISCRIMINATION in sports; FOOTBALL players; ATHLETES; AGGRESSION
               (Psychology); FOOTBALL referees; J71; L83; FOOTBALL Association
               Premier League (FAPL)"
}

@ARTICLE{Tas2017-eb,
  title     = "Inflation Target Credibility: Do the Financial Markets Find the
               Targets Believable?",
  author    = "Tas, Bedri Kamil Onur and Peker, Mustafa Cagri",
  abstract  = "We investigate the credibility of inflation targeting (IT)
               central banks (CBs) by estimating perceived inflation targets of
               the financial markets. We calculate financial markets' beliefs
               about the inflation targets of 24 IT countries. Then, we analyse
               whether the financial markets' beliefs about inflation targets
               match the announced targets. We conclude that the perceived
               upper bound of the inflation target is significantly higher than
               the announced one in many countries. Additionally, the perceived
               target band is narrower and asymmetric around the mid-point of
               the target for most CBs. We examine the implications of these
               findings and find that IT CBs are more likely to miss their
               targets when the perceptions of the financial markets are higher
               than the announced IT targets. These results indicate that IT
               CBs should pay attention to the perceptions of the announced
               targets when implementing policy actions.",
  journal   = "Oxf. Bull. Econ. Stat.",
  publisher = "Wiley Online Library",
  year      =  2017
}

@ARTICLE{Carrieri2016-nx,
  title     = "{Quasi-Experimental} Evidence on the Effects of Health
               Information on Preventive Behaviour in Europe",
  author    = "Carrieri, Vincenzo and Wuebker, Ansgar",
  abstract  = "We investigate the effect of information on preventive decisions
               in a quasi-experimental setting arising from the implementation
               of local breast cancer screening programmes in Europe. To
               identify the causal effect of invitation on preventive uptake,
               we link administrative public data on regional screening
               policies to individual data from the Survey of Health Ageing and
               Retirement in Europe (SHARE) and exploit regional variation in
               the availability of screening policies and in age eligibility
               criteria. We find home invitation increases mammography uptakes
               by around 24\%. Significant effects are found when at least 50\%
               of the population is reached by the invitation letter. The stock
               of health information and the ability to process the information
               received seem to play a significant role, as the effects of
               invitation are higher among low educated and lower among
               cognitively impaired women.",
  journal   = "Oxf. Bull. Econ. Stat.",
  publisher = "Wiley Online Library",
  volume    =  78,
  number    =  6,
  pages     = "765--791",
  month     =  dec,
  year      =  2016
}

@ARTICLE{Lee2015-ox,
  title     = "Do bank regulation and supervision matter?: International
               evidence from the recent financial crisis",
  author    = "Lee, Kangbok and Lu, Wenling",
  journal   = "Journal of Financial Economic Policy; Bingley",
  publisher = "Emerald Group Publishing Limited",
  volume    =  7,
  number    =  3,
  pages     = "275--288",
  year      =  2015,
  address   = "Bingley, United Kingdom",
  language  = "en"
}

@ARTICLE{Ben_Bouheni2014-ll,
  title     = "Banking regulation and supervision: can it enhance stability in
               Europe?",
  author    = "Ben Bouheni, Faten",
  journal   = "Journal of Financial Economic Policy; Bingley",
  publisher = "Emerald Group Publishing Limited",
  volume    =  6,
  number    =  3,
  pages     = "244--269",
  year      =  2014,
  address   = "France, United Kingdom",
  language  = "en"
}

@ARTICLE{Boudriga2009-uq,
  title     = "Banking supervision and nonperforming loans: a cross-country
               analysis",
  author    = "Boudriga, Abdelkader and Taktak, Neila Boulila and Jellouli,
               Sana",
  journal   = "Journal of Financial Economic Policy; Bingley",
  publisher = "Emerald Group Publishing Limited",
  volume    =  1,
  number    =  4,
  pages     = "286--318",
  year      =  2009,
  address   = "Bingley, United Kingdom",
  language  = "en"
}

@ARTICLE{French2016-zq,
  title    = "Financial literacy and over-indebtedness in low-income households",
  author   = "French, Declan and McKillop, Donal",
  abstract = "Financial literacy can explain a significant proportion of wealth
              inequality. Among the key components of financial literacy are
              numeracy and money management skills. Our study examines the
              relative importance of these components in the determination of
              consumer debt and household net worth among credit union members
              in socially disadvantaged areas. The main finding from our
              analysis is that money management skills are important
              determinants of financial outcomes but that numeracy has almost
              no role to play. This result adds to a recent US-based
              behavioural finance literature on the role of attention and
              planning in consumer finance. Findings are found to be robust
              when the sample is reduced to only those who have a clear role in
              household financial decision-making and also when controlling for
              potential endogeneity. Our findings have policy implications in
              the UK and elsewhere as credit unions across the world are
              important players in national financial literacy strategies.",
  journal  = "International Review of Financial Analysis",
  volume   =  48,
  pages    = "1--11",
  month    =  dec,
  year     =  2016,
  keywords = "Debt; Financial literacy; Money management"
}

@ARTICLE{Bhalotra2007-to,
  title     = "Is Child Work Necessary?*",
  author    = "Bhalotra, Sonia",
  abstract  = "This article investigates the hypothesis that child labour is
               compelled by poverty. It shows that a testable implication of
               this hypothesis is that the wage elasticity of child labour
               supply is negative. Using a large household survey for rural
               Pakistan, labour supply models for boys and girls in wage work
               are estimated. Conditioning on non-labour income and a range of
               demographic variables, the article finds a negative wage
               elasticity for boys and an elasticity that is insignificantly
               different from zero for girls. Thus, while boys appear to work
               on account of poverty compulsions, the evidence for girls is
               ambiguous.",
  journal   = "Oxf. Bull. Econ. Stat.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  69,
  number    =  1,
  pages     = "29--55",
  month     =  feb,
  year      =  2007,
  keywords  = "J22; J13; D12; O12"
}

@ARTICLE{Said2014-qi,
  title     = "Shadow banking: benefits, risks, and regulatory implications",
  author    = "Said, Z",
  abstract  = "Abstract Regular banks are no longer the center of attention in
               the financial system. After the global financial crisis of
               2007-2008, regulatory authorities are focusing on a new form of
               financial intermediation. This new form of credit intermediation
               is unregulated, market- based, and bank-alike. The crisis
               revealed that shadow banks have become one of the main actors in
               the financial system. Consequently, shadow banking system has
               the ability to ...",
  publisher = "Citeseer",
  year      =  2014
}

@BOOK{Mordel2018-qk,
  title     = "Prudential Liquidity Regulation in Banking: A Literature Review",
  author    = "Mordel, Adi and {Others}",
  publisher = "Bank of Canada",
  year      =  2018
}

@UNPUBLISHED{noauthor_2017-cq,
  title  = "The effectiveness of self-regulatory monitoring choices in
            community-based financial institutions",
  series = "BAR submission",
  year   =  2017
}

@ARTICLE{Hermalin1991-rc,
  title     = "The Effects of Board Composition and Direct Incentives on Firm
               Performance",
  author    = "Hermalin, Benjamin E and Weisbach, Michael S",
  abstract  = "This paper examines the relationship between top management
               compensation and corporate performance in public utilities.
               Previous researchers have argued that incentives for
               profitability are not needed in public utilities, since
               regulation provides assured profits. Earlier empirical work
               supports this claim. We reexamine this issue and provide several
               methodological improvements over prior studies. Our findings are
               consistent with the hypothesis that compensation packages for
               senior managers in public utilities are constructed to provide
               them with incentives to maximize stockholders' wealth.",
  journal   = "Financial Management",
  publisher = "[Financial Management Association International, Wiley]",
  volume    =  20,
  number    =  4,
  pages     = "101--112",
  year      =  1991
}

@UNPUBLISHED{Hermalin2001-rm,
  title       = "Boards of Directors as an Endogenously Determined Institution:
                 A Survey of the Economic Literature",
  author      = "Hermalin, Benjamin E and Weisbach, Michael S",
  abstract    = "This paper surveys the economic literature on boards of
                 directors. Although a legal requirement for many
                 organizations, boards are also an endogenously determined
                 governance mechanism for addressing agency problems inherent
                 to many organizations. Formal theory on boards of directors
                 has been quite limited to this point. Most empirical work on
                 boards has been aimed at answering one of three questions: 1)
                 How are board characteristics such as composition or size
                 related to profitability? 2) How do board characteristics
                 affect the observable actions of the board? 3) What factors
                 affect the makeup of boards and how they evolve over time? The
                 primary findings from the empirical literature on boards are:
                 Board composition is not related to corporate performance,
                 while board size has a negative relation to corporate
                 performance. Both board composition and size are correlated
                 with the board's decisions regarding CEO replacement,
                 acquisitions, poison pills, and executive compensation.
                 Finally, boards appear to evolve over time as a function of
                 the bargaining power of the CEO relative to the existing
                 directors. Firm performance, CEO turnover, and changes in
                 ownership structure appear to be important factors affecting
                 changes to boards.",
  number      =  8161,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  mar,
  year        =  2001
}

@ARTICLE{Boone2007-ag,
  title     = "The determinants of corporate board size and composition: An
               empirical analysis",
  author    = "Boone, Audra L and Casares Field, Laura and Karpoff, Jonathan M
               and Raheja, Charu G",
  abstract  = "Using a unique panel dataset that tracks corporate board
               development from a firm's IPO through 10 years later, we find
               that: (i) board size and independence increase as firms grow and
               diversify over time; (ii) board size---but not board
               independence---reflects a tradeoff between the firm-specific
               benefits and costs of monitoring; and (iii) board independence
               is negatively related to the manager's influence and positively
               related to constraints on that influence. These results indicate
               that economic considerations---in particular, the specific
               nature of the firm's competitive environment and managerial
               team---help explain cross-sectional variation in corporate board
               size and composition. Nonetheless, much of the variation in
               board structures remains unexplained, suggesting that
               idiosyncratic factors affect many individual boards'
               characteristics.",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  85,
  number    =  1,
  pages     = "66--101",
  month     =  jul,
  year      =  2007,
  keywords  = "Corporate boards; IPO; Board size; Board independence"
}

@ARTICLE{Zervas2014-am,
  title     = "The rise of the sharing economy: Estimating the impact of Airbnb
               on the hotel industry",
  author    = "Zervas, Georgios and Proserpio, Davide and Byers, John W",
  abstract  = "Peer-to-peer markets, collectively known as the sharing economy,
               have emerged as alternative suppliers of goods and services
               traditionally provided by long-established industries. The
               authors explore the economic impact of the sharing economy on
               incumbent",
  journal   = "J. Mark. Res.",
  publisher = "American Marketing Association",
  year      =  2014
}

@ARTICLE{Danielsson2001-mo,
  title    = "What Happens When You Regulate Risk? Evidence from a Simple
              Equilibrium Model",
  author   = "Danielsson, Jon and Zigrand, Jean-Pierre",
  abstract = "The implications of Value-at-Risk regulations are analyzed in a
              CARA--normal general equilibrium model. Financial institutions
              are heterogeneous in risk prefere",
  month    =  aug,
  year     =  2001,
  keywords = "General equilibrium; Value-at-risk; Risk regulation"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Danielsson2002-of,
  title    = "The emperor has no clothes: Limits to risk modelling",
  author   = "Dan{\i}ÃÅelsson, J{\'o}n",
  abstract = "This paper considers the properties of risk measures, primarily
              value-at-risk (VaR), from both internal and external (regulatory)
              points of view. It is argued that since market data is endogenous
              to market behavior, statistical analysis made in times of
              stability does not provide much guidance in times of crisis. In
              an extensive survey across data classes and risk models, the
              empirical properties of current risk forecasting models are found
              to be lacking in robustness while being excessively volatile. For
              regulatory use, the VaR measure may give misleading information
              about risk, and in some cases may actually increase both
              idiosyncratic and systemic risk.",
  journal  = "Journal of Banking \& Finance",
  volume   =  26,
  number   =  7,
  pages    = "1273--1296",
  month    =  jul,
  year     =  2002,
  keywords = "Value-at-risk; Capital adequacy; Financial regulations; Risk
              models"
}

@ARTICLE{Horvath2017-js,
  title     = "The Disturbing Interaction between Countercyclical Capital
               Requirements and Systemic Risk",
  author    = "Horv{\'a}th, B{\'a}lint L and Wagner, Wolf",
  abstract  = "We present a model in which flat (state-independent) capital
               requirements are undesirable because of shocks to bank capital.
               There is a rationale for countercyclical capital requirements
               that impose lower capital demands when aggregate bank capital is
               low. However, such capital requirements also have a cost as they
               increase systemic risk taking: by insulating banks against
               aggregate shocks (but not bank-specific ones), they create
               incentives to invest in correlated activities. As a result, the
               economy's sensitivity to shocks increases and systemic crises
               can become more likely. Capital requirements that directly
               incentivize banks to become less correlated dominate
               countercyclical policies as they reduce both systemic
               risk-taking and cyclicality.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  4,
  pages     = "1485--1511",
  month     =  jul,
  year      =  2017
}

@ARTICLE{Shive2017-gt,
  title     = "The Revolving Door for Financial Regulators",
  author    = "Shive, Sophie A and Forster, Margaret M",
  abstract  = "We investigate the motivations and effects of financial firms'
               hiring of former US financial regulatory employees. The number
               of top executives with regulatory experience per firm has
               increased 24\% over 2001--15, and hiring is associated with
               positive average announcement returns and a salary premium. In
               the quarter after hire, market and balance sheet measures of
               firm risk decrease significantly and measures of risk management
               activity increase, especially for hires from prudential
               regulators, who directly monitor financial firm risk. The
               absence of this result for unregulated firms and for exogenous
               shocks to regulatory experience suggests that firms hire
               ex-employees of their regulators when they perceive a need to
               reduce risk, consistent with a schooling hypothesis. We find
               little direct evidence of quid pro quo behavior in regulatory
               event frequency and fines.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  4,
  pages     = "1445--1484",
  month     =  jul,
  year      =  2017
}

@ARTICLE{Po-Chi_Chen2007-be,
  title    = "Productivity change in Taiwan's farmers' credit unions: a
              nonparametric risk-adjusted Malmquist approach",
  author   = "{Po-Chi Chen} and {Ming-Miin Yu} and {Ching-Cheng Chang} and
              {Shih-Hsun Hsu}",
  abstract = "This article proposes an extended three-stage DEA methodology
              similar to Fried et al. (2002) to improve the measurement of
              productivity growth when the assumption of free disposability of
              undesirable output does not apply. A directional distance
              function is used to construct adjusted Malmquist--Luenberger
              productivity indexes which simultaneously account for the impacts
              of undesirable outputs, environmental variables, and statistical
              noise. Panel data for 263 farmers' credit unions (FCUs) in Taiwan
              covering the 1998--2000 periods are employed to illustrate the
              advantages of this method. On average, the productivity of
              Taiwan's FCUs is found to have deteriorated over the 1998--2000
              period. Although an improvement in efficiency has been observed,
              the major reason for the deterioration is found to be due to the
              regression of technology. [ABSTRACT FROM AUTHOR]",
  journal  = "Agric. Econ.",
  volume   =  36,
  number   =  2,
  pages    = "221--231",
  month    =  mar,
  year     =  2007,
  keywords = "INDUSTRIAL productivity; CREDIT unions; COOPERATIVE banking
              industry; FARMERS; AGRICULTURISTS; TAIWAN; C61; D24; Directional
              distance function; Malmquist--Luenberger productivity index;
              Malmquist-Luenberger productivity index; Three-stage DEA;
              Undesirable outputs"
}

@ARTICLE{Banker2010-js,
  title    = "Differential impact of Korean banking system reforms on bank
              productivity",
  author   = "Banker, Rajiv D and Chang, Hsihui and Lee, Seok-Young",
  abstract = "Abstract: We study the impact of banking system reforms during a
              crisis following a period of undisciplined lending. Regulatory
              changes aimed at strengthening the banks' capital structure and
              risk management practices do not have a uniform impact on bank
              productivity, but rather favor financially sound or strategically
              privileged banks. We present evidence documenting the
              differential impact of regulatory reforms on Korean commercial
              bank productivity over the period 1995--2005. Average technical
              efficiency of banks decreased during the financial crisis of
              1997--1998. It improved following the subsequent bank
              restructuring and continued to improve through 2005. The capital
              adequacy ratio is positively associated with banks' technical
              efficiency. The non-performing loans ratio is negatively
              associated with technical efficiency. Both relationships are
              accentuated during the crisis but attenuated after the reforms.
              [Copyright \&y\& Elsevier]",
  journal  = "Journal of Banking \& Finance",
  volume   =  34,
  number   =  7,
  pages    = "1450--1460",
  month    =  jul,
  year     =  2010,
  keywords = "BANKING industry; ECONOMIC reform; CAPITAL structure; RISK
              management in business; PRODUCTION (Economic theory); FINANCIAL
              crises; LOANS; DATA envelopment analysis; KOREA; Bank
              restructuring; Capital adequacy ratio; Data Envelopment Analysis
              (DEA); Financial crisis; Korean commercial banks; Non-performing
              loans; Productivity"
}

@ARTICLE{Brooks2001-si,
  title    = "A trading strategy based on the lead--lag relationship between
              the spot index and futures contract for the {FTSE} 100",
  author   = "Brooks, Chris and Rew, Alistair G and Ritson, Stuart",
  abstract = "This paper examines the lead--lag relationship between the FTSE
              100 index and index futures price employing a number of time
              series models. Using 10-min observations from June 1996--1997, it
              is found that lagged changes in the futures price can help to
              predict changes in the spot price. The best forecasting model is
              of the error correction type, allowing for the theoretical
              difference between spot and futures prices according to the cost
              of carry relationship. This predictive ability is in turn
              utilised to derive a trading strategy which is tested under
              real-world conditions to search for systematic profitable trading
              opportunities. It is revealed that although the model forecasts
              produce significantly higher returns than a passive benchmark,
              the model was unable to outperform the benchmark after allowing
              for transaction costs.",
  journal  = "Int. J. Forecast.",
  volume   =  17,
  number   =  1,
  pages    = "31--44",
  month    =  jan,
  year     =  2001,
  keywords = "Stock index futures; FTSE 100; Error correction model; Trading
              rules; Forecasting accuracy; Cost of carry model"
}

@BOOK{Brooks2014-dk,
  title     = "Introductory Econometrics for Finance",
  author    = "Brooks, Chris",
  abstract  = "This bestselling and thoroughly classroom-tested textbook is a
               complete resource for finance students. A comprehensive and
               illustrated discussion of the most common empirical approaches
               in finance prepares students for using econometrics in practice,
               while detailed case studies help them understand how the
               techniques are used in relevant financial contexts. Worked
               examples from the latest version of the popular statistical
               software EViews guide students to implement their own models and
               interpret results. Learning outcomes, key concepts and
               end-of-chapter review questions (with full solutions online)
               highlight the main chapter takeaways and allow students to
               self-assess their understanding. Building on the successful
               data- and problem-driven approach of previous editions, this
               third edition has been updated with new data, extensive examples
               and additional introductory material on mathematics, making the
               book more accessible to students encountering econometrics for
               the first time. A companion website, with numerous student and
               instructor resources, completes the learning package.",
  publisher = "Cambridge University Press",
  month     =  may,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Carriero2015-nx,
  title    = "Bayesian {VARs}: Specification Choices and Forecast Accuracy",
  author   = "Carriero, Andrea and Clark, Todd E and Marcellino, Massimiliano",
  abstract = "In this paper we discuss how the point and density forecasting
              performance of Bayesian vector autoregressions (BVARs) is
              affected by a number of specification choices. We adopt as a
              benchmark a common specification in the literature, a BVAR with
              variables entering in levels and a prior modeled along the lines
              of Sims and Zha (International Economic Review 1998; 39:
              949--968). We then consider optimal choice of the tightness, of
              the lag length and of both; evaluate the relative merits of
              modeling in levels or growth rates; compare alternative
              approaches to h-step-ahead forecasting (direct, iterated and
              pseudo-iterated); discuss the treatment of the error variance and
              of cross-variable shrinkage; and assess rolling versus recursive
              estimation. Finally, we analyze the robustness of the results to
              the VAR size and composition (using also data for France, Canada
              and the UK, while the main analysis is for the USA). We obtain a
              large set of empirical results, but the overall message is that
              we find very small losses (and sometimes even gains) from the
              adoption of specification choices that make BVAR modeling quick
              and easy, in particular for point forecasting. This finding could
              therefore further enhance the diffusion of the BVAR as an
              econometric tool for a vast range of applications. Copyright
              \copyright{} 2013 John Wiley \& Sons, Ltd.",
  journal  = "J. Appl. Econ.",
  volume   =  30,
  number   =  1,
  pages    = "46--73",
  month    =  jan,
  year     =  2015
}

@ARTICLE{Harrison1976-iv,
  title     = "Bayesian Forecasting",
  author    = "Harrison, P J and Stevens, C F",
  abstract  = "This paper describes a Bayesian approach to forecasting. The
               principles of Bayesian forecasting are discussed and the formal
               inclusion of ``the forecaster'' in the forecasting system is
               emphasized as a major feature. The basic model, the dynamic
               linear model, is defined together with the Kalman filter
               recurrence relations and a number of model formulations are
               given. Multi-process models introduce uncertainty as to the
               underlying model itself, and this approach is described in a
               more general fashion than in our 1971 paper. Applications to
               four series are described in a sister paper. Although the
               results are far from exhaustive, the authors are convinced of
               the great benefits which the Bayesian approach offers to
               forecasters.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "[Royal Statistical Society, Wiley]",
  volume    =  38,
  number    =  3,
  pages     = "205--247",
  year      =  1976
}

@INCOLLECTION{West2004-qc,
  title     = "Bayesian Forecasting",
  booktitle = "Encyclopedia of Statistical Sciences",
  author    = "West, Mike",
  abstract  = "... is used routinely to process data and information, providing
               forecasts and inferences ... more general Bayesian sequential
               monitoring methods that are sensitive to forecast deterioration
               both ... techniques are central to the recent development of
               applied Bayesian forecasting systems ...",
  publisher = "John Wiley \& Sons, Inc.",
  year      =  2004
}

@ARTICLE{Geweke2006-im,
  title     = "Chapter 1 Bayesian Forecasting",
  author    = "Geweke, John and Whiteman, Charles",
  abstract  = "Bayesian forecasting is a natural product of a Bayesian approach
               to inference. The Bayesian approach in general requires explicit
               formulation of a model, and conditioning on known quantities, in
               order to draw inferences about unknown ones. In Bayesian
               forecasting, one simply takes a subset of the unknown quantities
               to be future values of some variables of interest. This chapter
               presents the principles of Bayesian forecasting, and describes
               recent advances in computational capabilities for applying them
               that have dramatically expanded the scope of applicability of
               the Bayesian approach. It describes historical developments and
               the analytic compromises that were necessary prior to recent
               developments, the application of the new procedures in a variety
               of examples, and reports on two long-term Bayesian forecasting
               exercises.",
  journal   = "Handbook of Economic Forecasting",
  publisher = "Elsevier",
  volume    =  1,
  pages     = "3--80",
  month     =  jan,
  year      =  2006,
  keywords  = "Markov chain Monte Carlo; predictive distribution; probability
               forecasting; simulation; vector autoregression"
}

@BOOK{Pole1994-xu,
  title     = "Applied Bayesian Forecasting and Time Series Analysis",
  author    = "Pole, Andy and West, Mike and Harrison, Jeff",
  abstract  = "Practical in its approach, Applied Bayesian Forecasting and Time
               Series Analysis provides the theories, methods, and tools
               necessary for forecasting and the analysis of time series. The
               authors unify the concepts, model forms, and modeling
               requirements within the framework of the dynamic linear mode
               (DLM). They include a complete theoretical development of the
               DLM and illustrate each step with analysis of time series data.
               Using real data sets the authors: Explore diverse aspects of
               time series, including how to identify, structure, explain
               observed behavior, model structures and behaviors, and interpret
               analyses to make informed forecasts Illustrate concepts such as
               component decomposition, fundamental model forms including
               trends and cycles, and practical modeling requirements for
               routine change and unusual events Conduct all analyses in the
               BATS computer programs, furnishing online that program and the
               more than 50 data sets used in the text The result is a clear
               presentation of the Bayesian paradigm: quantified subjective
               judgements derived from selected models applied to time series
               observations. Accessible to undergraduates, this unique volume
               also offers complete guidelines valuable to researchers,
               practitioners, and advanced students in statistics, operations
               research, and engineering.",
  publisher = "CRC Press",
  month     =  sep,
  year      =  1994,
  language  = "en"
}

@ARTICLE{Buhlmann2002-aj,
  title     = "Bootstraps for Time Series",
  author    = "B{\"u}hlmann, Peter",
  abstract  = "We review and compare block, sieve and local bootstraps for time
               series and thereby illuminate theoretical aspects of the
               procedures as well as their performance on finite-sample data.
               Our view is selective with the intention of providing a new and
               fair picture of some particular aspects of bootstrapping time
               series. The generality of the block bootstrap is contrasted with
               sieve bootstraps. We discuss implementational advantages and
               disadvantages. We argue that two types of sieve often outperform
               the block method, each of them in its own important niche,
               namely linear and categorical processes. Local bootstraps,
               designed for nonparametric smoothing problems, are easy to use
               and implement but exhibit in some cases low performance.",
  journal   = "Stat. Sci.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  17,
  number    =  1,
  pages     = "52--72",
  year      =  2002
}

@ARTICLE{Rosenthal2012-cd,
  title     = "Modeling Trade Direction",
  author    = "Rosenthal, Dale W R",
  abstract  = "I propose a modeling approach to classifying trades as buys or
               sells. Modeled classifications consider information strengths,
               microstructure effects, and classification correlations. I also
               propose estimators for quotes prevailing at trade time.
               Comparisons using 2800 U.S. stocks show modeled classifications
               are 1\%--2\% more accurate than current methods across dates,
               sectors, and the spread. For Nasdaq and New York Stock Exchange
               stocks, 1\% and 1.3\% of improvement comes from using
               information strengths; 0.9\% and 0.7\% of improvement comes from
               estimating quotes. I find evidence past studies used unclean
               data and indications of short-term price predictability. The
               method may help detect destabilizing order flow.",
  journal   = "Journal of Financial Econometrics",
  publisher = "Oxford University Press",
  volume    =  10,
  number    =  2,
  pages     = "390--415",
  month     =  mar,
  year      =  2012
}

@ARTICLE{Aiyar2014-tx,
  title    = "Does {Macro-Prudential} Regulation Leak? Evidence from a {UK}
              Policy Experiment",
  author   = "Aiyar, Shekhar and Calomiris, Charles W and Wieladek, Tomasz",
  abstract = "The regulation of bank capital as a means of smoothing the credit
              cycle is a central element of forthcoming macro-prudential
              regimes internationally. For such regulation to be effective in
              controlling the aggregate supply of credit it must be the case
              that: (i) changes in capital requirements affect loan supply by
              regulated banks, and (ii) unregulated substitute sources of
              credit are unable to offset changes in credit supply by affected
              banks. This paper examines micro evidence---lacking to date---on
              both questions, using a unique data set. In the UK, regulators
              have imposed time-varying, bank-specific minimum capital
              requirements since Basel I. It is found that regulated banks
              (UK-owned banks and resident foreign subsidiaries) reduce lending
              in response to tighter capital requirements. But unregulated
              banks (resident foreign branches) increase lending in response to
              tighter capital requirements on a relevant reference group of
              regulated banks. This ``leakage'' is substantial, amounting to
              about one-third of the initial impulse from the regulatory
              change.",
  journal  = "J. Money Credit Bank.",
  volume   =  46,
  number   = "s1",
  pages    = "181--214",
  month    =  feb,
  year     =  2014,
  keywords = "E32; E51; F30; G21; G28; macro-prudential regulation; credit
              cycles; regulatory arbitrage; transmission mechanism; bank
              lending; instrumental variables"
}

@ARTICLE{Bradley1995-tx,
  title     = "Bayesian Model Choice via Markov Chain Monte Carlo Methods",
  author    = "Bradley, P",
  abstract  = "SUMMARY Markov chain Monte Carlo (MCMC) integration methods
               enable the fitting of models of virtually unlimited complexity,
               and as such have revolutionized the practice of Bayesian data
               analysis. However, comparison across models may not proceed in a
               completely analogous fashion, owing to violations of the
               conditions sufficient to ensure convergence of the Markov chain.
               In this paper we present a framework for Bayesian ...",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "wwwf.imperial.ac.uk",
  volume    =  57,
  number    =  3,
  pages     = "473--484",
  year      =  1995
}

@ARTICLE{Bagehot1971-nm,
  title     = "The Only Game in Town",
  author    = "Bagehot, Walter",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  27,
  number    =  2,
  pages     = "12--14",
  month     =  mar,
  year      =  1971
}

@ARTICLE{De_Boef2008-ki,
  title    = "Taking Time Seriously",
  author   = "De Boef, Suzanna and Keele, Luke",
  abstract = "Dramatic world change has stimulated interest in research
              questions about the dynamics of politics. We have seen increases
              in the number of time series data sets and the length of typical
              time series. But three shortcomings are prevalent in published
              time series analysis. First, analysts often estimate models
              without testing restrictions implied by their specification.
              Second, researchers link the theoretical concept of equilibrium
              with cointegration and error correction models. Third, analysts
              often do a poor job of interpreting results. The consequences
              include weak connections between theory and tests, biased
              estimates, and incorrect inferences. We outline techniques for
              estimating linear dynamic regressions with stationary data and
              weakly exogenous regressors. We recommend analysts (1) start with
              general dynamic models and test restrictions before adopting a
              particular specification and (2) use the wide array of
              information available from dynamic specifications. We illustrate
              this strategy with data on Congressional approval and tax rates
              across OECD countries.",
  journal  = "Am. J. Pol. Sci.",
  volume   =  52,
  number   =  1,
  pages    = "184--200",
  month    =  jan,
  year     =  2008
}

@ARTICLE{Wagenvoort2006-fv,
  title     = "A Recursive Thick Frontier Approach to Estimating Production
               Efficiency*",
  author    = "Wagenvoort, Rien J L M and Schure, Paul H",
  abstract  = "We introduce a new panel data estimation technique for
               production and cost functions, the recursive thick frontier
               approach (RTFA). RTFA has two advantages over existing
               econometric frontier methods. First, technical inefficiency is
               allowed to be dependent on the explanatory variables of the
               frontier model. Secondly, RTFA does not hinge on distributional
               assumptions on the inefficiency component of the error term. We
               show by means of simulation experiments that RTFA outperforms
               the popular stochastic frontier approach and the `within'
               ordinary least squares estimator for realistic parameterizations
               of a productivity model. Although RTFAs formal statistical
               properties are unknown, we argue, based on these simulation
               experiments, that RTFA is a useful complement to existing
               methods.",
  journal   = "Oxf. Bull. Econ. Stat.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  68,
  number    =  2,
  pages     = "183--201",
  month     =  apr,
  year      =  2006,
  keywords  = "C23; D24"
}

@ARTICLE{Gneiting2007-xx,
  title     = "Strictly Proper Scoring Rules, Prediction, and Estimation",
  author    = "Gneiting, Tilmann and Raftery, Adrian E",
  abstract  = "Scoring rules assess the quality of probabilistic forecasts, by
               assigning a numerical score based on the predictive distribution
               and on the event or value that materializes. A scoring rule is
               proper if the forecaster maximizes the expected score for an
               observation drawn from the distributionF if he or she issues the
               probabilistic forecast F, rather than G ? F. It is strictly
               proper if the maximum is unique. In prediction problems, proper
               scoring rules encourage the forecaster to make careful
               assessments and to be honest. In estimation problems, strictly
               proper scoring rules provide attractive loss and utility
               functions that can be tailored to the problem at hand. This
               article reviews and develops the theory of proper scoring rules
               on general probability spaces, and proposes and discusses
               examples thereof. Proper scoring rules derive from convex
               functions and relate to information measures, entropy functions,
               and Bregman divergences. In the case of categorical variables,
               we prove a rigorous version of the Savage representation.
               Examples of scoring rules for probabilistic forecasts in the
               form of predictive densities include the logarithmic, spherical,
               pseudospherical, and quadratic scores. The continuous ranked
               probability score applies to probabilistic forecasts that take
               the form of predictive cumulative distribution functions. It
               generalizes the absolute error and forms a special case of a new
               and very general type of score, the energy score. Like many
               other scoring rules, the energy score admits a kernel
               representation in terms of negative definite functions, with
               links to inequalities of Hoeffding type, in both univariate and
               multivariate settings. Proper scoring rules for quantile and
               interval forecasts are also discussed. We relate proper scoring
               rules to Bayes factors and to cross-validation, and propose a
               novel form of cross-validation known as random-fold
               cross-validation. A case study on probabilistic weather
               forecasts in the North American Pacific Northwest illustrates
               the importance of propriety. We note optimum score approaches to
               point and quantile estimation, and propose the intuitively
               appealing interval score as a utility function in interval
               estimation that addresses width as well as coverage.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  102,
  number    =  477,
  pages     = "359--378",
  month     =  mar,
  year      =  2007
}

@ARTICLE{DeYoung2009-un,
  title     = "Mergers and Acquisitions of Financial Institutions: A Review of
               the Post-2000 Literature",
  author    = "DeYoung, Robert and Evanoff, Douglas D and Molyneux, Philip",
  abstract  = "This paper provides a review of the recent financial institution
               mergers and acquisition (M\&A) literature covering over 150
               studies. Several robust themes emerge in the post-2000
               literature. North American bank mergers are (or can be)
               efficiency improving, although the event-study literature
               presents a mixed picture regarding stockholder wealth creation.
               In contrast, European bank mergers appear to have resulted in
               both efficiency gains and stockholder value enhancement. There
               is robust evidence linking high CEO compensation to merger
               activity and strong implications that deals can be motivated by
               the desire to obtain `too-big-to-fail' status and reap the
               associated subsidies. Evidence on the impact of both geographic
               and product diversification via merger is mixed. There is
               growing evidence that financial institution M\&As can adversely
               impact certain types of borrowers, depositors, and other
               external stakeholders.",
  journal   = "J Financ Serv Res",
  publisher = "Springer US",
  volume    =  36,
  number    = "2-3",
  pages     = "87--110",
  month     =  dec,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Caiazza2012-up,
  title    = "What do bank acquirers want? Evidence from worldwide bank {M\&A}
              targets",
  author   = "Caiazza, Stefano and Clare, Andrew and Pozzolo, Alberto Franco",
  abstract = "What drives bankers to create larger and larger, often
              multinational banking groups? In this paper we investigate
              whether the targets in cross-border bank M\&As are materially
              different from those banks targeted in domestic M\&A deals. The
              main message of this paper is that, with few exceptions, domestic
              and foreign investors target similar banks. In particular, and
              contrary to what one might expect, bank size does not have a
              different effect on the probability of being a domestic or a
              cross-border target, instead it has a positive and highly
              significant effect in both cases. We find that the main
              differences between national and international M\&As are the
              characteristics of the countries where the banks operate.",
  journal  = "Journal of Banking \& Finance",
  volume   =  36,
  number   =  9,
  pages    = "2641--2659",
  month    =  sep,
  year     =  2012,
  keywords = "M\&A; Bank; Bank internationalisation"
}

@ARTICLE{Goddard2014-rc,
  title     = "The Size Distribution of {US} Banks and Credit Unions",
  author    = "Goddard, John and Liu, Hong and Mckillop, Donal and Wilson, John
               O S",
  abstract  = "AbstractThis study examines the firm size distribution of US
               banks and credit unions. A truncated lognormal distribution
               describes the size distribution, measured using assets data, of
               a large population of small, community-based commercial banks.
               The size distribution of a smaller but increasingly dominant
               cohort of large banks, which operate a high-volume low-cost
               retail banking model, exhibits power-law behaviour. There is a
               progressive increase in skewness over time, and Zipf?s Law is
               rejected as a descriptor of the size distribution in the upper
               tail. By contrast, the asset size distribution of the population
               of credit unions conforms closely to the lognormal distribution.",
  journal   = "International Journal of the Economics of Business",
  publisher = "Routledge",
  volume    =  21,
  number    =  1,
  pages     = "139--156",
  month     =  jan,
  year      =  2014
}

@ARTICLE{McKee2016-rx,
  title     = "Determinants of recent structural change for small asset {U.S}.
               credit unions",
  author    = "McKee, Gregory and Kagan, Albert",
  abstract  = "Credit union numbers in the United States have declined since
               1969. Changes in financial performance of U.S. credit unions
               have contributed to this decline. This paper discusses how small
               asset credit unions income, deposits, and loans have evolved
               during the period 1994--2011, with particular emphasis on
               2008--2011. The effect of asset growth on credit unions with
               below \$10 million in assets is distinct when compared with
               average asset sized U.S. credit unions . These credit unions
               face structural barriers to increase assets and will require
               unique managerial responses to macroeconomic changes in order to
               continue to provide member focused financial services
               profitably.",
  journal   = "Rev Quant Finan Acc",
  publisher = "Springer US",
  volume    =  47,
  number    =  3,
  pages     = "775--795",
  month     =  oct,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Ogura2014-fx,
  title     = "Bank Consolidation and Soft Information Acquisition in Small
               Business Lending",
  author    = "Ogura, Yoshiaki and Uchida, Hirofumi",
  abstract  = "We empirically examine the impact of bank consolidation on bank
               acquisition of soft information about borrowers. Using a dataset
               of small business financing, we find that mergers of small banks
               have a negative impact on soft information acquisition, whereas
               mergers of large banks have no impact. We also find some
               evidence that an increase in organizational complexity upon a
               merger, rather than a post-merger cost-cut, is likely to cause a
               negative and significant impact on soft information acquisition
               by small banks. These findings are consistent with the
               organizational theory that predicts a comparative advantage of
               simple and flat organizations in acquiring and processing soft
               information.",
  journal   = "J Financ Serv Res",
  publisher = "Springer US",
  volume    =  45,
  number    =  2,
  pages     = "173--200",
  month     =  apr,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Damar2010-fs,
  title    = "Credit Union Membership and Use of Internet Banking Technology",
  author   = "Damar, Evren and Hunnicutt, Lynn",
  abstract = "What makes households use internet banking? Bank adoption of
              internet banking technology has been widely considered, but
              relatively few papers address consumer usage of internet banking.
              This study looks at the determinants of internet banking usage
              among credit union members in the Western United States. We use
              call report data from the National Credit Union Administration to
              calculate the rate of internet banking usage among a credit
              union's members, which allows us to examine whether variations in
              institutional characteristics, local economic conditions and
              membership criteria have an impact on the internet usage rates
              among members of different credit unions. We find that members in
              credit unions that were early internet technology adopters have
              higher usage rates, and that the contribution to usage rates
              varies among types of online services offered.",
  journal  = "B. E. J. Econom. Anal. Policy",
  volume   =  10,
  number   =  1,
  month    =  jan,
  year     =  2010
}

@ARTICLE{Carmassi2013-eq,
  title     = "Living wills and cross-border resolution of systemically
               important banks",
  author    = "Carmassi, Jacopo and Herring, Richard John",
  abstract  = "Purpose -- The purpose of this paper is to analyze whether and
               how ``living wills'' and public disclosure of such resolution
               plans contribute to market discipline and the effective
               resolution of too big and too complex to fail banks.
               Design/methodology/approach -- The disorderly collapse of Lehman
               Brothers is analyzed. Large, systemically important banks are
               now required to prepare resolution plans (living wills). In the
               USA, parts of the living wills must be disclosed to the public.
               The public component is analyzed with respect to contribution to
               market discipline and effective resolution of banks considered
               too big and complex to fail. In a statistical analysis of the
               publicly available section of living wills, this information is
               contrasted with legislative requirements. Findings -- The
               analysis of public disclosures of resolution plans shows that
               they are insufficient to facilitate market discipline and, in
               some instances, fail to enhance public understanding of the
               financial institution and its business. When coupled with the
               uncertainty over how an internationally active financial
               institution will be resolved, the paper concludes that these
               reforms will do little to reduce market expectations that some
               financial firms are simply too big or too complex to fail.
               Research limitations/implications -- A very small data set and
               the necessity of cross-checking the authors' observations with
               all publicly available sources. The authors have also tried to
               infer a purpose for public disclosure of parts of resolution
               plans. The authorities are remarkably vague on the issue and so
               the authors have assumed they actually did have a specific
               intent that would strengthen the system. Practical implications
               -- The inference from the publicly available portion of living
               wills is that the authorities are a very long way from
               abolishing too-big-to-fail. Originality/value -- So far as the
               authors know, this is the first in-depth analysis of the
               information available in the public sections of living wills.",
  journal   = "Journal of Financial Economic Policy; Bingley",
  publisher = "Emerald Group Publishing Limited",
  volume    =  5,
  number    =  4,
  pages     = "361--387",
  year      =  2013,
  address   = "Bingley, United Kingdom",
  language  = "en"
}

@ARTICLE{Mcalevey2010-wy,
  title     = "{NEW} {ZEALAND} {CREDIT} {UNION} {MERGERS}: {NEW} {ZEALAND}
               {CREDIT} {UNION} {MERGERS}",
  author    = "Mcalevey, Lynn and Sibbald, Alexander and Tripe, David",
  abstract  = "ABSTRACT**: Research into the benefits of mergers in small
               financial institutions, in particular credit unions, is sparse.
               This study helps to fill this gap by analyzing recent intense
               merger activity in New Zealand credit unions. The major driver
               for these mergers was not the usual reason of attempting to
               increase efficiency for competitive purposes but rather enforced
               government action. Data envelopment analysis is used to explore
               changes in efficiency in merged credit unions between 1996 and
               2001. Those credit unions not involved in merger activity are
               used as a control group. Overall, credit unions have become more
               efficient over the period, notably in those that undertook
               mergers. The Malmquist index indicates significant technological
               progress over the period but a slight regression in terms of
               efficiency.",
  journal   = "Annals of Public and Cooperative Economics",
  publisher = "Blackwell Publishing Ltd",
  volume    =  81,
  number    =  3,
  pages     = "423--444",
  month     =  aug,
  year      =  2010,
  keywords  = "Production; Cost; Capital; Capital, Total Factor, and
               Multifactor Productivity; Capacity D24; Banks; Depository
               Institutions; Micro Finance Institutions; Mortgages G21;
               Mergers; Acquisitions; Restructuring; Voting; Proxy Contests;
               Corporate Governance G34; Firm Performance: Size,
               Diversification, and Scope L25; Technological Change: Choices
               and Consequences; Diffusion Processes O33"
}

@ARTICLE{Liu2012-sx,
  title     = "Estimating {Multiple-Input--Multiple-Output} Production
               Functions with an Analysis of Credit Unions",
  author    = "Liu, Long and Ondrich, Jan and Ruggerio, John",
  abstract  = "In this article, we develop a methodology to estimate production
               functions characterized by multiple inputs and multiple outputs.
               Aggregate output is estimated assuming a piecewise-linear
               production possibility set and by measuring distance to the
               boundary using nonparametric techniques. The parameters of the
               production function are then estimated in a second-stage using
               ordinary least squares (OLS). The primary advantage of this
               approach is the ability to measure the output aggregate
               nonparametrically. The approach developed in this article is
               applied to estimate credit union production. An alternative
               approach developed by Vinod (1968) specifies an extended
               Cobb-Douglas equation and estimates the parameters using
               canonical correlation. Our new approach is compared to Vinod's
               canonical correlation approach.",
  journal   = "Appl. Econ.",
  publisher = "Taylor \& Francis",
  volume    =  44,
  number    = "10-12",
  pages     = "1583--1589",
  month     =  apr,
  year      =  2012,
  keywords  = "Model Construction and Estimation C51; Production; Cost;
               Capital; Capital, Total Factor, and Multifactor Productivity;
               Capacity D24; Banks; Depository Institutions; Micro Finance
               Institutions; Mortgages G21"
}

@ARTICLE{Franck2014-us,
  title     = "A Theoretical Analysis of the Influence of Money Injections on
               Risk Taking in Football Clubs",
  author    = "Franck, Egon and Lang, Markus",
  abstract  = "Third party money injections of benefactors (sugar daddies)
               function as a bailout mechanism for otherwise insolvent football
               clubs. The successful implementation of the new UEFA ``financial
               fair play'' regulations will abrogate this bailout mechanism. We
               develop a theoretical model of a representative club and a sugar
               daddy to study the adverse incentive effects produced by the
               money injections of sugar daddies. We show that the existence of
               a sugar daddy induces the club to choose a riskier investment
               strategy and the more the sugar daddy commits to bailout the
               club, the more the clubs' optimal level of riskiness increases.
               Moreover, a private sugar daddy bails out the club less often
               than a public sugar daddy. Our model further shows that a
               ``too-big-to-fail'' phenomenon exists because it is optimal to
               always bailout a club if its market size is sufficiently large.
               Finally, we derive conditions under which the FFP and the
               pre-FFP regulations, respectively, are desirable from a welfare
               perspective.",
  journal   = "Scott. J. Polit. Econ.",
  publisher = "University of Zurich, Institute for Strategy and Business
               Economics (ISU)",
  volume    =  61,
  number    =  4,
  pages     = "430--454",
  month     =  sep,
  year      =  2014,
  keywords  = "FINANCE; MANAGEMENT; RISK management in business; BAILOUTS
               (Finance); SOCCER teams; SOCCER -- Economic aspects; UNION of
               European Football Associations"
}

@ARTICLE{Aiello2017-tb,
  title     = "{ON} {THE} {SOURCES} {OF} {HETEROGENEITY} {IN} {BANKING}
               {EFFICIENCY} {LITERATURE}: {HETEROGENEITY} {IN} {BANKING}
               {EFFICIENCY} {LITERATURE}",
  author    = "Aiello, Francesco and Bonanno, Graziella",
  abstract  = "One learns two main lessons from studying the great quantity of
               banking efficiency literature. These lessons regard the
               heterogeneity in results and the absence of a comprehensive
               review aimed at understanding the reasons for this variability.
               Surprisingly, although this issue is well-known, it has not been
               systematically analyzed before. In order to fill this gap, we
               perform a Meta-Regression-Analysis (MRA) by examining 1,661
               efficiency scores retrieved from 120 papers published over the
               period 2000-2014. The meta-regression is estimated by using the
               Random Effects Multilevel Model (REML), because it controls for
               within-study and between-study heterogeneity. The analysis
               yields four main results. Firstly, parametric methods yield
               lower levels of banking efficiency than nonparametric studies.
               This holds true even after controlling for the approach used in
               selecting the inputs and outputs of the frontier. Secondly, we
               show that banking efficiency is highest when using the value
               added approach, followed by estimates from studies based on the
               intermediation method, whereas those based on the hybrid
               approach are the lowest. Thirdly, efficiency scores also depend
               on the quality of studies and on the number of observations and
               variables used in the primary papers. As far as the effects of
               sample size, dimension and quality of papers are concerned,
               there are significant differences in sign and magnitude between
               parametric and nonparametric studies. Finally, cost efficiency
               is found to be higher than profit and production efficiency.
               Interestingly, MRA results are robust to the potential outliers
               in efficiency and sample size distributions",
  journal   = "J. Econ. Surv.",
  publisher = "mpra.ub.uni-muenchen.de",
  volume    =  13,
  pages     = "355",
  month     =  jan,
  year      =  2017,
  keywords  = "Banking; Efficiency; Frontier models; Meta-analysis; Regulation;
               Study design"
}

@ARTICLE{Bun2010-qk,
  title     = "The weak instrument problem of the system {GMM} estimator in
               dynamic panel data models",
  author    = "Bun, Maurice J G and Windmeijer, Frank",
  abstract  = "Summary The system GMM estimator for dynamic panel data models
               combines moment conditions for the model in first differences
               with moment conditions for the model in levels. It has been
               shown to improve on the GMM estimator in the first differenced
               model in terms of bias and root mean squared error. However, we
               show in this paper that in the covariance stationary panel data
               AR(1) model the expected values of the concentration parameters
               in the differenced and levels equations for the cross-section at
               time t are the same when the variances of the individual
               heterogeneity and idiosyncratic errors are the same. This
               indicates a weak instrument problem also for the equation in
               levels. We show that the 2SLS biases relative to that of the OLS
               biases are then similar for the equations in differences and
               levels, as are the size distortions of the Wald tests. These
               results are shown to extend to the panel data GMM estimators.",
  journal   = "Econom. J.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  13,
  number    =  1,
  pages     = "95--126",
  month     =  feb,
  year      =  2010,
  keywords  = "Dynamic panel data; System GMM; Weak instruments"
}

@ARTICLE{Fethi2010-ts,
  title     = "Assessing bank efficiency and performance with operational
               research and artificial intelligence techniques: A survey",
  author    = "Fethi, Meryem Duygun and Pasiouras, Fotios",
  abstract  = "This paper presents a comprehensive review of 196 studies which
               employ operational research (O.R.) and artificial intelligence
               (A.I.) techniques in the assessment of bank performance. Several
               key issues in the literature are highlighted. The paper also
               points to a number of directions for future research. We first
               discuss numerous applications of data envelopment analysis which
               is the most widely applied O.R. technique in the field. Then we
               discuss applications of other techniques such as neural
               networks, support vector machines, and multicriteria decision
               aid that have also been used in recent years, in bank failure
               prediction studies and the assessment of bank creditworthiness
               and underperformance.",
  journal   = "Eur. J. Oper. Res.",
  publisher = "Elsevier B.V.",
  volume    =  204,
  number    =  2,
  pages     = "189--198",
  month     =  jul,
  year      =  2010,
  keywords  = "Artificial intelligence; Banks; Data envelopment analysis;
               Efficiency; Operational research; Literature review"
}

@ARTICLE{Laeven2009-ck,
  title    = "Bank governance, regulation and risk taking",
  author   = "Laeven, Luc and Levine, Ross",
  abstract = "This paper conducts the first empirical assessment of theories
              concerning risk taking by banks, their ownership structures, and
              national bank regulations. We focus on conflicts between bank
              managers and owners over risk, and we show that bank risk taking
              varies positively with the comparative power of shareholders
              within the corporate governance structure of each bank. Moreover,
              we show that the relation between bank risk and capital
              regulations, deposit insurance policies, and restrictions on bank
              activities depends critically on each bank's ownership structure,
              such that the actual sign of the marginal effect of regulation on
              risk varies with ownership concentration. These findings show
              that the same regulation has different effects on bank risk
              taking depending on the bank's corporate governance structure.",
  journal  = "J. financ. econ.",
  volume   =  93,
  number   =  2,
  pages    = "259--275",
  series   = "Working Paper Series",
  month    =  aug,
  year     =  2009,
  keywords = "Corporate governance; Bank regulation; Financial institutions;
              Financial risk"
}

@ARTICLE{Bye1975-dk,
  title    = "Determination of 3-(5-tetrazolyl) thioxanthone 10,10-dioxide in
              human plasma, urine and faeces",
  author   = "Bye, A and Land, G",
  abstract = "A gas chromatographic method is described for assay of
              3-(5-tetrazolyl) thioxanthone 10,10-dioxide (BW 59C) in human
              plasma, urine and faeces. After extraction into
              1,2-dichloroethane from alkaline medium the compound is converted
              to the heptafluorobutyrate derivative which is injected into a
              gas chromatograph and measured using a 63Ni electron capture
              detector. The assay produces a linear calibration curve over the
              range 0-30 mug/ml when the internal standard method is used.
              Reproducibility is good and sensitivity down to 1 ng injected on
              column is possible. The method has been used to investigate the
              pharmacokinetic properties of BW 59C in man and has been
              semi-automated by the use of an autosampler and dedicated
              computer.",
  journal  = "J. Chromatogr.",
  volume   =  115,
  number   =  1,
  pages    = "93--99",
  month    =  dec,
  year     =  1975,
  keywords = "Momentum; Factor models",
  language = "en"
}

@ARTICLE{Veron2015-ry,
  title   = "Europe's radical banking union",
  author  = "V{\'e}ron, Nicolas and {Others}",
  journal = "Bruegel Essay and Lecture Series",
  volume  =  6,
  year    =  2015
}

@ARTICLE{Sueyoshi2015-za,
  title    = "Environmental assessment on coal-fired power plants in {U.S}.
              north-east region by {DEA} non-radial measurement",
  author   = "Sueyoshi, Toshiyuki and Goto, Mika",
  abstract = "This study discusses a new use of DEA (Data Envelopment Analysis)
              environmental assessment to measure unified (operational and
              environmental) and scale efficiencies among inputs, desirable and
              undesirable outputs. In particular, the measurement of scale
              efficiency is discussed by two non-radial models. That is a new
              methodological contribution. To discuss these efficiency
              measures, this study first examines a concept of disposability
              from the perspective of corporate strategies to adapt a
              regulation change on undesirable outputs. The concept is
              separated into natural and managerial disposability. After
              discussing how to measure the degree of scale efficiency within
              the non-radial approach, this study applies the proposed DEA
              environmental assessment to measure the performance of coal-fired
              power plants in the U.S. north-east region. The region has been
              long producing a large amount of coal from the Appalachian
              Mountains. The coal mining industry has supported U.S. energy
              utility and other industries. Because of the long history, the
              quality of coal became worse and the coal-fired power plants have
              been producing a large amount of undesirable gases. This study
              has statistically confirmed that there is a significant
              difference between the two types (BIT: bituminous coal and SUB:
              subbituminous coal) of coal-fired power plants in terms of their
              unified efficiency measures, including their scale efficiencies,
              under the concept of managerial disposability (the first
              priority: environment performance and the second priority:
              operational performance). In contrast, under the natural
              disposability (the first priority: operational performance and
              the second priority: environmental performance), this study
              cannot find such a statistical significance between them. The
              fact, in which BIT outperforms SUB in terms of their unified and
              scale efficiencies, suggests the policy implication that these
              power plants need to shift their coal combustions from SUB to BIT
              in the United States. Besides the empirical finding, this study
              cannot confirm the other hypothesis on whether coal-fired power
              plants with small operation (less than 50\% in plant capacity
              factor) outperform ones with large operation (more than 50\% in
              plant capacity factor), and vice versa, in terms of their unified
              and scale efficiency measures under natural and managerial
              disposability. An exception is found in environmental performance
              under variable returns to scale. The rationale is because their
              plant operations are frequently monitored by regulatory agencies.
              As a consequence, this study cannot find such a statistical
              difference between them on operational performance. This result
              implies that the regulation on coal-fired power plants has been
              effective on their unified performance but large power plants may
              have a potential to improve their environmental performance.",
  journal  = "Energy Econ.",
  volume   =  50,
  pages    = "125--139",
  month    =  jul,
  year     =  2015,
  keywords = "Electricity; DEA; Environmental assessment; PJM; Coal-fired power
              plant; Efficiency"
}

@ARTICLE{Lee2015-tu,
  title     = "Directional shadow price estimation of {CO} 2, {SO} 2 and {NO} x
               in the United States coal power industry 1990--2010",
  author    = "Lee, Chia-Yen and Zhou, Peng",
  journal   = "Energy Econ.",
  publisher = "Elsevier",
  volume    =  51,
  pages     = "493--502",
  year      =  2015
}

@BOOK{Goldblatt2008-hx,
  title     = "The ball is round: A global history of soccer",
  author    = "Goldblatt, David",
  publisher = "Penguin",
  year      =  2008
}

@TECHREPORT{Deloitte2017-jp,
  title       = "Annual Review of Football Finance",
  author      = "{Deloitte}",
  institution = "Deloitte",
  year        =  2017
}

@ARTICLE{Stigler1972-be,
  title   = "The Law and Economics of Public Policy: A Plea to the Scholars",
  author  = "Stigler, George J",
  journal = "J. Legal Stud.",
  volume  =  1,
  number  =  1,
  pages   = "1--12",
  year    =  1972
}

@ARTICLE{Stigler1982-gj,
  title     = "Economists and public policy",
  author    = "Stigler, G J",
  abstract  = "IN THIS ESSAY I attack a highly vulnerable target and do so with
               well-placed confidence, because the target is myself. More
               precisely, my target is a paper I once wrote (`` The Economist
               and the State'') in which I proposed two main theses. The first
               was that",
  journal   = "Regulation",
  publisher = "HeinOnline",
  volume    =  6,
  pages     = "13",
  year      =  1982
}

@ARTICLE{Berker2014-rx,
  title     = "Tie-breaking in round-robin soccer tournaments and its influence
               on the autonomy of relative rankings: {UEFA} vs. {FIFA}
               regulations",
  author    = "Berker, Yannick",
  abstract  = "Research questionMany sports tournaments, or tournament stages,
               are held in round-robin format. In establishing standings,
               tie-breaker criteria are often required to assign unique ranks
               to competitors that are equal on points. During the 2012 Union
               of European Football Associations (UEFA) European Football
               Championship, the relative ranking of two teams could possibly
               be influenced by a match in which neither team was involved,
               which we refer to as heteronomous relative ranking (HRR). We
               seek to shed light on the origin and the practical relevance of
               HRR in soccer competitions.Research methodsWe trace the
               appearance of HRR back to particularities in the UEFA EURO 2012
               tie-breaker criteria, which favor head-to-head records over goal
               difference, and relate the concept of autonomous relative
               ranking to Arrow's Independence of Irrelevant Alternatives.
               Using historical and Monte?Carlo simulated data, we compare HRR
               occurrence rates under EURO 2012 regulations to those under
               F{\'e}d{\'e}ration Internationale de Football Association (FIFA)
               2010 regulations, the latter of which give less importance to
               head-to-head records.Results and findingsHRR is well explained
               by tie-breaker criteria, namely the priorities of head-to-head
               records and goal difference. HRR occurs in more than 10\% of
               four-team soccer groups under EURO 2012 regulations; this rate
               is further increased in round-robin groups of six teams. FIFA
               2010 regulations lead to less than 0.1\%
               HRR.ImplicationsHead-to-head records may result in
               counterintuitive side effects that should be avoided when
               designing ranking systems.",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  14,
  number    =  2,
  pages     = "194--210",
  month     =  mar,
  year      =  2014
}

@ARTICLE{Hryckiewicz2017-lc,
  title    = "Banking business models and the nature of financial crisis",
  author   = "Hryckiewicz, Aneta and Koz{\l}owski, {\L}ukasz",
  abstract = "In our paper, we analyze heterogeneity among the various business
              models that incorporate systemically important banks in 65
              countries over the period of 2000--2012. For the first time, we
              identify true banking strategies in a portfolio context, that is,
              consisting of various combinations of bank assets and funding
              sources. Next, we estimate the way distinct strategies have
              affected bank profitability and risk before the crisis, as well
              as their impact on the mortgage crisis. We demonstrate that
              during the mortgage crisis, the investment model indicated the
              lowest individual risk and highest systemic risk simultaneously.
              Consequently, we document that the funding structure was
              responsible for the systemic effect of the mortgage crisis.
              Further, we demonstrate that countries with systemically
              important banks that rely on investment activities experience a
              greater, but more short-lived decline in GDP, when compared to
              countries that have predominantly traditional banking activities.",
  journal  = "J. Int. Money Finance",
  volume   =  71,
  pages    = "1--24",
  month    =  mar,
  year     =  2017,
  keywords = "Bank risk; Business model; Bank regulation; Financial crisis;
              Banking stability; Systemic risk"
}

@ARTICLE{Collard2017-jw,
  title    = "Optimal Monetary and Prudential Policies",
  author   = "Collard, Fabrice and Dellas, Harris and Diba, Behzad and Loisel,
              Olivier",
  journal  = "American Economic Journal: Macroeconomics",
  volume   =  9,
  number   =  1,
  pages    = "40--87",
  year     =  2017
}

@ARTICLE{Galati2017-sh,
  title     = "What do we know about the effects of macroprudential policy?",
  author    = "Galati, Gabriele and Moessner, Richhild",
  journal   = "Economica",
  publisher = "Wiley Online Library",
  year      =  2017
}

@UNPUBLISHED{Buch2016-qr,
  title       = "{Cross-Border} Prudential Policy Spillovers: How Much? How
                 Important? Evidence from the International Banking Research
                 Network",
  author      = "Buch, Claudia M and Goldberg, Linda",
  abstract    = "The development of macroprudential policy tools has been one
                 of the most significant changes in banking regulation in
                 recent years. In this multi-study initiative of the
                 International Banking Research Network, researchers from
                 fifteen central banks and two international organizations use
                 micro-banking data in conjunction with a novel dataset of
                 prudential instruments to study international spillovers of
                 prudential policy changes and their effects on bank lending
                 growth. The collective analysis has three main findings.
                 First, the effects of prudential instruments sometimes spill
                 over borders through bank lending. Second, international
                 spillovers vary across prudential instruments and are
                 heterogeneous across banks. Bank-specific factors like balance
                 sheet conditions and business models drive the amplitude and
                 direction of spillovers to lending growth rates. Third, the
                 effects of international spillovers of prudential policy on
                 loan growth rates have not been large on average. However, our
                 results tend to underestimate the full effect by focusing on
                 adjustment along the intensive margin and by analyzing a
                 period in which relatively few countries implemented
                 country-specific macroprudential policies.",
  number      =  22874,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  dec,
  year        =  2016
}

@UNPUBLISHED{Blinder2016-qc,
  title       = "Necessity as the Mother of Invention: Monetary Policy after
                 the Crisis",
  author      = "Blinder, Alan S and Ehrmann, Michael and de Haan, Jakob and
                 Jansen, David-Jan",
  abstract    = "We ask whether recent changes in monetary policy due to the
                 financial crisis will be temporary or permanent. We present
                 evidence from two surveys---one of central bank governors, the
                 other of academic specialists. We find that central banks in
                 crisis countries are more likely to have resorted to new
                 policies, to have had discussions about mandates, and to have
                 communicated more. But the thinking has changed more
                 broadly---for instance, central banks in non-crisis countries
                 also report having implemented macro-prudential measures.
                 Overall, we expect central banks in the future to have broader
                 mandates, use macro-prudential tools more widely, and
                 communicate more actively than before the crisis. While there
                 is no consensus yet about the usefulness of unconventional
                 monetary policies, we expect most of them will remain in
                 central banks' toolkits, as governors who gain experience with
                 a particular tool are more likely to assess that tool
                 positively. Finally, the relationship between central banks
                 and their governments might well have changed, with central
                 banks ``crossing the line'' more often than in the past.",
  number      =  22735,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  oct,
  year        =  2016
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Forbes2017-kv,
  title    = "The spillovers, interactions, and (un)intended consequences of
              monetary and regulatory policies",
  author   = "Forbes, Kristin and Reinhardt, Dennis and Wieladek, Tomasz",
  abstract = "Have bank regulatory policies and unconventional monetary
              policies---and any possible interactions---been a factor behind
              the recent ``deglobalisation'' in cross-border bank lending? To
              test this hypothesis, we use bank-level data from the UK---a
              country at the heart of the global financial system. Our results
              suggest that increases in microprudential capital requirements
              tend to reduce international bank lending and some forms of
              unconventional monetary policy can amplify this effect.
              Specifically, the UK◊≥s Funding for Lending Scheme (FLS)
              significantly amplified the effects of increased capital
              requirements on cross-border lending. Quantitative easing did not
              appear to have a similar effect and countries with stronger
              prudential capital regulations were partially insulated against
              the effects of these changes in UK policy. We find that this
              interaction between microprudential regulations and the FLS can
              explain roughly 30\% of the contraction in aggregate UK
              cross-border bank lending between mid-2012 and end-2013,
              corresponding to around 10\% of the global contraction in
              cross-border lending. This suggests that unconventional monetary
              policy designed to support domestic lending can have the
              unintended consequence of reducing foreign lending.",
  journal  = "J. Monet. Econ.",
  volume   =  85,
  pages    = "1--22",
  month    =  jan,
  year     =  2017,
  keywords = "Capital requirements; Funding for Lending Scheme; Financial
              deglobalisation"
}

@TECHREPORT{Damar2016-ku,
  title       = "International banking and cross-border effects of regulation:
                 Lessons from Canada",
  author      = "Damar, H Evren and Mordel, Adi",
  institution = "Bank of Canada Staff Working Paper",
  year        =  2016
}

@ARTICLE{Liu2017-ol,
  title    = "Sport business in China: current state and prospect",
  author   = "Liu, Dongfeng and Zhang, James J and Desbordes, Michel",
  abstract = "Purpose Growth of China's sport industry has brought tremendous
              opportunities to sport and non-sport organizations domestically
              and globally; nonetheless, the enlargement has also raised many
              challenges. To a great extent, China has chartered into
              unprecedented new sport business territories. Because of social,
              cultural, historical, and governmental differences, many theories
              and knowledge, professional experiences, best practices, and
              lessons learned in Western countries may or may not be directly
              applicable to the diverse setting(s) in China. Until now, only
              limited empirical evidence is available to address these
              challenges. Thus, formulating a special issue in the
              International Journal of Sports Marketing and Sponsorship to
              examine contemporary subject matters and concerns would be
              significantly meaningful to help understand, stimulate, and
              improve sport business operations in China, provide guidance to
              transnational organizations for doing sport-related business in
              China, offer constructive suggestions for Chinese corporations
              going global, and ultimately build up theories and best practices
              to address unique perspectives of China's sport industry. The
              paper aims to discuss these issues. Design/methodology/approach
              This paper is conceptual and presents a review of literature.
              Findings In addition to this leading paper, there are a total of
              eight manuscripts selected for this special issue inquiring on
              contemporary matters and development of China's sport industry,
              including four short articles that were formulated based on
              qualitative research information derived from case studies and
              interviews and four full-length articles that adopted a
              quantitative research protocol or a mixed research design
              involving both qualitative and quantitative information. Research
              limitations/implications While it is impossible to capture all
              contemporary topics in the development of China's sport industry
              within one journal issue, articles selected in this special issue
              of the International Journal of Sports Marketing and Sponsorship
              have provided useful highlights into some of the critical issues
              faced by the industry and research directions by academicians. It
              is sincerely expected that studies in this special issue would
              help inspire more scholarly inquires and ultimately improve the
              continued formulation and advancement of a strong sport industry
              in China. Practical implications While it is impossible to
              capture all contemporary topics in the development of China's
              sport industry within one journal issue, articles selected in
              this special issue of the International Journal of Sports
              Marketing and Sponsorship have provided useful highlights into
              some of the critical issues faced by the industry and research
              directions by academicians. It is sincerely expected that studies
              in this special issue would help inspire more scholarly inquires
              and ultimately improve the continued formulation and advancement
              of a strong sport industry in China. Social implications While it
              is impossible to capture all contemporary topics in the
              development of China's sport industry within one journal issue,
              articles selected in this special issue of the International
              Journal of Sports Marketing and Sponsorship have provided useful
              highlights into some of the critical issues faced by the industry
              and research directions by academicians. It is sincerely expected
              that studies in this special issue would help inspire more
              scholarly inquires and ultimately improve the continued
              formulation and advancement of a strong sport industry in China.
              Originality/value While it is impossible to capture all
              contemporary topics in the development of China's sport industry
              within one journal issue, articles selected in this special issue
              of the International Journal of Sports Marketing and Sponsorship
              have provided useful highlights into some of the critical issues
              faced by the industry and research directions by academicians. It
              is sincerely expected that studies in this special issue would
              help inspire more scholarly inquires and ultimately improve the
              continued formulation and advancement of a strong sport industry
              in China.",
  journal  = "International Journal of Sports Marketing and Sponsorship",
  volume   =  18,
  number   =  1,
  pages    = "2--10",
  year     =  2017
}

@BOOK{Nalbantis2016-gx,
  title     = "The Demand for International Football Telecasts in the United
               States",
  author    = "Nalbantis, Georgios and Pawlowski, Tim",
  abstract  = "This book provides a comprehensive overview and economic
               analysis of US consumer demand for televised football (soccer).
               Accounting for transnational demand, research is focused on the
               US consumers demand for the English Premier League, Spanish La
               Liga, Italian Serie A, German Bundesliga, French Ligue 1 and the
               UEFA Champions League, which represent the most popular and
               marketable football competitions worldwide, and have recently
               sealed lucrative media rights contracts in many large markets,
               including the US. The study also takes account of North American
               Major League Soccer (MLS) in order to provide a more
               comprehensive overview of the country's football market and to
               allow for direct comparisons with the aforementioned European
               competitions.These findings offer valuable insights for US
               broadcasters, European league organizers and managers to adjust
               existing strategies and/or develop new strategies in conquering
               the US football market.",
  publisher = "Springer",
  month     =  dec,
  year      =  2016,
  language  = "en"
}

@TECHREPORT{Gallagher2017-vf,
  title       = "Shadow Prices: Statistical Significance Testing",
  author      = "Gallagher, Ronan and Kuosmanen, Timo and Quinn, Barry",
  institution = "Queen's University Belfast Working Paper",
  year        =  2017
}

@ARTICLE{Akerlof1976-cb,
  title     = "The Economics of Caste and of the Rat Race and Other Woeful
               Tales",
  author    = "Akerlof, George",
  abstract  = "I. Introduction, 599.---II. Sharecropping, 601.---III. Work
               conditions: the rat race, 603.---IV. Statistical discrimination,
               606.---V. Caste and group organizations, 608.---VI. Conclusions,
               617.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  90,
  number    =  4,
  pages     = "599--617",
  month     =  nov,
  year      =  1976
}

@ARTICLE{Ayadi2016-kh,
  title    = "Does Basel compliance matter for bank performance?",
  author   = "Ayadi, Rym and Naceur, Sami Ben and Casu, Barbara and Quinn,
              Barry",
  abstract = "The global financial crisis underscored the importance of
              regulation and supervision to a well-functioning banking system
              that efficiently channels financial resources into investment. In
              this paper, we contribute to the ongoing policy debate by
              assessing whether compliance with international regulatory
              standards and protocols enhances bank operating efficiency. We
              focus specifically on the adoption of international capital
              standards and the Basel Core Principles for Effective Bank
              Supervision (BCP). The relationship between bank efficiency and
              regulatory compliance is investigated using the Simar and Wilson
              (2007. J. Econ. 136 (1), 31) double bootstrapping approach on an
              international sample of publicly listed banks. Our results
              indicate that overall BCP compliance, or indeed compliance with
              any of its individual chapters, has no association with bank
              efficiency.",
  journal  = "Journal of Financial Stability",
  volume   =  23,
  pages    = "15--32",
  month    =  apr,
  year     =  2016,
  keywords = "BCP; Efficiency; Regulatory Compliance"
}

@ARTICLE{Chatfield1996-sf,
  title     = "Model uncertainty and forecast accuracy",
  author    = "Chatfield, Chris",
  abstract  = "In time-series analysis, a model is rarely pre-specified but
               rather is typically formulated in an iterative, interactive way
               using the given time-series data. Unfortunately the properties
               of the fitted model, and the forecasts from it, are generally
               calculated as if the model were known in the first place. This
               is theoretically incorrect, as least squares theory, for
               example, does not apply when the same data are used to
               formulates and fit a model. Ignoring prior model selection leads
               to biases, not only in estimates of model parameters but also in
               the subsequent construction of prediction intervals. The latter
               are typically too narrow, partly because they do not allow for
               model uncertainty. Empirical results also suggest that more
               complicated models tend to give a better fit but poorer ex-ante
               forecasts. The reasons behind these phenomena are reviewed. When
               comparing different forecasting models, the BIC is preferred to
               the AIC for identifying a model on the basis of within-sample
               fit, but out-of-sample forecasting accuracy provides the real
               test. Alternative approaches to forecasting, which avoid
               conditioning on a single model, include Bayesian model averaging
               and using a forecasting method which is not model-based but
               which is designed to be adaptable and robust.",
  journal   = "J. Forecast.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  15,
  number    =  7,
  pages     = "495--508",
  month     =  dec,
  year      =  1996,
  keywords  = "AIC; Bayesian model averaging; BIC; forecasting; model
               selection; model uncertainty; neural networks; prediction
               intervals"
}

@ARTICLE{Barth2013-zn,
  title     = "Bank regulation and supervision in 180 countries from 1999 to
               2011",
  author    = "Barth, James R and Caprio, Jr, Gerard and Levine, Ross",
  journal   = "Journal of Financial Economic Policy",
  publisher = "Emerald Group Publishing Limited",
  volume    =  5,
  number    =  2,
  pages     = "111--219",
  year      =  2013,
  address   = "Bingley, United Kingdom",
  language  = "en"
}

@ARTICLE{Paxton2007-hd,
  title     = "Technical Efficiency in a {Semi-Formal} Financial Sector: The
               Case of Mexico",
  author    = "Paxton, Julia",
  abstract  = "The semi-formal financial sector in Mexico is playing an
               increasingly important role in serving a largely poor, rural
               clientele. A stochastic frontier with non-monotonic marginal
               effects [ Wang, Journal of Productivity Analysis (2002 ), Vol.
               18, pp. 241--253] reveals a wide disparity in technical
               efficiency levels among 190 Mexican semi-formal financial
               intermediaries. The results show that technology, average loan
               size, rural outreach and institutional age are all positively
               associated with technical efficiency. The marginal effects vary
               widely and, in some cases, the effects are non-monotonic over
               percentile groups. The results indicate that strengthening
               younger, technologically undeveloped financial institutions will
               have the strongest marginal benefit in revitalizing the rural
               financial sector. [ABSTRACT FROM AUTHOR]",
  journal   = "Oxford Bull Econ \& Stats",
  publisher = "Wiley-Blackwell",
  volume    =  69,
  number    =  1,
  pages     = "57--74",
  month     =  feb,
  year      =  2007,
  keywords  = "BANKING research; FINANCIAL services industry; ASSOCIATIONS,
               institutions, etc.; MONOTONIC functions; RURAL geography; MEXICO
               -- Economic conditions -- 1994-; MEXICO; C1; G14; O16"
}

@ARTICLE{Sussmuth2010-wx,
  title     = "Induced Civic Pride and Integration",
  author    = "S{\"u}ssmuth, Bernd and Heyne, Malte and Maennig, Wolfgang",
  abstract  = "Does a nation's contingent value of hosting a mega event depend
               on past experience with implied benefits? Applying data from
               ex-ante and ex-post queries, we use the FIFA World Cup 2006 to
               address this question. The ex-post increase in valuation is
               shown to be owing to citizens requiring an involving experience.
               We also use the first mega-event hosted by reunified Germany to
               investigate how the integration of the two parts of Germany
               progressed after 18 years of reunification. We find that civic
               pride induced by collective experience can considerably
               accelerate the convergence of East Germans' preferences towards
               those of West Germans. [ABSTRACT FROM AUTHOR]",
  journal   = "Oxf. Bull. Econ. Stat.",
  publisher = "Wiley-Blackwell",
  volume    =  72,
  number    =  2,
  pages     = "202--220",
  month     =  apr,
  year      =  2010,
  keywords  = "GERMANY -- Social conditions; GERMANY -- History -- Unification,
               1990; PRIDE \& vanity; GERMANS; GERMANY; L83; FEDERATION
               internationale de football association"
}

@ARTICLE{Dickson2016-ej,
  title     = "Domestic Violence and Football in Glasgow: Are Reference Points
               Relevant?",
  author    = "Dickson, Alex and Jennings, Colin and Koop, Gary",
  abstract  = "A growing body of evidence suggests that people exhibit loss
               aversion - the displeasure from suffering a loss is larger than
               the pleasure enjoyed from an equivalent-sized gain - and that
               expectations are important in determining what is perceived as a
               loss. Recent research suggests that disappointing results in
               sporting fixtures relative to prematch expectations play an
               important role in triggering domestic violence (Card and Dahl,
               2011), consistent with the idea of loss aversion around
               expectations-based reference points. This paper seeks to
               investigate whether such behaviour is exhibited by football fans
               in Glasgow by looking at the relationship between match outcomes
               relative to expectations and levels of domestic violence using a
               data set that contains every domestic violence incident in
               Glasgow over a period of more than eight years. Whilst we find
               that when the 'Old-Firm' Glasgow rivals Celtic and Rangers play
               there are large increases in domestic violence (regardless of
               the outcome of the match), in other matches disappointing
               results relative to expectations are not linked to increased
               domestic violence, except when those matches occur at the very
               end of the season where the title is still being contended.
               [ABSTRACT FROM AUTHOR]",
  journal   = "Oxf. Bull. Econ. Stat.",
  publisher = "Wiley-Blackwell",
  volume    =  78,
  number    =  1,
  pages     = "1--21",
  month     =  feb,
  year      =  2016,
  keywords  = "UTILITY theory; FAMILY violence; SPECTATOR sports; LOSS
               aversion; SPORTS -- Scotland"
}

@ARTICLE{Buraimo2016-ab,
  title     = "An Analysis of Consumer Response to Corruption: Italy's
               Calciopoli Scandal",
  author    = "Buraimo, Babatunde and Migali, Giuseppe and Simmons, Robert",
  abstract  = "The Calciopoli episode affecting Italian football in the 2005-6
               season serves as an opportunity for an empirical investigation
               into consumer (fan) behavior, following leagueimposed
               punishments on clubs whose officials were found guilty of
               corrupt practices. Using a difference-in-differences estimation
               method, we find that home attendances for convicted teams fell
               by around 16\%, relative to those clubs not subject to
               punishment.We show further that the fall in attendances resulted
               in non-trivial gate revenue reductions. Our results suggest that
               a sizeable number of fans of the punished clubs were
               subsequently deterred from supporting their teams inside the
               stadium. [ABSTRACT FROM AUTHOR]",
  journal   = "Oxf. Bull. Econ. Stat.",
  publisher = "Wiley-Blackwell",
  volume    =  78,
  number    =  1,
  pages     = "22--41",
  month     =  feb,
  year      =  2016,
  keywords  = "CONSUMER behavior; SPORTS -- Corrupt practices; FOOTBALL teams
               -- Corrupt practices; ITALY; UNION of European Football
               Associations"
}

@ARTICLE{Athanasoglou2009-pf,
  title    = "Assessing output and productivity growth in the banking industry",
  author   = "Athanasoglou, Panayiotis P and Georgiou, Evangelia A and
              Staikouras, Christos C",
  abstract = "Abstract: This paper assesses the evolution of output and
              productivity in the Greek banking industry for the period
              1990--2006. Three main categories of bank output were estimated
              based on modern theoretical approaches, while for the estimation
              of output and productivity (partial and total factor) we relied
              on the index number method (Tornqvist index). We also considered
              the effect of labor quality on banks' productivity and the
              contribution of total factor productivity to bank output growth.
              Bank output and labor productivity outpaced considerably the
              respective GDP growth and labor productivity of the Greek economy
              during the period under examination. Capital and total factor
              productivity have also improved remarkably mainly since 1999, due
              to the structural changes that took place within the industry,
              capital (mainly IT) investments and improvement in the quality of
              human capital. [Copyright \&y\& Elsevier]",
  journal  = "Q. Rev. Econ. Finance",
  volume   =  49,
  number   =  4,
  pages    = "1317--1340",
  month    =  nov,
  year     =  2009,
  keywords = "PRODUCTIVITY incentives; ECONOMIC development; BANKING industry;
              LABOR productivity; PRODUCTION (Economic theory); PRODUCTION
              standards; INDEX numbers (Economics); ESTIMATION theory; Bank
              output; Labor quality; Tornqvist index; Total factor
              productivity; User-cost approach"
}

@ARTICLE{Fried2002-au,
  title     = "Accounting for Environmental Effects and Statistical Noise in
               Data Envelopment Analysis",
  author    = "Fried, H O and Lovell, C A K and Schmidt, S S and Yaisawarng, S",
  abstract  = "In this paper we propose a new technique for incorporating
               environmental effects and statistical noise into a producer
               performance evaluation based on data envelopment analysis (DEA).
               The technique involves a three-stage analysis. In the first
               stage, DEA is applied to outputs and inputs only, to obtain
               initial measures of producer performance. In the second stage,
               stochastic frontier analysis (SFA) is used to regress first
               stage performance measures against a set of environmental
               variables. This provides, for each input or output (depending on
               the orientation of the first stage DEA model), a three-way
               decomposition of the variation in performance into a part
               attributable to environmental effects, a part attributable to
               managerial inefficiency, and a part attributable to statistical
               noise. In the third stage, either inputs or outputs (again
               depending on the orientation of the first stage DEA model) are
               adjusted to account for the impact of the environmental effects
               and the statistical noise uncovered in the second stage, and DEA
               is used to re-evaluate producer performance. Throughout the
               analysis emphasis is placed on slacks, rather than on radial
               efficiency scores, as appropriate measures of producer
               performance. An application to nursing homes is provided to
               illustrate the power of the three-stage methodology.",
  journal   = "Journal of Productivity Analysis",
  publisher = "Kluwer Academic Publishers",
  volume    =  17,
  number    = "1-2",
  pages     = "157--174",
  month     =  jan,
  year      =  2002,
  keywords  = "Production; Cost; Capital; Capital, Total Factor, and
               Multifactor Productivity; Capacity D24; Analysis of Health Care
               Markets I11; Operations Research; Statistical Decision Theory
               C44",
  language  = "en"
}

@ARTICLE{Ramaswamy2011-el,
  title     = "Market Structures and Systemic Risks of {Exchange-Traded} Funds",
  author    = "Ramaswamy, Srichander",
  abstract  = "Crisis experience has shown that as the financial intermediation
               chain lengthens, it becomes complicated to assess the risks of
               financial products due to a lack",
  journal   = "BIS working paper No.343",
  publisher = "papers.ssrn.com",
  month     =  apr,
  year      =  2011,
  keywords  = "Mutual funds, total return swaps, securities lending, systemic
               risk"
}

@ARTICLE{Yao2017-ia,
  title    = "Expected default based score for identifying systemically
              important banks",
  author   = "Yao, Yanzhen and Li, Jianping and Zhu, Xiaoqian and Wei, Lu",
  abstract = "The issue of identifying systemically important banks has gained
              prominence since the recent global financial crisis in 2007.
              However, the extant methods either neglect the adverse impact on
              the financial system posed by a bank or ignore the various
              interactions among banks. To resolve this issue, the objective of
              this study is to put forward an expected default based score
              (EDBS) that overcomes the drawbacks of the existing methods from
              the perspective of contagion risk. This indicator measures the
              systemic importance of a bank by calculating the expected bank
              defaults triggered by its initial failure. In the empirical
              study, the expected default based score is applied to identify
              the systemically important banks in the Chinese banking system.
              Both the quantitative comparison with other major methods and the
              qualitative evaluation of the Delphi method validate the
              reliability of the EDBS method. The empirical results also
              demonstrate that interconnectedness among banks is an important
              and complementary driver of systemic importance in addition to
              asset size.",
  journal  = "Econ. Model.",
  volume   =  64,
  pages    = "589--600",
  month    =  aug,
  year     =  2017,
  keywords = "Systemically important banks; Contagion risk; Systemic risk; Bank
              risk"
}

@ARTICLE{McKillop2005-fl,
  title     = "Financial Cooperatives: Structure, Conduct and Performance",
  author    = "McKillop, Donal G",
  abstract  = "Hermann Schulze-Delitzsch founded the first urban credit
               cooperative in Germany in 1850 while Friedrich Wilhelm
               Raiffeisen formed the first rural credit cooperative in Germany
               in 1864. A principal purpose of these early people's banks was
               to draw outside funds into",
  journal   = "Annals of Public and Cooperative Economics",
  publisher = "Blackwell Publishing Ltd/Inc.",
  volume    =  76,
  number    =  3,
  pages     = "301--305",
  month     =  sep,
  year      =  2005
}

@INCOLLECTION{Duffey2013-wm,
  title     = "The Quantification of Systemic Risk and Stability",
  booktitle = "Quantifying Systemic Risk",
  author    = "Duffey, Romney B",
  editor    = "Haubrich, Joseph G and Lo, Andrew W",
  publisher = "University of Chicago Press",
  pages     = "222--262",
  year      =  2013
}

@INCOLLECTION{Acharya2012-hp,
  title     = "How to Calculate Systemic Risk Surcharges",
  booktitle = "Quantifying Systemic Risk",
  author    = "Acharya, Viral V and Pedersen, Lasse H and Philippon, Thomas and
               Richardson, Matthew",
  publisher = "University of Chicago Press",
  pages     = "175--212",
  month     =  feb,
  year      =  2012
}

@INCOLLECTION{De_Nicolo2013-ut,
  title     = "Systemic Risks and the Macroeconomy",
  booktitle = "Quantifying Systemic Risk",
  author    = "De Nicol{\`o}, Gianni and Lucchetta, Marcella",
  editor    = "Haubrich, Joseph G and Lo, Andrew W",
  publisher = "University of Chicago Press",
  pages     = "113--153",
  year      =  2013
}

@INCOLLECTION{Danielsson2012-ml,
  title     = "Endogenous and Systemic Risk",
  booktitle = "Quantifying Systemic Risk",
  author    = "Danielsson, Jon and Shin, Hyun Song and Zigrand, Jean-Pierre",
  publisher = "University of Chicago Press",
  pages     = "73--94",
  month     =  apr,
  year      =  2012
}

@INCOLLECTION{Adrian2013-ga,
  title     = "Hedge Fund Tail Risk",
  booktitle = "Quantifying Systemic Risk",
  author    = "Adrian, Tobias and Brunnermeier, Markus K and Nguyen, Hoai-Luu Q",
  editor    = "Haubrich, Joseph G and Lo, Andrew W",
  publisher = "University of Chicago Press",
  pages     = "154--172",
  year      =  2013
}

@ARTICLE{Bhagat2008-yv,
  title     = "Corporate governance and firm performance",
  author    = "Bhagat, Sanjai and Bolton, Brian",
  abstract  = "How is corporate governance measured? What is the relationship
               between corporate governance and performance? This paper sheds
               light on these questions while taking into account the
               endogeneity of the relationships among corporate governance,
               corporate performance, corporate capital structure, and
               corporate ownership structure. We make three additional
               contributions to the literature: First, we find that better
               governance as measured by the Gompers, Ishii, and Metrick
               [Gompers, P.A., Ishii, J.L., and Metrick, A., 2003, Corporate
               governance and equity prices, Quarterly Journal of Economics
               118(1), 107--155.] and Bebchuk, Cohen and Ferrell [Bebchuk, L.,
               Cohen, A., and Ferrell, A., 2004, What matters in corporate
               governance?, Working paper, Harvard Law School] indices, stock
               ownership of board members, and CEO-Chair separation is
               significantly positively correlated with better contemporaneous
               and subsequent operating performance. Second, contrary to claims
               in GIM and BCF, none of the governance measures are correlated
               with future stock market performance. In several instances
               inferences regarding the (stock market) performance and
               governance relationship do depend on whether or not one takes
               into account the endogenous nature of the relationship between
               governance and (stock market) performance. Third, given poor
               firm performance, the probability of disciplinary management
               turnover is positively correlated with stock ownership of board
               members, and board independence. However, better governed firms
               as measured by the GIM and BCF indices are less likely to
               experience disciplinary management turnover in spite of their
               poor performance.",
  journal   = "Journal of Corporate Finance",
  publisher = "Elsevier",
  volume    =  14,
  number    =  3,
  pages     = "257--273",
  month     =  jun,
  year      =  2008,
  keywords  = "Corporate governance; Corporate ownership; CEO Turnover;
               Endogeneity; Simultaneous Equations"
}

@ARTICLE{Li2017-dj,
  title     = "Robust Jump Regressions",
  author    = "Li, Jia and Todorov, Viktor and Tauchen, George",
  abstract  = "ABSTRACTWe develop robust inference methods for studying linear
               dependence between the jumps of discretely observed processes at
               high frequency. Unlike classical linear regressions, jump
               regressions are determined by a small number of jumps occurring
               over a fixed time interval and the rest of the components of the
               processes around the jump times. The latter are the continuous
               martingale parts of the processes as well as observation noise.
               By sampling more frequently the role of these components, which
               are hidden in the observed price, shrinks asymptotically. The
               robustness of our inference procedure is with respect to
               outliers, which are of particular importance in the current
               setting of relatively small number of jump observations. This is
               achieved by using nonsmooth loss functions (like L1) in the
               estimation. Unlike classical robust methods, the limit of the
               objective function here remains nonsmooth. The proposed method
               is also robust to measurement error in the observed processes,
               which is achieved by locally smoothing the high-frequency
               increments. In an empirical application to financial data, we
               illustrate the usefulness of the robust techniques by
               contrasting the behavior of robust and ordinary least regression
               (OLS)-type jump regressions in periods including disruptions of
               the financial markets such as so-called ?flash crashes.?
               Supplementary materials for this article are available online.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  112,
  number    =  517,
  pages     = "332--341",
  month     =  jan,
  year      =  2017
}

@ARTICLE{Krauss2017-qq,
  title    = "{STATISTICAL} {ARBITRAGE} {PAIRS} {TRADING} {STRATEGIES}:
              {REVIEW} {AND} {OUTLOOK}",
  author   = "Krauss, Christopher",
  abstract = "This survey reviews the growing literature on pairs trading
              frameworks, i.e., relative-value arbitrage strategies involving
              two or more securities. Research is categorized into five groups:
              The distance approach uses nonparametric distance metrics to
              identify pairs trading opportunities. The cointegration approach
              relies on formal cointegration testing to unveil stationary
              spread time series. The time-series approach focuses on finding
              optimal trading rules for mean-reverting spreads. The stochastic
              control approach aims at identifying optimal portfolio holdings
              in the legs of a pairs trade relative to other available
              securities. The category ``other approaches'' contains further
              relevant pairs trading frameworks with only a limited set of
              supporting literature. Finally, pairs trading profitability is
              reviewed in the light of market frictions. Drawing from a large
              set of research consisting of over 100 references, an in-depth
              assessment of each approach is performed, ultimately revealing
              strengths and weaknesses relevant for further research and for
              implementation.",
  journal  = "J. Econ. Surv.",
  volume   =  31,
  number   =  2,
  pages    = "513--545",
  month    =  apr,
  year     =  2017,
  keywords = "Mean-reversion; Pairs Trading; Spread Trading; Relative-value
              Arbitrage"
}

@INCOLLECTION{Almanidis2016-js,
  title     = "Banking Crises, Early Warning Models, and Efficiency",
  booktitle = "Advances in Efficiency and Productivity",
  author    = "Almanidis, Pavlos and Sickles, Robin C",
  editor    = "Aparicio, Juan and Knox Lovell, C A and Pastor, Jesus T",
  abstract  = "This paper proposes a general model that combines the Mixture
               Hazard Model with the Stochastic Frontier Model for the purposes
               of investigating the main determinants of the failures and
               performances of a panel of U.S. commercial banks during the
               financial crisis that began in 2007. The combined model provides
               measures of the probability and time to failure conditional on a
               bank's performance and vice versa. Both continuous-time and
               discrete-time specifications of the model are considered in the
               paper. The estimation is carried out via the
               expectation-maximization algorithm due to incomplete information
               regarding the identity of at-risk banks. In- and out-of-sample
               predictive accuracy of the proposed models is investigated in
               order to assess their potential to serve as early warning tools.",
  publisher = "Springer, Cham",
  pages     = "331--364",
  series    = "International Series in Operations Research \& Management
               Science",
  year      =  2016,
  language  = "en"
}

@ARTICLE{Black1986-mr,
  title     = "Noise",
  author    = "Black, Fischer",
  abstract  = "The effects of noise on the world, and on our views of the
               world, are profound. Noise in the sense of a large number of
               small events is often a causal factor much more powerful than a
               small number of large events can be. Noise makes trading in
               financial markets possible, and thus allows us to observe prices
               for financial assets. Noise causes markets to be somewhat
               inefficient, but often prevents us from taking advantage of
               inefficiencies. Noise in the form of uncertainty about future
               tastes and technology by sector causes business cycles, and
               makes them highly resistant to improvement through government
               intervention. Noise in the form of expectations that need not
               follow rational rules causes inflation to be what it is, at
               least in the absence of a gold standard or fixed exchange rates.
               Noise in the form of uncertainty about what relative prices
               would be with other exchange rates makes us think incorrectly
               that changes in exchange rates or inflation rates cause changes
               in trade or investment flows or economic activity. Most
               generally, noise makes it very difficult to test either
               practical or academic theories about the way that financial or
               economic markets work. We are forced to act largely in the dark.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  41,
  number    =  3,
  pages     = "528--543",
  month     =  jul,
  year      =  1986
}

@ARTICLE{Stoll2006-ov,
  title     = "Electronic Trading in Stock Markets",
  author    = "Stoll, Hans R",
  abstract  = "Modern trading technology clashes with the traditional
               organization of a stock exchange, where transactions were
               consummated via face-to-face negotiation. The modern trading
               facility is no longer a place. Rather, it is a computer system
               over which transactions are entered, routed, executed and
               cleared electronically with little or no human intervention. In
               this article, I examine how electronic trading has altered stock
               markets. I begin with an overview of how the stock trading
               process works and then address a number of questions. How have
               the jobs of traditional stock market dealers on the NYSE and on
               Nasdaq been affected by electronic trading? How do electronic
               communications networks differ from traditional markets? How has
               electronic trading affected bid-ask spreads and commission
               costs? What subtle issues arise in electronic trading when
               dealer and customer interests diverge? Will computer programs
               replace human judgment? What is the effect of electronic trading
               on the number and types of securities markets? What is the role
               of regulation in electronic markets?",
  journal   = "J. Econ. Perspect.",
  publisher = "ingentaconnect.com",
  volume    =  20,
  number    =  1,
  pages     = "153--174",
  month     =  mar,
  year      =  2006
}

@ARTICLE{Forker2012-vr,
  title     = "Prudence and financial self-regulation in credit unions in
               Northern Ireland",
  author    = "Forker, John and Ward, Anne Marie",
  abstract  = "Credit unions in Northern Ireland are subject to a unique
               combination of statutory oversight and self-regulation. This
               paper investigates the association between prudence and the
               monitoring of financial ratios by credit union trade
               associations. We find that compliance with the mandated level of
               capital reserves is uniformly high, regardless of the existence
               or extent of self-regulation. However, after controlling for
               cross-sectional differences in profitability, age, size, growth
               and common bond type a positive association exists between
               self-regulation and financial ratios measuring prudence and loan
               book quality. These findings have policy implications for the
               regulation of credit unions in Northern Ireland and elsewhere
               regarding potential regulatory cost savings from reliance on
               self-regulation provided by trade associations.",
  journal   = "The British Accounting Review",
  publisher = "Elsevier",
  volume    =  44,
  number    =  4,
  pages     = "221--234",
  month     =  dec,
  year      =  2012,
  keywords  = "Credit unions; Financial ratios; Loan book quality; Prudence;
               Self-regulation"
}

@ARTICLE{Gauthier2012-wc,
  title     = "Macroprudential capital requirements and systemic risk",
  author    = "Gauthier, C{\'e}line and Lehar, Alfred and Souissi, Moez",
  abstract  = "When setting banks' regulatory capital requirement based on
               their contribution to the overall risk of the banking system we
               have to consider that the risk of the banking system as well as
               each bank's risk contribution changes once bank equity capital
               gets reallocated. We define macroprudential capital requirements
               as the fixed point at which each bank's capital requirement
               equals its contribution to the risk of the system under the
               proposed capital requirements. We use a network based structural
               model to measure systemic risk and how it changes with bank
               capital and allocate risk to individual banks based on five risk
               allocation mechanisms used in the literature. Using a sample of
               Canadian banks we find that macroprudential capital allocations
               can differ by as much as 25\% from observed capital levels, are
               not trivially related to bank size or individual bank default
               probability, increase in interbank assets, and differ
               substantially from a simple risk attribution analysis. We
               further find that across all risk allocation mechanisms
               macroprudential capital requirements reduce the default
               probabilities of individual banks as well as the probability of
               a systemic crisis by about 25\%. Macroprudential capital
               requirements are robust to model risk and are positively
               correlated to future capital raised by banks as well as future
               losses in equity value. Our results suggest that financial
               stability can be substantially enhanced by implementing a
               systemic perspective on bank regulation.",
  journal   = "Journal of Financial Intermediation",
  publisher = "Elsevier",
  volume    =  21,
  number    =  4,
  pages     = "594--618",
  month     =  oct,
  year      =  2012,
  keywords  = "Systemic risk; Financial stability; Bank regulation; Risk
               management; Interbank market;"
}

@ARTICLE{Zhang2016-wm,
  title    = "Research on Credit Scoring by Fusing Social Media Information in
              Online {Peer-to-Peer} Lending",
  author   = "Zhang, Yuejin and Jia, Hengyue and Diao, Yunfei and Hai, Mo and
              Li, Haifeng",
  abstract = "In recent years, online Peer-to-Peer (P2P) lending market is
              rapidly expanding in China. In this paper, we use public dataset
              from PPDai, a leading online P2P platform in China to study the
              loan default. We construct a credit scoring model by fusing
              social media information based on decision tree. The experimental
              result shows that our model has good classification accuracy.
              From the credit scoring model and classification rules, we get a
              conclusion that the loan information, social media information,
              and credit information are most important factors for predicting
              the default. However, the credit rating is not as important as
              the platform described.",
  journal  = "Procedia Comput. Sci.",
  volume   =  91,
  pages    = "168--174",
  month    =  jan,
  year     =  2016,
  keywords = "credit scoring; default risk; decision tree; Peer-to-Peer
              lending; social media information"
}

@ARTICLE{Mi2017-ze,
  title     = "Can funding platforms' self-initiated financial innovation
               improve credit availability? Evidence from China's {P2P} market",
  author    = "Mi, Jinhong Jackson and Zhu, Hongfei",
  abstract  = "ABSTRACTIn this article, we used loan transaction data from a
               Chinese Peer-to-Peer (P2P) platform and employed a
               Difference-In-Differences (DID) approach to detect the effect of
               a self-initiated financial innovation introduced via P2P
               borrowing and lending. We found that the self-initiated
               financial innovation improved the availability of credit.",
  journal   = "Appl. Econ. Lett.",
  publisher = "Routledge",
  volume    =  24,
  number    =  6,
  pages     = "396--398",
  month     =  mar,
  year      =  2017
}

@ARTICLE{Jin2014-vo,
  title    = "Banking systemic vulnerabilities: A tail-risk dynamic {CIMDO}
              approach",
  author   = "Jin, Xisong and Nadal De Simone, Francisco de A",
  abstract = "This study proposes a novel framework which combines marginal
              probabilities of default estimated from a structural credit risk
              model with the consistent information multivariate density
              optimization (CIMDO) methodology and the generalized dynamic
              factor model (GDFM) supplemented by a dynamic t-copula. The
              framework models banks' default dependence explicitly and
              captures the time-varying non-linearities and feedback effects
              typical of financial markets. It measures banking systemic credit
              risk in the three forms categorized by the European Central Bank:
              (1) credit risk common to all banks; (2) credit risk in the
              banking system conditional on distress on a specific bank or
              combinations of banks; and (3) the buildup of banking system
              vulnerabilities over time which may unravel disorderly. In
              addition, the estimates of the common components of the banking
              sector short-term and conditional forward default measures
              contain early warning features, and the identification of their
              drivers is useful for macroprudential policy. Finally, the
              framework produces robust out-of-sample forecasts of the banking
              systemic credit risk measures. This paper advances the agenda of
              making macroprudential policy operational.",
  journal  = "Journal of Financial Stability",
  volume   =  14,
  pages    = "81--101",
  month    =  oct,
  year     =  2014,
  keywords = "Financial stability; Procyclicality; Macroprudential policy;
              Credit risk; Early warning indicators; Default probability;
              Non-linearities; Generalized dynamic factor model; Dynamic
              copulas; GARCH"
}

@ARTICLE{Nathan1992-aq,
  title     = "Operating efficiency of Canadian banks",
  author    = "Nathan, Alli and Neave, Edwin H",
  abstract  = "This article assesses Canadian banks' ability to realize scale
               economies and cost complementarities in joint production. The
               Canadian banking system, with its 10 or so large banks and 50
               smaller ones, offers a good database for a study of efficiency,
               especially since previous work suggests that the system's
               concentration has had little impact on system competitiveness.
               This article estimates a system of cost and cost share equations
               using Zellner's iterative seemingly unrelated regression
               technique, then evaluates scale economies and cost
               complementarities from the estimated cost functions' first and
               second partial derivatives. The article compares a model that
               classifies deposits as inputs with another that classifies them
               as outputs. The empirical findings indicate that deposits are
               better modelled as outputs than inputs; that Canadian banks
               organize to exhaust available sources of scale economies and
               economies in joint production; and that conclusions regarding
               scale economies and cost complementarities differ importantly
               according to whether deposits are modelled as inputs or as
               outputs.",
  journal   = "J Finan Serv Res",
  publisher = "Kluwer Academic Publishers",
  volume    =  6,
  number    =  3,
  pages     = "265--276",
  month     =  sep,
  year      =  1992,
  address   = "Dordrecht, Netherlands",
  language  = "en"
}

@ARTICLE{Frederickson1990-ag,
  title     = "Public Administration and Social Equity",
  author    = "Frederickson, H George",
  abstract  = "In 1968 a theory of social equity was developed and put forward
               as the ``third pillar'' for public administration, with the same
               status as economy and efficiency as values or principles to
               which public administration should adhere. Research on public
               administration finds that bureaucratic decision rules and the
               processes of policy implementation tend to favor principles of
               social equity. (Author)",
  journal   = "Public Adm. Rev.",
  publisher = "ERIC",
  volume    =  50,
  number    =  2,
  pages     = "228--237",
  year      =  1990,
  keywords  = "Equal Opportunities (Jobs); Public Administration; Public
               Policy; Social Discrimination",
  language  = "en"
}

@ARTICLE{Parhizgari2004-gx,
  title     = "Measures of organizational effectiveness: private and public
               sector performance",
  author    = "Parhizgari, A M and Ronald Gilbert, G",
  abstract  = "This study provides a platform for the comparison of measures of
               internal structures and processes associated with organizational
               effectiveness in the private and public sectors. The study is
               based on a sample of 11,352 cases from 28 private and 41 public
               sector organizations. Nine measures associated with
               organizational effectiveness are identified and then empirically
               derived for each sector. These measures are then compared across
               the two sectors. Following the application of rigorous
               statistical procedures, the authors conclude that the
               effectiveness measures applied in both the private and public
               sectors are significantly different. The ramifications of these
               results are discussed.",
  journal   = "Omega",
  publisher = "Elsevier",
  volume    =  32,
  number    =  3,
  pages     = "221--229",
  month     =  jun,
  year      =  2004,
  keywords  = "Organizational design; Organizational effectiveness measurement;
               Quality management; Operations and logistic management;
               Structural equation models; Survey research/design"
}

@ARTICLE{Doumpos2014-mm,
  title     = "Applying data envelopment analysis on accounting data to assess
               and optimize the efficiency of Greek local governments",
  author    = "Doumpos, Michael and Cohen, Sandra",
  abstract  = "The efficiency and effectiveness of local governments has become
               one of the main points of interest in public sector
               administration, as decision and policy making gradually move
               from the central to the local level. This paper introduces an
               efficiency analysis framework based on accrual accounting data
               obtained from the local governments' financial statements. Data
               envelopment analysis is used to obtain efficiency estimates,
               which are analyzed through a second stage regression against a
               set of efficiency explanatory factors. Furthermore, the optimal
               reallocation of the municipalities' inputs and outputs is
               explored to provide policy recommendations that a central
               government could implement in a budget reduction context.
               Detailed empirical results are presented from a panel data set
               of Greek municipalities over the period 2002--2009.",
  journal   = "Omega",
  publisher = "Elsevier",
  volume    =  46,
  pages     = "74--85",
  month     =  jul,
  year      =  2014,
  keywords  = "Local governments; Efficiency; Data envelopment analysis;
               Accrual accounting data"
}

@ARTICLE{Casu2006-ib,
  title     = "Evaluating cost efficiency in central administrative services in
               {UK} universities",
  author    = "Casu, B and Thanassoulis, E",
  abstract  = "This paper describes an attempt to evaluate cost efficiency in
               UK university central administration. The funding councils of
               higher education institutions have progressively evolved
               elaborate systems for measuring university performance in
               teaching quality and research. Indeed, funding of universities
               is linked to their performance in research. The allocation of
               resources between academic and administrative activities, on the
               other hand, has so far not been subject to scrutiny. Yet,
               expenditure on administration is typically some 30\% of that
               allocated to academic activities. This paper sets up a data
               envelopment analysis (DEA) framework to identify practices
               leading to cost-efficient central administrative services in UK
               universities. The problems in defining the unit of assessment
               and the relationship between the inputs and the outputs are
               clearly demonstrated.",
  journal   = "Omega",
  publisher = "Elsevier",
  volume    =  34,
  number    =  5,
  pages     = "417--426",
  month     =  oct,
  year      =  2006,
  keywords  = "Higher education; Administration; DEA; Performance measurement"
}

@ARTICLE{Da_Cruz2014-bp,
  title    = "Revisiting the determinants of local government performance",
  author   = "da Cruz, Nuno Ferreira and Marques, Rui Cunha",
  abstract = "Managing financial resources efficiently is a requirement for all
              levels of government. However, measuring the performance of
              governments or other public authorities is usually highly
              complex. The results of this type of assessment are likely to be
              biased or perverse. This study attempts to identify
              non-discretionary or exogenous variables that are associated with
              better/worse economic performance of local governments (the
              determinants of efficiency). Based on past research, the paper
              starts by providing a classification for the different types of
              determinants of local government performance. Afterwards, using
              data from all Portuguese municipalities, the relationship between
              a large number of factors and the efficiency scores is assessed.
              To accomplish this, several Tobit, OLS and double-bootstrap
              models were implemented. The efficiency scores are computed
              through non-parametric frontier methodologies. The results
              indicate that analysts must be prudent while interpreting the
              economic results achieved by each municipality. To be impartial
              and robust any performance evaluation model should (at least)
              consider the effects of the determinants of cost efficiency
              identified in this paper.",
  journal  = "Omega",
  volume   =  44,
  pages    = "91--103",
  month    =  apr,
  year     =  2014,
  keywords = "DEA; Efficiency; Local government; Operational environment;
              Policy analysis"
}

@INCOLLECTION{Johnson2015-bo,
  title     = "Stochastic Nonparametric Approach to Efficiency Analysis: A
               Unified Framework",
  booktitle = "Data Envelopment Analysis",
  author    = "Johnson, Andrew and Kuosmanen, Timo and Saastamoinen, Antti",
  editor    = "Zhu, Joe",
  abstract  = "Bridging the gap between axiomatic Data Envelopment Analysis
               (DEA) and econometric Stochastic Frontier Analysis (SFA) has
               been one of the most vexing problems in the field of efficiency
               analysis. Recent developments in multivariate convex regression,
               particularly Convex Nonparametric Least Squares (CNLS) method,
               have led to the full integration of DEA and SFA into a unified
               framework of productivity analysis, referred to as Stochastic
               Nonparametric Envelopment of Data (StoNED). The unified
               framework of StoNED offers a general and flexible platform for
               efficiency analysis and related themes such as frontier
               estimation and production analysis, allowing one to combine
               existing tools of efficiency analysis in novel ways across the
               DEA-SFA spectrum, facilitating new opportunities for further
               methodological development. This chapter provides an updated and
               elaborated presentation of the CNLS and StoNED methods. This
               chapter also extends the scope of the StoNED method in several
               directions. Most notably, this chapter examines quantile
               estimation using StoNED and an extension of the StoNED method to
               the general case of multiple inputs and multiple outputs. This
               chapter also provides a detailed discussion of how to model
               heteroscedasticity in the inefficiency and noise terms.",
  publisher = "Springer, Boston, MA",
  pages     = "191--244",
  series    = "International Series in Operations Research \& Management
               Science",
  year      =  2015,
  language  = "en"
}

@ARTICLE{Lopez-Espinosa2012-qi,
  title    = "Short-term wholesale funding and systemic risk: A global {CoVaR}
              approach",
  author   = "L{\'o}pez-Espinosa, Germ{\'a}n and Moreno, Antonio and Rubia,
              Antonio and Valderrama, Laura",
  abstract = "We use the CoVaR approach to identify the main factors behind
              systemic risk in a set of large international banks. We find that
              short-term wholesale funding is a key determinant in triggering
              systemic risk episodes. In contrast, we find weaker evidence that
              either size or leverage contributes to systemic risk within the
              class of large international banks. We also show that asymmetries
              based on the sign of bank returns play an important role in
              capturing the sensitivity of system-wide risk to individual bank
              returns. Since short-term wholesale funding emerges as the most
              relevant systemic factor, our results support the Basel
              Committee's proposal to introduce a net stable funding ratio,
              penalizing excessive exposure to liquidity risk.",
  journal  = "Journal of Banking \& Finance",
  volume   =  36,
  number   =  12,
  pages    = "3150--3162",
  month    =  dec,
  year     =  2012,
  keywords = "Systemic importance; Liquidity risk; Macroprudential regulation"
}

@ARTICLE{Mensah1993-ld,
  title     = "Measuring Production Efficiency in a {Not-for-Profit} Setting:
               An Extension",
  author    = "Mensah, Yaw M and Li, Shu-Hsing",
  abstract  = "In a recent study, Hayes and Millar (1990) presented empirical
               evidence on the cost function and apparent cost-optimizing
               behavior of the local managers of 33 county jails in Tennessee.
               Two main arguments were advanced: (1) line-item budgeting (LIB)
               was an ineffective control because the cost shares are assumed
               fixed in such budgetary settings, and (2) the budgetary control
               and performance evaluation process in not-for-profit settings
               could be improved if the underlying cost function was estimated
               by using a flexible functional form such as the translog to gain
               knowledge of possible input substitution and output
               transformation. This translog budget model was viewed implicitly
               as a superior alternative to the more traditional LIB approach.
               The main objective of this study is to extend Hayes and Millar's
               idea of a translog budget model by outlining a more complete
               budgetary system. It is shown that a frontier cost function
               generated from the ordinary least squares (OLS) translog
               function can be used to identify four types of inefficiencies:
               the degree of technical, allocative, and scale inefficiency as
               well as institutional X-inefficiency (Liebenstein 1966). These
               four types of inefficiencies are then linked to both the
               long-term and short-term objectives of budgeting; namely,
               performance evaluation, subordinate manager motivation,
               planning, and control. The second objective of the study is to
               compare the translog budget model (as revised) against the most
               frequently applied alternative technique, the nonparametric data
               envelopment analysis (DEA), in the same context. From the
               standpoint of a routine budgetary control system, it was found
               that the DEA model is more suitable than the translog model. In
               not-for-profit settings, however, an econometric model such as
               the translog may be needed initially in the specification of the
               most appropriate input and output measures. Although the
               effectiveness of LIB as a control is not addressed directly, our
               analysis shows that the line-item data mandatory with this
               approach is an essential part of any effective budgetary system
               designed for a not-for-profit setting.",
  journal   = "The Accounting Review",
  publisher = "American Accounting Association",
  volume    =  68,
  number    =  1,
  pages     = "66--88",
  year      =  1993
}

@ARTICLE{Szymanski2003-lg,
  title     = "The Economic Design of Sporting Contests",
  author    = "Szymanski, Stefan",
  abstract  = "This paper reviews the literature on commercial sport through
               the lens of the economic theory of contests/tournaments. It
               seeks to draw together research on incentives in individualistic
               sports such as golf and footraces with the research on
               uncertainty of outcome and competitive balance in team sports
               such as baseball and soccer. The contest framework is used to
               analyze issues such as the optimal distribution of prizes, the
               impact of revenue sharing, salary caps, draft rules, and a
               variety of other restraints commonly employed in sports leagues.
               The paper draws heavily on a comparative analysis of contest
               organization, in particular between North America and Europe.",
  journal   = "J. Econ. Lit.",
  publisher = "ingentaconnect.com",
  volume    =  41,
  number    =  4,
  pages     = "1137--1187",
  month     =  dec,
  year      =  2003
}

@ARTICLE{Ghosh2001-zu,
  title    = "Does operating performance really improve following corporate
              acquisitions?",
  author   = "Ghosh, Aloke",
  abstract = "Previous research indicates that operating performance improves
              following corporate acquisitions relative to industry-median
              firms. Such performance results are likely to be biased because
              acquiring firms undertake acquisitions following a period of
              superior performance and they are generally larger than
              industry-median firms. Using firms matched on performance and
              size as a benchmark, I find no evidence that operating
              performance improves following acquisitions. I also analyze if
              performance is higher in cash acquisitions as suggested by
              various studies. The results indicate that cash flows increase
              significantly following acquisitions that are made with cash, but
              decline for stock acquisitions.",
  journal  = "Journal of Corporate Finance",
  volume   =  7,
  number   =  2,
  pages    = "151--178",
  month    =  jun,
  year     =  2001,
  keywords = "Acquisitions; Operating performance"
}

@ARTICLE{Powell2005-hu,
  title    = "Does operating performance increase post-takeover for {UK}
              takeovers? A comparison of performance measures and benchmarks",
  author   = "Powell, Ronan G and Stark, Andrew W",
  abstract = "Using several benchmarks and operating performance measures, the
              results from this paper suggest that takeovers completed in the
              UK over the period 1985 to 1993 result in modest improvements in
              operating performance. Using a matching procedure similar to that
              employed by Loughran and Ritter [J. Finance 52 (1997) 1823], in
              which benchmark firms are selected on the basis of several
              pre-takeover characteristics, the median increase in
              post-takeover performance for acquiring firms ranges from 0.13\%
              per annum to a statistically significant 1.78\% per annum,
              depending on the definition of operating performance used and
              choice of deflator. Using the same matching scheme in a Healy et
              al. [J. Financ. Econ. 31 (1992) 135] methodology, in which
              post-takeover performance is regressed on a combined target and
              acquirer pre-takeover performance, reveals larger improvements in
              operating performance, ranging from 0.80\% to a statistically
              significant 3.1\%, again depending on the definition of operating
              performance employed and deflator chosen. While there is some
              evidence that factors such as industrial relatedness and the
              removal of the target CEO have an impact on post-takeover
              performance, method of payment is found to have an insignificant
              impact.",
  journal  = "Journal of Corporate Finance",
  volume   =  11,
  number   =  1,
  pages    = "293--317",
  month    =  mar,
  year     =  2005,
  keywords = "Takeovers; Operating cash flows; Benchmarks; Market validation"
}

@ARTICLE{Martynova2008-jz,
  title    = "A century of corporate takeovers: What have we learned and where
              do we stand?",
  author   = "Martynova, Marina and Renneboog, Luc",
  abstract = "This paper reviews the vast academic literature on the market for
              corporate control. Our main focus is the cyclical wave pattern
              that this market exhibits. We address the following questions:
              Why do we observe recurring surges and downfalls in M\&A
              activity? Why do managers herd in their takeover decisions? Is
              takeover activity fuelled by capital market developments? Does a
              transfer of control generate shareholder gains and do such gains
              differ across takeover waves? What caused the formation of
              conglomerate firms in the wave of the 1960s and their
              de-conglomeration in the 1980s and 1990s? And, why do we observe
              time- and country-clustering of hostile takeover activity? We
              find that the patterns of takeover activity and their
              profitability vary significantly across takeover waves. Despite
              such diversity, all waves still have some common factors: they
              are preceded by technological or industrial shocks, and occur in
              a positive economic and political environment, amidst rapid
              credit expansion and stock market booms. Takeovers towards the
              end of each wave are usually driven by non-rational, frequently
              self-interested managerial decision-making.",
  journal  = "Journal of Banking \& Finance",
  volume   =  32,
  number   =  10,
  pages    = "2148--2177",
  month    =  oct,
  year     =  2008,
  keywords = "G34; Takeovers; Mergers and acquisitions; Takeover waves; Market
              timing; Industry shocks"
}

@ARTICLE{Huizinga2008-dd,
  title    = "International profit shifting within multinationals: A
              multi-country perspective",
  author   = "Huizinga, Harry and Laeven, Luc",
  abstract = "We model the opportunities and incentives generated by
              international tax differences for international profit shifting
              by multinationals. The model considers not only profit shifting
              arising from international tax differences between affiliates and
              parent companies, but also from tax differences between
              affiliates in different host countries. Our model yields the
              prediction that a multinational's profit shifting in a country
              depends on a weighted average of international tax rate
              differences between all countries where the multinational is
              active. Using a unique dataset containing detailed firm-level
              information on the parent companies and subsidiaries of European
              multinationals and information about the international tax
              system, we test our model and empirically examine the extent of
              intra-European profit shifting by European multinationals. On
              average, we find a semi-elasticity of reported profits with
              respect to the top statutory tax rate of 1.3, while shifting
              costs are estimated to be 0.6\% of the tax base. International
              profit shifting leads to a substantial redistribution of national
              corporate tax revenues. Many European nations appear to gain
              revenues from profit shifting by multinationals largely at the
              expense of Germany.",
  journal  = "J. Public Econ.",
  volume   =  92,
  number   =  5,
  pages    = "1164--1182",
  month    =  jun,
  year     =  2008,
  keywords = "Corporate taxation; International profit shifting"
}

@ARTICLE{Bai1998-je,
  title     = "Testing For and Dating Common Breaks in Multivariate Time Series",
  author    = "Bai, Jushan and Lumsdaine, Robin L and Stock, James H",
  abstract  = "This paper develops methods for constructing asymptotically
               valid confidence intervals for the date of a single break in
               multivariate time series, including I(0), I(1), and
               deterministically trending regressors. Although the width of the
               asymptotic confidence interval does not decrease as the sample
               size increases, it is inversely related to the number of series
               which have a common break date, so there are substantial gains
               to multivariate inference about break dates. These methods are
               applied to two empirical examples: the mean growth rate of
               output in three European countries, and the mean growth rate of
               U.S. consumption, investment, and output.",
  journal   = "Rev. Econ. Stud.",
  publisher = "Oxford University Press",
  volume    =  65,
  number    =  3,
  pages     = "395--432",
  month     =  jul,
  year      =  1998
}

@INCOLLECTION{Blundell2003-nt,
  title     = "Endogeneity in Nonparametric and Semiparametric Regression
               Models",
  booktitle = "Advances in Economics and Econometrics",
  author    = "Blundell, Richard and Powell, James L",
  editor    = "Dewatripont, Mathias and Hansen, Lars Peter and Turnovsky,
               Stephen J and Dewatripont, Mathias and Hansen, Lars Peter and
               Turnovsky, Stephen J",
  publisher = "Cambridge University Press",
  pages     = "312--357",
  year      =  2003,
  address   = "Cambridge",
  keywords  = "Economics: General Interest, Mathematical Modeling and Methods,
               Econometric Society Monographs,"
}

@ARTICLE{Hand2016-ig,
  title   = "Editorial: `Big data' and data sharing",
  author  = "Hand, David J",
  journal = "J. R. Stat. Soc. A",
  volume  =  179,
  number  =  3,
  pages   = "629--631",
  month   =  jun,
  year    =  2016
}

@ARTICLE{Brogaard2014-bs,
  title     = "{High-Frequency} Trading and Price Discovery",
  author    = "Brogaard, Jonathan and Hendershott, Terrence and Riordan, Ryan",
  abstract  = "We examine the role of high-frequency traders (HFTs) in price
               discovery and price efficiency. Overall HFTs facilitate price
               efficiency by trading in the direction of permanent price
               changes and in the opposite direction of transitory pricing
               errors, both on average and on the highest volatility days. This
               is done through their liquidity demanding orders. In contrast,
               HFTs' liquidity supplying orders are adversely selected. The
               direction of HFTs' trading predicts price changes over short
               horizons measured in seconds. The direction of HFTs' trading is
               correlated with public information, such as macro news
               announcements, market-wide price movements, and limit order book
               imbalances.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  27,
  number    =  8,
  pages     = "2267--2306",
  month     =  aug,
  year      =  2014
}

@ARTICLE{Mourao2016-hv,
  title     = "Soccer transfers, team efficiency and the sports cycle in the
               most valued European soccer leagues -- have European soccer
               teams been efficient in trading players?",
  author    = "Mourao, Paulo Reis",
  abstract  = "ABSTRACTThe sale of soccer players is a serious issue for the
               sustainability of professional teams. This article discusses the
               efficiency of the values that 183 European soccer teams have
               received for the sales of their players since 2007. We estimated
               stochastic frontiers for these soccer teams using stochastic
               frontier analysis. We found that teams with higher numbers of
               titles, with huge past acquisitions of players, and achieving
               good rankings in the previous season tend to receive more
               transfer inflows. The efficiency of these inflows can be
               significantly influenced if the team exhibits a long sports
               history or if the team participates in the Champions League or
               in the Europa League.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  48,
  number    =  56,
  pages     = "5513--5524",
  month     =  dec,
  year      =  2016
}

@ARTICLE{Anderson1952-fr,
  title     = "Asymptotic Theory of Certain ``Goodness of Fit'' Criteria Based
               on Stochastic Processes",
  author    = "Anderson, T W and Darling, D A",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Math. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  23,
  number    =  2,
  pages     = "193--212",
  month     =  jun,
  year      =  1952,
  language  = "en"
}

@ARTICLE{Nowy2015-mf,
  title     = "Organizational performance of nonprofit and for-profit sport
               organizations",
  author    = "Nowy, Tobias and Wicker, Pamela and Feiler, Svenja and Breuer,
               Christoph",
  abstract  = "Research question: This study contributes to the organizational
               performance literature by conceptualizing differences in
               performance between nonprofit and for-profit organizations using
               property rights theory and suggesting comparative measures for
               multiple performance dimensions. It advances the following
               research question: Are there significant differences in
               organizational performance between nonprofit and for-profit
               sport organizations?Research methods: Data from a nationwide
               online survey of nonprofit (n = 1640) and for-profit sport
               organizations (n = 732) in German equestrian sport are used for
               the empirical analysis. The challenge was to find adequate
               measures for the comparison of the two types of legal forms.
               Altogether, 22 regression models for performance measures across
               four dimensions (financial, product, customer, strategic) are
               estimated with the legal form (nonprofit vs. for-profit) and
               some controls as independent variables.Results and findings: The
               results show that for-profit organizations outperform nonprofit
               organizations in terms of overall financial performance, while
               nonprofits excel with regard to price structure. For-profits
               attach more importance to program quality, employee
               qualifications, and strategies. No significant differences can
               be observed in the product dimension. However, for-profits tend
               to focus on customer groups that have typically been targeted by
               nonprofits. Since the models also control for organizational
               size and resources, the results indicate that differences are
               not always attributable to the legal form.Implications: The
               findings imply that for-profits do not outperform nonprofits in
               all performance dimensions. Nonprofits should consider that
               for-profits have invaded some of their typical domains and
               should be more flexible when market demand changes, while
               for-profits should reconsider their price structure.",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  15,
  number    =  2,
  pages     = "155--175",
  month     =  mar,
  year      =  2015
}

@ARTICLE{Adair2000-jb,
  title     = "House Prices and Accessibility: The Testing of Relationships
               within the Belfast Urban Area",
  author    = "Adair, Alastair and McGreal, Stanley and Smyth, Austin and
               Cooper, James and Ryley, Tim",
  abstract  = "The debate surrounding the sustainability of urban areas and the
               need to reduce dependency upon the car as the primary mode of
               transportation has rekindled interest in the relationship
               between accessibility, price and location of owner occupied
               housing. Traditionally, attempts at quantification have used
               hedonic analysis employing straight-line distance measurements
               to focal points such as the CBD as an indicator of
               accessibility. This paper focuses upon factors affecting the
               price structure of residential property in the Belfast Urban
               Area, examining the relative influence of property
               characteristics, socio-economic factors and the impact of
               accessibility. The model employs an accessibility index computed
               for each of 182 traffic zones and uses transaction data for a
               sample of 2648 residential properties sold during 1996. Results
               indicate that accessibility is of little significance in
               explaining variation in house prices at a city-wide scale but at
               a sub-market level, particularly in lower-income areas,
               accessibility can be an important influence. The analysis
               highlights the importance of investigation at a sub-market level
               and draws conclusions regarding the complexity of relationships
               within an urban area.",
  journal   = "Housing Studies",
  publisher = "Routledge",
  volume    =  15,
  number    =  5,
  pages     = "699--716",
  month     =  sep,
  year      =  2000
}

@ARTICLE{Altunbas2000-qr,
  title    = "Efficiency and risk in Japanese banking",
  author   = "Altunbas, Yener and Liu, Ming-Hau and Molyneux, Philip and Seth,
              Rama",
  abstract = "This paper investigates the impact of risk and quality factors on
              banks' cost by using the stochastic cost frontier methodology to
              evaluate scale and X-inefficiencies, as well as technical change
              for a sample of Japanese commercial banks between 1993 and 1996.
              Loan-loss provisions are included in the cost frontier model to
              control for output quality, with a financial capital and a
              liquidity ratio included to control risk. Following the approach
              suggested in Mester (1996) we show that if risk and quality
              factors are not taken into account optimal bank size tends to be
              overstated. That is, optimal bank size is considerably smaller
              when risk and quality factors are taken into account when
              modelling the cost characteristics of Japanese banks. We also
              find that the level of financial capital has the biggest
              influence on the scale efficiency estimates. X-inefficiency
              estimates, in contrast, appear less sensitive to risk and quality
              factors. Our results also suggest that scale inefficiencies
              dominate X-inefficiencies. These are important findings because
              they contrast with the results of previous studies on Japanese
              banking. In particular, the results indicate an alternative
              policy prescription, namely, that the largest banks should shrink
              to benefit from scale advantages. It also seems that financial
              capital has the largest influence on optimal bank size.",
  journal  = "Journal of Banking \& Finance",
  volume   =  24,
  number   =  10,
  pages    = "1605--1628",
  month    =  oct,
  year     =  2000,
  keywords = "Japanese banks; Cost functions; Economies of scale;
              Inefficiencies; Technical change"
}

@ARTICLE{Girardi2013-jj,
  title    = "Systemic risk measurement: Multivariate {GARCH} estimation of
              {CoVaR}",
  author   = "Girardi, Giulio and Tolga Erg{\"u}n, A",
  abstract = "We modify Adrian and Brunnermeier's (2011) CoVaR, the VaR of the
              financial system conditional on an institution being in financial
              distress. We change the definition of financial distress from an
              institution being exactly at its VaR to being at most at its VaR.
              This change allows us to consider more severe distress events, to
              backtest CoVaR, and to improve its consistency (monotonicity)
              with respect to the dependence parameter. We define the systemic
              risk contribution of an institution as the change from its CoVaR
              in its benchmark state (defined as a one-standard deviation
              event) to its CoVaR under financial distress. We estimate the
              systemic risk contributions of four financial industry groups
              consisting of a large number of institutions for the sample
              period June 2000 to February 2008 and the 12months prior to the
              beginning of the crisis. We also investigate the link between
              institutions' contributions to systemic risk and their
              characteristics.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  8,
  pages    = "3169--3180",
  month    =  aug,
  year     =  2013,
  keywords = "Value-at-Risk; Conditional Value-at-Risk; Systemic Risk; DCC
              model"
}

@ARTICLE{Demirguc-Kunt2008-oq,
  title     = "Banking on the principles: Compliance with Basel Core Principles
               and bank soundness",
  author    = "Demirg{\"u}{\c c}-Kunt, Asl{\i} and Detragiache, Enrica and
               Tressel, Thierry",
  abstract  = "This study finds that banks receive more favorable Moody's
               financial strength ratings in countries with better compliance
               with Basel Core Principles related to information provision. The
               results are robust to controlling for broad indexes of
               institutional quality, macroeconomic variables, sovereign
               ratings, and reverse causality. Compliance with other Core
               Principles does not affect ratings robustly. Measuring bank
               soundness through Z-scores yields broadly similar results for
               advanced and emerging markets. Countries aiming to upgrade
               banking regulation and supervision should consider giving
               priority to information provision over other elements of the
               core principles.",
  journal   = "Journal of Financial Intermediation",
  publisher = "Elsevier",
  volume    =  17,
  number    =  4,
  pages     = "511--542",
  month     =  oct,
  year      =  2008,
  keywords  = "Bank soundness; Basel Core Principles; Regulation and
               supervision"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Johnson2011-fe,
  title     = "One-stage estimation of the effects of operational conditions
               and practices on productive performance: asymptotically normal
               and efficient, root-n consistent {StoNEZD} method",
  author    = "Johnson, Andrew L and Kuosmanen, Timo",
  abstract  = "Understanding the effects of operational conditions and
               practices on productive efficiency can provide valuable economic
               and managerial insights. The conventional approach is to use a
               two-stage method where the efficiency estimates are regressed on
               contextual variables representing the operational conditions.
               The main problem of the two-stage approach is that it ignores
               the correlations between inputs and contextual variables. To
               address this shortcoming, we build on the recently developed
               regression interpretation of data envelopment analysis (DEA) to
               develop a new one-stage semi-nonparametric estimator that
               combines the nonparametric DEA-style frontier with a regression
               model of the contextual variables. The new method is referred to
               as stochastic semi-nonparametric envelopment of z variables data
               (StoNEZD). The StoNEZD estimator for the contextual variables is
               shown to be statistically consistent under less restrictive
               assumptions than those required by the two-stage DEA estimator.
               Further, the StoNEZD estimator is shown to be unbiased,
               asymptotically efficient, asymptotically normally distributed,
               and converge at the standard parametric rate of order n‚àí1/2.
               Therefore, the conventional methods of statistical testing and
               confidence intervals apply for asymptotic inference. Finite
               sample performance of the proposed estimators is examined
               through Monte Carlo simulations.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  36,
  number    =  2,
  pages     = "219--230",
  month     =  oct,
  year      =  2011,
  keywords  = "Data envelopment analysis; Partial linear model; Two-stage
               method",
  language  = "en"
}

@ARTICLE{Leclerc2003-mr,
  title     = "Production et rationalisation des interm{\'e}diaires financiers:
               Le{\c c}ons {\`a} tirer de l'exp{\'e}rience des Caisses
               Populaires Acadiennes",
  author    = "Leclerc, Andr{\'e} and Fortin, Mario",
  abstract  = "Nous mesurons {\`a} l'aide du DEA (Data Envelopment Analysis)
               l'efficacit{\'e} des caisses populaires acadiennes en utilisant
               la valeur des produits d'interm{\'e}diation ainsi que le nombre
               de transactions r{\'e}alis{\'e}es par chaque caisse entre 1997
               et 2000, au moment o{\`u} un important programme de fusions et
               de r{\'e}ing{\'e}nierie financi{\`e}re {\'e}tait mis en place.
               Cette analyse permet d'{\'e}tablir plusieurs r{\'e}sultats. Tout
               d'abord, l'inclusion des produits transactionnels r{\'e}duit
               environ de moiti{\'e} l'inefficacit{\'e} technique et
               {\'e}conomique par rapport {\`a} l'inefficacit{\'e} obtenue
               lorsque l'output des caisses est limit{\'e} seulement aux
               produits transactionnels. Un algorithme d'auto-amor{\c c}age
               permet de v{\'e}rifier que ce r{\'e}sultat est statistiquement
               significatif. Ensuite, nous montrons que le programme de fusions
               a touch{\'e} surtout les caisses moins efficaces et a permis
               d'augmenter de fa{\c c}on importante leur efficacit{\'e}
               gr{\^a}ce {\`a} une baisse du nombre d'employ{\'e}s en
               {\'e}quivalent temps complet. Finalement, nous montrons que
               d'importants progr{\`e}s technologiques ont {\'e}t{\'e}
               r{\'e}alis{\'e}s entre 1997 et 2000 en raison de l'accroissement
               du nombre de transactions informatis{\'e}es. Des gains
               additionnels de productivit{\'e} ont {\'e}t{\'e} rendus possible
               gr{\^a}ce {\`a} l'am{\'e}lioration de l'efficacit{\'e} des
               caisses ayant particip{\'e}{\`a} une fusion tandis que les
               caisses n'ayant pas fusionn{\'e}, bien que plus efficaces au
               d{\'e}part que les autres, n'ont pas r{\'e}alis{\'e} de gains
               d'efficacit{\'e}.",
  journal   = "Annals of Public and Cooperative Economics",
  publisher = "Blackwell Publishing Ltd",
  volume    =  74,
  number    =  3,
  pages     = "397--432",
  month     =  sep,
  year      =  2003
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Fortin2011-ht,
  title     = "{L'EFFICIENCE} {DES} {COOPERATIVES} {DE} {SERVICES}
               {FINANCIERS}: {UNE} {ANALYSE} {DE} {LA} {CONTRIBUTION} {DU}
               {MILIEU*}",
  author    = "Fortin, Mario and Leclerc, Andr{\'e}",
  abstract  = "R{\'E}SUM{\'E}**: Le faible niveau moyen d'efficience des
               institutions bancaires habituellement trouv{\'e} dans les
               {\'e}tudes empiriques a {\'e}t{\'e} qualifi{\'e} de ¬´ bo{\^\i}te
               noire ¬ª par Berger et Mester (1997). Cette {\'e}tude cherche
               {\`a} identifier si les caract{\'e}ristiques de l'environnement
               de m{\^e}me que celles qui sont propres {\`a} une
               coop{\'e}rative d'{\'e}pargne et de cr{\'e}dit pourraient
               expliquer une partie des {\'e}carts de performance apparaissant
               dans les scores d'efficience. Le mod{\`e}le de co{\^u}t que nous
               avons estim{\'e} est bas{\'e} sur la valeur ajout{\'e}e par
               l'interm{\'e}diation financi{\`e}re. Par ailleurs, pour
               {\'e}viter la perte d'informations d{\'e}coulant de la borne
               {\`a} l'unit{\'e} des scores d'efficience d{\'e}coulant du DEA,
               nous avons compar{\'e} les r{\'e}sultats d'une analyse avec le
               score d'efficience et de superefficience. Nos r{\'e}sultats
               montrent qu'au moins 34\% des {\'e}carts de scores peuvent
               {\^e}tre expliqu{\'e}s par un ensemble limit{\'e} de variables:
               taille de la coop{\'e}rative, taux de capitalisation,
               {\'e}pargne par membre, nombre de membres et type de march{\'e}.",
  journal   = "Annals of Public and Cooperative Economics",
  publisher = "Blackwell Publishing Ltd",
  volume    =  82,
  number    =  1,
  pages     = "45--62",
  month     =  mar,
  year      =  2011
}

@ARTICLE{Kassay2016-bn,
  title    = "Competition and Cooperation in European Professional Club
              Football",
  author   = "Kassay, Lili and G{\'e}czi, G{\'a}bor",
  abstract = "The aim of the article is to highlight the theoretical assumption
              that the relative competitive disadvantage between markets with
              different maximum sizes can be partially counteracted with
              strategic thinking and business-like operations. The research
              question presented in this article is whether there are any
              management tools available for clubs which they can use to
              improve their business competitiveness above and beyond the
              limitations of their maximum market size. According to the
              research hypothesis, there is such a tool available for clubs:
              the management and operation of a football club in a
              well-organized and business-like manner. The method of analysis
              is the so-called Grounded Theory (Glaser, \& Strauss, 1967;
              Locke, 2001), which is an abstract analytical schema with a
              systematic data analysis process. The data collection
              incorporates desk research, comparative analysis, organizational
              review, and in-depth interviews. The results are presented
              according to the following dimensions: a) relevance of the
              research question; b) specification of groups of relevant
              management tools; c) the role of the national football
              association in the process of market development; d) the role of
              the owners in this process; and e) how the structure of the
              sector and its operating processes can guarantee the efficient
              utilization of all the material resources which have been plowed
              into the sector over the last four to five years. According to
              the conclusions, the creation of an operating system and club
              model that allows for the utilization of resources - maximized
              market revenues and governmental sources - in the most effective
              way is an unavoidable challenge. The formation of the sustainable
              operations of clubs is fundamentally influenced by club owners.
              An inspection of the interaction between club owners, head
              coaches, and players is a key task in the process of creating a
              new club model.",
  journal  = "Physical Culture and Sport. Studies and Research",
  volume   =  69,
  number   =  1,
  pages    = "1499",
  month    =  jan,
  year     =  2016
}

@ARTICLE{Anand2013-za,
  title    = "A network model of financial system resilience",
  author   = "Anand, Kartik and Gai, Prasanna and Kapadia, Sujit and Brennan,
              Simon and Willison, Matthew",
  abstract = "We examine the role of macroeconomic fluctuations, asset market
              liquidity, and network structure in determining contagion and
              aggregate losses in a stylized financial system. Systemic
              instability is explored in a financial network comprising three
              distinct, but interconnected, sets of agents -- domestic banks,
              overseas banks, and firms. Calibrating the model to advanced
              country banking sector data, this preliminarily model generates
              broadly sensible aggregate loss distributions which are bimodal
              in nature. We demonstrate how systemic crises may occur and
              analyse how our results are influenced by firesale externalities
              and the feedback effects from curtailed lending in the
              macroeconomy. We also illustrate the resilience of our model
              financial system to stress scenarios with sharply rising
              corporate default rates and falling asset prices.",
  journal  = "J. Econ. Behav. Organ.",
  volume   =  85,
  pages    = "219--235",
  month    =  jan,
  year     =  2013,
  keywords = "Contagion; Financial crises; Network models; Systemic risk"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wilson2013-xz,
  title    = "The relationship between ownership structure and club performance
              in the English Premier League",
  author   = "Wilson, Robert and Plumley, Daniel and Ramchandani, Girish",
  abstract = "Purpose -- The purpose of this paper is three‚Äêfold. First, to
              explore the relationship between the financial and sporting
              performance of clubs competing in the English Premier League
              (EPL). Second, to investigate the effect of different models of
              EPL club ownership on financial and league performance. Third, to
              review the finances of EPL clubs in the context of UEFA's
              Financial Fair Play regulations.Design/methodology/approach --
              Financial data from annual reports for the period 2001‚Äê2010 was
              collected for 20 EPL clubs. Correlation analysis was conducted to
              examine the relationship between the finances of EPL clubs and
              their league position. One‚Äêway analysis of variance (ANOVA) tests
              were then used to examine the effect of ownership type on clubs'
              financial and league performances. Where the results of ANOVA
              testing revealed statistically significant differences between
              groups, these were investigated further using appropriate post
              hoc procedures.Findings -- The stock market model of ownership
              returned better financial health relative to privately owned
              (domestic and foreign) clubs. However, clubs owned privately by
              foreign investors or on the stock market performed better in the
              league in comparison with domestically owned clubs. The stock
              market model was more likely to comply with Financial Fair Play
              regulations.Originality/value -- The paper confirms empirically
              that football clubs that float on the stock market are in better
              financial health and that clubs in pursuit of short‚Äêterm sporting
              excellence are reliant on substantial investment, in this case
              from foreign investors.",
  journal  = "Sport, Business and Management: An International Journal",
  volume   =  3,
  number   =  1,
  pages    = "19--36",
  year     =  2013
}

@ARTICLE{Schindler2014-he,
  title     = "Persistence and Predictability in {UK} House Price Movements",
  author    = "Schindler, Felix",
  abstract  = "This paper extends the analysis of predictability and
               persistence of inflation-adjusted house price movements in the
               UK housing market both on a regional level across 13 regions and
               on a nationwide level. Applying a univariate time series
               approach, the results from the quarterly transaction-based
               Nationwide Building Society indices from 1974 to 2009 provide
               empirical evidence for a high persistence of house price
               movements. In addition to conducting parametric and
               non-parametric tests, we provide technical trading strategies as
               a robustness check to compare predictability across markets and
               to test whether or not the detected persistence can also be used
               for detecting turning points in the market. The empirical
               findings from the technical trading strategies support the
               results from the statistical tests. Moving average-based trading
               strategies perform extremely well in the southern regions, while
               trading strategies are less profitable for the northern regions
               and Wales. Thus, from an investors' perspective, there are
               excess real returns from moving average-based strategies
               compared to a buy-and-hold strategy for most regional markets.
               From a household perspective, the findings support the
               importance of derivative markets where households could hedge
               their risk exposure from being homeowner.",
  journal   = "J. Real Estate Fin. Econ.",
  publisher = "Springer US",
  volume    =  48,
  number    =  1,
  pages     = "132--163",
  month     =  jan,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Atkinson1998-vo,
  title     = "Estimating Radial Measures of Productivity Growth: Frontier vs
               {Non-Frontier} Approaches",
  author    = "Atkinson, Scott E and Cornwell, Christopher",
  abstract  = "In this paper, we propose an alternative econometric framework
               for estimating and decomposing productivity change that does not
               require a distribution for inefficiency or the uncorrelatedness
               between inefficiency and the regressors. We develop our
               methodology for the input-oriented radial measure of
               productivity change and establish that this equals the negative
               of the time change in the log cost function. Our econometric
               framework is based on a fixed-effects, multiple-output cost
               frontier, where we decompose productivity change into discrete
               shifts in the frontier and changes in firm efficiency levels
               relative to the frontier. We also show that the standard
               non-frontier specification is nested within our frontier model
               and thus can produce different estimates of productivity change.
               Using a panel of twelve US railroads from 1951 to 1975, our
               estimated cost frontier suggests average annual productivity
               growth of roughly 0.3 percent, with efficiency change rising
               then falling over the period. Specification tests reject the
               non-frontier model, which yields smaller gains in productivity.",
  journal   = "Journal of Productivity Analysis",
  publisher = "Kluwer Academic Publishers",
  volume    =  10,
  number    =  1,
  pages     = "35--46",
  month     =  jul,
  year      =  1998,
  language  = "en"
}

@BOOK{Szymanski2015-od,
  title     = "Money and Soccer: A Soccernomics Guide: Why Chievo Verona,
               Unterhaching, and Scunthorpe United Will Never Win the Champions
               League, Why Manchester City, Roma, and Paris St. Germain Can,
               and Why Real Madrid, Bayern Munich, and Manchester United Cannot
               Be Stopped",
  author    = "Szymanski, Stefan and Kuper, Simon",
  abstract  = "Modern soccer is big business. From the ill-received takeover of
               Manchester United by the Glazer family to Paris Saint Germain's
               current shopping spree for the best footballers on the planet,
               soccer finance has become an increasingly important part of the
               game.Barely a summer goes by now without a cherished club going
               into administration or a wealthy businessman funding a mid table
               team's ascension to Champions League competitor. Meanwhile, the
               twice-annual multi-million dollar merry-go-round of transfer
               season sees players (and now managers) signed for sums thought
               impossible just a decade ago. Understanding soccer finance has
               become essential for comprehending the beautiful game. But for
               many fans, soccer finance remains, frustratingly, a world that
               is opaque and difficult to grasp.Stefan Szymanski, co-author of
               the bestselling Soccernomics, tackles every soccer fan's burning
               questions in Money and Soccer: A Soccernomics Guide. From the
               abolition of the maximum wage in the 1960s, through to the
               impact of TV money both at home and abroad in the 1990s and
               2000s, Szymanski explains how money, or lack of, affects your
               favorite club. Drawing on extensive research into financial
               records dating back to the 1970s, Szymanski provides clear
               analysis of the way that clubs have transformed in the modern
               era.This book isn't limited to European clubs. Szymanski, a
               renowned expert on sports management and economics, looks at
               what we can learn from comparing the ascension of Europe's
               biggest clubs to their lofty perches and with new financial
               models across the world. Through careful research and
               informative stories drawn from around the globe, Szymanski
               provides an accessible guide to the world of soccer finance.",
  publisher = "PublicAffairs",
  month     =  jun,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Aggarwal2016-xe,
  title    = "The performance of female hedge fund managers",
  author   = "Aggarwal, Rajesh and Boyson, Nicole M",
  abstract = "Using data for the period 1994--2013, we examine the return and
              risk-taking behavior of hedge funds having at least one female
              portfolio manager and funds that have all female portfolio
              managers. Funds with all female managers perform no differently
              than all male-managed funds and have similar risk profiles. For
              single-style funds, those with mixed teams of both genders
              underperform male-only funds on both a raw and risk-adjusted
              basis, although mixed funds incur less risk and their Sharpe
              ratios do not differ. For funds of funds, both all-female and
              mixed funds have similar performance to male-managed funds. We
              then consider the failure rate across all fund styles. Funds with
              at least one female manager fail at higher rates, driven by
              difficulty in raising capital---these funds are smaller and are
              less likely to be closed to new investment. Surviving funds with
              at least one female manager have better performance than
              male-managed surviving funds, consistent with the idea that
              female managers need to perform better for their funds to
              survive. Yet, female-managed surviving funds have fewer assets
              under management than surviving male-managed funds. Using media
              mentions as a proxy for investor interest, female-managed funds
              receive proportionately less attention. Our results suggest that
              there are no inherent differences in skill between female and
              male managers, but that only the best performing female managers
              manage to survive.",
  journal  = "Rev. Financ. Econ.",
  volume   =  29,
  pages    = "23--36",
  month    =  apr,
  year     =  2016
}

@ARTICLE{Mergaerts2016-ut,
  title    = "Business models and bank performance: A long-term perspective",
  author   = "Mergaerts, Frederik and Vander Vennet, Rudi",
  abstract = "This paper examines the effects of bank business models on
              performance and risk for a sample of 505 banks from 30 European
              countries over the period from 1998 to 2013. We document that
              business models in the European banking sector are characterized
              by a continuum, rather than a discrete set, of possible
              strategies. Using factor analysis to identify business models, we
              can account for this continuity. To estimate the impact of
              business models on performance, we use a methodology that is able
              to separate short-run effects from the longer-term impact of
              business model choices. Our findings show that retail-oriented
              banks perform better in terms of both profitability and stability
              and that diversification is associated with higher profitability.
              We report substantial variation of business model effects over
              different bank types. Our results lend support to the new capital
              regulations proposed in the Basel III framework, but we also
              argue that business model considerations should be more
              fundamentally integrated in the post-crisis regulatory and
              supervisory practice.",
  journal  = "Journal of Financial Stability",
  volume   =  22,
  pages    = "57--75",
  month    =  feb,
  year     =  2016,
  keywords = "Banking; Business model; Bank performance; Factor analysis"
}

@ARTICLE{Dungey2015-im,
  title    = "Contagion and banking crisis -- International evidence for
              2007--2009",
  author   = "Dungey, Mardi and Gajurel, Dinesh",
  abstract = "Policy makers aim to avoid banking crises, and although they can
              to some extent control domestic conditions, internationally
              transmitted crises are difficult to tackle. This paper identifies
              international contagion in banking during the 2007--2009 crisis
              for 54 economies. We identify three channels of contagion --
              systematic, idiosyncratic and volatility -- and find evidence for
              these in 45 countries. Banking crises are overwhelmingly
              associated with the presence of both systematic and idiosyncratic
              contagion. The results reveal that crisis shocks transmitted from
              a foreign jurisdiction via idiosyncratic contagion increase the
              likelihood of a systemic crisis in the domestic banking system by
              almost 37 percent, whereas increased exposure via systematic
              contagion does not necessarily destabilize the domestic banking
              system. Thus while policy makers and regulatory authorities are
              rightly concerned with the systematic transmission of banking
              crises, reducing the potential for idiosyncratic contagion can
              importantly reduce the consequences for the domestic economy.",
  journal  = "Journal of Banking \& Finance",
  volume   =  60,
  pages    = "271--283",
  month    =  nov,
  year     =  2015,
  keywords = "Global financial crisis; Financial contagion; Banking
              institutions; Asset pricing"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Black2016-qj,
  title    = "The systemic risk of European banks during the financial and
              sovereign debt crises",
  author   = "Black, Lamont and Correa, Ricardo and Huang, Xin and Zhou, Hao",
  abstract = "European banks became a source of risk to global financial
              markets during the financial crisis and attention to the European
              banking sector increased during the sovereign debt crisis. To
              measure the systemic risk of European banks, we calculate a
              distress insurance premium (DIP), which integrates the
              characteristics of bank size, probability of default, and
              correlation. Based on this measure, the systemic risk of European
              banks reached its height in late 2011 around ‚Ç¨500 billion. We
              find that this was largely due to sovereign default risk. The DIP
              methodology is also used to measure the systemic contribution of
              individual banks. This approach identifies the large systemically
              important European banks, but Italian and Spanish banks as a
              group notably increased in systemic importance during the sample
              period. Bank-specific fundamentals like capital-asset ratios
              predict the one-year-ahead systemic risk contributions.",
  journal  = "Journal of Banking \& Finance",
  volume   =  63,
  pages    = "107--125",
  month    =  feb,
  year     =  2016,
  keywords = "Banking systemic risk; European debt crisis; Too-big-to-fail;
              Leverage; Correlation; Credit default swap; Macroprudential
              regulation"
}

@ARTICLE{Gray1997-hy,
  title     = "Testing Market Efficiency: Evidence From The {NFL} Sports
               Betting Market",
  author    = "Gray, Philip K and Gray, Stephen F",
  abstract  = "This article examines the efficiency of the National Football
               League (NFL) betting market. The standard ordinary least squares
               (OLS) regression methodology is replaced by a probit model. This
               circumvents potential econometric problems, and allows us to
               implement more sophisticated betting strategies where bets are
               placed only when there is a relatively high probability of
               success. In-sample tests indicate that probit-based betting
               strategies generate statistically significant profits. Whereas
               the profitability of a number of these betting strategies is
               confirmed by out-of-sample testing, there is some inconsistency
               among the remaining out-of-sample predictions. Our results also
               suggest that widely documented inefficiencies in this market
               tend to dissipate over time.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  52,
  number    =  4,
  pages     = "1725--1737",
  month     =  sep,
  year      =  1997
}

@ARTICLE{Walters2014-my,
  title    = "The {Black-Litterman} Model in Detail",
  author   = "{Walters} and {CFA} and {Jay}",
  abstract = "In this paper we survey the literature on the Black-Litterman
              model. This survey is provided both as a chronology and a
              taxonomy as there are many claims on the",
  month    =  jun,
  year     =  2014,
  keywords = "quantitative portfolio management, asset allocation,
              Black-Litterman, mixed-estimation"
}

@ARTICLE{Goetzmann2015-ea,
  title     = "{Weather-Induced} Mood, Institutional Investors, and Stock
               Returns",
  author    = "Goetzmann, William N and Kim, Dasol and Kumar, Alok and Wang,
               Qin",
  abstract  = "This study shows that weather-based indicators of mood impact
               perceptions of mispricing and trading decisions of institutional
               investors. Using survey and disaggregated trade data, we show
               that relatively cloudier days increase perceived overpricing in
               individual stocks and the Dow Jones Industrial Index and
               increase selling propensities of institutions. We introduce
               stock-level measures of investor mood; investor optimism
               positively impacts stock returns among stocks with higher
               arbitrage costs, and stocks experiencing similar investor mood
               exhibit return comovement. These findings complement existing
               studies on how weather impacts stock index returns and identify
               another channel through which it can manifest.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  28,
  number    =  1,
  pages     = "73--111",
  month     =  jan,
  year      =  2015
}

@ARTICLE{Da2015-sn,
  title     = "The Sum of All {FEARS} Investor Sentiment and Asset Prices",
  author    = "Da, Zhi and Engelberg, Joseph and Gao, Pengjie",
  abstract  = "We use daily Internet search volume from millions of households
               to reveal market-level sentiment. By aggregating the volume of
               queries related to household concerns (e.g., ``recession,''
               ``unemployment,'' and ``bankruptcy''), we construct a Financial
               and Economic Attitudes Revealed by Search (FEARS) index as a new
               measure of investor sentiment. Between 2004 and 2011, we find
               FEARS (i) predict short-term return reversals, (ii) predict
               temporary increases in volatility, and (iii) predict mutual fund
               flows out of equity funds and into bond funds. Taken together,
               the results are broadly consistent with theories of investor
               sentiment.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  28,
  number    =  1,
  pages     = "1--32",
  month     =  jan,
  year      =  2015
}

@ARTICLE{Fukuyama2010-li,
  title    = "A slacks-based inefficiency measure for a two-stage system with
              bad outputs",
  author   = "Fukuyama, Hirofumi and Weber, William L",
  abstract = "We model the performance of DMUs (decision-making units) using a
              two-stage network model. In the first stage of production DMUs
              use inputs to produce an intermediate output that becomes an
              input to a second stage where final outputs are produced.
              Previous black box DEA models allowed for non-radial scaling of
              outputs and inputs and accounted for slacks in the constraints
              that define the technology. We extend these models and build a
              performance measure that accounts for a network structure of
              production. We use our method to estimate the performance of
              Japanese banks, which use labor, physical capital, and financial
              equity capital in a first stage to produce an intermediate output
              of deposits. In the second stage, those deposits become an input
              in the production of loans and securities investments. The
              network estimates reveal greater bank inefficiency than do the
              estimates that treat the bank production process as a black box
              with all production taking place in a single stage.",
  journal  = "Omega",
  volume   =  38,
  number   =  5,
  pages    = "398--409",
  month    =  oct,
  year     =  2010,
  keywords = "Two-stage DEA; Slacks-based inefficiency; Japanese banking; Bad
              outputs"
}

@ARTICLE{Barros2012-qq,
  title    = "The technical efficiency of the Japanese banks: Non-radial
              directional performance measurement with undesirable output",
  author   = "Barros, Carlos Pestana and Managi, Shunsuke and Matousek, Roman",
  abstract = "The paper analyses technical efficiency of the Japanese banks
              from 2000 to 2007. The estimation technique is based on the
              Russell directional distance function that takes into
              consideration not only desirable outputs but also an undesirable
              output that is represented by non-performing loans (NPLs). The
              results indicate that NPLs remain a significant burden as for
              banks' performance. We show that banks' inputs have to be
              utilised more efficiently, particularly labour and premises. We
              also argue that a further restructuring process is needed in the
              segment of Regional Banks. We conclude that the Japanese banking
              system is still far away from being fully consolidated and
              restructured.",
  journal  = "Omega",
  volume   =  40,
  number   =  1,
  pages    = "1--8",
  month    =  jan,
  year     =  2012,
  keywords = "DEA; Japan; Banks; Non-performing loans"
}

@ARTICLE{Lozano2016-jg,
  title    = "Slacks-based inefficiency approach for general networks with bad
              outputs: An application to the banking sector",
  author   = "Lozano, Sebasti{\'a}n",
  abstract = "In this paper the efficiency assessment of general networks of
              processes that produce both desirable and undesirable outputs is
              addressed. This problem arises in many contexts (e.g.
              transportation, energy generation, etc.). A general networks
              slacks-based inefficiency (GNSBI) measure can be computed using a
              simple linear program that takes into account the weak
              disposability of the bad outputs. The slacks-based inefficiency
              (SBI) of each process is also calculated. Target values for all
              inputs, outputs (both desirable and undesirable) and even
              intermediate products are also provided. The proposed approach is
              rather general and can accommodate many different network
              topologies and returns to scale assumptions. Two applications to
              the banking sector are presented: one to assess banks
              efficiencies and another to assess bank branches.",
  journal  = "Omega",
  volume   =  60,
  pages    = "73--84",
  month    =  apr,
  year     =  2016,
  keywords = "Efficiency; Network DEA; Undesirable outputs; Slacks-based
              inefficiency; Banks; Bank branches"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sharma2013-gp,
  title     = "Efficiency and productivity of banking sector: A critical
               analysis of literature and design of conceptual model",
  author    = "Sharma, Dipasha and Sharma, Anil K and Barua, Mukesh K",
  abstract  = "Purpose -- The purpose of this paper is to discuss a
               comprehensive literature survey of studies focusing on the
               efficiency and productivity of the banking sector using
               parametric and non‚Äêparametric frontier
               techniques.Design/methodology/approach -- Critically reviewing
               106 studies published across the world from 1994 to 2011, a
               conceptual framework is developed for the studies assessing the
               efficiency and productivity of the banking industry using
               non‚Äêparametric DEA frontier approach.Findings -- Both the
               frontier approaches, parametric and non‚Äêparametric, are gaining
               an edge over the traditional financial performance measures. In
               the non‚Äêparametric approach, data envelopment analysis (DEA) is
               widely applied to measure a bank's efficiency and productivity.
               Studies conducted in developed countries such as the USA, the UK
               and Europe are now emerging with the new concepts of banking
               efficiency.Research limitations/implications -- These findings
               are based only on the critical review of 106 studies. This study
               suggests the direction for future research and identifies the
               gap in existing literature with the development of a conceptual
               model.Originality/value -- This study is original in nature and
               included literature published in recent issues of 2011.",
  journal   = "Qualitative Research in Financial Markets",
  publisher = "emeraldinsight.com",
  volume    =  5,
  number    =  2,
  pages     = "195--224",
  year      =  2013
}

@ARTICLE{Chaffai2015-og,
  title     = "Modelling and measuring business risk and the resiliency of
               retail banks",
  author    = "Chaffai, Mohamed and Dietsch, Michel",
  abstract  = "The recent banking crisis has revealed the existence of strong
               resiliency factors in the retail banking business model. On
               average, retail banks suffered less than other financial
               institutions from unexpected market changes. This paper proposes
               a new methodology to measure retail banks' business risk, which
               is defined as the risk of adverse and unexpected changes in
               banks' profits coming from sudden changes in the banks'
               activities. This methodology is based on the efficiency frontier
               methodology, and, more specifically, on the duality property
               between the directional distance function and the profit
               function. Using the distance function to compute banks'
               profitability, we take the distance to the frontier of best
               practices as a measure of profit inefficiency, i.e. of
               unexpected losses related to underperformance. In this approach,
               shifts in the efficiency frontier induced by adverse shocks to
               banks' volumes serve as a measure of business risk. This measure
               of profit volatility allows a measurement to be made of the
               impact of volume changes on banks' profits. This method is
               applied to a database containing half yearly regulatory
               accounting reports over the 1993--2011 period for a sample of
               quite all French banks running a retail banking business model.
               Our results verify a low level of business risk in retail
               banking, thus confirming the resiliency of the retail banks'
               business model.",
  journal   = "Journal of Financial Stability",
  publisher = "Elsevier",
  volume    =  16,
  pages     = "173--182",
  month     =  feb,
  year      =  2015,
  keywords  = "Bank solvency; Retail banking; Business risk; Efficiency
               analysis; Profit"
}

@ARTICLE{Juo2014-tg,
  title     = "Decomposing the change in profit of Taiwanese banks:
               incorporating risk",
  author    = "Juo, Jia-Ching",
  abstract  = "This study follows the structure of Grifell-Tatj{\'e} and Lovell
               (Manag Sci 45:1177--1193, 1999) and uses the non-parametric
               approach to decompose the change in profit of Taiwanese banks
               into various drivers. However, risk was never considered in the
               papers based on profit decomposition. Without considering risk,
               the empirical results will be biased while decomposing the
               change in profit. In fact, risk is a joint but undesirable
               output which cannot be freely disposed of by various
               regulations. The non-performing loan (NPL) is employed as a risk
               indicator for decomposing the change in profit in this study.
               This study also performs a three-way comparison among (1) the
               original Grifell-Tatj{\'e} and Lovell (Manag Sci 45:1177--1193,
               1999) analysis (OGLA) model that ignores NPL, (2) the extended
               Grifell-Tatj{\'e} and Lovell (Manag Sci 45:1177--1193, 1999)
               analysis (EGLA) model that is based on the OGLA model and
               incorporates NPL, and (3) the directional distance function
               (DDF) model that is based on Juo et al. (Omega 40:550--561,
               2012) and incorporates NPLs to see if incorporating the
               undesirable output matters. The decomposition of the change in
               profit in the above three models is then illustrated using
               Taiwanese banks over the period 2006--2010.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  41,
  number    =  2,
  pages     = "247--262",
  month     =  apr,
  year      =  2014,
  language  = "en"
}

@INCOLLECTION{Kumar2014-lu,
  title     = "A Survey of Empirical Literature on Bank Efficiency",
  booktitle = "Deregulation and Efficiency of Indian Banks",
  author    = "Kumar, Sunil and Gulati, Rachita",
  abstract  = "This chapter presents a survey of empirical literature on bank
               efficiency. Four key research areas have been identified where
               most of the research efforts have been devoted in last two
               decades. These research areas are (i) impact of financial
               deregulation on bank efficiency, (ii) bank efficiency across
               ownership types, (iii) cross-country differences in bank
               efficiency, and (iv) impact of mergers and acquisitions on bank
               efficiency. In addition, two prominent research issues in the
               banking efficiency literature have also been discussed in this
               chapter. The first issue relates with the selection and
               specification of inputs and outputs in a study on the subject
               matter. The second issue is on the selection of appropriate
               frontier efficiency technique to measure bank efficiency.
               Regarding the first issue, we note that though both
               intermediation and production approaches are not perfect to
               model the production process of banking firms, but the former
               dominates the latter in the empirical applications because it is
               better suited to capture the decisions taken to minimize the
               cost of the financing mix. On the second issue, we note that a
               bank's efficiency score can differ completely due to the
               measurement technique, and there is virtually no consensus on
               the preferred estimation method of bank efficiency.",
  publisher = "Springer, New Delhi",
  pages     = "119--165",
  series    = "India Studies in Business and Economics",
  year      =  2014,
  language  = "en"
}

@ARTICLE{Fujii2014-cy,
  title     = "Indian bank efficiency and productivity changes with undesirable
               outputs: A disaggregated approach",
  author    = "Fujii, Hidemichi and Managi, Shunsuke and Matousek, Roman",
  abstract  = "The objective of this study is to examine technical efficiency
               and productivity growth in the Indian banking sector over the
               period from 2004 to 2011. We apply an innovative methodological
               approach introduced by Chen et al. (2011) and Barros et al.
               (2012), who use a weighted Russell directional distance model to
               measure technical inefficiency. We further modify and extend
               that model to measure TFP change with NPLs. We find that the
               inefficiency levels are significantly different among the three
               ownership structure of banks in India. Foreign banks have strong
               market position in India and they pull the production frontier
               in a more efficient direction. SPBs and domestic private banks
               show considerably higher inefficiency. We conclude that the
               restructuring policy applied in the late 1990s and early 2000s
               by the Indian government has not had a long-lasting effect.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  38,
  pages     = "41--50",
  month     =  jan,
  year      =  2014,
  keywords  = "DEA; Efficiency; Bank; India; Non-performing loan"
}

@ARTICLE{Aiello2016-fe,
  title     = "Efficiency in banking: a meta-regression analysis",
  author    = "Aiello, Francesco and Bonanno, Graziella",
  abstract  = "AbstractOne learns two main lessons from studying the great
               quantity of banking efficiency literature. These lessons regard
               the heterogeneity in results and the absence of a comprehensive
               review aimed at understanding the reasons for this variability.
               Surprisingly, although this issue is well-known, it has not been
               systematically analyzed before. In order to fill this gap, we
               perform a Meta-Regression-Analysis (MRA) by examining 1661
               efficiency scores retrieved from 120 papers published over the
               period 2000?2014. The meta-regression is estimated by using the
               Random Effects Multilevel Model (REML) because it controls for
               within- and between-study heterogeneity. The analysis yields
               four main results. First, parametric methods yield lower levels
               of banking efficiency than nonparametric studies. This holds
               true even after controlling for the approach used in selecting
               the inputs and outputs of the frontier. Secondly, we show that
               banking efficiency is highest when using the value-added
               approach, followed by estimates from studies based on the
               intermediation method, whereas those based on the hybrid
               approach are the lowest. Thirdly, efficiency scores are also
               determined by the quality of studies and the number of
               observations and variables used in the primary papers. As far as
               the effects of sample size, dimension and quality of papers are
               concerned, there are significant differences in sign and
               magnitude between parametric and nonparametric studies. Finally,
               cost efficiency is found to be higher than profit efficiency.
               Interestingly, MRA results are robust to the potential outliers
               in efficiency and sample size distributions.",
  journal   = "International Review of Applied Economics",
  publisher = "Routledge",
  volume    =  30,
  number    =  1,
  pages     = "112--149",
  month     =  jan,
  year      =  2016
}

@ARTICLE{Glaser2007-xf,
  title     = "Overconfidence and trading volume",
  author    = "Glaser, Markus and Weber, Martin",
  abstract  = "Theoretical models predict that overconfident investors will
               trade more than rational investors. We directly test this
               hypothesis by correlating individual overconfidence scores with
               several measures of trading volume of individual investors.
               Approximately 3,000 online broker investors were asked to answer
               an internet questionnaire which was designed to measure various
               facets of overconfidence (miscalibration, volatility estimates,
               better than average effect). The measures of trading volume were
               calculated by the trades of 215 individual investors who
               answered the questionnaire. We find that investors who think
               that they are above average in terms of investment skills or
               past performance (but who did not have above average performance
               in the past) trade more. Measures of miscalibration are,
               contrary to theory, unrelated to measures of trading volume.
               This result is striking as theoretical models that incorporate
               overconfident investors mainly motivate this assumption by the
               calibration literature and model overconfidence as
               underestimation of the variance of signals. In connection with
               other recent findings, we conclude that the usual way of
               motivating and modeling overconfidence which is mainly based on
               the calibration literature has to be treated with caution.
               Moreover, our way of empirically evaluating behavioral finance
               models---the correlation of economic and psychological variables
               and the combination of psychometric measures of judgment biases
               (such as overconfidence scores) and field data'seems to be a
               promising way to better understand which psychological phenomena
               actually drive economic behavior.",
  journal   = "Geneva Risk Insur. Rev.",
  publisher = "Palgrave Macmillan UK",
  volume    =  32,
  number    =  1,
  pages     = "1--36",
  month     =  jun,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Beccalli2015-zq,
  title    = "Are European banks too big? Evidence on economies of scale",
  author   = "Beccalli, Elena and Anolli, Mario and Borello, Giuliana",
  abstract = "In light of the policy debate on too-big-to-fail we investigate
              evidence of economies of scale for 103 European listed banks over
              2000--2011. Using the Stochastic Frontier Approach, the results
              show that economies of scale are widespread across different size
              classes of banks and are especially large for the biggest banks.
              At the country level, banks operating in the smallest financial
              systems and the countries most affected by the financial crises
              realize the lowest scale economies (including diseconomies) due
              to the reduction in production capacity. As for the determinants
              of scale economies, these mainly emanate from banks oriented
              toward investment banking, with higher liquidity, lower Tier 1
              capital, those that contributed less to systemic risk during the
              crises, and those with too-big-to-fail status.",
  journal  = "Journal of Banking \& Finance",
  volume   =  58,
  number   =  0,
  pages    = "232--246",
  month    =  sep,
  year     =  2015,
  keywords = "Bank; Economies of scale; Regulation; Too-big-to-fail; EU"
}

@ARTICLE{Nakashima2016-er,
  title     = "An econometric evaluation of bank recapitalization programs with
               bank- and loan-level data",
  author    = "Nakashima, Kiyotaka",
  abstract  = "Public capital injections into the banking system are a
               comprehensive policy program aimed at reducing the financial
               risks faced by capital-injected banks, thereby stimulating their
               lending and profitability. This paper evaluates empirically
               Japan's two large-scale capital injections in 1998 and 1999. We
               begin by extracting the treatment effects of the public
               injections from bank-level panel data. Using a
               difference-in-difference estimator in two-way fixed-effects
               regression models, we find that the public injections
               significantly reduced the financial risks faced by the
               capital-injected banks but did not stimulate their lending or
               profitability. Next, we investigate what factors impeded bank
               lending after the public injections using a matched sample of
               Japanese banks and their borrowers. By employing three-way
               fixed-effects regression models corresponding to the matched
               sample, we provide evidence that the deterioration of borrowers'
               creditworthiness inhibited not only the injected banks but also
               the noninjected banks from lending more.",
  journal   = "Journal of Banking \& Finance",
  publisher = "papers.ssrn.com",
  volume    =  63,
  pages     = "1--24",
  month     =  feb,
  year      =  2016,
  keywords  = "Public capital injection; Treatment effect; Capital crunch;
               Default risk difference-in-difference estimator; Three-way
               fixed-effects model"
}

@ARTICLE{Worthington2010-id,
  title     = "{FRONTIER} {EFFICIENCY} {MEASUREMENT} {IN} {DEPOSIT-TAKING}
               {FINANCIAL} {MUTUALS}: A {REVIEW} {OF} {TECHNIQUES},
               {APPLICATIONS}, {AND} {FUTURE} {RESEARCH} {DIRECTIONS}",
  author    = "Worthington, Andrew C",
  abstract  = "ABSTRACT**: Despite the global importance of mutuals in
               financial services, and the universal need to measure and
               improve organizational efficiency in all deposit-taking
               institutions, it is only relatively recently that the most
               advanced econometric and mathematical programming frontier
               techniques have been applied. This paper provides a synoptic
               survey of the comparatively few empirical analyses of frontier
               efficiency measurement in deposit-taking financial mutuals,
               comprising savings and loans, building societies and credit
               unions in Australia, the UK, and the USA. Both estimation and
               measurement techniques and the determinants of efficiency are
               examined. Particular focus is placed on how the results of these
               studies may help inform regulatory policy and managerial
               behaviour.",
  journal   = "Annals of Public and Cooperative Economics",
  publisher = "Blackwell Publishing Ltd",
  volume    =  81,
  number    =  1,
  pages     = "39--75",
  month     =  mar,
  year      =  2010
}

@ARTICLE{Chortareas2012-fx,
  title    = "Bank supervision, regulation, and efficiency: Evidence from the
              European Union",
  author   = "Chortareas, Georgios E and Girardone, Claudia and Ventouri,
              Alexia",
  abstract = "This paper investigates the dynamics between key regulatory and
              supervisory policies and various aspects of commercial bank
              efficiency and performance for a sample of 22 EU countries over
              2000--2008. In the first stage of the analysis we measure
              efficiency by employing the Data Envelopment Analysis (DEA)
              technique. In addition, we employ two distinct accounting ratios
              to capture the costs of intermediation (net interest margin) and
              cost effectiveness (cost-to-income ratio). Our regression
              framework includes truncated regressions and generalized linear
              models. Moreover, we carry out a sensitivity analysis for
              robustness using a fractional logit estimator. Our results show
              that strengthening capital restrictions and official supervisory
              powers can improve the efficient operations of banks. Evidence
              also indicates that interventionist supervisory and regulatory
              policies such as private sector monitoring and restricting bank
              activities can result in higher bank inefficiency levels.
              Finally, the evidence produced suggests that the beneficial
              effects of capital restrictions and official supervisory powers
              (interventionist supervisory and regulatory policies) on bank
              efficiency are more pronounced in countries with higher quality
              institutions.",
  journal  = "Journal of Financial Stability",
  volume   =  8,
  number   =  4,
  pages    = "292--302",
  month    =  dec,
  year     =  2012,
  keywords = "Bootstrap; Bank performance; Regulation and supervision; European
              Banks; DEA; Truncated regression"
}

@ARTICLE{Ramakrishnan1984-fs,
  title     = "Information Reliability and a Theory of Financial Intermediation",
  author    = "Ramakrishnan, Ram T S and Thakor, Anjan V",
  abstract  = "This paper is an analysis of when it will be beneficial for
               agents engaged in the production of information to form
               coalitions. The model is cast in a financial market framework,
               thus leading to an identification of conditions sufficient for
               the existence of financial intermediaries. Intermediation is
               shown to improve welfare if informational asymmetries are
               present, and the information generated to rectify these
               asymmetries is potentially unreliable. The usual appeal to
               transactions costs to explain intermediation is not needed.",
  journal   = "Rev. Econ. Stud.",
  publisher = "Oxford University Press",
  volume    =  51,
  number    =  3,
  pages     = "415--432",
  month     =  jul,
  year      =  1984
}

@ARTICLE{Charreaux2001-ug,
  title     = "Corporate Governance: Stakeholder Value Versus Shareholder Value",
  author    = "Charreaux, G{\'e}rard and Desbri{\`e}res, Philippe",
  abstract  = "Unsatisfied with the dominatingshareholders' point of view, that
               appears to betoo limited to build a relevant theory ofcorporate
               governance, we propose an enlargeddefinition of the value which
               may be called,the stakeholder value. This definition and
               itsassociated measure are more suitable for thestakeholder
               approach to the firm and morerelevant to understand the value
               creation andsharing mechanisms.",
  journal   = "Journal of Management \& Governance",
  publisher = "Kluwer Academic Publishers",
  volume    =  5,
  number    =  2,
  pages     = "107--128",
  month     =  jun,
  year      =  2001,
  language  = "en"
}

@BOOK{Fare2012-tx,
  title     = "Cost and Revenue Constrained Production",
  author    = "F{\"a}re, Rolf and Grosskopf, Shawna",
  abstract  = "The basic notion underlying this monograph - budget or revenue
               constrained models of production - we owe to Ronald W. Shephard,
               who recognized its fundamental importance in modeling behavior
               in a wide variety of settings including the service and public
               sector. Our endeavor here is to extend Shephard's earlier work
               in several directions while maintaining his axiomatic approach.
               Our contributions include an expanded set of duality results and
               a general bent toward empirical implementation: including
               various parameterizations, applications to efficiency and
               productivity measurement, and shadow pricing. We hope to provide
               those engaged in empirical work with some powerful and useful
               tools which have received relatively little attention. The
               nature of the material in this monograph is somewhat technical,
               however, the level of mathematical difficulty is standard.
               Although we have tried to keep the monograph fairly
               self-contained, we have also kept technical detail to a minimum
               in the body of the text. Many technical extensions appear as
               problems at the ends of Chapters. The reader is also referred to
               the notes at the end of each chapter for references to
               additional literature. A prepublication draft of this manuscript
               was used as lecture notes in a graduate course in production
               theory at the Department of Economics at Bilkent University. We
               thank our students as well as faculty members for their patience
               and interest. Special thanks go to Dean Togan, Zeynap Koksal and
               Ali Dogramaci for making our stay in Ankara not only productive,
               but also enjoyable.",
  publisher = "Springer Science \& Business Media",
  series    = "Bilkent University Lecture Series",
  month     =  dec,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Atkinson2009-su,
  title     = "Feasible estimation of firm-specific allocative inefficiency
               through Bayesian numerical methods",
  author    = "Atkinson, Scott E and Dorfman, Jeffrey H",
  abstract  = "Both the theoretical and empirical literature on the estimation
               of allocative and technical inefficiency has grown enormously.
               To minimize aggregation bias, ideally one should estimate firm
               and input-specific parameters describing allocative
               inefficiency. However, identifying these parameters has often
               proven difficult. For a panel of Chilean hydroelectric power
               plants, we obtain a full set of such parameters using Gibbs
               sampling, which draws sequentially from conditional generalized
               method of moments (GMM) estimates obtained via instrumental
               variables estimation. We find an economically significant range
               of firm-specific efficiency estimates with differing degrees of
               precision. The standard GMM approach estimates virtually no
               allocative inefficiency for industry-wide parameters. Copyright
               \copyright{} 2009 John Wiley \& Sons, Ltd.",
  journal   = "J. Appl. Econ.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  24,
  number    =  4,
  pages     = "675--697",
  month     =  jun,
  year      =  2009
}

@ARTICLE{Cuesta2009-cn,
  title    = "Environmental efficiency measurement with translog distance
              functions: A parametric approach",
  author   = "Cuesta, Rafael A and Lovell, C A Knox and Zof{\'\i}o, Jos{\'e} L",
  abstract = "We introduce new specifications and estimation procedures of
              traditional distance functions that allow researchers to
              undertake environmental efficiency and productivity studies
              within a parametric stochastic framework. Relying on a translog
              distance function specification that treats the outputs' vector
              asymmetrically by allowing equiproportional desirable outputs
              expansion and undesirable outputs contraction, we discuss the
              relevant properties that characterize the environmental
              hyperbolic distance function, and show that it can be easily
              implemented using conventional econometric techniques based on
              maximum likelihood estimation. We illustrate our model estimating
              technical efficiency scores for a panel of U.S. electricity
              generating units that produce electricity and SO2 emissions as
              byproduct.",
  journal  = "Ecol. Econ.",
  volume   =  68,
  number   =  8,
  pages    = "2232--2242",
  month    =  jun,
  year     =  2009,
  keywords = "Parametric distance functions; Stochastic frontier analysis;
              Environmental efficiency; Undesirable outputs"
}

@ARTICLE{Atkinson2002-uk,
  title    = "Stochastic estimation of firm technology, inefficiency, and
              productivity growth using shadow cost and distance functions",
  author   = "Atkinson, Scott E and Primont, Daniel",
  abstract = "It is well-known that a firm's technology, allocative efficiency,
              technical efficiency, and productivity growth can be estimated
              using a shadow cost system, comprised of a shadow cost function
              and its share equations, whose arguments are outputs and shadow
              input prices (prices internal to the firm). We provide a dual
              characterization to estimate these measures using a shadow
              distance system. This system is comprised of a shadow distance
              function, expressed in terms of shadow input quantities and
              output quantities, plus the first-order conditions from the
              cost-minimization problem. An advantage of the distance system
              over the cost system is that we obtain direct estimates of input
              inefficiency with the former, but indirect estimates with the
              latter. We also show how to express cost function derivatives in
              terms of distance function derivatives, which allows calculation
              of returns to scale and price elasticities of demand from the
              estimated distance system. Using panel data on US electric
              utilities, we estimate both systems and find a strong similarity
              between their associated measures.",
  journal  = "J. Econom.",
  volume   =  108,
  number   =  2,
  pages    = "203--225",
  month    =  jun,
  year     =  2002,
  keywords = "Distance and cost frontiers; Allocative inefficiency; Technical
              inefficiency; Productivity change; Technical change"
}

@ARTICLE{Phillips2015-ve,
  title     = "Halbert White Jr. Memorial {JFEC} Lecture: Pitfalls and
               Possibilities in Predictive Regression",
  author    = "Phillips, Peter C B",
  abstract  = "Financial theory and econometric methodology both struggle in
               formulating models that are logically sound in reconciling
               short-run martingale behavior for financial assets with
               predictable long-run behavior, leaving much of the research to
               be empirically driven. The present article overviews recent
               contributions to this subject, focusing on the main pitfalls in
               conducting predictive regression and on some of the
               possibilities offered by modern econometric methods. The latter
               options include indirect inference and techniques of endogenous
               instrumentation that use convenient temporal transforms of
               persistent regressors. Some additional suggestions are made for
               bias elimination, quantile crossing amelioration, and control of
               predictive model misspecification.",
  journal   = "Journal of Financial Econometrics",
  publisher = "Oxford University Press",
  volume    =  13,
  number    =  3,
  pages     = "521--555",
  month     =  jun,
  year      =  2015
}

@ARTICLE{McKillop2015-gy,
  title     = "Credit Unions as Cooperative Institutions: Distinctiveness,
               Performance and Prospects",
  author    = "McKillop, Donal G and Wilson, John O S",
  abstract  = "AbstractCredit unions are not for profit cooperative financial
               institutions which provide financial services to a membership
               defined on the basis of a common bond. In 2013, there were
               56,904 credit unions across 103 countries with 207.9 million
               members. There is a great diversity within the credit union
               movement across these countries. This reflects the various
               economic, historic and cultural contexts within which credit
               unions operate. This paper traces the historical development of
               credit unions in different parts of the world. We investigate
               what sets credit unions apart from other financial services
               organisations, placing a particular focus on the role they play
               in building social capital and community relations and
               empowering members. We also discuss the challenges to the future
               development of credit unions. These include increased regulatory
               burdens, capital constraints, declining membership involvement,
               and tensions between social and economic objectives.",
  journal   = "Social and Environmental Accountability Journal",
  publisher = "Routledge",
  volume    =  35,
  number    =  2,
  pages     = "96--112",
  month     =  may,
  year      =  2015
}

@ARTICLE{Ben_Taieb2014-zl,
  title    = "A gradient boosting approach to the Kaggle load forecasting
              competition",
  author   = "Ben Taieb, Souhaib and Hyndman, Rob J",
  abstract = "We describe and analyse the approach used by Team TinTin (Souhaib
              Ben Taieb and Rob J Hyndman) in the Load Forecasting track of the
              Kaggle Global Energy Forecasting Competition 2012. The
              competition involved a hierarchical load forecasting problem for
              a US utility with 20 geographical zones. The data available
              consisted of the hourly loads for the 20 zones and hourly
              temperatures from 11 weather stations, for four and a half years.
              For each zone, the hourly electricity loads for nine different
              weeks needed to be predicted without having the locations of
              either the zones or stations. We used separate models for each
              hourly period, with component-wise gradient boosting for
              estimating each model using univariate penalised regression
              splines as base learners. The models allow for the electricity
              demand changing with the time-of-year, day-of-week, time-of-day,
              and on public holidays, with the main predictors being current
              and past temperatures, and past demand. Team TinTin ranked fifth
              out of 105 participating teams.",
  journal  = "Int. J. Forecast.",
  volume   =  30,
  number   =  2,
  pages    = "382--394",
  month    =  apr,
  year     =  2014,
  keywords = "Short-term load forecasting; Multi-step forecasting; Additive
              models; Gradient boosting; Machine learning; Kaggle competition"
}

@ARTICLE{Brabec2008-dq,
  title    = "A nonlinear mixed effects model for the prediction of natural gas
              consumption by individual customers",
  author   = "Brabec, Marek and Kon{\'a}r, Ond{\v r}ej and Pelik{\'a}n, Emil
              and Mal{\'y}, Marek",
  abstract = "This study deals with the description and prediction of the daily
              consumption of natural gas at the level of individual customers.
              Unlike traditional group averaging approaches, we are faced with
              the irregularities of individual consumption series posed by
              inter-individual heterogeneity, including zeros, missing data,
              and abrupt consumption pattern changes. Our model is of the
              nonlinear regression type, with individual customer-specific
              parameters that, nevertheless, have a common distribution
              corresponding to the nonlinear mixed effects model framework. It
              is advantageous to build the model conditionally. The first
              condition, whether a particular customer has consumed or not, is
              modeled as a consumption status in an individual fashion. The
              prediction performance of the proposed model is demonstrated
              using a real dataset of 62 individual customers, and compared
              with two more traditional approaches: ARIMAX and ARX.",
  journal  = "Int. J. Forecast.",
  volume   =  24,
  number   =  4,
  pages    = "659--678",
  month    =  oct,
  year     =  2008,
  keywords = "Individual gas consumption; Nonlinear mixed effects model;
              ARIMAX; ARX; Generalized linear mixed model; Conditional modeling"
}

@ARTICLE{Matthews2007-uj,
  title    = "Competitive conditions among the major British banks",
  author   = "Matthews, Kent and Murinde, Victor and Zhao, Tianshu",
  abstract = "This paper reports an empirical assessment of competitive
              conditions among the major British banks, during a period of
              major structural change. Specifically, estimates of the
              Rosse--Panzar H-statistic are reported for a panel of 12 banks
              for the period 1980--2004. The sample banks correspond closely to
              the major British banking groups specified by the British Banking
              Association. The robustness of the results of the Rosse--Panzar
              methodology is tested by estimating the ratio of Lerner indices
              obtained from interest rate setting equations. The results
              confirm the consensus finding that competition in British banking
              is most accurately characterised by the theoretical model of
              monopolistic competition. There is evidence that the intensity of
              competition in the core market for bank lending remained
              approximately unchanged throughout the 1980s and 1990s. However,
              competition appears to have become less intense in the non-core
              (off-balance sheet) business of British banks.",
  journal  = "Journal of Banking \& Finance",
  volume   =  31,
  number   =  7,
  pages    = "2025--2042",
  month    =  jul,
  year     =  2007,
  keywords = "Competitive conditions; British banking"
}

@ARTICLE{Hutton2012-rj,
  title     = "The Future of Public Sector Pensions",
  author    = "Hutton, John",
  abstract  = "The dramatic rise in life expectancy and longer retirement has
               created serious concerns about the long-term affordability of
               public sector pensions. Drawing on insights from a recent
               inquiry into public sector pension reform, commissioned by the
               Conservative--Liberal Democrat Coalition and led by the author,
               this article outlines the challenge faced by policy makers and
               sets out how it should be addressed. It argues that, with 12
               million people active in this part of the pensions system, and
               estimates that the gap between contributions and payments will
               grow from \pounds{}3bn to \pounds{}10bn in the next decade, the
               government has no choice but to enter the `lions' den' of
               pensions' policy. It contends that any long-term solution is to
               be crafted cannot be based solely on economics, but must also
               answer fundamental questions of ethics and equity.",
  journal   = "Polit. Q.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  83,
  number    =  2,
  pages     = "311--317",
  month     =  apr,
  year      =  2012,
  keywords  = "Future; Pension costs; Pensions; Public sector; public pensions;
               pensions reform; retirement"
}

@ARTICLE{Burbidge1980-bl,
  title     = "Pensions and Retirement Behaviour",
  author    = "Burbidge, John B and Robb, A Leslie",
  abstract  = "This paper employs a lifecycle model to analyse the theoretical
               effects on an individual's retirement behaviour of pension plans
               that are not actuarially fair. In addition it examines public
               and private pension plans in Canada and indicates that income
               effects are likely to be at least as important as substitution
               effects and that new plans may have effects which are very
               different from those of mature plans. In particular, the
               analysis suggests that empirical evidence drawn from the
               phase-in period of the Canada Pension Plan (1966-75) is
               misleading as to the effects of the mature plan on the age of
               retirement. /// R{\'e}gimes de rentes et d{\'e}cision de prendre
               sa retraite. Ce m{\'e}moire utilise un mod{\`e}le dit du
               cycle-de-vie pour analyser les effets, au plan th{\'e}orique,
               sur la d{\'e}cision de prendre sa retraite du fait qu'un
               individu participe {\`a} un r{\'e}gime de rentes qui n'est pas
               `{\'e}quitable' au plan actuariel (i.e., {\`a} un r{\'e}gime
               pour lequel la valeur pr{\'e}sente des contributions n'est pas
               {\'e}gale {\`a} la valeur pr{\'e}sente des prestations). De plus
               le m{\'e}moire examine les r{\'e}gimes de rentes tant publics
               que priv{\'e}s au Canada et montre que les effets de revenus
               sont susceptibles d'{\^e}tre au moins tout aussi importants que
               les effets de substitution. Il appert aussi que les r{\'e}gimes
               nouveaux peuvent avoir des effets fort diff{\'e}rents de ceux
               des r{\'e}gimes arriv{\'e}s {\`a} maturation. En particulier,
               l'analyse sugg{\`e}re que les donn{\'e}es empiriques tir{\'e}es
               de la p{\'e}riode d'introduction et de mise en place du
               r{\'e}gime de rentes du Canada (1966-75) et les
               interpr{\'e}tations auxquelles elles ont donn{\'e} lieu quant
               {\`a} l'effet du r{\'e}gime sur l'{\^a}ge de la retraite peuvent
               fort bien porter {\`a} faux et ne pas s'appliquer au moment
               o{\`u} le r{\'e}gime atteint sa maturit{\'e}.",
  journal   = "Can. J. Econ.",
  publisher = "[Wiley, Canadian Economics Association]",
  volume    =  13,
  number    =  3,
  pages     = "421--437",
  year      =  1980
}

@ARTICLE{Halkos2012-kb,
  title     = "Industry performance evaluation with the use of financial
               ratios: An application of bootstrapped {DEA}",
  author    = "Halkos, George E and Tzeremes, Nickolaos G",
  abstract  = "In data envelopment analysis (DEA) context financial data/ratios
               have been used in order to produce a unified measure of
               performance metric. However, several scholars have indicated
               that the inclusion of financial ratios create biased efficiency
               estimates with implications on firms' and industries'
               performance evaluation. By applying bootstrap techniques the
               paper provides an application of evaluating the performance of
               23 Greek manufacturing sectors with the use of financial data.
               The results reveal that in the first stage of our sensitivity
               analysis the efficiencies obtained are biased. However, after
               applying the bootstrap techniques the sensitivity analysis
               reveals that the efficiency scores have been significantly
               improved.",
  journal   = "Expert Syst. Appl.",
  publisher = "Elsevier Ltd",
  volume    =  39,
  number    =  5,
  pages     = "5872--5880",
  month     =  apr,
  year      =  2012,
  keywords  = "DEA; Mathematical programming; Sensitivity analysis; Financial
               ratios"
}

@ARTICLE{Pasiouras2008-ln,
  title     = "International evidence on the impact of regulations and
               supervision on banks' technical efficiency: an application of
               two-stage data envelopment analysis",
  author    = "Pasiouras, Fotios",
  abstract  = "This study uses a sample of 715 banks from 95 countries and
               two-stage data envelopment analysis (DEA) to provide
               international evidence on the impact of regulations and
               supervision approaches on banks' efficiency. We first use DEA to
               estimate technical and scale efficiency. We then use Tobit
               regression to investigate the impact of several regulations
               related to capital adequacy, private monitoring, banks'
               activities, deposit insurance schemes, disciplinary power of the
               authorities, and entry into banking on banks' technical
               efficiency. We estimate several specifications while controlling
               for bank-specific attributes and country-level characteristics
               accounting for macroeconomic conditions, financial development,
               market structure, overall institutional development, and access
               to banking services. In several cases, the results provide
               evidence in favour of all three pillars of Basel II that promote
               the adoption of strict capital adequacy standards, the
               development of powerful supervisory agencies, and the creation
               of market disciplining mechanisms. However, only the latter one
               is significant in all of our specifications. While the remaining
               regulations do not appear to have a robust impact on efficiency,
               several other country-specific characteristics are significantly
               related to efficiency.",
  journal   = "Rev Quant Finan Acc",
  publisher = "Springer US",
  volume    =  30,
  number    =  2,
  pages     = "187--223",
  month     =  feb,
  year      =  2008,
  language  = "en"
}

@ARTICLE{Field1990-yr,
  title     = "Production efficiency of British building societies",
  author    = "Field, Kamal",
  abstract  = "The purpose of this paper is to examine the relative efficiency
               of British building societies and to find out whether efficiency
               factor is the driving force behind the merging of small building
               societies. Efficiency indices for societies are derived from a
               1981 sample of cross-section data. The results indicate wide
               disparities in efficiency. These differences are not related to
               the size of building societies, but apparently to managers'
               skill and motivation.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  22,
  number    =  3,
  pages     = "415--426",
  month     =  mar,
  year      =  1990
}

@ARTICLE{Fried1996-gn,
  title    = "An analysis of the performance of university-affiliated credit
              unions",
  author   = "Fried, H O and Lovell, C A K and Turner, J A",
  abstract = "In this paper we analyze the operating efficiency of a group of
              university-affiliated credit unions in 1990. We use free disposal
              hull (FDH) techniques, which generalize data envelopment analysis
              (DEA) techniques by dispensing with the convexity assumption
              imposed in DEA, to measure the operating efficiency of
              university-affiliated credit unions and to compare their
              efficiency with that of credit unions not affiliated with a
              university. The purpose of the analysis is to test the hypothesis
              that university-affiliated credit unions, by virtue of the
              superior educational attainment of their members, some of whom
              sit on boards of directors that monitor managements, are thereby
              better managed and so perform better. In the second stage of the
              analysis we use seemingly unrelated regression (SUR) techniques
              to identify exogenous factors that might explain variation in
              operating efficiency among the university-affiliated credit
              unions.",
  journal  = "Comput. Oper. Res.",
  volume   =  23,
  number   =  4,
  pages    = "375--384",
  month    =  apr,
  year     =  1996
}

@ARTICLE{Glass2000-qd,
  title     = "A Post Deregulation Analysis of the Sources of Productivity
               Growth in {UK} Building Societies",
  author    = "Glass, J Colin and McKillop, Donal G",
  abstract  = "In this study we employ a distance function approach to
               investigate sources of productivity growth in UK building
               societies in the post-deregulation period 1989--93. Productivity
               growth is decomposed into technical change and change in
               efficiency, with the latter change also being decomposed into
               change in pure technical efficiency, change in scale efficiency
               and change in input congestion. As a scale-inefficient society
               may be able to obtain size efficiency gains even when the
               attainment of scale efficiency is impractical, we also measure
               the change in size efficiency over the period. The finding of
               substantial productivity growth was largely due to progressive
               shifts in technology, with the relatively small improvements in
               efficiency being largely due to improvements in scale
               efficiency. A marked increase in the attainment of size
               efficiency over the period was also found.",
  journal   = "Manchester Sch. Econ. Soc. Stud.",
  publisher = "Blackwell Publishers Ltd",
  volume    =  68,
  number    =  3,
  pages     = "360--385",
  month     =  jun,
  year      =  2000
}

@ARTICLE{Sinan_Cebenoyan1993-yd,
  title     = "The relative efficiency of stock versus mutual {S\&Ls}: A
               stochastic cost frontier approach",
  author    = "Sinan Cebenoyan, A and Cooperman, Elizabeth S and Register,
               Charles A and Hudgins, Sylvia C",
  abstract  = "From an agency theory perspective, recent conversion activity of
               savings and loan associations (S\&Ls) from mutual to stock
               organizations should improve the overall performance of the
               thrift industry. We employ a two-step approach to examine this
               issue using a sample of 559 S\&Ls in the Atlanta Federal Home
               Loan Bank District in 1988. In the first step, we estimate
               inefficiency scores for individual S\&Ls using a stochastic cost
               frontier methodology. In a second step Tobit model we use these
               inefficiency scores to examine the relationship between firm
               inefficiency and organizational form. We find three important
               results: (1) that the mutual and stock S\&Ls in our sample have
               similar cost structures, allowing the pooling of S\&L data; (2)
               that S\&Ls have a wide range of inefficiency scores, with a mean
               score of 16 percent indicating that the average S\&L could
               produce its output with only 84 percent of the inputs actually
               used; and (3) that operating inefficiency was not significantly
               related to form of ownership.",
  journal   = "J Finan Serv Res",
  publisher = "Kluwer Academic Publishers",
  volume    =  7,
  number    =  2,
  pages     = "151--170",
  month     =  jun,
  year      =  1993,
  language  = "en"
}

@ARTICLE{Cook2009-pr,
  title    = "Data envelopment analysis ({DEA}) -- Thirty years on",
  author   = "Cook, Wade D and Seiford, Larry M",
  abstract = "This paper provides a sketch of some of the major research
              thrusts in data envelopment analysis (DEA) over the three decades
              since the appearance of the seminal work of Charnes et al. (1978)
              [Charnes, A., Cooper, W.W., Rhodes, E.L., 1978. Measuring the
              efficiency of decision making units. European Journal of
              Operational Research 2, 429--444]. The focus herein is primarily
              on methodological developments, and in no manner does the paper
              address the many excellent applications that have appeared during
              that period. Specifically, attention is primarily paid to (1) the
              various models for measuring efficiency, (2) approaches to
              incorporating restrictions on multipliers, (3) considerations
              regarding the status of variables, and (4) modeling of data
              variation.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  192,
  number   =  1,
  pages    = "1--17",
  month    =  jan,
  year     =  2009,
  keywords = "data variation; dea; models; multiplier restrictions"
}

@ARTICLE{Simar2000-hh,
  title     = "Statistical Inference in Nonparametric Frontier Models: The
               State of the Art",
  author    = "Simar, L{\'e}opold and Wilson, Paul W",
  abstract  = "Efficiency scores of firms are measured by their distance to an
               estimated production frontier. The economic literature proposes
               several nonparametric frontier estimators based on the idea of
               enveloping the data (FDH and DEA-type estimators). Many have
               claimed that FDH and DEA techniques are non-statistical, as
               opposed to econometric approaches where particular parametric
               expressions are posited to model the frontier. We can now define
               a statistical model allowing determination of the statistical
               properties of the nonparametric estimators in the multi-output
               and multi-input case. New results provide the asymptotic
               sampling distribution of the FDH estimator in a multivariate
               setting and of the DEA estimator in the bivariate case. Sampling
               distributions may also be approximated by bootstrap
               distributions in very general situations. Consequently,
               statistical inference based on DEA/FDH-type estimators is now
               possible. These techniques allow correction for the bias of the
               efficiency estimators and estimation of confidence intervals for
               the efficiency measures. This paper summarizes the results which
               are now available, and provides a brief guide to the existing
               literature. Emphasizing the role of hypotheses and inference, we
               show how the results can be used or adapted for practical
               purposes.",
  journal   = "Journal of Productivity Analysis",
  publisher = "Kluwer Academic Publishers",
  volume    =  13,
  number    =  1,
  pages     = "49--78",
  month     =  jan,
  year      =  2000,
  language  = "en"
}

@ARTICLE{Banker1984-xe,
  title     = "Some Models for Estimating Technical and Scale Inefficiencies in
               Data Envelopment Analysis",
  author    = "Banker, R D and Charnes, A and Cooper, W W",
  abstract  = "In management contexts, mathematical programming is usually used
               to evaluate a collection of possible alternative courses of
               action en route to selecting one which is best. In this
               capacity, mathematical programming serves as a planning aid to
               management. Data Envelopment Analysis reverses this role and
               employs mathematical programming to obtain ex post facto
               evaluations of the relative efficiency of management
               accomplishments, however they may have been planned or executed.
               Mathematical programming is thereby extended for use as a tool
               for control and evaluation of past accomplishments as well as a
               tool to aid in planning future activities. The CCR ratio form
               introduced by Charnes, Cooper and Rhodes, as part of their Data
               Envelopment Analysis approach, comprehends both technical and
               scale inefficiencies via the optimal value of the ratio form, as
               obtained directly from the data without requiring a priori
               specification of weights and/or explicit delineation of assumed
               functional forms of relations between inputs and outputs. A
               separation into technical and scale efficiencies is accomplished
               by the methods developed in this paper without altering the
               latter conditions for use of DEA directly on observational data.
               Technical inefficiencies are identified with failures to achieve
               best possible output levels and/or usage of excessive amounts of
               inputs. Methods for identifying and correcting the magnitudes of
               these inefficiencies, as supplied in prior work, are
               illustrated. In the present paper, a new separate variable is
               introduced which makes it possible to determine whether
               operations were conducted in regions of increasing, constant or
               decreasing returns to scale (in multiple input and multiple
               output situations). The results are discussed and related not
               only to classical (single output) economics but also to more
               modern versions of economics which are identified with
               ?contestable market theories.?",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  30,
  number    =  9,
  pages     = "1078--1092",
  month     =  sep,
  year      =  1984
}

@ARTICLE{Drake2006-tk,
  title    = "The impact of macroeconomic and regulatory factors on bank
              efficiency: A non-parametric analysis of Hong Kong's banking
              system",
  author   = "Drake, Leigh and Hall, Maximilian J B and Simper, Richard",
  abstract = "This paper assesses the relative technical efficiency of
              institutions operating in a market that has been significantly
              affected by environmental and market factors in recent years, the
              Hong Kong banking system. These environmental factors are
              specifically incorporated into the efficiency analysis using the
              innovative slacks-based, second stage Tobit regression approach
              advocated by Fried et al. [Fried, H.O., Schmidt, S.S.,
              Yaisawarng, S., 1999. Incorporating the operating environment
              into a nonparametric measure of technical efficiency. Journal of
              Productivity Analysis 12, 249--267]. A further innovation is that
              we also employ Tone's [Tone, K., 2001. A slacks-based measure of
              efficiency in data envelopment analysis. European Journal of
              Operational Research 130, 498--509] slacks-based model (SBM) to
              conduct the data envelopment analysis (DEA), in addition to the
              more traditional approach attributable to Banker, Charnes and
              Cooper (BCC) [Banker, R.D., Charnes, A., Cooper, W.W., 1984. Some
              models for estimating technical and scale efficiencies in data
              envelopment analysis. Management Science 30, 1078--1092]. The
              results indicate: high levels of technical inefficiency for many
              institutions; considerable variations in efficiency levels and
              trends across size groups and banking sectors; and also
              differential impacts of environmental factors on different size
              groups and financial sectors. Surprisingly, the accession of Hong
              Kong to the People's Republic of China, episodes of financial
              deregulation, and the 1997/1998 South East Asian crisis do not
              seem to have had a significant independent impact on relative
              efficiency. However, the results suggest that the impact of the
              last-mentioned may have come via the adverse developments in the
              macroeconomy and in the housing market.",
  journal  = "Journal of Banking \& Finance",
  volume   =  30,
  number   =  5,
  pages    = "1443--1466",
  month    =  may,
  year     =  2006,
  keywords = "c23; c52; g21"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Delis2009-th,
  title     = "Determinants of bank efficiency: evidence from a semi‚Äêparametric
               methodology",
  author    = "Delis, Manthos D and Papanikolaou, Nikolaos I",
  abstract  = "Purpose -- This paper aims to analyze bank efficiency into a
               number of bank‚Äêspecific, industry‚Äêspecific and macroeconomic
               determinants.Design/methodology/approach -- The authors follow a
               semi‚Äêparametric two‚Äêstage methodology, where productive
               efficiency is derived via a non‚Äêparametric technique in the
               first stage and then the scores obtained are linked to a series
               of determinants of bank efficiency, using a double bootstrapping
               procedure.Findings -- Overall, it is found that the banking
               sectors of almost all the sample countries show a gradual
               improvement in their efficiency levels. The model used shows
               that a number of determinants like bank size, industry
               concentration and the investment environment have a positive
               impact on bank efficiency, which is not the case when standard
               Tobit models are employed.Research limitations/implications --
               The findings have important implications for the relevance of
               well‚Äêknown hypotheses that refer to the performance of the
               banking sectors, like the structure‚Äêconduct‚Äêperformance and the
               efficient structure hypotheses. These implications are not
               necessarily verified when past conventional econometric
               methodologies are used.Practical implications -- The paper
               offers new insights to policy makers, bank managers and
               practitioners on the relevance of a number of driving factors of
               bank efficiency that might help them to improve the performance
               of the banking system and enhance the quality of services
               provided.Originality/value -- This is the first paper in the
               bank efficiency literature that employs a semi‚Äêparametric
               two‚Äêstage model, which relaxes several deficiencies of previous
               two‚Äêstage empirical approaches thus, offering a solution to the
               many problematic features of standard censored regressions.",
  journal   = "Managerial Finance",
  publisher = "Emerald Group Publishing Limited",
  volume    =  35,
  number    =  3,
  pages     = "260--275",
  year      =  2009,
  keywords  = "Accounting; Auditing \& Economics"
}

@ARTICLE{Lakonishok1994-ia,
  title     = "Contrarian Investment, Extrapolation, and Risk",
  author    = "Lakonishok, Josef and Shleifer, Andrei and Vishny, Robert W",
  abstract  = "For many years, scholars and investment professionals have
               argued that value strategies outperform the market. These value
               strategies call for buying stocks that have low prices relative
               to earnings, dividends, book assets, or other measures of
               fundamental value. While there is some agreement that value
               strategies produce higher returns, the interpretation of why
               they do so is more controversial. This article provides evidence
               that value strategies yield higher returns because these
               strategies exploit the suboptimal behavior of the typical
               investor and not because these strategies are fundamentally
               riskier.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  49,
  number    =  5,
  pages     = "1541--1578",
  month     =  dec,
  year      =  1994
}

@INCOLLECTION{Simar2011-ir,
  title     = "Performance of the Bootstrap for {DEA} Estimators and Iterating
               the Principle",
  booktitle = "Handbook on Data Envelopment Analysis",
  author    = "Simar, L{\'e}opold and Wilson, Paul W",
  editor    = "Cooper, William W and Seiford, Lawrence M and Zhu, Joe",
  abstract  = "This chapter further examines the bootstrap method proposed by
               Simar and Wilson (Manag Sci 44(11):49--61, 1998) for DEA
               efficiency estimators. Some simplifications as well as Monte
               Carlo evidence on the coverage probabilities of confidence
               intervals estimated by the method are offered. In addition, we
               present similar evidence for confidence intervals estimated with
               the so-called naive bootstrap to illustrate the fact that the
               naive bootstrap is inconsistent in the DEA setting. Finally, we
               propose an iterated version of the bootstrap which may be used
               to improve bootstrap estimates of confidence intervals.",
  publisher = "Springer, Boston, MA",
  volume    =  164,
  pages     = "241--271",
  chapter   =  10,
  series    = "International Series in Operations Research \& Management
               Science",
  year      =  2011,
  address   = "Boston, MA",
  keywords  = "bootstrap; data envelopment analysis; distance function;
               efficiency; frontier models",
  language  = "en"
}

@ARTICLE{Kwan1997-lv,
  title     = "Bank Risk, Capitalization, and Operating Efficiency",
  author    = "Kwan, Simon and Eisenbeis, Robert",
  abstract  = "No abstract is available for this item.",
  journal   = "Journal of Financial Services Research",
  publisher = "Springer \& Western Finance Association",
  volume    =  12,
  number    =  2,
  pages     = "117--131",
  year      =  1997
}

@ARTICLE{Lozano-Vivas2010-ab,
  title    = "The impact of non-traditional activities on the estimation of
              bank efficiency: International evidence",
  author   = "Lozano-Vivas, Ana and Pasiouras, Fotios",
  abstract = "This paper investigates the relevance of non-traditional
              activities in the estimation of bank efficiency levels using a
              sample of 752 publicly quoted commercial banks from 87 countries
              around the world, allowing comparison of the impact of such
              activities under different levels of economic development,
              geographical regions and other country characteristics. We
              estimate both cost and profit efficiency of banks using a
              traditional function that considers loans and other earnings
              assets as the only outputs, and two additional functions to
              account for non-traditional activities, one with off-balance
              sheet (OBS) items and the other with non-interest income as an
              additional output. Controlling for cross-country differences in
              regulatory and environmental conditions, we find that, on
              average, cost efficiency increases irrespective of whether we use
              OBS or non-interest income, although the results for profit
              efficiency are mixed. Our results also reveal that while the
              inclusion of non-traditional outputs does not alter the
              directional impact of environmental variables on bank
              inefficiency, regulations that restrict bank activities and
              enhance monitoring and supervision provisions improve both cost
              and profit efficiency.",
  journal  = "Journal of Banking \& Finance",
  volume   =  34,
  number   =  7,
  pages    = "1436--1449",
  month    =  jul,
  year     =  2010,
  keywords = "Efficiency; Non-traditional activities; Off-balance sheet;
              Environmental variables; Regulatory conditions"
}

@ARTICLE{Zellner1966-sn,
  title     = "Specification and Estimation of {Cobb-Douglas} Production
               Function Models",
  author    = "Zellner, A and Kmenta, J and Dr{\`e}ze, J",
  abstract  = "In this paper we consider the specification and estimation of
               the Cobb-Douglas production function model. After reviewing the
               ``traditional'' specifying assumptions for the model which are
               based on deterministic profit maximization, we develop a model
               in which profits are stochastic and in which maximization of the
               mathematical expectation of profits is posited. ``Sampling
               theory'' and Bayesian estimation techniques for this model are
               presented.",
  journal   = "Econometrica",
  publisher = "[Wiley, Econometric Society]",
  volume    =  34,
  number    =  4,
  pages     = "784--795",
  year      =  1966
}

@ARTICLE{David_Sherman2006-si,
  title     = "Benchmarking with quality-adjusted {DEA} ({Q-DEA}) to seek
               lower-cost high-quality service: Evidence from a {U.S.bank}
               application",
  author    = "David Sherman, H and Zhu, Joe",
  abstract  = "Benchmarking is a widely cited method to identify and adopt
               best-practices as a means to improve performance. Data
               envelopment analysis (DEA) has been demonstrated to be a
               powerful benchmarking methodology for situations where multiple
               inputs and outputs need to be assessed to identify
               best-practices and improve productivity in organizations. Most
               DEA benchmarking studies have excluded quality, even in
               service-sector applications such as health care where quality is
               a key element of performance. This limits the practical value of
               DEA in organizations where maintaining and improving service
               quality is critical to achieving performance objectives. In this
               paper, alternative methods incorporating quality in DEA
               benchmarking are demonstrated and evaluated. It is shown that
               simply treating the quality measures as DEA outputs does not
               help in discriminating the performance. Thus, the current study
               presents a new, more sensitive, quality-adjusted DEA (Q-DEA),
               which effectively deals with quality measures in benchmarking.
               We report the results of applying Q-DEA to a U.S. bank's
               200-branch network that required a method for benchmarking to
               help manage operating costs and service quality. Q-DEA findings
               helped the bank achieve cost savings and improved operations
               while preserving service quality, a dimension critical to its
               mission. New insights about ways to improve branch operations
               based on the best-practice (high-quality low-cost) benchmarks
               identified with Q-DEA are also described in the paper. This
               demonstrates the practical need and potential benefits of Q-DEA
               and its efficacy in one application, and also suggests the need
               for further research on measuring and incorporating quality into
               DEA benchmarking.",
  journal   = "Ann. Oper. Res.",
  publisher = "Springer US",
  volume    =  145,
  number    =  1,
  pages     = "301--319",
  month     =  jul,
  year      =  2006,
  language  = "en"
}

@ARTICLE{Fukuyama2002-ve,
  title    = "Estimating output allocative efficiency and productivity change:
              Application to Japanese banks",
  author   = "Fukuyama, Hirofumi and Weber, William L",
  journal  = "Eur. J. Oper. Res.",
  volume   =  137,
  number   =  1,
  pages    = "177--190",
  month    =  feb,
  year     =  2002,
  keywords = "allocative e; ciency; dea; indirect production theory; japanese
              banking; productivity change; quasi distance function"
}

@ARTICLE{Lensink2008-um,
  title    = "Bank efficiency and foreign ownership: Do good institutions
              matter?",
  author   = "Lensink, Robert and Meesters, Aljar and Naaborg, Ilko",
  journal  = "Journal of Banking \& Finance",
  volume   =  32,
  number   =  5,
  pages    = "834--844",
  month    =  may,
  year     =  2008,
  keywords = "banks; cost efficiency; foreign ownership; governance"
}

@ARTICLE{Berger2003-af,
  title    = "Explaining the dramatic changes in performance of {US} banks:
              technological change, deregulation, and dynamic changes in
              competition",
  author   = "Berger, Allen N and Mester, Loretta J",
  journal  = "Journal of Financial Intermediation",
  volume   =  12,
  number   =  1,
  pages    = "57--95",
  month    =  jan,
  year     =  2003,
  keywords = "12; 2003; 57; 95; and; com; deregulation; elsevier; explaining
              the dramatic changes; in performance of; jfi; locate; rnal of
              financial intermediation; technological change; us banks; www"
}

@ARTICLE{Imbens1994-am,
  title     = "Identification and Estimation of Local Average Treatment Effects",
  author    = "Imbens, Guido W and Angrist, Joshua D",
  abstract  = "We investigate conditions sufficient for identification of
               average treatment effects using instrumental variables. First we
               show that the existence of valid instruments is not sufficient
               to identify any meaningful average treatment effect. We then
               establish that the combination of an instrument and a condition
               on the relation between the instrument and the participation
               status is sufficient for identification of a local average
               treatment effect for those who can be induced to change their
               participation status by changing the value of the instrument.
               Finally we derive the probability limit of the standard IV
               estimator under these conditions. It is seen to be a weighted
               average of local average treatment effects.(This abstract was
               borrowed from another version of this item.)",
  journal   = "Econometrica",
  publisher = "Econometric Society",
  volume    =  62,
  number    =  2,
  pages     = "467--475",
  year      =  1994
}

@ARTICLE{Rubin1980-nf,
  title     = "Bias Reduction Using {Mahalanobis-Metric} Matching",
  author    = "Rubin, Donald B",
  abstract  = "Monte Carlo methods are used to study the ability of
               nearest-available, Mahalanobis-metric matching to make the means
               of matching variables more similar in matched samples than in
               random samples.",
  journal   = "Biometrics",
  publisher = "[Wiley, International Biometric Society]",
  volume    =  36,
  number    =  2,
  pages     = "293--298",
  year      =  1980
}

@ARTICLE{Chung1999-sp,
  title    = "Limit orders and the bid--ask spread",
  author   = "Chung, Kee H and Van Ness, Bonnie F and Van Ness, Robert A",
  abstract = "We examine the role of limit-order traders and specialists in the
              market-making process. We find that a large portion of posted
              bid--ask quotes originates from the limit-order book without
              direct participation by specialists, and that competition between
              traders and specialists has a significant impact on the bid--ask
              spread. Specialists' spreads are widest at the open, narrow until
              late morning, and then level off. The U-shaped intraday pattern
              of spreads largely reflects the intraday variation in spreads
              established by limit-order traders. Lastly, the intraday
              variation in limit-order spreads is significantly related to the
              intraday variation in limit-order placements and executions.",
  journal  = "J. financ. econ.",
  volume   =  53,
  number   =  2,
  pages    = "255--287",
  month    =  aug,
  year     =  1999,
  keywords = "Bid--ask spread; Limit order; Specialists"
}

@ARTICLE{Hirshleifer2003-mw,
  title     = "Good Day Sunshine: Stock Returns and the Weather",
  author    = "Hirshleifer, David and Shumway, Tyler",
  abstract  = "Psychological evidence and casual intuition predict that sunny
               weather is associated with upbeat mood. This paper examines the
               relationship between morning sunshine in the city of a country's
               leading stock exchange and daily market index returns across 26
               countries from 1982 to 1997. Sunshine is strongly significantly
               correlated with stock returns. After controlling for sunshine,
               rain and snow are unrelated to returns. Substantial use of
               weather-based strategies was optimal for a trader with very low
               transactions costs. However, because these strategies involve
               frequent trades, fairly modest costs eliminate the gains. These
               findings are difficult to reconcile with fully rational price
               setting.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  58,
  number    =  3,
  pages     = "1009--1032",
  month     =  jun,
  year      =  2003
}

@ARTICLE{Gatev2006-bs,
  title     = "Pairs Trading: Performance of a {Relative-Value} Arbitrage Rule",
  author    = "Gatev, Evan and Goetzmann, William N and Rouwenhorst, K Geert",
  abstract  = "We test a Wall Street investment strategy, ``pairs trading,''
               with daily data over 1962--2002. Stocks are matched into pairs
               with minimum distance between normalized historical prices. A
               simple trading rule yields average annualized excess returns of
               up to 11\% for self-financing portfolios of pairs. The profits
               typically exceed conservative transaction-cost estimates.
               Bootstrap results suggest that the ``pairs'' effect differs from
               previously documented reversal profits. Robustness of the excess
               returns indicates that pairs trading profits from temporary
               mispricing of close substitutes. We link the profitability to
               the presence of a common factor in the returns, different from
               conventional risk measures.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  19,
  number    =  3,
  pages     = "797--827",
  month     =  oct,
  year      =  2006
}

@ARTICLE{Michel1978-he,
  title    = "Immunologic and genetic factors predisposing to allergy",
  author   = "Michel, F B and Bousquet, J and Prefaut, C and Wagner, E",
  abstract = "Allergy often begins with a subtle and/or transient T cell
              defect. This defect is first responsible for an IgA deficiency.
              The normal function of IgA is immune exclusion. In its absence,
              allergens can pass through the mucosa and stimulate the
              immunocompetent cells. The T cell defect may also be implied by
              the synthesis of IgE directed against the allergens which passed
              through. Clinical, biological and immunological findings (T cell
              defect in allergic disease, low range of IgA in the early life of
              atopics) are in agreement. The genetic factor for pollinosis and
              house dust allergy are segregated. In ragweed allergy there is an
              Ir gene coding for antigen-specific Ig of different classes and a
              group of non-linked major histocompatibility complex alleles
              coding for non antigen-specific IgE. There are some links with
              HLA. In house dust allergy the Ir gene is very common and almost
              everyone can produce an allergy under some conditions (T cell
              defect). Whatever the immunologic and genetic factors are, they
              need allergens and environmental factors to induce allergy.
              Allergy is a complex state in which several mechanisms, often
              associated and sometimes unclear, are involved. Some of them may
              be an abnormality of the autonomic nervous system, and/or an
              increase in the mucous membrane permeability, and/or a subtle
              immunodeficiency. All these mechanisms are regulated by genetic
              factors and modulated by environmental ones.",
  journal  = "Allergol. Immunopathol.",
  volume   =  6,
  number   =  2,
  pages    = "169--179",
  month    =  mar,
  year     =  1978,
  language = "en"
}

@BOOK{Dobson2011-ve,
  title     = "The Economics of Football",
  author    = "Dobson, Stephen and Goddard, John",
  abstract  = "The second edition of this popular book presents a detailed
               economic analysis of professional football at club level, with
               new material included to reflect the development of the
               economics of professional football over the past ten years.
               Using a combination of economic reasoning and statistical and
               econometric analysis, the authors build upon the successes and
               strengths of the first edition to guide readers through the
               economic complexities and peculiarities of English club
               football. It uses a wide range of international comparisons to
               help emphasize both the broader relevance as well as the unique
               characteristics of the English experience. Topics covered
               include some of the most hotly debated issues currently
               surrounding professional football, including player salaries,
               the effects of management on team performance, betting on
               football, racial discrimination and the performance of football
               referees. This edition also features new chapters on the
               economics of international football, including the World Cup.",
  publisher = "Cambridge University Press",
  month     =  feb,
  year      =  2011,
  address   = "New York",
  language  = "en"
}

@ARTICLE{Andor2014-sk,
  title     = "The {StoNED} age: the departure into a new era of efficiency
               analysis? A monte carlo comparison of {StoNED} and the
               ``oldies'' ({SFA} and {DEA})",
  author    = "Andor, Mark and Hesse, Frederik",
  abstract  = "Based on the seminal paper of Farrell (J R Stat Soc Ser A
               (General) 120(3):253--290, 1957), researchers have developed
               several methods for measuring efficiency. Nowadays, the most
               prominent representatives are nonparametric data envelopment
               analysis (DEA) and parametric stochastic frontier analysis
               (SFA), both introduced in the late 1970s. Researchers have been
               attempting to develop a method which combines the virtues---both
               nonparametric and stochastic---of these ``oldies''. The recently
               introduced Stochastic non-smooth envelopment of data (StoNED) by
               Kuosmanen and Kortelainen (J Prod Anal 38(1):11--28, 2012) is
               such a promising method. This paper compares the StoNED method
               with the two ``oldies'' DEA and SFA and extends the initial
               Monte Carlo simulation of Kuosmanen and Kortelainen (J Prod Anal
               38(1):11--28, 2012) in several directions. We show, among
               others, that, in scenarios without noise, the rivalry is still
               between the ``oldies'', while in noisy scenarios, the
               nonparametric StoNED PL now constitutes a promising alternative
               to the SFA ML.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  41,
  number    =  1,
  pages     = "85--109",
  month     =  feb,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Badunenko2012-bm,
  title     = "When, where and how to perform efficiency estimation",
  author    = "Badunenko, Oleg and Henderson, Daniel J and Kumbhakar, Subal C",
  abstract  = "Summary. We compare two flexible estimators of technical
               efficiency in a cross-sectional setting: the non-parametric
               kernel stochastic frontier analysis estimator of Fan, Li and
               Weersink with the non-parametric bias-corrected data envelopment
               analysis estimator of Kneip, Simar and Wilson. We assess the
               finite sample performance of each estimator via Monte Carlo
               simulations and empirical examples. We find that the reliability
               of efficiency scores critically hinges on the ratio of the
               variation in efficiency to the variation in noise. These results
               should be a valuable resource to both academic researchers and
               practitioners.",
  journal   = "J. R. Stat. Soc. Ser. A Stat. Soc.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  175,
  number    =  4,
  pages     = "863--892",
  month     =  oct,
  year      =  2012,
  keywords  = "Bootstrap; Non-parametric kernel; Technical efficiency"
}

@ARTICLE{Hughes2000-ew,
  title     = "Recovering Risky Technologies Using the Almost Ideal Demand
               System: An Application to {U.S}. Banking",
  author    = "Hughes, Joseph P and Lang, William and Mester, Loretta J and
               Moon, Choon-Geol",
  abstract  = "We present and estimate a model that shifts the focus of
               modeling production from the traditional assumptions of profit
               maximization and cost minimization to a more general assumption
               of managerial utility maximization that can incorporate risk
               incentives into the analysis of production and recover
               value-maximizing technologies. We implement the model using the
               almost ideal demand system. In addition, we use the model to
               measure efficiency in a more general way that can incorporate a
               concern for the market value of firms' assets and equity and
               identify value-maximizing firms. This shift in focus bridges the
               gap between the risk incentives literature in banking that
               ignores the microeconomics of production and the production
               literature that ignores the relationship between production
               decisions and risk. Our estimation of the model for a sample of
               U.S. commercial banks illustrates that results obtained from our
               generalized model can differ significantly from those obtained
               from the standard profit-maximization model, which ignores risk.",
  journal   = "Journal of Financial Services Research",
  publisher = "Kluwer Academic Publishers",
  volume    =  18,
  number    =  1,
  pages     = "5--27",
  month     =  oct,
  year      =  2000,
  language  = "en"
}

@ARTICLE{Badin2010-vv,
  title    = "Optimal bandwidth selection for conditional efficiency measures:
              A data-driven approach",
  author   = "B{\u a}din, Luiza and Daraio, Cinzia and Simar, L{\'e}opold",
  abstract = "In productivity analysis an important issue is to detect how
              external (environmental) factors, exogenous to the production
              process and not under the control of the producer, might
              influence the production process and the resulting efficiency of
              the firms. Most of the traditional approaches proposed in the
              literature have serious drawbacks. An alternative approach is to
              describe the production process as being conditioned by a given
              value of the environmental variables (Cazals, C., Florens, J.P.,
              Simar, L., 2002. Nonparametric Frontier estimation: A robust
              approach. Journal of Econometrics 106, 1--25; Daraio, C., Simar,
              L., 2005. Introducing environmental variables in nonparametric
              Frontier models: A probabilistic approach. Journal of
              Productivity Analysis 24(1), 93--121). This defines conditional
              efficiency measures where the production set in the
              input$\times$output space may depend on the value of the external
              variables. The statistical properties of nonparametric estimators
              of these conditional measures are now established (Jeong, S.O.,
              Park, B.U., Simar, L., 2008. Nonparametric conditional efficiency
              measures: Asymptotic properties. Annals of Operations Research
              doi: 10.1007/s10479-008-0359-5). These involve the estimation of
              a nonstandard conditional distribution function which requires
              the specification of a smoothing parameter (a bandwidth). So far,
              only the asymptotic optimal order of this bandwidth has been
              established. This is of little interest for the practitioner. In
              this paper we fill this gap and we propose a data-driven
              technique for selecting this parameter in practice. The approach,
              based on a Least Squares Cross Validation procedure (LSCV),
              provides an optimal bandwidth that minimizes an appropriate
              (weighted) integrated Squared Error (ISE). The method is
              carefully described and exemplified with some simulated data with
              univariate and multivariate environmental factors. An application
              on real data (performances of Mutual Funds) illustrates how this
              new optimal method of bandwidth selection works in practice.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  201,
  number   =  2,
  pages    = "633--640",
  month    =  mar,
  year     =  2010,
  keywords = "Bandwidth; Conditional distribution function; Conditional
              efficiency measures; Environmental factors; Nonparametric
              efficiency estimation"
}

@ARTICLE{Tone2010-sj,
  title    = "Dynamic {DEA}: A slacks-based measure approach",
  author   = "Tone, Kaoru and Tsutsui, Miki",
  abstract = "In data envelopment analysis, there are several methods for
              measuring efficiency changes over time, e.g. the window analysis
              and the Malmquist index. However, they usually neglect carry-over
              activities between two consecutive terms and only focus on the
              separate time period independently aiming local optimization in a
              single period, even if these models can take into account the
              time change effect. In the actual business world, a long time
              planning and investment is a subject of great concern. For these
              cases, single period optimization model is not suitable for
              performance evaluation. To cope with long time point of view, the
              dynamic DEA model incorporates carry-over activities into the
              model and enables us to measure period specific efficiency based
              on the long time optimization during the whole period. Dynamic
              DEA model proposed by F{\"a}re and Grosskopf is the first
              innovative contribution for such purpose. In this paper we
              develop their model in the slacks-based measure (SBM) framework,
              called dynamic SBM (DSBM). The SBM model is non-radial and can
              deal with inputs/outputs individually, contrary to the radial
              approaches that assume proportional changes in inputs/outputs.
              Furthermore, according to the characteristics of carry-overs, we
              classify them into four categories, i.e. desirable, undesirable,
              free and fixed. Desirable carry-overs correspond, for example, to
              profit carried forward and net earned surplus carried to the next
              term, while undesirable carry-overs include, for example, loss
              carried forward, bad debt and dead stock. Free and fixed
              carry-overs indicate, respectively, discretionary and
              non-discretionary ones. We develop dynamic SBM models that can
              evaluate the overall efficiency of decision making units for the
              whole terms as well as the term efficiencies.",
  journal  = "Omega",
  volume   =  38,
  number   =  3,
  pages    = "145--156",
  month    =  jun,
  year     =  2010,
  keywords = "Carry-over; DEA; DSBM; Dynamic DEA"
}

@ARTICLE{Badin2012-bc,
  title    = "How to measure the impact of environmental factors in a
              nonparametric production model",
  author   = "B{\u a}din, Luiza and Daraio, Cinzia and Simar, L{\'e}opold",
  abstract = "The measurement of technical efficiency allows managers and
              policy makers to enhance existing differentials and potential
              improvements across a sample of analyzed units. The next step
              involves relating the obtained efficiency estimates to some
              external or environmental factors which may influence the
              production process, affect the performances and explain the
              efficiency differentials. Recently introduced conditional
              efficiency measures (Daraio and Simar, 2005, 2007a,b), including
              conditional FDH, conditional DEA, conditional order-m and
              conditional order-$\alpha$, have rapidly developed into a useful
              tool to explore the impact of exogenous factors on the
              performance of Decision Making Units in a nonparametric
              framework. This paper contributes in a twofold fashion. It first
              extends previous studies by showing that a careful analysis of
              both full and partial conditional measures allows the
              disentangling of the impact of environmental factors on the
              production process in its two components: impact on the
              attainable set and/or impact on the distribution of the
              efficiency scores. The authors investigate these
              interrelationships, both from an individual and a global
              perspective. Second, this paper examines the impact of
              environmental factors on the production process in a new
              two-stage type approach but using conditional measures to avoid
              the flaws of the traditional two-stage analysis. This novel
              approach also provides a measure of inefficiency whitened from
              the main effect of the environmental factors allowing a ranking
              of units according to their managerial efficiency, even when
              facing heterogeneous environmental conditions. The paper includes
              an illustration on simulated samples and a real data set from the
              banking industry.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  223,
  number   =  3,
  pages    = "818--833",
  month    =  dec,
  year     =  2012,
  keywords = "Conditional efficiency measures; Data Envelopment Analysis (DEA);
              Free Disposal Hull (FDH); Managerial efficiency; Nonparametric
              frontiers; Two-stage efficiency analysis"
}

@ARTICLE{Yang2014-wr,
  title     = "A predictive {DEA} model for outlier detection",
  author    = "Yang, Mingwen and Wan, Guohua and Zheng, Eric",
  abstract  = "Outlier detection is one of the key issues in any data-driven
               analytics. In this paper, we propose Bi-super DEA, a super
               DEA-based method that constructs both efficient and inefficient
               frontiers for outlier detection. In evaluating its predictive
               performance, we develop a novel predictive DEA procedure, PDEA,
               which extends the conventional DEA approaches that have been
               primarily used for in-sample efficiency estimation, to predict
               outputs for the out-of-sample. This enables us to compare the
               predictive performance of our approach against several popular
               outlier detection methods including the parametric robust
               regression in statistics and non-parametric k-means in data
               mining. We conduct comprehensive simulation experiments to
               examine the relative performance of these outlier detection
               methods under the influence of five factors: sample size,
               linearity of production function, normality of noise
               distribution, homogeneity of data, and levels of random noise
               contaminating the data generating process (DGP). We find that,
               somewhat surprisingly, Bi-super CCR consistently outperforms
               Bi-super BCC in detecting outliers. Under the linearity,
               normality and homogeneity conditions, the parametric robust
               regression method works best. However, when the DGP violates
               these conditions, Bi-super DEA emerges as the better choice due
               to its distribution-free property. Our results shed light on the
               conditions that each method excels or fails and provide users
               with practical guidelines on how to choose appropriate methods
               to detect outliers.",
  journal   = "Journal of Management Analytics",
  publisher = "Taylor \& Francis",
  volume    =  1,
  number    =  1,
  pages     = "20--41",
  month     =  jan,
  year      =  2014
}

@ARTICLE{Sloane1971-tv,
  title     = "The Economics of Professional Football: The Football Club as a
               Utility Maximiser",
  author    = "Sloane, Peter J",
  journal   = "Scott. J. Polit. Econ.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  18,
  number    =  2,
  pages     = "121--146",
  month     =  jun,
  year      =  1971
}

@ARTICLE{Neale1964-ar,
  title     = "The Peculiar Economics of Professional Sports",
  author    = "Neale, Walter C",
  abstract  = "Louis-Schmelling paradox, 1. --- The inverted joint product or
               the product joint, 2. --- League standing effect, 3. --- Fourth
               estate benefit, 3. --- Multifirm plants, 5. --- Diminishing
               quality returns, 8. --- Input-enthusiasm effect, 8. --- Roger
               Maris cobweb, 12. --- Bobby Layne rigidity, 12. --- Archie Moore
               invisibility, 13.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  78,
  number    =  1,
  pages     = "1--14",
  month     =  feb,
  year      =  1964
}

@ARTICLE{Rubin2013-hg,
  title     = "A dynamic theory of the credit union",
  author    = "Rubin, Geoffrey M and Overstreet, George A and Beling, Peter and
               Rajaratnam, Kanshukan",
  abstract  = "A topic of recent interest in the retail financial sector has
               been the growth of credit unions or ``pure cooperatives''. Past
               credit union researchers built mathematical models of credit
               union operations. These models identified important operating
               characteristics but were modeled under assumptions of static
               operating environments. The model presented in this paper
               departs from the traditional static models and examines dynamic
               operation for a United States credit union. Its inter-temporal
               structure clarifies a number of issues---such as optimal equity
               retention and inter-temporal rate policy---not addressed by
               earlier studies. Given initial conditions, the model specifies
               equity retention and inter-temporal deposit and loan rate
               policies until an equilibrium state is reached.",
  journal   = "Ann. Oper. Res.",
  publisher = "Springer US",
  volume    =  205,
  number    =  1,
  pages     = "29--53",
  month     =  may,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Frijns2012-cl,
  title     = "Firm efficiency and stock returns",
  author    = "Frijns, Bart and Margaritis, Dimitris and Psillaki, Maria",
  abstract  = "In this paper, we investigate the role of firm efficiency in
               asset pricing using a sample of US publicly listed companies for
               the period 1988--2007. We employ non-parametric data envelopment
               analysis (DEA) on various input/output combinations, focusing on
               sales and market value as output measures in the construction of
               the frontier technologies. Using these performance measures, we
               examine whether efficient firms perform differently from
               inefficient firms following standard financial analysis
               procedures. First, we employ performance attribution
               regressions, by forming portfolios based on efficiency scores
               and tracking the performance of the various portfolios over
               time. Second, we perform cross-sectional/panel regressions to
               determine whether firm efficiency indeed has explanatory power
               for the cross-section of stock returns. Our results suggest that
               firm efficiency plays an important role in asset pricing and
               that efficient firms significantly outperform inefficient firms
               even after controlling for known risk factors.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  37,
  number    =  3,
  pages     = "295--306",
  month     =  jun,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Lewbel2007-uk,
  title     = "Estimation of Average Treatment Effects with Misclassification",
  author    = "Lewbel, Arthur",
  abstract  = "This paper considers identification and estimation of the effect
               of a mismeasured binary regressor in a nonparametric or
               semiparametric regression, or the conditional average effect of
               a binary treatment or policy on some outcome where treatment may
               be misclassified. Failure to account for misclassification is
               shown to result in attenuation bias in the estimated treatment
               effect. An identifying assumption that overcomes this bias is
               the existence of an instrument for the binary regressor that is
               conditionally independent of the treatment effect. A discrete
               instrument suffices for nonparametric identification.",
  journal   = "Econometrica",
  publisher = "Blackwell Publishing Ltd",
  volume    =  75,
  number    =  2,
  pages     = "537--551",
  month     =  mar,
  year      =  2007,
  keywords  = "Binary regressor; program evaluation; treatment effects;
               misclassification; contamination bias; measurement error; binary
               choice; binomial response"
}

@ARTICLE{Dawson2000-gx,
  title     = "Estimating Coaching Efficiency in Professional Team Sports:
               Evidence from English Association Football",
  author    = "Dawson, Peter and Dobson, Stephen and Gerrard, Bill",
  abstract  = "This study provides a review of the literature on sporting
               production functions with an emphasis on different input-output
               specifications and alternative estimation procedures. Empirical
               evidence is reported for English association football on the
               robustness of estimates of coaching efficiency to changes in
               estimation methods and the definition of team performance and
               playing talent inputs. A measure of player quality based on
               predicted start-of-season transfer values is developed. It is
               found that the estimation of coaching efficiency is sensitive to
               the choice of time-invariant efficiency models versus
               time-varying and inefficiency effects models. It is also found
               that the results are little affected by different measures of
               team performance but are highly sensitive to the use of an ex
               post financial expenditure input measure. Ex ante input measures
               based on start-of-season player characteristics or predicted
               transfer values are recommended as more appropriate on both
               theoretical and empirical grounds.",
  journal   = "Scott. J. Polit. Econ.",
  publisher = "Blackwell Publishers Ltd",
  volume    =  47,
  number    =  4,
  pages     = "399--421",
  month     =  sep,
  year      =  2000
}

@ARTICLE{Barros2008-as,
  title    = "Efficiency measurement of the English football Premier League
              with a random frontier model",
  author   = "Barros, Carlos Pestana and Garcia-del-Barrio, Pedro",
  abstract = "Using the random stochastic frontier model, this paper examines
              the technical efficiency of the English football Premier League
              from 1998/99 to 2003/04. The model disentangles homogenous and
              heterogeneous variables in the cost function, which leads us to
              advise the implementation of common policies as well as policies
              by clusters.",
  journal  = "Econ. Model.",
  volume   =  25,
  number   =  5,
  pages    = "994--1002",
  month    =  sep,
  year     =  2008,
  keywords = "Efficiency; Football; Policy implications; Random frontier models"
}

@ARTICLE{Barros2006-vh,
  title     = "Performance evaluation of the English Premier Football League
               with data envelopment analysis",
  author    = "Barros, Carlos Pestana and Leach, Stephanie",
  abstract  = "This paper uses data envelopment analysis (DEA) to evaluate the
               performance of English Premier League football clubs from
               1998/99 to 2002/03 combining sport and financial variables. The
               paper evaluates how close the clubs are relative to the frontier
               of best practices, analysing how they manage sport as well as
               financial results. Managerial implications of the research are
               devised.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  38,
  number    =  12,
  pages     = "1449--1458",
  month     =  jul,
  year      =  2006
}

@ARTICLE{Barros2007-gz,
  title     = "Technical efficiency in the English Football Association Premier
               League with a stochastic cost frontier",
  author    = "Barros, Carlos Pestana and Leach, Stephanie",
  abstract  = "This article uses an econometric frontier model to evaluate the
               technical efficiency of English Premier League clubs from
               1998/99 to 2002/03 combining sport and financial variables. A
               Cobb?Douglas cost specification of the technical efficiency
               effects model is used to generate football club efficiency
               scores, allowing for contextual variables which affect
               inefficiency. We conclude that the efficiency scores are mixed.
               A policy is devised for the management of this sector.",
  journal   = "Appl. Econ. Lett.",
  publisher = "Routledge",
  volume    =  14,
  number    =  10,
  pages     = "731--741",
  month     =  aug,
  year      =  2007
}

@ARTICLE{Dawson2002-ae,
  title     = "Managerial efficiency and human capital: an application to
               English association football",
  author    = "Dawson, Peter and Dobson, Stephen",
  abstract  = "The problem of hidden action in organizations makes direct
               measurement of managerial performance problematic. But in
               English association football hidden action is unlikely to be as
               serious a problem because the owner observes the manager's
               performance each time the team plays. In this situation
               production frontier analysis may be used to measure managerial
               performance and analyze the variation in performance across
               managers in terms of manager human capital. Having some kind of
               prior affiliation with the club and achieving international
               recognition as a player are especially important. Overall,
               initial experience matters more than specific and general
               managerial experience. Copyright \copyright{} 2002 John Wiley \&
               Sons, Ltd.",
  journal   = "Manage. Decis. Econ.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  23,
  number    =  8,
  pages     = "471--486",
  month     =  dec,
  year      =  2002
}

@ARTICLE{Lee2009-ch,
  title     = "Identification of Segments of Soccer Clubs in the Spanish League
               First Division with a Latent Class Model Comments",
  author    = "Lee, Young Hoon",
  abstract  = "In this brief commentary, I critically evaluate the empirical
               application of the stochastic frontier latent class model to
               panel data of the Spanish Football League presented in an
               article by Barros, Corral, and Garcia-del-Barrio, published in a
               previous issue of this journal. Errors from the selection of
               input price variable to the interpretation of the cost function
               estimates are discussed, and empirical evidence that their
               estimates are not consistent to basic microeconomic theories is
               presented.",
  journal   = "J. Sports Econom.",
  publisher = "SAGE Publications",
  volume    =  10,
  number    =  6,
  pages     = "651--659",
  month     =  dec,
  year      =  2009
}

@ARTICLE{Pestana_Barros2007-qz,
  title     = "Identification of Segments of Soccer Clubs in the Spanish League
               First Division With a Latent Class Model",
  author    = "Pestana Barros, Carlos and del Corral, Julio and
               Garcia-del-Barrio, Pedro",
  abstract  = "This article identifies different groups in a cost function
               framework of soccer clubs in the Spanish Football League First
               Division. In particular, we have clustered the sample?
               comprising data for seasons 1994/1995 to 2004/2005?into three
               groups. To do so, we have implemented a stochastic frontier
               latent class model, a procedure that also permits us to analyze
               the efficiency of the clubs with respect to their own frontiers.
               The results reveal that some of the clubs could improve their
               efficiency levels substantially.",
  journal   = "J. Sports Econom.",
  publisher = "SAGE Publications",
  volume    =  9,
  number    =  5,
  pages     = "451--469",
  month     =  dec,
  year      =  2007
}

@ARTICLE{Barros2009-ht,
  title     = "Analysing the technical efficiency of the Spanish Football
               League First Division with a random frontier model",
  author    = "Barros, Carlos Pestana and Garcia-del-Barrio, Pedro and Leach,
               Stephanie",
  abstract  = "This article analyses technical efficiency of football clubs in
               the Spanish Football League Division 1 (Primera Liga) from the
               seasons 1995?1996 to 2004?2005 with an unbalanced panel data.
               The random frontier model is used, allowing the identification
               of random variables in the cost frontier. It is concluded that
               the price of capital investment, the number of points won and
               attendance are heterogeneous variables. Therefore, no common
               public policy aiming to improve efficiency can embrace all of
               the clubs, so that policies by clusters are required.",
  journal   = "Appl. Econ.",
  publisher = "Routledge",
  volume    =  41,
  number    =  25,
  pages     = "3239--3247",
  month     =  nov,
  year      =  2009
}

@ARTICLE{McKillop1996-ja,
  title    = "The composite cost function and efficiency in giant Japanese
              banks",
  author   = "McKillop, D G and Glass, J C and Morikawa, Y",
  abstract = "This study provides further empirical investigation, in the
              context of giant Japanese banks, of the recent claim by Pulley
              and Braunstein (1992, A composite cost function for multiproduct
              firms with an application to economies of scope in banking,
              Review of Economics and Statistics 74, 221-230), that their new
              composite model for the multiproduct cost function has important
              advantages over the separable quadratic, generalized translog and
              standard translog models. In addition to assessing the composite
              model's relative ability in measuring global scope and scale
              economies, the study also extends the P-B analysis to assess
              measurement of product-specific scope and scale economies,
              pairwise cost complementarities between outputs, changes in the
              marginal costs of outputs and technological change. The results
              appear to confirm P-B's claim. The persistent finding of scale
              economies for large Japanese banks is also investigated and
              confirmed.",
  journal  = "Journal of Banking and Finance",
  volume   =  20,
  number   =  10,
  pages    = "1651--1671",
  year     =  1996,
  keywords = "Cost functions; Economies of scale; Economies of scope; Japanese
              banks"
}

@ARTICLE{Dai2014-zy,
  title    = "Best-practice benchmarking using clustering methods: Application
              to energy regulation",
  author   = "Dai, Xiaofeng and Kuosmanen, Timo",
  abstract = "Data envelopment analysis (DEA) is widely used as a benchmarking
              tool for improving productive performance of decision making
              units (DMUs). The benchmarks produced by DEA are obtained as a
              side-product of computing efficiency scores. As a result, the
              benchmark units may differ from the evaluated DMU in terms of
              their input--output profiles and the scale size. Moreover, the
              DEA benchmarks may operate in a more favorable environment than
              the evaluated DMU. Further, DEA is sensitive to stochastic noise,
              which can affect the benchmarking exercise. In this paper we
              propose a new approach to benchmarking that combines the frontier
              estimation techniques with clustering methods. More specifically,
              we propose to apply some clustering methods to identify groups of
              DMUs that are similar in terms of their input--output profiles or
              other observed characteristics. We then rank DMUs in the
              descending order of efficiency within each cluster. The
              cluster-specific efficiency rankings enable the management to
              identify not only the most efficient benchmark, but also other
              peers that operate more efficiently within the same cluster. The
              proposed approach is flexible to combine any clustering method
              with any frontier estimation technique. The inputs of clustering
              and efficiency analysis are user-specified and can be
              multi-dimensional. We present a real world application to the
              regulation of electricity distribution networks in Finland, where
              the regulator uses the semi-nonparametric StoNED method
              (stochastic non-parametric envelopment of data). StoNED can be
              seen as a stochastic extension of DEA that takes the noise term
              explicitly into account. We find that the cluster-specific
              efficiency rankings provide more meaningful benchmarks than the
              conventional approach of using the intensity weights obtained as
              a side-product of efficiency analysis.",
  journal  = "Omega",
  volume   =  42,
  number   =  1,
  pages    = "179--188",
  month    =  jan,
  year     =  2014,
  keywords = "Benchmark regulation; CNLS; CRS; Clustering; DEA; DMU; Data
              envelopment analysis (DEA); Electricity distribution; NMM;
              Non-parametric methods including convex non-parame; Productive
              efficiency; SFA; StoNED; Stochastic non-smooth envelopment of
              data (StoNED); VRS; cnls; constant returns to scale; crs; data
              envelopment analysis; dea; decision making unit; dmu; nmm;
              non-parametric methods including convex non-; normal mixture
              model; sfa; stochastic frontier analysis; stochastic non-smooth
              envelopment of data; stoned; variable returns to scale; vrs"
}

@ARTICLE{Tsionas2002-ey,
  title     = "Stochastic frontier models with random coefficients",
  author    = "Tsionas, Efthymios G",
  abstract  = "The paper proposes a stochastic frontier model with random
               coefficients to separate technical inefficiency from
               technological differences across firms, and free the frontier
               model from the restrictive assumption that all firms must share
               exactly the same technological possibilities. Inference
               procedures for the new model are developed based on Bayesian
               techniques, and computations are performed using Gibbs sampling
               with data augmentation to allow finite-sample inference for
               underlying parameters and latent efficiencies. An empirical
               example illustrates the procedure. Copyright \copyright{} 2002
               John Wiley \& Sons, Ltd.",
  journal   = "J. Appl. Econ.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  17,
  number    =  2,
  pages     = "127--147",
  month     =  mar,
  year      =  2002
}

@ARTICLE{Hadri2003-en,
  title     = "Estimation of technical inefficiency effects using panel data
               and doubly heteroscedastic stochastic production frontiers",
  author    = "Hadri, K and Guermat, C and Whittaker, J",
  abstract  = ". In previous studies, measures of technical inefficiency
               effects derived from stochastic production frontiers have been
               estimated from residuals which are sensitive to specification
               errors. This study corrects for this inaccuracy by extending the
               doubly heteroscedastic stochastic cost frontier suggested by
               Hadri (1999) to the model for technical inefficiency effects.
               This model is a stochastic frontier production function for
               panel data as proposed by Battese and Coelli (1995). The study
               uses, for illustration of the techniques, data on 101 mainly
               cereal farms in England. We find that the correction for
               heteroscedasticity is supported by the data. Both point
               estimates and confidence intervals for technical efficiencies
               are provided. The confidence intervals are constructed by
               extending the ``Battese-Coelli'' method reported by Horrace and
               Schmidt (1996) by allowing the technical inefficiency to be time
               varying and the disturbance terms to be heteroscedastic. The
               confidence intervals reveal the precision of technical
               efficiency estimates and show the deficiencies of making
               inferences based exclusively on point estimates.",
  journal   = "Empir. Econ.",
  publisher = "Springer-Verlag",
  volume    =  28,
  number    =  1,
  pages     = "203--222",
  month     =  jan,
  year      =  2003,
  language  = "en"
}

@ARTICLE{Kuosmanen2003-fr,
  title     = "Duality Theory of Non-convex Technologies",
  author    = "Kuosmanen, Timo",
  abstract  = "Duality Theory of production imposes a number of simplifying
               assumptions regarding the production technology, including
               various maintained convexity assumptions. Emphasizing the
               technological information content of alternative models, this
               paper challenges some widely held views on the role of
               convexity. The role of convexity in Duality Theory is
               asymmetric. While convexity is of importance in recovering
               technology information from economic models, cost functions are
               concave and profit functions are convex irrespective of
               convexity of the underlying technology. For recovering
               technology information from economic models and data, we discuss
               two alternative approaches: recovering inexact outer-bound
               approximations; and enriching standard economic models by
               additional quantity/financial constraints. The main conclusion
               is that non-convexities should not stop one from applying the
               Duality Theory.",
  journal   = "Journal of Productivity Analysis",
  publisher = "Kluwer Academic Publishers",
  volume    =  20,
  number    =  3,
  pages     = "273--304",
  month     =  nov,
  year      =  2003,
  language  = "en"
}

@UNPUBLISHED{Chetty2012-fs,
  title       = "Active vs. Passive Decisions and Crowdout in Retirement
                 Savings Accounts: Evidence from Denmark",
  author      = "Chetty, Raj and Friedman, John N and Leth-Petersen, Soren and
                 Nielsen, Torben and Olsen, Tore",
  abstract    = "Using 41 million observations on savings for the population of
                 Denmark, we show that the impacts of retirement savings
                 policies on wealth accumulation depend on whether they change
                 savings rates by active or passive choice. Subsidies for
                 retirement accounts, which rely upon individuals to take an
                 action to raise savings, primarily induce individuals to shift
                 assets from taxable accounts to retirement accounts. We
                 estimate that each \$1 of government expenditure on subsidies
                 increases total saving by only 1 cent. In contrast, policies
                 that raise retirement contributions if individuals take no
                 action - such as automatic employer contributions to
                 retirement accounts - increase wealth accumulation
                 substantially. We estimate that approximately 15\% of
                 individuals are ``active savers'' who respond to tax subsidies
                 primarily by shifting assets across accounts. 85\% of
                 individuals are ``passive savers'' who are unresponsive to
                 subsidies but are instead heavily influenced by automatic
                 contributions made on their behalf. Active savers tend to be
                 wealthier and more financially sophisticated. We conclude that
                 automatic contributions are more effective at increasing
                 savings rates than subsidies for three reasons: (1) subsidies
                 induce relatively few individuals to respond, (2) they
                 generate substantial crowd-out conditional on response, and
                 (3) they do not increase the savings of passive individuals,
                 who are least prepared for retirement.",
  number      =  18565,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  nov,
  year        =  2012
}

@ARTICLE{Bollen2011-uf,
  title    = "Twitter mood predicts the stock market",
  author   = "Bollen, Johan and Mao, Huina and Zeng, Xiaojun",
  abstract = "Behavioral economics tells us that emotions can profoundly affect
              individual behavior and decision-making. Does this also apply to
              societies at large, i.e. can societies experience mood states
              that affect their collective decision making? By extension is the
              public mood correlated or even predictive of economic indicators?
              Here we investigate whether measurements of collective mood
              states derived from large-scale Twitter feeds are correlated to
              the value of the Dow Jones Industrial Average (DJIA) over time.
              We analyze the text content of daily Twitter feeds by two mood
              tracking tools, namely OpinionFinder that measures positive vs.
              negative mood and Google-Profile of Mood States (GPOMS) that
              measures mood in terms of 6 dimensions (Calm, Alert, Sure, Vital,
              Kind, and Happy). We cross-validate the resulting mood time
              series by comparing their ability to detect the public's response
              to the presidential election and Thanksgiving day in 2008. A
              Granger causality analysis and a Self-Organizing Fuzzy Neural
              Network are then used to investigate the hypothesis that public
              mood states, as measured by the OpinionFinder and GPOMS mood time
              series, are predictive of changes in DJIA closing values. Our
              results indicate that the accuracy of DJIA predictions can be
              significantly improved by the inclusion of specific public mood
              dimensions but not others. We find an accuracy of 86.7\% in
              predicting the daily up and down changes in the closing values of
              the DJIA and a reduction of the Mean Average Percentage Error
              (MAPE) by more than 6\%.",
  journal  = "J. Comput. Sci.",
  volume   =  2,
  number   =  1,
  pages    = "1--8",
  month    =  mar,
  year     =  2011,
  keywords = "Collective mood; Sentiment tracking; Social networks; Stock
              market"
}

@ARTICLE{Bessler2015-jr,
  title    = "Time-varying systematic and idiosyncratic risk exposures of {US}
              bank holding companies",
  author   = "Bessler, Wolfgang and Kurmann, Philipp and Nohel, Tom",
  abstract = "We study the time-varying risk exposures of US bank holding
              companies for the 1986--2012 period by decomposing total bank
              risk into systematic banking-industry risk, systematic
              market-wide risk, and idiosyncratic bank risk. Banking-industry
              risk factors directly relate to the banks' financial
              intermediation functions while market-wide risk factors affect
              all stocks. In contrast, idiosyncratic risk is driven by
              individual bank risk characteristics. We analyze the systematic
              bank risk factors in time series regressions, determine their
              importance with a democratic orthogonalization technique, and
              explore the idiosyncratic risk in a panel regression framework.
              Our results suggest that corporate credit risk and real estate
              risk are most detrimental in crisis periods, while banks'
              interest rate risk sensitivity has changed over the last decade.
              The banks' equity ratio, loan-loss provisions, fraction of real
              estate loans, and proportion of non-interest income relate to the
              differences in individual bank risk. Moreover, banks'
              idiosyncratic risk contains a strong state-level business cycle
              component. Our results are robust to alternative risk factor
              specifications. Overall, our study contributes to understanding
              the structure and time-variation of banks' systematic and
              idiosyncratic risks.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  35,
  pages    = "45--68",
  month    =  mar,
  year     =  2015,
  keywords = "Bank risk exposures; Financial crises; G01; G21; Systematic and
              idiosyncratic risk"
}

@ARTICLE{Haas2003-rg,
  title     = "Productive efficiency of English football teams---a data
               envelopment analysis approach",
  author    = "Haas, Dieter J",
  abstract  = "This paper investigates how close to their potential English
               Premier League Clubs play. Using a deterministic Data
               Envelopment Analysis Approach, the productive efficiency of 20
               teams in the 2000/2001 season is measured and weaknesses of
               individual teams are disclosed. The sensitivity of results is
               analyzed with regard to different model specifications and
               variable combinations. Copyright \copyright{} 2003 John Wiley \&
               Sons, Ltd.",
  journal   = "Manage. Decis. Econ.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  24,
  number    =  5,
  pages     = "403--410",
  month     =  aug,
  year      =  2003
}

@ARTICLE{Leach2015-du,
  title    = "Making Money Out of Football",
  author   = "Leach, Stephanie and Szymanski, Stefan",
  abstract = "In the US, most economists argue that professional sports teams
              are profit-maximising businesses, but it is a widely held view in
              Europe that professional football clubs are not run on a
              profit-maximising basis. This belief has important implications
              for the impact of widely-advocated policy measures, such as
              revenue sharing. This paper looks at the performance of 16
              English football clubs that acquired a stock exchange listing in
              the mid-1990s. If the European story is true, we should have
              observed a shift toward profit-maximising behaviour at these
              clubs, under the assumption that investors were attracted to
              these football clubs to earn a positive return. This paper finds
              no evidence of any shift in the behaviour of these 16 clubs after
              flotation. This result is consistent with the view that football
              clubs in England have been much more oriented toward profit
              objectives than is normally assumed.",
  journal  = "Scott. J. Polit. Econ.",
  volume   =  62,
  number   =  1,
  pages    = "25--50",
  month    =  feb,
  year     =  2015
}

@ARTICLE{Brunnermeier2015-wc,
  title     = "International credit flows and pecuniary externalities",
  author    = "Brunnermeier, Markus K and Sannikov, Yuliy",
  journal   = "American Economic Journal: Macroeconomics",
  publisher = "American Economic Association",
  volume    =  7,
  number    =  1,
  pages     = "297--338",
  year      =  2015
}

@ARTICLE{Brunnermeier2014-hc,
  title   = "Monetary analysis: price and financial stability",
  author  = "Brunnermeier, Markus K and Sannikov, Yuliy",
  journal = "Navigating Monetary Policy in the New Normal 8",
  pages   = "61",
  year    =  2014
}

@ARTICLE{Brunnermeier2014-tc,
  title   = "International credit flows, pecuniary externalities, and capital
             controls",
  author  = "Brunnermeier, Markus K and Sannikov, Yuliy",
  journal = "Princeton University",
  year    =  2014
}

@ARTICLE{Acharya2009-ve,
  title    = "A theory of systemic risk and design of prudential bank
              regulation",
  author   = "Acharya, Viral V",
  abstract = "Systemic risk is modeled as the endogenously chosen correlation
              of returns on assets held by banks. The limited liability of
              banks and the presence of a negative externality of one bank's
              failure on the health of other banks give rise to a systemic
              risk-shifting incentive where all banks undertake correlated
              investments, thereby increasing economy-wide aggregate risk.
              Regulatory mechanisms such as bank closure policy and capital
              adequacy requirements that are commonly based only on a bank's
              own risk fail to mitigate aggregate risk-shifting incentives, and
              can, in fact, accentuate systemic risk. Prudential regulation is
              shown to operate at a collective level, regulating each bank as a
              function of both its joint (correlated) risk with other banks as
              well as its individual (bank-specific) risk.",
  journal  = "Journal of Financial Stability",
  volume   =  5,
  number   =  3,
  pages    = "224--255",
  month    =  sep,
  year     =  2009,
  keywords = "Systemic risk; Crisis; Risk-shifting; Capital adequacy; Bank
              regulation"
}

@ARTICLE{Simar2011-yn,
  title     = "Two-stage {DEA}: caveat emptor",
  author    = "Simar, L{\'e}opold and Wilson, Paul W",
  abstract  = "This paper examines the wide-spread practice where data
               envelopment analysis (DEA) efficiency estimates are regressed on
               some environmental variables in a second-stage analysis. In the
               literature, only two statistical models have been proposed in
               which second-stage regressions are well-defined and meaningful.
               In the model considered by Simar and Wilson (J Prod Anal
               13:49--78, 2007), truncated regression provides consistent
               estimation in the second stage, where as in the model proposed
               by Banker and Natarajan (Oper Res 56: 48--58, 2008a), ordinary
               least squares (OLS) provides consistent estimation. This paper
               examines, compares, and contrasts the very different assumptions
               underlying these two models, and makes clear that second-stage
               OLS estimation is consistent only under very peculiar and
               unusual assumptions on the data-generating process that limit
               its applicability. In addition, we show that in either case,
               bootstrap methods provide the only feasible means for inference
               in the second stage. We also comment on ad hoc specifications of
               second-stage regression equations that ignore the part of the
               data-generating process that yields data used to obtain the
               initial DEA estimates.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  36,
  number    =  2,
  pages     = "205",
  month     =  oct,
  year      =  2011,
  language  = "en"
}

@ARTICLE{Donaldson2017-yc,
  title    = "Warehouse Banking",
  author   = "Donaldson, Jason Roderick and Piacentino, Giorgia and Thakor,
              Anjan V",
  abstract = "We develop a theory of banking that explains why banks started
              out as commodities warehouses. We show that warehouses become
              banks because their superior storag",
  month    =  may,
  year     =  2017,
  keywords = "warehousing, liquidity creation, bank capital, bank existence"
}

@ARTICLE{Galariotis2017-ua,
  title     = "A combined methodology for the concurrent evaluation of the
               business, financial and sports performance of football clubs:
               the case of France",
  author    = "Galariotis, Emilios and Germain, Christophe and Zopounidis,
               Constantin",
  abstract  = "The recent transformation of football clubs to businesses and
               the challenges posed by this transformation motivate us to study
               the financial, business, and sports performance of French
               football clubs. We propose a two-stage method that can be
               applied to other settings, especially when there exist sample
               size and theoretical/model specification issues: first,
               Multicriteria Analysis is used to rank clubs on their financial
               and business performance dimensions; second, these rankings and
               the league standing (capturing sports performance) are used to
               assess the interrelationships of the different dimensions by
               means of a Partial Least Squares Structural Equation Modeling
               Approach. We find an amphidromous positive relationship between
               business performance and sports performance, and a one-way
               inverse relationship where financial performance affects sports
               performance. Put simply, more revenues affect sports
               achievements positively and these in turn impact positively on
               revenues in a virtuous cycle. The higher revenues do not aid
               financial performance given a race for success that can be
               possibly augmented by stakeholder myopia: the inherent to the
               sport pursuit of short term objectives to the detriment of long
               term sustainability. Consequently, the role of regulators
               (national authorities, UEFA Financial Fair Play) as custodians,
               is ever more important in protecting clubs from financial
               distress.",
  journal   = "Ann. Oper. Res.",
  publisher = "Springer US",
  pages     = "1--24",
  month     =  aug,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Kalemli-Ozcan2013-zb,
  title     = "Financial regulation, financial globalization, and the
               synchronization of economic activity",
  author    = "Kalemli-Ozcan, Sebnem and Papaioannou, Elias and Peydr{\'o},
               Jos{\'e}-Luis",
  abstract  = "ABSTRACT We analyze the impact of financial globalization on
               business cycle synchronization using a proprietary database on
               banks' international exposure for industrialized countries
               during 1978 to 2006. Theory makes ambiguous predictions and",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  68,
  number    =  3,
  pages     = "1179--1228",
  year      =  2013
}

@ARTICLE{Benmelech2010-cl,
  title     = "The Political Economy of Financial Regulation: Evidence from
               {U.S}. State Usury Laws in the 19th Century",
  author    = "Benmelech, Efraim and Moskowitz, Tobias J",
  abstract  = "Financial regulation was as hotly debated a political issue in
               the 19th century as it is today. We study the political economy
               of state usury laws in 19th century America. Exploiting the wide
               variation in regulation, enforcement, and economic conditions
               across states and time, we find that usury laws when binding
               reduce credit and economic activity, especially for smaller
               firms. We examine the motives of regulation and find that usury
               laws coincide with other economic and political policies
               favoring wealthy political incumbents, particularly when they
               have more voting power. The evidence suggests financial
               regulation is driven by private interests capturing rents from
               others rather than public interests protecting the underserved.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  65,
  number    =  3,
  pages     = "1029--1073",
  month     =  jun,
  year      =  2010
}

@ARTICLE{Merton1995-ak,
  title     = "Financial innovation and the management and regulation of
               financial institutions",
  author    = "Merton, Robert C",
  abstract  = "New security designs, improvements in computer and
               telecommunications technology and advances in the theory of
               finance have led to revolutionary changes in the structure of
               financial markets and institutions. This paper provides a
               functional perspective on the dynamics of institutional change
               and uses a series of examples to illustrate the breadth and
               depth of institutional change that is likely to occur. These
               examples emphasize the role of hedging versus equity capital in
               managing risk, the need for risk accounting and changes in
               methods for implementing both regulatory and stabilization
               public policy.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  19,
  number    =  3,
  pages     = "461--481",
  month     =  jun,
  year      =  1995,
  keywords  = "Financial innovation; Banking; Finance"
}

@ARTICLE{Kahane1977-xm,
  title     = "Capital adequacy and the regulation of financial intermediaries",
  author    = "Kahane, Yehuda",
  abstract  = "Financial intermediaries are subject to various forms of
               regulation , intended to ensure their solvency. Commonly,
               regulators seek to achieve this goal by imposing an upper bound
               on the intermediary's leverage (eg through the use of a minimum
               capital requirement). Another",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  1,
  number    =  2,
  pages     = "207--218",
  month     =  oct,
  year      =  1977
}

@ARTICLE{Tomes2013-rx,
  title     = "The law of unintended (financial) consequences: the expansion of
               {HIPAA} business associate liability",
  author    = "Tomes, Jonathan P",
  abstract  = "The recent Omnibus Rule published by the Department of Health
               and Human Services greatly expanded liability for breaches of
               health information privacy and security under the HIPAA statute
               and regulations. This expansion could have dire financial
               consequences for the health care industry. The Rule expanded the
               definition of business associates to include subcontractors of
               business associates and made covered entities and business
               associates liable for breaches of the entities who perform a
               service for them involving the use of individually identifiable
               health information under the federal common law of agency. Thus,
               if a covered entity or its ``do wnstream'' business associate
               breaches security or privacy, the covered entity or ``upstream''
               business associate may face HIPAA's civil money penalties or a
               lawsuit. Financial managers need to be aware of these changes
               both to protect against the greater liability and to plan for
               the compliance costs inherent in effectively, if not legally,
               making business associates into covered entities.",
  journal   = "J. Health Care Finance",
  publisher = "europepmc.org",
  volume    =  39,
  number    =  4,
  pages     = "28--35",
  year      =  2013,
  language  = "en"
}

@ARTICLE{Todd_Jewell2017-me,
  title     = "Technical efficiency with multi-output, heterogeneous
               production: a latent class, distance function model of english
               football",
  author    = "Todd Jewell, R",
  abstract  = "This study combines the output distance function approach with a
               latent class model to estimate technical efficiency in English
               football in the presence of productive heterogeneity within a
               stochastic frontier analysis framework. The distance function
               approach allows the researcher to estimate technical efficiency
               including both on-field and off-field production, which is
               important in the case of English football where clubs are
               generally thought to maximize something other than profit.
               On-field production is measured using total league points, and
               off-field production is measured using total revenue. The data
               set consists of 2177 club-level observations on 88 clubs that
               competed in the four divisions of professional football in
               England over the 29-season period from 1981/82 to 2009/10. The
               results show evidence of three separate productivity classes in
               English football. As might be expected, technical efficiency
               estimated using the latent class model is, on average, higher
               than technical efficiency using an alternative method which
               confines heterogeneity to the intercept coefficient.
               Specifically, average efficiency for the sample is 87.3 and
               93.2\% for the random-intercept model and the latent class model
               respectively.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  48,
  number    =  1,
  pages     = "37--50",
  month     =  aug,
  year      =  2017,
  language  = "en"
}

@BOOK{Kahn1988-xa,
  title     = "The Economics of Regulation: Principles and Institutions",
  author    = "Kahn, Alfred Edward",
  abstract  = "As Chairman of the Civil Aeronautics Board in the late 1970s,
               Alfred E. Kahn presided over the deregulation of the airlines
               and his book, published earlier in that decade, presented the
               first comprehensive integration of the economic theory and
               institutional practice of economic regulation. In his lengthy
               new introduction to this edition Kahn surveys and analyzes the
               deregulation revolution that has not only swept the airlines but
               has transformed American public utilities and private industries
               generally over the past seventeen years.While attitudes toward
               regulation have changed several times in the intervening years
               and government regulation has waxed and waned, the question of
               whether to regulate more or to regulate less is a topic of
               constant debate, one that The Economics of Regulation addresses
               incisively. It clearly remains the standard work in the field, a
               starting point and reference tool for anyone working in
               regulation.Kahn points out that while dramatic changes have come
               about in the structurally competitive industries - the airlines,
               trucking, stock exchange brokerage services, railroads, buses,
               cable television, oil and natural gas - the consensus about the
               desirability and necessity for regulated monopoly in public
               utilities has likewise been dissolving, under the burdens of
               inflation, fuel crises, and the traumatic experience with
               nuclear plants. Kahn reviews and assesses the changes in both
               areas: he is particularly frank in his appraisal of the effect
               of deregulation on the airlines.His conclusion today mirrors
               that of his original, seminal work - that different industries
               need different mixes of institutional arrangements that cannot
               be decided on the basis of ideology.Alfred E. Kahn is Robert
               Julius Thorne Professor of Political Economy at Cornell
               University and Special Consultant to National Economic Research
               Associates.",
  publisher = "MIT Press",
  year      =  1988,
  language  = "en"
}

@ARTICLE{Bhattacharya1998-tp,
  title     = "The Economics of Bank Regulation",
  author    = "Bhattacharya, Sudipto and {Arnoud W. A. Boot} and Thakor, Anjan
               V",
  abstract  = "We review the economics of bank regulation as developed in the
               contemporary literature. We begin with an examination of the
               central aspects of modem banking theories in explaining the
               asset transformation function of intermediaries, optimal bank
               liability contracts, coordination problems leading to bank
               failures and their empirical significance, and the regulatory
               interventions suggested by these considerations. In particular,
               we focus on regulations aimed primarily at ameliorating
               deposit-insurance-related moral hazards, such as cash-asset
               reserve requirements, risk-sensitive capital requirements and
               deposit insurance premia, and bank closure policy. Moreover, we
               examine the impact of the competitive environment (bank charter
               value) and industry structure (scope of banks) on these moral
               hazards. We also examine the implications of banking theory for
               alternatives to deposit insurance.",
  journal   = "J. Money Credit Bank.",
  publisher = "[Wiley, Ohio State University Press]",
  volume    =  30,
  number    =  4,
  pages     = "745--770",
  year      =  1998
}

@ARTICLE{Hagg1997-gd,
  title     = "Theories on the Economics of Regulation: A Survey of the
               Literature from a European Perspective",
  author    = "H{\"a}gg, P G{\"o}ran",
  abstract  = "Abstract This article is, on one hand, a survey of the core
               theoretical literature on the economics of public regulation ;
               and, on the other hand, a cohesive discussion about how the
               literature has matured and how recent contributions on political
               and regulatory",
  journal   = "European Journal of Law and Economics",
  publisher = "Springer",
  volume    =  4,
  number    =  4,
  pages     = "337--370",
  year      =  1997
}

@ARTICLE{Posner1975-nz,
  title     = "The Social Costs of Monopoly and Regulation",
  author    = "Posner, Richard A",
  abstract  = "This paper presents a model and some highly tentative empirical
               estimates of the social costs of monopoly and monopoly-inducing
               regulation in the United States. Unlike the previous studies, it
               assumes that competition to obtain a monopoly results in the
               transformation of expected monopoly profits into social costs. A
               major conclusion is that public regulation is probably a larger
               source of social costs than private monopoly. The implications
               of the analysis for several public policy issues, such as
               appropriate policy toward mergers and price discrimination, are
               also discussed.",
  journal   = "J. Polit. Econ.",
  publisher = "journals.uchicago.edu",
  volume    =  83,
  number    =  4,
  pages     = "807--827",
  year      =  1975
}

@BOOK{Posner2014-mz,
  title     = "Economic Analysis of Law",
  author    = "Posner, Richard A",
  abstract  = "Lucid, comprehensive, and definitive in its field, this text
               covers every aspect of economic analysis of the law. Features:
               Two new chapters, one on intellectual property, one on
               international and comparative law, both exploding fields of
               great importance. Earlier editions' questions have been
               converted to answers, making the book more accessible and
               informative. Revised to be clearer and less technical. More
               eclectic, reflecting recent criticisms of ``rational choice''
               theory, in particular the need to supplement it with insights
               from psychology. Greater attention paid to judicial behavior,
               realistically modeled and explained in economic terms.
               Incorporates insights from the veritable explosion of books and
               articles published in the last few years on economic analysis of
               law.",
  publisher = "Wolters Kluwer Law \& Business",
  month     =  jan,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Weisman2016-yi,
  title     = "What Do Regulators Value?",
  author    = "Weisman, Dennis L",
  abstract  = "Abstract: In a pioneering article entitled ``Taxation by
               Regulation,'' Judge Richard Posner challenged the prevailing
               orthodoxy that regulation emulates competition along the lines
               of the Public Interest Theory of regulation. He argued that
               regulation is best",
  journal   = "B. E. J. Econom. Anal. Policy",
  publisher = "degruyter.com",
  volume    =  16,
  number    =  4,
  year      =  2016
}

@ARTICLE{Hanson2017-zo,
  title    = "Safety regulation in professional football: Empirical evidence of
              intended and unintended consequences",
  author   = "Hanson, Andrew and Jolly, Nicholas A and Peterson, Jeremy",
  abstract = "In response to increasing public awareness and negative long-term
              health effects of concussions, the National Football League
              implemented the ``Crown-of-the-Helmet Rule'' (CHR). The CHR
              imposes penalties on players who initiate contact using the top
              of the helmet. This paper examines the intended effect of this
              policy and its potential for unintended consequences. We find
              evidence supporting the intended effect of the policy- a
              reduction in weekly concussion reports among defensive players by
              as much as 32\% (34\% for all head and neck injuries), but also
              evidence of an increase in weekly lower extremity injury reports
              for offensive players by as much as 34\%.",
  journal  = "J. Health Econ.",
  volume   =  53,
  pages    = "87--99",
  month    =  may,
  year     =  2017,
  keywords = "Health outcomes; Safety regulation; Unintended consequences",
  language = "en"
}

@ARTICLE{Carpenter2011-lf,
  title     = "Intended and Unintended Consequences of Youth Bicycle Helmet
               Laws",
  author    = "Carpenter, Christopher S and Stehr, Mark",
  abstract  = "AbstractMore than 20 states have adopted laws requiring youths
               to wear a helmet when riding a bicycle. We confirm previous
               research indicating that these laws reduced fatalities and
               increased helmet use, but we also show that the laws
               significantly reduced youth bicycling. We find this result in
               standard two-way fixed-effects models of parental reports of
               youth bicycling and in triple-difference models of self-reported
               bicycling among high school youths that explicitly account for
               bicycling by youths just above the age threshold of the helmet
               law. Our results highlight important intended and unintended
               consequences of a well-intentioned public policy.",
  journal   = "J. Law Econ.",
  publisher = "[University of Chicago Press, Booth School of Business,
               University of Chicago, University of Chicago Law School]",
  volume    =  54,
  number    =  2,
  pages     = "305--324",
  year      =  2011
}

@ARTICLE{Hahn2008-we,
  title     = "Has Economic Analysis Improved Regulatory Decisions?",
  author    = "Hahn, Robert W and Tetlock, Paul C",
  journal   = "J. Econ. Perspect.",
  publisher = "American Economic Association",
  volume    =  22,
  number    =  1,
  pages     = "67--84",
  year      =  2008
}

@TECHREPORT{noauthor_2012-ru,
  title       = "Cost-benefit analysis of monetary and financial statistics",
  institution = "Bank of England",
  year        =  2012
}

@ARTICLE{Strauss1991-vh,
  title     = "Review Essay: Sunstein, Statutes, and the Common Law:
               Reconciling Markets, the Communal Impulse, and the Mammoth State",
  author    = "Strauss, Peter L",
  journal   = "Mich. Law Rev.",
  publisher = "The Michigan Law Review Association",
  volume    =  89,
  number    =  4,
  pages     = "907--935",
  year      =  1991
}

@ARTICLE{Davidson2017-zc,
  title     = "{Bank-Owned} Life Insurance and Bank Risk",
  author    = "Davidson, T R",
  abstract  = "The use of bank-owned life insurance (BOLI) has more than
               tripled since 2001 and has caught the attention of the Office of
               the Comptroller of the Currency. I find increases in BOLI lead
               to higher levels of liquidity risk, credit risk, and interest
               rate risk. Robustness tests confirm these results and suggest
               over- and underinvestment in BOLI and use of BOLI as a tax
               shelter contribute to risk increases. Results indicate that the
               concerns expressed by regulators are warranted, and suggest
               insurance may not always have the intended effect of reducing
               firm risk because of unintended consequences or misuse.
               \copyright{} 2017 The Eastern Finance Association",
  journal   = "Financial Review",
  publisher = "Wiley-Blackwell",
  volume    =  52,
  number    =  3,
  pages     = "459--498",
  year      =  2017,
  keywords  = "bank risk; bank-owned life insurance; corporate-owned life
               insurance; G21; G22; G28; key employee insurance"
}

@UNPUBLISHED{Bakkar2017-nc,
  title  = "Does banks' systemic importance affect their capital structure and
            balance sheet adjustment processes?",
  author = "Bakkar, Yassine and De Jonghe, Olivier and Tarazi, Amine",
  year   =  2017
}

@ARTICLE{Tanna2017-df,
  title    = "What is the net effect of financial liberalization on bank
              productivity? A decomposition analysis of bank total factor
              productivity growth",
  author   = "Tanna, Sailesh and Luo, Yun and De Vita, Glauco",
  abstract = "We employ a unique framework to quantify the net effect of
              financial liberalization on banks' total factor productivity
              (TFP) growth through a decomposition analysis of two effects: a
              positive direct effect of financial liberalization on bank TFP
              growth; and a negative indirect effect operating through a higher
              propensity to systemic banking crisis. The empirical
              decomposition is based on a sample of 1530 banks operating in 88
              countries over the period 1999--2011. We find that the net effect
              of financial liberalization on bank TFP growth is positive: the
              direct positive effect outweighs the negative one. An important
              policy implication flows from these findings.",
  journal  = "Journal of Financial Stability",
  volume   =  30,
  pages    = "67--78",
  month    =  jun,
  year     =  2017,
  keywords = "Financial liberalization; Banking crisis; Systemic risk; Bank
              productivity; Total factor productivity"
}

@ARTICLE{Rohde2017-dy,
  title     = "The market for football club investors: a review of theory and
               empirical evidence from professional European football",
  author    = "Rohde, Marc and Breuer, Christoph",
  abstract  = "ABSTRACTResearch objectives: The European market for football
               club investors is undergoing a significant transformation, with
               German clubs opening up for strategic investors, French clubs
               being taken over by private majority investors, and English
               top-league clubs experiencing an influx of foreign investors.
               Economic and legal politics have played an important role in the
               deregulation of closed member associations.Research methods:
               This paper aims to summarize the history and market situation of
               the ?Big Five? European leagues, review available theory and
               empirical evidence on incorporations and public and private
               investors, and suggest research gaps that deserve further
               attention. The authors have also constructed a unique database
               covering all owners in the two premium divisions in England,
               France, Germany, and Italy for the period from 2003 to
               2014.Results and findings: The available articles in the growing
               research field of football club investors cover various
               theoretical areas, such as the application of property rights
               theory to European football clubs. In addition, several
               empirical papers analyze the financial and sporting impact of
               domestic and foreign private investors and public listings. All
               these studies highlight the increasing importance of club
               ownership in the rat race of European football.Implications:
               Nevertheless, some research gaps remain to be studied at an
               appropriate depth. First, further empirical studies should
               analyze the impact of incorporations in German football and the
               entry of private majority investors in France. Furthermore,
               future research may address the paradox of de-listings in
               England and additional listings in continental Europe. Finally,
               this article identifies the impact of foreign investors and
               multi-ownership synergies as promising research fields. In this
               respect, the article provides some managerial implications for
               football club owners, managers, and regulators.",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  17,
  number    =  3,
  pages     = "265--289",
  month     =  may,
  year      =  2017
}

@ARTICLE{Caglio2016-jl,
  title     = "Does Fair Play Matter? {UEFA} Regulation and Financial
               Sustainability in the European Football Industry",
  author    = "Caglio, A and D'Andrea, A and Masciandaro, D and Ottaviano, GIP",
  abstract  = "Abstract: In 2009 the Union of European Football Associations
               (UEFA) launched its Financial Fair Play Regulations (FFPR) aimed
               at preventing professional football clubs from overspending in
               the quest of sporting success to the detriment of their long-run
               financial",
  publisher = "papers.ssrn.com",
  year      =  2016
}

@ARTICLE{Madden2012-gq,
  title    = "Fan welfare maximization as a club objective in a professional
              sports league",
  author   = "Madden, Paul",
  abstract = "Motivated by aspects of European soccer club governance (members'
              clubs supporters' trusts), a first formal analysis of fan welfare
              maximization as a club objective in a sports league is provided,
              with comparisons to objectives studied previously (profit and win
              maximization). Positive comparisons focus on team qualities,
              ticket prices, attendances and the impact of capacity crowds;
              empirically observed ticket black markets and inelastic pricing
              are consistent only with fan welfare maximization. Normatively,
              social welfare (aggregate league surplus) is well-served by a
              league of fan welfare maximizers, or sometimes win maximizers,
              but not profit maximizers; leagues should not normally make
              profits.",
  journal  = "Eur. Econ. Rev.",
  volume   =  56,
  number   =  3,
  pages    = "560--578",
  month    =  apr,
  year     =  2012,
  keywords = "Fan welfare maximization; Aggregate surplus; Sports league"
}

@ARTICLE{Madden2012-lw,
  title    = "Supporter Influence on Club Governance in a Sports League; a
              ``Utility Maximization'' Model",
  author   = "Madden, Paul and Robinson, Terry",
  abstract = "The article formalizes a seminal suggestion of Sloane (),
              studying a sports league in which club objectives are
              multi-argument utility functions defined over profits, win
              percentages and fan (=supporter) welfare, thus combining the
              three objectives that have been addressed separately in previous
              models. Particular focus is on the consequences of increasing the
              utility weight on fan welfare, to capture the recent increasing
              supporter involvement in club governance in UK football. Positive
              consequences are unambiguously higher attendances, with more
              nuanced affects on ticket prices and player expenditure. A
              normative consequence is that positive profits for club owners
              indicate social sub-optimality.",
  journal  = "Scott. J. Polit. Econ.",
  volume   =  59,
  number   =  4,
  pages    = "339--360",
  month    =  sep,
  year     =  2012
}

@ARTICLE{Becker1983-wg,
  title     = "A Theory of Competition Among Pressure Groups for Political
               Influence",
  author    = "Becker, Gary S",
  abstract  = "This paper presents a theory of competition among pressure
               groups for political influence. Political equilibrium depends on
               the efficiency of each group in producing pressure, the effect
               of additional pressure on their influence, the number of persons
               in different groups, and the deadweight cost of taxes and
               subsidies. An increase in deadweight costs discourages pressure
               by subsidized groups and encourages pressure by taxpayers. This
               analysis unifies the view that governments correct market
               failures with the view that they favor the politically powerful:
               both are produced by the competition for political favors.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  98,
  number    =  3,
  pages     = "371--400",
  month     =  aug,
  year      =  1983
}

@ARTICLE{Feng2010-jg,
  title    = "Efficiency, technical change, and returns to scale in large {US}
              banks: Panel data evidence from an output distance function
              satisfying theoretical regularity",
  author   = "Feng, Guohua and Serletis, Apostolos",
  abstract = "Abstract: This paper provides parametric estimates of technical
              change, efficiency change, economies of scale, and total factor
              productivity growth for large banks (those with assets in excess
              of \$1 billion) in the United States, over the period from 2000
              to 2005. This is done by estimating an output distance function
              subject to theoretical regularity within a Bayesian framework. We
              find that failure to incorporate theoretical regularity
              conditions results in mismeasured shadow revenue and/or cost
              shares, which in turn leads to perverse conclusions regarding
              productivity growth. Our results from the regularity-constrained
              model show that total factor productivity of the large US banks
              grew at an average rate of 1.98\% over the sample period.
              However, our estimates also show a clear downward trend in the
              growth rate of total factor productivity and our decomposition of
              the primal Divisia total factor productivity growth index into
              its three components -- technical change, efficiency change, and
              economies of scale -- indicates that technical change is the
              driving force behind this decline. [Copyright \&y\& Elsevier]",
  journal  = "Journal of Banking \& Finance",
  volume   =  34,
  number   =  1,
  pages    = "127--138",
  month    =  jan,
  year     =  2010,
  keywords = "INDUSTRIAL efficiency; RETURNS to scale; BANKING industry --
              United States; CAPITAL productivity; SHADOW prices; STOCK price
              indexes; PANEL analysis; REVENUE; UNITED States; C11;
              Productivity decomposition; Translog output distance function"
}

@MISC{Glosten1988-cc,
  title    = "Estimating the components of the bid/ask spread",
  author   = "Glosten, Lawrence R and Harris, Lawrence E",
  abstract = "This paper develops and implements a technique for estimating a
              model of the bid/ask spread. The spread is decomposed into two
              components, one due to asymmetric information and one due to
              inventory costs, specialist monopoly power, and clearing costs.
              The model is estimated using NYSE common stock transaction prices
              in the period 1981-1983. Cross-sectionaI regrzsion analysis is
              then used to relate time-series estimated spread components to
              other stock \_\%xzacteris= tics. The rest\&s cannot r\&ct the
              hypothesis that si8nGant amoiij of NYSE common stock spreads are
              due to asymmetric information.",
  journal  = "J. financ. econ.",
  volume   =  21,
  pages    = "123--142",
  year     =  1988
}

@ARTICLE{Hirakata2017-iv,
  title   = "Empirical Evidence on ``Systemic as a Herd'': The Case of Japanese
             Regional Banks",
  author  = "Hirakata, Naohisa and Kido, Yosuke and Thum, Jie Liang",
  journal = "Bank of Japan Working Paper Series",
  year    =  2017
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Billio_undated-ki,
  title  = "Measuring Systemic Risk in the Finance and Insurance Sectors‚àó",
  author = "Billio, Monica and Getmansky, Mila and Lo, Andrew W and Pelizzon,
            Loriana"
}

@TECHREPORT{Den_Hertog2010-lc,
  title       = "Review of Economic Theories of Regulation",
  author      = "den Hertog, Johan",
  number      = "Discussion Paper Series 10-18",
  institution = "Tjalling C Koopmans Research Institute",
  month       =  dec,
  year        =  2010
}

@ARTICLE{Admati2010-uu,
  title   = "Healthy banking system is the goal, not profitable banks",
  author  = "Admati, Anat and Allen, Franklin and Brealey, Richard and Brennan,
             Michael and Brunnermeier, Markus K and Boot, Arnoud and Cochrane,
             John H and DeMarzo, Peter M and Fama, Eugene F and Fishman,
             Michael and {Others}",
  journal = "Financ. Times",
  volume  =  9,
  year    =  2010
}

@ARTICLE{Posner1974-ae,
  title   = "Theories of Economic Regulation",
  author  = "Posner, Richard A",
  journal = "The Bell Journal of Economics and Management Science",
  volume  =  5,
  number  =  2,
  pages   = "335",
  year    =  1974
}

@BOOK{Patterson2012-lt,
  title     = "Dark Pools: The rise of {A.I}. trading machines and the looming
               threat to Wall Street",
  author    = "Patterson, Scott",
  abstract  = "Dark Pools is the pacy, revealing, and profoundly chilling tale
               of how global markets have been hijacked by trading robots --
               many so self-directed that humans can't predict what they'll do
               next.It's the story of the blisteringly intelligent computer
               programmers behind the rise of these `bots'. And it's a timely
               warning that as artificial intelligence gradually takes over, we
               could be on the verge of global meltdown.`Scott Patterson has
               the ability to see things you and I don't notice.' Nassim
               Nicholas Taleb, New York Times bestselling author of
               Antifragile, Fooled by Randomness and The Black Swan",
  publisher = "Random House",
  month     =  jul,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Joskow1989-ta,
  title     = "The effects of economic regulation",
  author    = "Joskow, Paul L and Rose, Nancy L",
  abstract  = "... The ICC, the IBT, and the cartelization of the American
               trucking industry . Quarterly Review of Economics and Business,
               13 (1973), pp. 33--47. View Record in Scopus. Arnold, 1970; TR
               Arnold; The Teamsters Union as a determinant of the structure of
               the trucking industry . ...",
  journal   = "Handbook of industrial organization",
  publisher = "Elsevier",
  volume    =  2,
  pages     = "1449--1506",
  year      =  1989
}

@ARTICLE{noauthor_undated-ay,
  title = "[{PDF]Bank} Size, Capital Requirements, and Systemic Risk: Some .."
}

@ARTICLE{Kumar2016-wa,
  title     = "Lifecycle-consistent female labor supply with nonlinear taxes:
               evidence from unobserved effects panel data models with
               censoring, selection and endogeneity",
  author    = "Kumar, Anil",
  abstract  = "This paper uses the Panel Study of Income Dynamics (PSID) from
               1979 to 2007 to estimate within-period lifecycle-consistent
               labor supply elasticities of US females in a two-stage budgeting
               framework. The paper combines a variety of econometric
               approaches to estimate unobserved effects panel data models with
               censoring, selection and endogeneity. The paper finds evidence
               of substantial upward bias in estimated wage elasticities from
               pooled panel models which do not account for unobserved effects,
               as fixed effects and correlated random effects (CRE)
               specifications yield smaller elasticities. Estimates are also
               somewhat sensitive to using a lifecycle-consistent specification
               versus a standard static model. The lifecycle-consistent wage
               elasticity from a CRE model with instrumental variables is 0.56
               on the extensive margin and 0.31 on the intensive margin for an
               overall wage elasticity of 0.87. The standard static model, on
               the other hand, yields a wage elasticity of 0.46 on the
               extensive margin and 0.13 on the intensive margin for an overall
               elasticity of 0.59.",
  journal   = "Rev. Econ. Househ.",
  publisher = "Springer US",
  volume    =  14,
  number    =  1,
  pages     = "207--229",
  month     =  mar,
  year      =  2016,
  language  = "en"
}

@MISC{Spatt2006-da,
  title        = "{SEC} Speech: Financial Regulation: Economic Margins and
                  ``Unintended Consequences''; Washington, {DC}: Mar. 17, 2006
                  (Chester S. Spatt)",
  author       = "Spatt, Chester S",
  year         =  2006,
  howpublished = "\url{https://www.sec.gov/news/speech/spch031706css.htm}",
  note         = "Accessed: 2017-9-26"
}

@BOOK{Merton1976-cc,
  title     = "Sociological Ambivalence and Other Essays",
  author    = "Merton, Robert King",
  publisher = "Simon and Schuster",
  year      =  1976,
  language  = "en"
}

@ARTICLE{Haldane2011-fb,
  title     = "Systemic risk in banking ecosystems",
  author    = "Haldane, Andrew G and May, Robert M",
  abstract  = "In the run-up to the recent financial crisis, an increasingly
               elaborate set of financial instruments emerged, intended to
               optimize returns to individual institutions with seemingly
               minimal risk. Essentially no attention was given to their
               possible effects on the stability of the system as a whole.
               Drawing analogies with the dynamics of ecological food webs and
               with networks within which infectious diseases spread, we
               explore the interplay between complexity and stability in
               deliberately simplified models of financial networks. We suggest
               some policy lessons that can be drawn from such models, with the
               explicit aim of minimizing systemic risk.",
  journal   = "Nature",
  publisher = "search.proquest.com",
  volume    =  469,
  number    =  7330,
  pages     = "351--355",
  month     =  jan,
  year      =  2011,
  language  = "en"
}

@ARTICLE{Kim1988-to,
  title     = "Risk in Banking and Capital Regulation",
  author    = "Kim, Daesik and Santomero, Anthony M",
  abstract  = "This paper investigates the role of bank capital regulation in
               risk control. It is known that banks choose portfolios of higher
               risk because of inefficiently priced deposit insurance. Bank
               capital regulation is a way to redress this bias toward risk.
               Utilizing the mean-variance model, the following results are
               shown: (a) the use of simple capital ratios in regulation is an
               ineffective means to bound the insolvency risk of banks; (b) as
               a solution to problems of the capital ratio regulation, the
               ``theoretically correct'' risk weights under the risk-based
               capital plan are explicitly derived; and (c) the ``theoretically
               correct'' risk weights are restrictions on asset composition,
               which alters the optimal portfolio choice of banking firms.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  43,
  number    =  5,
  pages     = "1219--1233",
  month     =  dec,
  year      =  1988
}

@ARTICLE{Lehar2005-fg,
  title     = "Measuring systemic risk: A risk management approach",
  author    = "Lehar, Alfred",
  abstract  = "Abstract This paper proposes a new method to measure and monitor
               the risk in a banking system. Standard tools that regulators
               require banks to use for their internal risk management are
               applied at the level of the banking system to measure the risk
               of a regulator's portfolio. Using a sample of international
               banks from 1988 until 2002, I estimate the dynamics and
               correlations between bank asset portfolios. To obtain measures
               for the risk of a regulator's portfolio, I model the individual
               liabilities that the regulator has to each bank as contingent
               claims on the bank's assets. The portfolio aspect of the
               regulator's liability is explicitly considered and the
               methodology allows a comparison of sub-samples from different
               countries. Correlations, bank asset volatility, and bank
               capitalization increase for North American and somewhat for
               European banks, while Japanese banks face deteriorating capital
               levels. In the sample period, the North American banking system
               gains stability while the Japanese banking sector becomes more
               fragile. The expected future liability of the regulator varies
               substantially over time and is especially high during the Asian
               crisis starting in 1997. Further analysis shows that the
               Japanese banks contribute most to the volatility of the
               regulator's liability at that time. Larger and more profitable
               banks have lower systemic risk and additional equity capital
               reduces systemic risk only for banks that are constrained by
               regulatory capital requirements.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  29,
  number    =  10,
  pages     = "2577--2603",
  month     =  oct,
  year      =  2005,
  keywords  = "Systemic risk; Financial stability; Risk management"
}

@ARTICLE{Koehn1980-zf,
  title     = "Regulation of Bank Capital and Portfolio Risk",
  author    = "Koehn, Michael and Santomero, Anthony M",
  abstract  = "... an optimal fee schedule can be derived such that the agent,
               ie, bank management , acting so ... It will be assumed that the
               central purpose of bank regulation is to reduce the riskiness of
               ... some arguments to the contrary may be offered, the alleged
               purpose of regulatory control over ...",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  35,
  number    =  5,
  pages     = "1235--1244",
  month     =  dec,
  year      =  1980
}

@BOOK{Bessis2011-bb,
  title     = "Risk Management in Banking",
  author    = "Bessis, Jo{\"e}l",
  abstract  = "Never before has risk management been so important. Now in its
               third edition, this seminal work by Jo{\"e}l Bessis has been
               comprehensively revised and updated to take into account the
               changing face of risk management. Fully restructured, featuring
               new material and discussions on new financial products,
               derivatives, Basel II, credit models based on time intensity
               models, implementing risk systems and intensity models of
               default, it also includes a section on Subprime that discusses
               the crisis mechanisms and makes numerous references throughout
               to the recent stressed financial conditions. The book postulates
               that risk management practices and techniques remain of major
               importance, if implemented in a sound economic way with proper
               governance. Risk Management in Banking, Third Edition considers
               all aspects of risk management emphasizing the need to
               understand conceptual and implementation issues of risk
               management and examining the latest techniques and practical
               issues, including: Asset-Liability Management Risk regulations
               and accounting standards Market risk models Credit risk models
               Dependencies modeling Credit portfolio models Capital Allocation
               Risk-adjusted performance Credit portfolio management Building
               on the considerable success of this classic work, the third
               edition is an indispensable text for MBA students, practitioners
               in banking and financial services, bank regulators and auditors
               alike.",
  publisher = "John Wiley \& Sons",
  month     =  dec,
  year      =  2011,
  language  = "en"
}

@ARTICLE{Mook2014-tg,
  title     = "Credit Unions: Market Niche or Market Accommodation?",
  author    = "Mook, Laurie and Maiorano, John and Quarter, Jack",
  abstract  = "Credit unions, nonprofit mutual associations also called
               financial cooperatives, have a lengthy history. The World
               Council of Credit Unions reports that credit unions are found in
               101 countries representing 56,000 credit unions, more than 200
               million members, and \$1.7 trillion in assets. This study,
               following earlier research in Canada that found that credit
               unions are more prevalent in rural communities and small towns
               relative to the general population and to banks, examines credit
               union and bank branches in three U.S. states (Arizona, New
               Hampshire, and Wisconsin). We find that credit union branches
               are strongly represented in sizable urban communities, and are
               more likely to be located in low-income zip code areas than
               banks. The data show not only evidence of a credit union niche
               market but also a tension between social and economic
               objectives, and that credit unions accommodate themselves to
               profit norms, what we refer to as market accommodation.",
  journal   = "Nonprofit and Voluntary Sector Quarterly",
  publisher = "SAGE Publications Inc",
  volume    =  44,
  number    =  4,
  pages     = "814--831",
  month     =  jun,
  year      =  2014
}

@ARTICLE{Loffler2013-dg,
  title    = "Robustness and Informativeness of Systemic Risk Measures",
  author   = "L{\"o}ffler, Gunter and Raupach, Peter",
  abstract = "Please note that this paper has been replaced by ``Pitfalls in
              the Use of Systemic Risk Measures,'' available via <a
              href='http://ssrn.com/abstract=2593",
  month    =  apr,
  year     =  2013,
  keywords = "Systemic Risk, CoVaR, Marginal Expected Shortfall, Tail Risk"
}

@ARTICLE{Rixon2016-fy,
  title   = "Credit Union Commercial Lending: Mitigating Risk through
             Recording, Monitoring, and Reporting",
  author  = "Rixon, Daphne and Goth, Peter",
  journal = "Canadian Central for Credit Unions Filene Report",
  year    =  2016
}

@ARTICLE{Berger1987-es,
  title     = "Testing a Point Null Hypothesis: The Irreconcilability of {P}
               Values and Evidence",
  author    = "Berger, James O and Sellke, Thomas",
  abstract  = "Abstract The problem of testing a point null hypothesis (or a
               ?small interval? null hypothesis) is considered. Of interest is
               the relationship between the P value (or observed significance
               level) and conditional and Bayesian measures of evidence against
               the null hypothesis. Although one might presume that a small P
               value indicates the presence of strong evidence against the
               null, such is not necessarily the case. Expanding on earlier
               work [especially Edwards, Lindman, and Savage (1963) and Dickey
               (1977)], it is shown that actual evidence against a null (as
               measured, say, by posterior probability or comparative
               likelihood) can differ by an order of magnitude from the P
               value. For instance, data that yield a P value of .05, when
               testing a normal mean, result in a posterior probability of the
               null of at least .30 for any objective prior distribution.
               (?Objective? here means that equal prior weight is given the two
               hypotheses and that the prior is symmetric and nonincreasing
               away from the null; other definitions of ?objective? will be
               seen to yield qualitatively similar results.) The overall
               conclusion is that P values can be highly misleading measures of
               the evidence provided by the data against the null hypothesis.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  82,
  number    =  397,
  pages     = "112--122",
  month     =  mar,
  year      =  1987
}

@ARTICLE{Therneau2016-bq,
  title   = "Mixed Effects Cox Models (the coxme package)",
  author  = "Therneau, Terry",
  journal = "R Vignette",
  year    =  2016
}

@ARTICLE{Peltzman1989-tt,
  title     = "The Economic Theory of Regulation after a Decade of Deregulation",
  author    = "Peltzman, Sam and Levine, Michael E and Noll, Roger G",
  journal   = "Brookings Papers on Economic Activity. Microeconomics",
  publisher = "Brookings Institution Press",
  volume    =  1989,
  pages     = "1--59",
  year      =  1989
}

@ARTICLE{Peltzman1993-so,
  title   = "George Stigler's Contribution to the Economic Analysis of
             Regulation",
  author  = "Peltzman, Sam",
  journal = "J. Polit. Econ.",
  volume  =  101,
  number  =  5,
  pages   = "818--832",
  year    =  1993
}

@ARTICLE{Ross2002-hc,
  title     = "Open competition in league sports",
  author    = "Ross, Stephen F and Szymanski, Stefan",
  journal   = "Wis. L. Rev.",
  publisher = "HeinOnline",
  pages     = "625",
  year      =  2002
}

@ARTICLE{Smith1988-kf,
  title     = "Credit Union Rate and Earnings Retention Decisions under
               Uncertainty and Taxation",
  author    = "Smith, Donald J",
  journal   = "J. Money Credit Bank.",
  publisher = "[Wiley, Ohio State University Press]",
  volume    =  20,
  number    =  1,
  pages     = "119--131",
  year      =  1988
}

@ARTICLE{Davis2001-yv,
  title     = "Credit union governance and survival of the cooperative form",
  author    = "Davis, Kevin",
  journal   = "Journal of financial services research",
  publisher = "Springer",
  volume    =  19,
  number    =  2,
  pages     = "197--210",
  year      =  2001
}

@ARTICLE{Heath1999-jt,
  title   = "Coherent measures of risk",
  author  = "Heath, David and Delbaen, F and Eber, J M and Artzner, P",
  journal = "Math. Finance",
  volume  =  9,
  number  =  3,
  pages   = "203--228",
  year    =  1999
}

@ARTICLE{Kim2016-we,
  title    = "Pricing derivatives with counterparty risk and collateralization:
              A fixed point approach",
  author   = "Kim, Jinbeom and Leung, Tim",
  abstract = "This paper studies a valuation framework for financial contracts
              subject to reference and counterparty default risks with
              collateralization requirement. We propose a fixed point approach
              to analyze the mark-to-market contract value with counterparty
              risk provision, and show that it is a unique bounded and
              continuous fixed point via contraction mapping. This leads us to
              develop an accurate iterative numerical scheme for valuation.
              Specifically, we solve a sequence of linear inhomogeneous PDEs,
              whose solutions converge to the fixed point price function. We
              apply our methodology to compute the bid and ask prices for both
              defaultable equity and fixed-income derivatives, and illustrate
              the non-trivial effects of counterparty risk, collateralization
              ratio and liquidation convention on the bid-ask spreads.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  249,
  number   =  2,
  pages    = "525--539",
  month    =  mar,
  year     =  2016,
  keywords = "Bilateral counterparty risk; Collateralization; Credit valuation
              adjustment; Fixed point method Contraction mapping,"
}

@ARTICLE{Fare1990-qd,
  title     = "A distance function approach to price efficiency",
  author    = "F{\"a}re, R and Grosskopf, S",
  journal   = "J. Public Econ.",
  publisher = "North-Holland",
  year      =  1990
}

@ARTICLE{Puzanova2013-ei,
  title    = "Systemic risk contributions: A credit portfolio approach",
  author   = "Puzanova, Natalia and D{\"u}llmann, Klaus",
  abstract = "Abstract We put forward a framework for measuring systemic risk
              and attributing it to individual banks. Systemic risk is
              coherently measured as the expected loss to depositors and
              investors when a systemic event occurs. The risk contributions
              are calculated so as to ensure a full risk allocation among
              institutions. Applying our methodology to a panel of 54--86 of
              the world's major commercial banks for a 13-year time span with
              monthly frequency not only allows us to closely match the list of
              G-SIBs; we can also use individual risk contributions to compute
              bank-specific surcharges: systemic capital charges as well as
              countercyclical buffers. We therefore address both dimensions of
              systemic risk -- cross-sectional and time-series -- in a single
              integrated approach. As the analysis of risk drivers confirms,
              the main focus of macroprudential supervision should be on a
              solid capital base throughout the financial cycle and
              de-correlation of banks' asset values.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  4,
  pages    = "1243--1257",
  month    =  apr,
  year     =  2013,
  keywords = "Systemic risk; Systemic risk contributions; Systemic capital
              charge; Countercyclical capital buffer; Expected shortfall;
              Importance sampling"
}

@ARTICLE{Austin2017-pz,
  title    = "A Tutorial on Multilevel Survival Analysis: Methods, Models and
              Applications",
  author   = "Austin, Peter C",
  abstract = "Data that have a multilevel structure occur frequently across a
              range of disciplines, including epidemiology, health services
              research, public health, education and sociology. We describe
              three families of regression models for the analysis of
              multilevel survival data. First, Cox proportional hazards models
              with mixed effects incorporate cluster-specific random effects
              that modify the baseline hazard function. Second, piecewise
              exponential survival models partition the duration of follow-up
              into mutually exclusive intervals and fit a model that assumes
              that the hazard function is constant within each interval. This
              is equivalent to a Poisson regression model that incorporates the
              duration of exposure within each interval. By incorporating
              cluster-specific random effects, generalised linear mixed models
              can be used to analyse these data. Third, after partitioning the
              duration of follow-up into mutually exclusive intervals, one can
              use discrete time survival models that use a complementary
              log--log generalised linear model to model the occurrence of the
              outcome of interest within each interval. Random effects can be
              incorporated to account for within-cluster homogeneity in
              outcomes. We illustrate the application of these methods using
              data consisting of patients hospitalised with a heart attack. We
              illustrate the application of these methods using three
              statistical programming languages (R, SAS and Stata).",
  journal  = "Int. Stat. Rev.",
  volume   =  85,
  number   =  2,
  pages    = "185--203",
  month    =  aug,
  year     =  2017,
  keywords = "Multilevel models; hierarchical regression model; survival
              analysis; event history models; Cox proportional hazards model;
              clustered data; health services research; statistical software;
              frailty models"
}

@ARTICLE{Pankratz2005-sr,
  title    = "Random-effects Cox proportional hazards model: general variance
              components methods for time-to-event data",
  author   = "Pankratz, V Shane and de Andrade, Mariza and Therneau, Terry M",
  abstract = "Proportional hazards regression models are commonly used to study
              factors associated with time-to-event data. Because many complex
              genetic diseases exhibit variation in age at onset, it is
              important to have the capability to perform survival analyses on
              data collected from individuals whose observations are correlated
              due to shared genes or environment. While there are widely
              accepted methods for variance components analysis for simple
              quantitative traits, a parallel methodology for survival data has
              not been available. This manuscript outlines a method to perform
              variance component analyses under general random effects
              proportional hazards models. This method is based on a Laplace
              approximation, and makes computation for correlated time-to-event
              data feasible. The correlated frailty models described here can
              be used to perform genetic analyses, and other analyses with
              structured random effects, on age-at-onset data in a manner
              analogous to standard variance components methods for
              quantitative traits. We illustrate the use of the method by
              examining the heritability of breast cancer in a large familial
              cohort study. We also perform variance components linkage
              analyses on data simulated for the Twelfth Genetic Analysis
              Workshop (GAW12), and further examine the performance of this
              method for linkage analysis in a simulation study. The breast
              cancer analyses support significant heritability of disease
              age-at-onset that is of moderate size. The variance component
              linkage analyses successfully identify the location of the
              disease genes that were simulated to have a direct impact on
              age-at-onset. The methods outlined here make it possible to
              perform general variance components analyses on time-to-event
              endpoints, even on large data sets, in a computationally
              efficient manner.",
  journal  = "Genet. Epidemiol.",
  volume   =  28,
  number   =  2,
  pages    = "97--109",
  month    =  feb,
  year     =  2005,
  language = "en"
}

@ARTICLE{Sedunov2016-rw,
  title    = "What is the systemic risk exposure of financial institutions?",
  author   = "Sedunov, John",
  abstract = "Abstract I compare the performance of three measures of
              institution-level systemic risk exposure --- Exposure CoVaR
              (Adrian and Brunnermeier, 2016), systemic expected shortfall
              (Acharya et al., 2016), and Granger causality (Billio et al.,
              2012). I modify Exposure CoVaR to allow for forecasting, and
              estimate the ability of each measure to forecast the performance
              of financial institutions during systemic crisis periods in 1998
              (LTCM) and 2008 (Lehman Brothers). I find that Exposure CoVaR
              forecasts the within-crisis performance of financial
              institutions, and provides useful forecasts of future systemic
              risk exposures. Systemic expected shortfall and Granger causality
              do not forecast the performance of financial institutions
              reliably during crises. I also find, using cross-sectional
              regressions, that foreign equity exposure and securitization
              income determine systemic risk exposure during the 1998 and 2008
              crises, respectively; financial institution size determines
              systemic risk exposure during both crisis periods; and executive
              compensation does not determine systemic risk exposure.",
  journal  = "Journal of Financial Stability",
  volume   =  24,
  number   = "Supplement C",
  pages    = "71--87",
  month    =  jun,
  year     =  2016,
  keywords = "Systemic risk; Banking crises; Financial institutions"
}

@ARTICLE{Pagano2016-te,
  title    = "A comprehensive approach to measuring the relation between
              systemic risk exposure and sovereign debt",
  author   = "Pagano, Michael S and Sedunov, John",
  abstract = "Abstract Using an integrated model to control for simultaneity,
              as well as new risk measurement techniques such as Adapted
              Exposure CoVaR and Marginal Expected Shortfall (MES), we show
              that the aggregate systemic risk exposure of financial
              institutions is positively related to sovereign debt yields in
              European countries in an episodic manner, varying positively with
              the intensity of the financial crisis facing a particular nation.
              We find evidence of a simultaneous relation between systemic risk
              exposure and sovereign debt yields. This suggests that models of
              sovereign debt yields should also include the systemic risk of a
              country's financial system in order to avoid potentially
              important mis-specification errors. We find evidence that
              systemic risk of a country's financial institutions and the risk
              of sovereign governments are inter-related and shocks to these
              domestic linkages are stronger and longer lasting than
              international risk spillovers. Thus, the channel in which
              domestic sovereign debt yields can be affected by another
              nation's sovereign debt is mostly an indirect one in that shocks
              to a foreign country's government finances are transmitted to
              that country's financial system which, in turn, can spill over to
              the domestic financial system and, ultimately, have a
              destabilizing effect on the domestic sovereign debt market.",
  journal  = "Journal of Financial Stability",
  volume   =  23,
  number   = "Supplement C",
  pages    = "62--78",
  month    =  apr,
  year     =  2016,
  keywords = "Systemic risk; Banking crises; Sovereign debt; Contagion;
              Financial institutions"
}

@ARTICLE{Birkhauser2017-gk,
  title     = "Did {UEFA's} financial fair play harm competition in European
               football leagues?",
  author    = "Birkh{\"a}user, Stephan and Kaserer, Christoph and Urban, Daniel",
  abstract  = "When introducing UEFA's Financial Fair Play (FFP) it was argued
               that as a beneficial side effect competition in European
               football leagues should become more equilibrated and perceived
               as being fairer. Based on a hand-collected dataset on league
               results, player market values as well as investor payments of
               more than 300 European football clubs, we scrutinize the impact
               of FFP on the competitive landscape in major European football
               leagues. By applying a fixed-effect panel regression
               difference-in-differences approach, we find results that are
               consistent with the view that FFP might have further amplified
               the competitive imbalance. This might be caused by the fact that
               FFP raises some barriers against the entrance of new investors.
               Moreover, we present evidence that FFP supports the former
               season's winner in terms of budget shares in the upcoming
               season. Overall, our results support the view that FFP turns
               European football leagues less equilibrated and even tends to
               freeze current hierarchies.",
  journal   = "Rev Manag Sci",
  publisher = "Springer Berlin Heidelberg",
  pages     = "1--33",
  month     =  jul,
  year      =  2017,
  keywords  = "Football, competition, investors, inequality, fair play",
  language  = "en"
}

@ARTICLE{Beck2018-bk,
  title    = "When arm's length is too far: Relationship banking over the
              credit cycle",
  author   = "Beck, Thorsten and Degryse, Hans and De Haas, Ralph and van
              Horen, Neeltje",
  abstract = "Abstract We conduct face-to-face interviews with bank chief
              executive officers to classify 397 banks across 21 countries as
              relationship or transaction lenders. We then use the geographic
              coordinates of these banks' branches and of 14,100 businesses to
              analyze how the lending techniques of banks near firms are
              related to credit constraints at two contrasting points of the
              credit cycle. We find that while relationship lending is not
              associated with credit constraints during a credit boom, it
              alleviates such constraints during a downturn. This positive role
              of relationship lending is stronger for small and opaque firms
              and in regions with a more severe economic downturn. Moreover,
              relationship lending mitigates the impact of a downturn on firm
              growth and does not constitute evergreening of loans.",
  journal  = "J. financ. econ.",
  volume   =  127,
  number   =  1,
  pages    = "174--196",
  month    =  jan,
  year     =  2018,
  keywords = "Relationship banking; Credit constraints; Credit cycle"
}

@ARTICLE{Sweeney1997-zc,
  title     = "The Market Value of Debt, Market versus Book Value of Debt, and
               Returns to Assets",
  author    = "Sweeney, Richard J and Warga, Arthur D and Winters, Drew",
  abstract  = "Empirical research typically relies on book rather than market
               value of debt, though theory is virtually always in terms of
               market values. This paper documents how book value measurements
               of debt distort debtequity ratios and cost of capital
               calculations. We focus on three key issues. First,
               mismeasurement can influence cross-sectional studies of capital
               structure, though the errors introduced may not be important
               because the cross-sectional correlation is very high each month
               between book and market-based measures. Second, mismeasurement
               can influence time-series studies of capital structure; this
               influence can be quite important. Third, mismeasurement can
               importantly influence calculations of cost of capital.",
  journal   = "Financial Management",
  publisher = "[Financial Management Association International, Wiley]",
  volume    =  26,
  number    =  1,
  pages     = "5--21",
  year      =  1997
}

@ARTICLE{Silva2017-fc,
  title    = "An analysis of the literature on systemic financial risk: A
              survey",
  author   = "Silva, Walmir and Kimura, Herbert and Sobreiro, Vinicius Amorim",
  abstract = "Abstract This article presents an analysis of the literature on
              systemic financial risk. To that end, we analyze and classify 266
              articles that were published no later than September 2016 in the
              databases Scopus and Web of Knowledge; these articles were
              identified using the keywords ``systemic risk'', ``financial
              stability'', ``financial'', ``measure'', ``indicator'', and
              ``index''. They were evaluated based on 10 categories, namely,
              type of study, type of approach, object of study, method, spatial
              scope, temporal scope, context, focus, type of data used, and
              results. The analysis and classification of this literature made
              it possible to identify the remaining gaps in the literature on
              systemic risk; this contributes to a future research agenda on
              the topic. Moreover, the most influential articles in this field
              of research and the articles that compose the mainstream research
              on systemic financial risk were identified.",
  journal  = "Journal of Financial Stability",
  volume   =  28,
  number   = "Supplement C",
  pages    = "91--114",
  month    =  feb,
  year     =  2017,
  keywords = "Systemic risk; Financial stability; Bibliometry; Financial;
              Measure"
}

@ARTICLE{Weinke2003-io,
  title   = "Frailty Models",
  author  = "Weinke, Andreas",
  journal = "Max Planck Institute for Demographic Research",
  year    =  2003
}

@ARTICLE{Ferrarini2013-uj,
  title     = "Reforming Securities and Derivatives Trading in the Eu: From
               Emir to {MiFIR}",
  author    = "Ferrarini, Guido and Saguato, Paolo",
  abstract  = "The financial crisis has generated a deep revision of the
               regulation of securities and derivatives markets. In this paper,
               we critically examine the extent to which current reforms, such
               as the European Market Infrastructure Regulation and the
               proposed new Markets in Financial Instruments Directive and
               Regulation, will expand ?public? securities and derivatives
               markets, while correspondingly reducing the scope of ?private?
               markets (which broadly coincide with the ?unregulated?
               over-the-counter markets). We also ask whether these reforms
               will on the whole reduce systemic risks and transaction costs of
               securities and derivatives trading in Europe. For these
               purposes, we formulate conjectures that are partly based on the
               experience of past reforms in the area of equity trading.",
  journal   = "Journal of Corporate Law Studies",
  publisher = "Routledge",
  volume    =  13,
  number    =  2,
  pages     = "319--359",
  month     =  oct,
  year      =  2013
}

@BOOK{Busch2017-un,
  title     = "Regulation of the {EU} Financial Markets: {MiFID} {II} and
               {MiFIR}",
  author    = "Busch, Danny and Ferrarini, Guido",
  abstract  = "This book provides a comprehensive and expert examination of the
               Markets in Financial Instruments Directive II, which comes into
               force in January 2018 and will have a major impact on investment
               firms and financial markets. It offers detailed guidance on
               interpretation of MiFID II, its measure and aims which include:
               to increase transparency; better protect investors; reinforce
               confidence; address unregulated areas; and ensure that
               supervisors are granted adequate powers to fulfil their tasks.
               After a thorough overview of the various innovative features of
               the new legislative framework in comparison with the former
               MiFID, the book's chapters are grouped thematically to cover the
               following areas: general aspects; investment firms and
               investment services; trading; supervision and enforcement; and
               reform perspectives. Offering high-quality analysis of both the
               theoretical and practical aspects of MiFID II, this book is an
               essential guide to this major EU legislation. It brings together
               the expert opinions of leading practitioners and legal and
               economic scholars with access to practice, providing a variety
               of perspectives on the new regime and the likely effect of the
               increased regulation.",
  publisher = "Oxford University Press",
  year      =  2017,
  language  = "en"
}

@BOOK{Haentjens2015-sp,
  title     = "European Banking and Financial Law",
  author    = "Haentjens, Matthias and de Gioia-Carabellese, Pierre",
  abstract  = "In recent decades, the volume of EU legislation on financial law
               has increased exponentially. Banks, insurers, pension funds,
               investment firms and other financial institutions all are
               increasingly subject to European regulatory rules, as are day to
               day financial transactions. Serving as a comprehensive and
               authoritative introduction to European banking and? financial
               law, the book is organized around the three economic themes that
               are central to the financial industry: (i) financial markets;
               (ii) financial institutions; and (iii) financial transactions.
               It covers not only regulatory law, but also commercial law that
               is relevant for the most important financial transactions. It
               also explains the most important international standard
               contracts such as LMA loan contracts and the GMRA repurchase
               agreements. Covering a broad range of aspects of financial law
               from a European perspective, it is essential reading for
               students of financial law and European regulation.",
  publisher = "Routledge",
  month     =  jun,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Zhu2013-xo,
  title     = "Do dark pools harm price discovery?",
  author    = "Zhu, Haoxiang",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  27,
  number    =  3,
  pages     = "747--789",
  year      =  2013
}

@ARTICLE{Prorokowski2015-ip,
  title     = "{MiFID} {II} compliance -- are we ready?",
  author    = "Prorokowski, Lukasz",
  abstract  = "Purpose -- This paper aims to discuss the impact of nascent
               Markets in Financial Instruments Directive (MiFID II)
               initiatives and, thus, to deliver practical insights into MiFID
               II implementation, compliance and cost reduction MiFID II
               constitutes the backbone for the upcoming financial market
               reforms. With the first proposal of MiFID drafted in October
               2011, this regulatory framework has undergone over 2,000
               amendments. As MiFID II currently stands, this Directive
               attempts to address issues exposed by the global financial
               crisis. Design/methodology/approach -- This study, based on
               secondary research and an in-depth analysis of the MiFID II
               framework, investigates structural and technological challenges
               entailed by this Directive. The analysis is broken down into the
               following sections: technological and structural challenges;
               costs of implementation; MiFID II teams; facilitating near
               real-time regulatory reporting; increased transparency
               requirements; and information technology (IT) initiatives for
               MiFID II compliance. Findings -- MiFID II commands significant
               changes in business and operating models. With this in mind, the
               study indicates current technological and structural challenges
               faced by financial institutions and advises on ways of
               mitigating MiFID II risks. Although it is too early to assess
               the costs of implementing MiFID II, this paper suggests ways of
               reducing MiFID II-related costs. The study also advises on
               organising dedicated teams to deal with MiFID II. Furthermore,
               this paper argues that early investments in IT systems and
               processes would allow financial services firms to gain a
               competitive advantage and, hence, scoop up market share or
               launch new, lucrative services -- especially in the area of
               collateralisation and market data processing. Originality/value
               -- This paper shows that the current version of MiFID II still
               requires a great deal of attention from the regulators that need
               to readdress contentious issues revolving around the links
               between MiFID II and other regulatory frameworks such as
               European Market Infrastructure Regulation and Dodd--Frank. This
               study addresses the MiFID II compliance issues by adopting
               European Union and non-European Union banks' and asset managers'
               perspectives and, hence, delivers practical implications for
               risk managers and compliance officers of various financial
               institutions.",
  journal   = "Journal of Financial Regulation and Compliance",
  publisher = "Emerald Group Publishing Limited",
  volume    =  23,
  number    =  2,
  pages     = "196--206",
  year      =  2015
}

@ARTICLE{Plantin-_The_Review_of_Financial_Studies2014-wc,
  title    = "Shadow banking and bank capital regulation",
  author   = "Plantin - The Review of Financial Studies, G and {2014}",
  abstract = "Abstract Banks are subject to capital requirements because their
              privately optimal leverage is higher than the socially optimal
              one. This is in turn because banks fail to internalize all costs
              that their insolvency creates for agents who use their money-like
              liabilities to settle transactions. If banks can bypass capital
              regulation in an opaque shadow banking sector, it may be optimal
              to relax capital requirements so that liquidity dries up in the
              shadow ...",
  journal  = "academic.oup.com",
  year     =  2014
}

@ARTICLE{Loffler2016-fr,
  title    = "Pitfalls in the Use of Systemic Risk Measures",
  author   = "L{\"o}ffler, Gunter and Raupach, Peter",
  abstract = "We examine pitfalls in the use of return-based measures of
              systemic risk contributions (SRCs). For both linear and
              non-linear return frameworks, assuming normal",
  month    =  aug,
  year     =  2016,
  keywords = "Systemic Risk; CoVaR; Marginal Expected Shortfall; Tail Risk"
}

@ARTICLE{Amel2004-kl,
  title    = "Consolidation and efficiency in the financial sector: A review of
              the international evidence",
  author   = "Amel, Dean and Barnes, Colleen and Panetta, Fabio and Salleo,
              Carmelo",
  abstract = "Abstract In response to fundamental changes in regulation and
              technology, the financial industry is undergoing an unprecedented
              wave of consolidation. A growing body of empirical literature
              measures the efficiency gains from mergers and acquisitions;
              however there is little sense of how the results might depend on
              the country, industry and time period analyzed. In this paper we
              review critically works that cover the main sectors of the
              financial industry (commercial and investment banks, insurance
              and asset management companies) in the major industrialized
              countries over the last 20 years, searching for common patterns
              that transcend national and sectoral peculiarities. We find that
              consolidation in the financial sector is beneficial up to a
              relatively small size, but there is little evidence that mergers
              yield economies of scope or gains in managerial efficiency.",
  journal  = "Journal of Banking \& Finance",
  volume   =  28,
  number   =  10,
  pages    = "2493--2519",
  month    =  oct,
  year     =  2004,
  keywords = "Mergers; Efficiency; Bank mergers"
}

@ARTICLE{Berger2001-dx,
  title     = "Efficiency Barriers to the Consolidation of the European
               Financial Services Industry",
  author    = "Berger, Allen N and De Young, Robert and Udell, Gregory F",
  abstract  = "Cross-border consolidation of financial institutions within
               Europe has been relatively limited, possibly reflecting
               efficiency barriers to operating across borders, including
               distance; differences in language, culture, currency, and
               regulatory/supervisory structures; and explicit or implicit
               rules against foreign competitors. EU policies such as the
               Single Market Programme and European Monetary Union attenuate
               some but not all of these barriers. The evidence is consistent
               with the hypothesis that these barriers offset most of any
               potential efficiency gains from cross-border consolidation.
               Banks headquartered in other EU nations have slightly lower
               average measured efficiency than domestic banks and non-EU-based
               foreign banks.",
  journal   = "European Financial Management",
  publisher = "Blackwell Publishers Ltd",
  volume    =  7,
  number    =  1,
  pages     = "117--130",
  month     =  mar,
  year      =  2001,
  keywords  = "banks; mergers; efficiency; Europe; financial institutions"
}

@ARTICLE{Berger2003-eb,
  title    = "The efficiency effects of a single market for financial services
              in Europe",
  author   = "Berger, Allen N",
  abstract = "Abstract This paper examines the potential efficiency effects of
              a single market for financial services in Europe. The topics
              covered include universal banking, the merger and acquisition
              process itself, cross-border ownership and management of
              financial institutions, and the effects of consolidation of
              financial institutions on the supply of relationship lending
              services to informationally opaque small businesses. The research
              reviewed here suggests that the creation of a single market for
              the European financial services industry is not likely to bring
              about strong efficiency gains and that cross-border efficiency
              barriers may prevent the single market from becoming a reality.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  150,
  number   =  3,
  pages    = "466--481",
  month    =  nov,
  year     =  2003,
  keywords = "Banks; Securities firms; Insurance; Mergers; Efficiency;
              International finance"
}

@ARTICLE{Frame2001-jk,
  title     = "{US} financial services consolidation: The case of corporate
               credit unions",
  author    = "Frame, W Scott and Coelli, Tim J",
  journal   = "Review of Industrial Organization",
  publisher = "Springer",
  volume    =  18,
  number    =  2,
  pages     = "229--241",
  year      =  2001
}

@ARTICLE{Gleason2003-nm,
  title     = "Evidence on Value Creation in the Financial Services Industries
               through the Use of Joint Ventures and Strategic Alliances",
  author    = "Gleason, Kimberly C and Mathur, Ike and {Wiggins} and {III} and
               Roy, A",
  abstract  = "While an extensive body of literature has examined merger,
               acquisition, and consolidation activity in commercial banks and
               other financial services firms, little attention has been paid
               to examining how these institutions use the cooperative
               activities of joint ventures and strategic alliances to
               accomplish their growth objectives. We analyze the effects of
               the use of joint ventures and strategic alliances by a sample of
               firms in the banking, investment services, and insurance
               industries. Our results show that commercial banks, investment
               services firms, and insurance companies experience significant
               abnormal returns of 0.66\% on average when they announce their
               participation in a joint venture or strategic alliance. These
               abnormal returns are significantly positive across the four
               strategic motives of domestic, international, horizontal, and
               diversifying cooperative activities. Using a matched sample, we
               also show that our sample firms enjoy significant, positive,
               abnormal returns for holding periods of six, 12, and 18 months
               after the announcement of the cooperative activity.",
  journal   = "Financial Review",
  publisher = "Blackwell Publishing Inc",
  volume    =  38,
  number    =  2,
  pages     = "213--234",
  month     =  may,
  year      =  2003,
  keywords  = "joint ventures; strategic alliances; long-horizon performance;
               G21/G29/G14"
}

@ARTICLE{Ireland2017-ba,
  title   = "{EXCHANGE} {TRADED} {FUNDS}",
  author  = "Ireland, Bank of",
  journal = "Bank of Ireland Discussion paper",
  year    =  2017
}

@ARTICLE{Leung2015-bv,
  title     = "Implied Volatility of Leveraged {ETF} Options",
  author    = "Leung, Tim and Sircar, Ronnie",
  abstract  = "AbstractThis paper studies the problem of understanding implied
               volatilities from options written on leveraged exchanged-traded
               funds (LETFs), with an emphasis on the relations between LETF
               options with different leverage ratios. We first examine from
               empirical data the implied volatility skews for LETF options
               based on the S\&P 500. In order to enhance their comparison with
               non-leveraged ETFs, we introduce the concept of moneyness
               scaling and provide a new formula that links option implied
               volatilities between leveraged and unleveraged ETFs. Under a
               multiscale stochastic volatility framework, we apply asymptotic
               techniques to derive an approximation for both the LETF option
               price and implied volatility. The approximation formula reflects
               the role of the leverage ratio, and thus allows us to link
               implied volatilities of options on an ETF and its leveraged
               counterparts. We apply our result to quantify matches and
               mismatches in the level and slope of the implied volatility
               skews for various LETF options using data from the underlying
               ETF option prices. This reveals some apparent biases in the
               leverage implied by the market prices of different products,
               long and short with leverage ratios two times and three times.",
  journal   = "Appl. Math. Finance",
  publisher = "Routledge",
  volume    =  22,
  number    =  2,
  pages     = "162--188",
  month     =  mar,
  year      =  2015
}

@ARTICLE{Halme2002-ut,
  title    = "Dealing with interval scale data in data envelopment analysis",
  author   = "Halme, Merja and Joro, Tarja and Koivu, Matti",
  abstract = "Abstract This paper considers the problem of interval scale data
              in the most widely used models of data envelopment analysis
              (DEA), the CCR and BCC models. Radial models require inputs and
              outputs measured on the ratio scale. Our focus is on how to deal
              with interval scale variables especially when the interval scale
              variable is a difference of two ratio scale variables like profit
              or the decrease/increase in bank accounts. We suggest the use of
              these ratio scale variables in a radial DEA model.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  137,
  number   =  1,
  pages    = "22--27",
  month    =  feb,
  year     =  2002,
  keywords = "Efficiency analysis; Data envelopment analysis; Interval scale
              variables; Negative variables"
}

@ARTICLE{Wong2014-mx,
  title    = "A {STOCHASTIC} {SETTING} {TO} {BANK} {FINANCIAL} {PERFORMANCE}
              {FOR} {REFINING} {EFFICIENCY} {ESTIMATES}",
  author   = "Wong, Wai-Peng and Deng, Qiang and Tseng, Ming-Lang and Lee,
              Loo-Hay and Hooy, Chee-Wooi",
  abstract = "This study contributes to develop a framework to measure the
              financial performance of banks in a stochastic setting. The
              framework comprises several steps, the first of which is the
              development of a financial performance measurement model to
              evaluate a bank's financial performance using a set of factors
              from the CAMEL (Capital adequacy, Assets, Management Capability,
              Earning and Liquidity) system. Second, the stochastic setting of
              the efficiency measurement is handled using the data collection
              budget allocation approach, whereby Monte Carlo simulations are
              used to analyse additional generated data and a genetic algorithm
              is used to refine the accuracy of the efficiency estimates. The
              results show that the accuracy of the model is greatly improved
              using the proposed approach. In contrast to the conventional
              deterministic model, the proposed framework is more useful to
              managers in determining the bank's future financial operations to
              improve the overall financial soundness of the bank. Copyright
              \copyright{} 2014 John Wiley \& Sons, Ltd.",
  journal  = "Intell. Sys. Acc. Fin. Mgmt.",
  volume   =  21,
  number   =  4,
  pages    = "225--245",
  month    =  oct,
  year     =  2014,
  keywords = "Monte Carlo simulation; bank financial performance; genetic
              algorithm (GA)"
}

@ARTICLE{Emrouznejad2017-fz,
  title    = "A survey and analysis of the first 40 years of scholarly
              literature in {DEA}: 1978--2016",
  author   = "Emrouznejad, Ali and Yang, Guo-Liang",
  abstract = "Abstract In recent years there has been an exponential growth in
              the number of publications related to theory and applications of
              Data Envelopment Analysis (DEA). Charnes, Cooper, and Rhodes
              (1978) introduced DEA as a tool for measuring efficiency and
              productivity of decision making units. DEA has immediately been
              recognized as a modern tool for performance measurement. Since
              then, a large and considerable amount of articles has been
              appeared, including significant breakthroughs in theory and a
              great portion of works on DEA applications, both public and
              private sectors, to assess the efficiency and productivity of
              their activities. Although there have been several bibliographic
              collections reported, a comprehensive analysis and listing of
              DEA-related articles covering its first four decades of history
              is still missing. This paper, thus, aims to report an extensive
              listing of DEA-related articles including theory and methodology
              developments and ``real'' applications in diversified scenarios
              from 1978 to end of 2016. Some summary statistics of the
              publications' growth, the most utilized academic journals,
              authorship analysis, as well as keywords analysis are also
              provided.",
  journal  = "Socioecon. Plann. Sci.",
  month    =  jan,
  year     =  2017,
  keywords = "Data envelopment analysis; Efficiency and productivity;
              Bibliography; Survey"
}

@ARTICLE{Mahapatra2015-su,
  title    = "Assessment of Proactive Environmental Initiatives: Evaluation of
              Efficiency Based on {Interval-Scale} Data",
  author   = "Mahapatra, S and Pal, R and Hult, T and Talluri, S",
  abstract = "In recent years, firms have pursued a host of environmental
              operational practices to derive superior environmental and
              business performance. However, the relative effectiveness of
              these practices has not been explored adequately. Past empirical
              studies have mainly assessed the performance impact of various
              practices across firms without accounting for firm-specific
              considerations. Consequently, it is not clear how the costs and
              benefits of various practices compare across firms that may weigh
              the practices differently. We examine this issue by analyzing the
              financial and environmental performance impact of five commonly
              adopted operational practices of 30 firms across a diverse set of
              industries. The operational practices are measured in interval
              scale through content analysis. Subsequently, we apply a data
              envelopment analysis-based technique proposed by Dehnokhalaji et
              al . [22] that can incorporate both ratio and interval-scale
              data. We find that although the emphases on various environmental
              practices vary among firms, mainly two of the five practices
              (i.e., proactive waste reduction and remanufacturing)
              differentiate firms' business and environmental performance. The
              theoretical and managerial implications of the findings are also
              discussed.",
  journal  = "IEEE Trans. Eng. Manage.",
  volume   =  62,
  number   =  2,
  pages    = "280--293",
  month    =  may,
  year     =  2015,
  keywords = "data envelopment analysis;environmental management;data
              envelopment analysis-based technique;environmental
              performance;environmental practices;firm-specific
              considerations;interval-scale data;proactive environmental
              initiatives assessment;Business;Input
              variables;Packaging;Pollution;Pollution
              measurement;Standards;Waste reduction;Content analysis;data
              envelopment analysis (DEA);environmental operations;financial
              performance;interval-scale data"
}

@ARTICLE{White2005-so,
  title     = "Assessing the role of the international financial services
               centre in Irish regional development",
  author    = "White, Mark C",
  abstract  = "Abstract This paper examines the manner and extent to which the
               International Financial Services Centre (IFSC) in Dublin
               contributes to regional development in Ireland. Since its 1987
               launch, the IFSC created over 10,000 jobs and promoted urban
               renewal in a previously derelict section of Dublin. Although it
               stands as one of Ireland's most prominent development projects,
               empirical examination of the IFSC remains limited. This study
               looks specifically at issues such as the kinds of activities and
               employment created at the IFSC, as well as local linkage
               formation. Based on published data and research interviews, this
               paper shows that the IFSC contributed to Ireland's economic
               development at a time when industrial policy focused primarily
               on employment creation. To date the IFSC has proven to be a
               successful policy intervention, the routine nature of many
               IFSC-related activities raises questions about the IFSC's
               ability to become something more than a centre for back-office
               financial services.",
  journal   = "European Planning Studies",
  publisher = "Routledge",
  volume    =  13,
  number    =  3,
  pages     = "387--405",
  month     =  apr,
  year      =  2005
}

@ARTICLE{Keeble1991-jg,
  title     = "Small Firms, Business Services Growth and Regional Development
               in the United Kingdom: Some Empirical Findings",
  author    = "Keeble, David and Bryson, John and Wood, Peter",
  abstract  = "KEEBLE D., BRYSON J. and WOOD P. (1991) Small firms, business
               services growth and regional development in the United Kingdom:
               some empirical findings, Reg. Studies 25, 439?457. Since 1980,
               the United Kingdom has experienced very rapid growth in firms
               and employment in information-intensive business services. This
               paper documents the nature and extent of this growth, with
               particular reference to small and new firms in management
               consultancy and market research, and reviews its locational
               impact. Numbers of business service firms, most of which are
               small independent companies, have grown much faster than
               consumer service firms. Despite a significant death rate, many
               small businesses in these sectors have grown rapidly, as have
               very large firms. Business service growth has contributed
               powerfully to the UK's north?south divide, being focused on
               London and the outer South East. Possible causes of these trends
               and key future research issues are reviewed. KEEBLE D., BRYSON
               J. et WOOD P. (1991) Petites entreprises, croissance des
               services aux entreprises et am{\'e}nagement du territoire au
               Royaume-Uni: quelques r{\'e}sultats empiriques, Reg. Studies 25,
               439?457. Depuis 1980 le Royaume-Uni {\`a} joui d'une croissance
               vertigineuse du parc d'entreprises et du nombre d'emplois dans
               les services d'information aux entreprises. Cet article fait un
               compterendu du caract{\`e}re et de la port{\'e}e de cette
               croissance se r{\'e}f{\'e}rant en particulier aux petites et
               nouvelles entreprises dans les domaines du conseil en gestion et
               de l'{\'e}tude de march{\'e}, et consid{\`e}re son impact
               g{\'e}ographique. De nombreuses firmes offrant des services aux
               entreprises et dont la plupart sont de petites entreprises
               individuelles, se sont d{\'e}velopp{\'e}es plus vite que les
               firmes offrant des services aux particuliers. En d{\'e}pit d'une
               mortalit{\'e} sensible, beaucoup des firmes dans ces secteurs se
               sont d{\'e}velopp{\'e}es rapidement, comme les tr{\`e}s grandes
               entreprises. La croissance des services aux entreprises a
               contribu{\'e} fortement {\`a} la scission Nord-Sud au R-U, se
               concentrant sur Londres et le Sud-Est. On fait la critique des
               causes {\'e}ventuelles de ces tendances et consid{\`e}re des
               questions cl{\'e} {\`a} rechercher dans l'avenir. KEEBLE D.,
               BRYSON J. und WOOD P. (1991) Kleinfirmen, Zunahme
               gesch{\"a}ftlicher Dienstleistungen und regionale Entwicklung im
               Vereinigten K{\"o}nigreich: empirische Befunde, Reg. Studies 25,
               439?457. Im Vereinigten K{\"o}nigreich hat seit 1980 eine rasche
               Zunahme von Firmen und Erwerbst{\"a}tigkeiten in
               informationsintensiven Gesch{\"a}ftsdienstleistungen
               stattgefunden. Dieser Aufsatz dokumentiert die Natur und das
               Ausmass dieser Zunahme, besonders im Hinblick auf kleine und neu
               er{\"o}ffnete Firmen in der Gesch{\"a}ftsf{\"u}hrungsberatung
               und Marktforschung, und bespricht ihre Standortwirkung. Die
               Anzahl der Firmen, die Gesch{\"a}ftsdienstleistungen anbieten,
               wobei die meisten kleine, unabh{\"a}ngige Gesellschaften sind,
               hat viel schneller zugenommen als die der
               Verbraucherdienstleistungen. Trotz einer beachtlichen
               Schliessungsrate sind viele kleine Unternehmen dieser
               Sektoren?wie die der sehr grossen?schnell gewachsen. Die Zunahme
               der gesch{\"a}ftlichen Dienstleistungen hat kr{\"a}ftig zum
               Nord-S{\"u}dgef{\"a}lle des Vereinigten K{\"o}nigreichs
               beigetragen. Die Hauptzunahme konzentriert sich auf London und
               den weiteren S{\"u}dosten. M{\"o}gliche Gr{\"u}nde f{\"u}r diese
               Tendenzen und wichtige zuk{\"u}nftige Forschungsfragen werden
               er{\"o}rtert.",
  journal   = "Reg. Stud.",
  publisher = "Routledge",
  volume    =  25,
  number    =  5,
  pages     = "439--457",
  month     =  oct,
  year      =  1991
}

@ARTICLE{Chen2014-ft,
  title    = "Systemic Risk and the Interconnectedness Between Banks and
              Insurers: An Econometric Analysis",
  author   = "Chen, Hua and Cummins, J David and Viswanathan, Krupa S and
              Weiss, Mary A",
  abstract = "This article uses daily market value data on credit default swap
              spreads and intraday stock prices to measure systemic risk in the
              insurance sector. Using the systemic risk measure, we examine the
              interconnectedness between banks and insurers with Granger
              causality tests. Based on linear and nonlinear causality tests,
              we find evidence of significant bidirectional causality between
              insurers and banks. However, after correcting for conditional
              heteroskedasticity, the impact of banks on insurers is stronger
              and of longer duration than the impact of insurers on banks.
              Stress tests confirm that banks create significant systemic risk
              for insurers but not vice versa.",
  journal  = "J. Risk Insur.",
  volume   =  81,
  number   =  3,
  pages    = "623--652",
  month    =  sep,
  year     =  2014
}

@ARTICLE{Kohler2015-wi,
  title    = "Which banks are more risky? The impact of business models on bank
              stability",
  author   = "K{\"o}hler, Matthias",
  abstract = "Abstract In this paper, we analyze the impact of business models
              on bank stability in 15 EU countries between 2002 and 2011. We
              represent banks' business models by the share of non-interest
              income in total operating income and the share of non-deposit
              funding in total liabilities. In contrast to the literature, we
              include in our sample a large number of unlisted banks, which
              represent the majority of banks in the EU. We believe this to be
              important, since many unlisted banks typically have a more
              retail-oriented business model. We show that banks will be
              significantly more stable and profitable if they increase their
              share of non-interest income, indicating that substantial
              benefits are to be gained from income diversification. Such
              benefits are particularly large for savings and cooperative
              banks. Investment banks, in contrast, become significantly more
              risky. Diversifying into non-deposit funding has a different
              impact as well. While retail-oriented banks will be significantly
              less stable if they increase their share of non-deposit funding,
              investment banks will be significantly more stable. These
              findings indicate that it is important to enlarge the sample of
              banks and to include different types of banks with different
              business models in order to arrive at general conclusions about
              the effect of non-interest income and non-deposit funding on bank
              stability.",
  journal  = "Journal of Financial Stability",
  volume   =  16,
  number   = "Supplement C",
  pages    = "195--212",
  month    =  feb,
  year     =  2015,
  keywords = "Banks; Risk-taking; Business model; Non-interest income;
              Non-deposit funding"
}

@BOOK{Coelli2005-qr,
  title     = "An Introduction to Efficiency and Productivity Analysis",
  author    = "Coelli, Timothy J and Rao, Dodla Sai Prasada and O'Donnell,
               Christopher J and Battese, George Edward",
  abstract  = "The second edition of this book has been written for the same
               audience as the first edition. It is designed to be a ``first
               port of call'' for people wishing to study efficiency and
               productivity analysis. The book provides an accessible
               introduction to the four principal methods involved: econometric
               estimation of average response models; index numbers; data
               envelopment analysis (DEA); and stochastic firontier analysis
               (SFA). For each method, we provide a detailed introduction to
               the basic concepts, give some simple numerical examples, discuss
               some of the more important extensions to the basic methods, and
               provide references for further reading. In addition, we provide
               a number of detailed empirical applications using real-world
               data. The book can be used as a textbook or as a reference text.
               As a textbook, it probably contains too much material to cover
               in a single semester, so most instructors will want to design a
               course around a subset of chapters. For example, Chapter 2 is
               devoted to a review of production economics and could probably
               be skipped in a course for graduate economics majors. However,
               it should prove useful to undergraduate students and those doing
               a major in another field, such as business management or health
               studies.",
  publisher = "Springer Science \& Business Media",
  edition   = "2nd",
  month     =  jul,
  year      =  2005,
  address   = "New",
  keywords  = "Battese - O'Donnell - Rao - analysis - efficiency",
  language  = "en"
}

@INCOLLECTION{Barros2014-ud,
  title     = "Management ability, tactics, strategy and team performance",
  booktitle = "Handbook on the Economics of Professional Football",
  author    = "Barros, Carlos Pestana and Couto, Eduardo and Samagaio, Antonio",
  editor    = "Goddard, John and Sloane, Peter",
  publisher = "Edward Elgar Publishing",
  pages     = "166--188",
  year      =  2014
}

@MISC{PricewaterhouseCoopers2016-xm,
  title     = "{IFRS} 9: Impairment--global banking industry benchmark",
  author    = "PricewaterhouseCoopers, U K",
  publisher = "May",
  year      =  2016
}

@ARTICLE{Demyanyk2010-py,
  title    = "Financial crises and bank failures: A review of prediction
              methods",
  author   = "Demyanyk, Yuliya and Hasan, Iftekhar",
  abstract = "Abstract In this article we provide a summary of empirical
              results obtained in several economics and operations research
              papers that attempt to explain, predict, or suggest remedies for
              financial crises or banking defaults; we also outline the
              methodologies used in them. We analyze financial and economic
              circumstances associated with the US subprime mortgage crisis and
              the global financial turmoil that has led to severe crises in
              many countries. The intent of this article is to promote future
              empirical research for preventing bank failures and financial
              crises.",
  journal  = "Omega",
  volume   =  38,
  number   =  5,
  pages    = "315--324",
  month    =  oct,
  year     =  2010,
  keywords = "Subprime; Mortgage; Financial crisis; Default"
}

@ARTICLE{Hamid2017-fk,
  title     = "Efficiency measurement of the banking sector in the presence of
               non-performing loan",
  author    = "Hamid, Nurhayati and Ramli, Noor Asiah and Hussin, Siti Aida
               Sheikh",
  abstract  = "Bank industry plays a vital role in a country?s economic
               development. In the banking industry, the non-performing loans
               which are acknowledged as being undesirable outputs and usually
               ignored in most of the analysis should be taken into account
               since they are undesirable by-products of producing loans and
               may lead to the bank inefficiency. Modelling the efficiency
               measurement without undesirable outputs can provide misleading
               results and unfair assessment. The Directional Distance Function
               (DDF) approach which extended from the Data Envelopment Analysis
               (DEA) framework is one of the enhancement efficiency approaches
               to handle a situation when there is a joint production of the
               desirable and undesirable outputs. The comparison of both
               results between the domestic and foreign banks shows that the
               DEA technical efficiency score for domestic banks is marginally
               higher than the Malaysian foreign banks. However, when
               incorporating the undesirable output, the DDF technical
               efficiency for foreign banks is slightly higher than domestic
               banks.",
  journal   = "AIP Conf. Proc.",
  publisher = "American Institute of Physics",
  volume    =  1795,
  number    =  1,
  pages     = "020001",
  month     =  jan,
  year      =  2017
}

@ARTICLE{Scheel2001-oo,
  title    = "Undesirable outputs in efficiency valuations",
  author   = "Scheel, Holger",
  abstract = "Abstract Efficiency measurement is usually based on the
              assumption that inputs have to be minimized and outputs have to
              be maximized. In a growing number of applications, however,
              undesirable outputs are incorporated into the production model
              which have to be minimized. In this paper various approaches for
              treating such outputs in the framework of Data Envelopment
              Analysis (DEA) are discussed and the resulting efficient
              frontiers are compared. New radial measures are introduced which
              assume that any change of the output level will involve both
              undesirable and desirable outputs.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  132,
  number   =  2,
  pages    = "400--410",
  month    =  jul,
  year     =  2001,
  keywords = "Data envelopment analysis; Radial efficiency measurement;
              Undesirable outputs; Linear programming; Pollution"
}

@BOOK{Fare1994-ie,
  title     = "Production Frontiers",
  author    = "Fare, Rolf and Grosskopf, Shawna and Knox Lovell, C A",
  abstract  = "This book presents a mathematical programming approach to the
               analysis of production frontiers and efficiency measurement. The
               authors construct a variety of production frontiers, and by
               measuring distances to them are able to develop a model of
               efficient producer behaviour and a taxonomy of possible types of
               departure from efficiency in various environments. Linear
               programming is used as an analytical and computational technique
               in order to accomplish this. The approach developed is then
               applied to modelling producer behaviour. By focusing on the
               empirical relevance of production frontiers and distances to
               them, and applying linear programming techniques to artificial
               data to illustrate the type of information they can generate,
               this book provides a unique study in applied production
               analysis. It will be of interest to scholars and students of
               economics and operations research, and analysts in business and
               government.",
  publisher = "Cambridge University Press",
  year      =  1994,
  language  = "en"
}

@ARTICLE{Christensen1973-mj,
  title     = "Transcendental Logarithmic Production Frontiers",
  author    = "Christensen, Laurits R and Jorgenson, Dale W and Lau, Lawrence J",
  journal   = "Rev. Econ. Stat.",
  publisher = "The MIT Press",
  volume    =  55,
  number    =  1,
  pages     = "28--45",
  year      =  1973
}

@ARTICLE{Kumbhakar1990-oa,
  title    = "Production frontiers, panel data, and time-varying technical
              inefficiency",
  author   = "Kumbhakar, Subal C",
  abstract = "Abstract This paper uses a panel-data framework and models
              firm-specific technical inefficiency which is allowed to vary
              over time. The specification is flexible enough to accommodate
              increasing, decreasing, and time-invariant behavior of technical
              inefficiency. Time-varying firm- and input-specific allocative
              inefficiency is also incorporated. The estimation method
              suggested uses a parametric production function and
              cost-minimization hypothesis.",
  journal  = "J. Econom.",
  volume   =  46,
  number   =  1,
  pages    = "201--211",
  month    =  oct,
  year     =  1990
}

@ARTICLE{Lee2014-je,
  title    = "A new approach to measuring shadow price: Reconciling engineering
              and economic perspectives",
  author   = "Lee, Sang-Choon and Oh, Dong-Hyun and Lee, Jeong-Dong",
  abstract = "Shadow price is one of the most important pieces of information
              in environmental decision making. Two different
              approaches---namely, economic and engineering---have been applied
              to obtain the shadow price of undesirable outputs, while using
              different methodological backgrounds and perspectives. The
              current study proposes a new conceptual framework and an economic
              estimation model to reconcile the shadow price estimates derived
              via the two approaches. We also suggest a new mapping rule that
              incorporates the concept of abatement level, which is a basic
              element in the engineering approach. As a result, the proposed
              model generates continuously changing estimates---i.e.,
              comprising a shadow price curve---based on the abatement level.
              We further investigate the determinant factors of shadow price by
              using second-step regression. The suggested methodology is used
              to investigate the shadow price of carbon emissions in South
              Korean electricity generating plants, thus yielding relevant
              policy implications.",
  journal  = "Energy Econ.",
  volume   =  46,
  pages    = "66--77",
  month    =  nov,
  year     =  2014,
  keywords = "Shadow price; Undesirable outputs; Carbon abatement; Power plants"
}

@ARTICLE{Leleu2013-ig,
  title    = "Shadow pricing of undesirable outputs in nonparametric analysis",
  author   = "Leleu, Herv{\'e}",
  abstract = "Abstract For three decades a growing interest in the modeling of
              desirable and undesirable outputs has led to a theoretical and
              methodological debate in the nonparametric literature on
              production technology and efficiency. The first main discussion
              is about the way of modeling `bad/undesirables' as inputs or
              outputs, or by transformation functions. The second debate
              concerns the implications of the weak disposability assumption in
              the modeling of bad outputs, in particular the possibility of
              assigning unexpected signs to shadow prices of bad outputs. In
              addition, we point out a current error in the modeling of weak
              disposability under a variable returns to scale technology. In
              this paper we introduce a hybrid model to ensure the economically
              meaningful jointness of good and bad outputs while constraining
              shadow prices of bad outputs to their expected sign. We argue
              that it is a sound compromise to model undesirable outputs with a
              meaningful primal/dual economic interpretation. Finally we
              propose an extension to define shadow prices for undesirable
              outputs following the Law of One Price (LoOP) rule.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  231,
  number   =  2,
  pages    = "474--480",
  month    =  dec,
  year     =  2013,
  keywords = "Data Envelopment Analysis; Undesirable outputs; Weak
              disposability; Shadow prices"
}

@ARTICLE{Fukuyama2008-lp,
  title    = "Japanese banking inefficiency and shadow pricing",
  author   = "Fukuyama, Hirofumi and Weber, William L",
  abstract = "Abstract We estimate Japanese banking inefficiency and the shadow
              price of problem loans by treating problem loans as a jointly
              produced undesirable by-product of the loan production process.
              Our method uses the directional output distance function of
              F{\"a}re et al. [R. F{\"a}re, S. Grosskopf, D.-W. Noh, W.L.
              Weber, Characteristics of a polluting technology: Theory and
              practice, Journal of Econometrics 126 (2005) 469--492] and seeks
              the maximum expansion of desirable outputs, such as loans and
              securities investments, and the simultaneous contraction in
              undesirable outputs, such as problem loans. The directional
              output distance function is estimated using data envelopment
              analysis and a parametric linear programming method. While the
              two methods give similar estimates of inefficiency, the shadow
              price estimates for the two methods diverge.",
  journal  = "Math. Comput. Model.",
  volume   =  48,
  number   =  11,
  pages    = "1854--1867",
  month    =  dec,
  year     =  2008,
  keywords = "DEA; Directional DEA specification; Parametric LP specification;
              Shadow pricing; Japanese banking; Bad (undesirable) output"
}

@ARTICLE{Beugelsdijk2010-vr,
  title     = "Introduction: Place, space and organization--- economic
               geography and the multinational enterprise",
  author    = "Beugelsdijk, Sjoerd and McCann, Philip and Mudambi, Ram",
  abstract  = "This article discusses the current links between international
               trade theory, economic geography and strategy and international
               business. We offer a way forward for building further links
               between these literatures that focusses on the notions of place,
               space and organization, and we document how the papers in this
               special issue contribute to this debate.",
  journal   = "J. Econ. Geogr.",
  publisher = "Oxford University Press",
  volume    =  10,
  number    =  4,
  pages     = "485--493",
  month     =  jul,
  year      =  2010
}

@ARTICLE{Chan2010-hc,
  title     = "{DOES} {SUBNATIONAL} {REGION} {MATTER}? {FOREIGN} {AFFILIATE}
               {PERFORMANCE} {IN} {THE} {UNITED} {STATES} {AND} {CHINA}",
  author    = "Chan, Christine M and Makino, Shige and Isobe, Takehiko",
  abstract  = "This study examines the extent to which subnational regions can
               explain foreign affiliate performance in two host country
               settings, the United States and China, the world's two largest
               economies at polar ends of the economic spectrum (i. e., an
               advanced versus an emerging economy). Our results suggest that
               the subnational region is significant in explaining foreign
               affiliate performance, thus confirming its importance as an
               additional unit of analysis for firm performance. This study
               also shows that the effects of subnational region are far
               stronger in China than they are in the United States, thus
               suggesting that regional differences are more critical in their
               explanatory power for firm performance in emerging economies
               than they are in advanced economies.",
  journal   = "Strategic Manage. J.",
  publisher = "Wiley",
  volume    =  31,
  number    =  11,
  pages     = "1226--1243",
  year      =  2010
}

@ARTICLE{Goerzen2013-nv,
  title     = "Global cities and multinational enterprise location strategy",
  author    = "Goerzen, Anthony and Asmussen, Christian Geisler and Nielsen, Bo
               Bernhard",
  abstract  = "We combine the concept of location derived by economic
               geographers with theories of the multinational enterprise (MNE)
               and the liability of foreignness developed by international
               business scholars, to examine the factors that propel MNEs
               toward, or away from, ``global cities''. We argue that three
               distinctive characteristics of global cities -- global
               interconnectedness, cosmopolitanism, and abundance of advanced
               producer services -- help MNEs overcome the costs of doing
               business abroad, and we identify the contingencies under which
               these characteristics combine with firm attributes to exert
               their strongest influence. Consistent with these arguments, our
               analysis of a large sample of MNE location decisions using a
               multilevel multinomial model suggests not only that MNEs have a
               strong propensity to locate within global cities, but also that
               these choices are associated with a nuanced interplay of firm-
               and subsidiary-level factors, including investment motives,
               proprietary capabilities, and business strategy. Our study
               provides important insights for international business scholars
               by shedding new light on MNE location choices and also
               contributes to our understanding of economic geography by
               examining the heterogeneous strategies and capabilities of MNEs
               -- the primary agents of economic globalization -- that shape
               the nature of global cities.",
  journal   = "J Int Bus Stud",
  publisher = "Palgrave Macmillan UK",
  volume    =  44,
  number    =  5,
  pages     = "427--450",
  month     =  jun,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Linton2007-qj,
  title    = "The quantilogram: With an application to evaluating directional
              predictability",
  author   = "Linton, O and Whang, Yoon-Jae",
  abstract = "Abstract We propose a new diagnostic tool for time series called
              the quantilogram. The tool can be used formally and we provide
              the inference tools to do this under general conditions, and it
              can also be used as a simple graphical device. We apply our
              method to measure directional predictability and to test the
              hypothesis that a given time series has no directional
              predictability. The test is based on comparing the correlogram of
              quantile hits to a pointwise confidence interval or on comparing
              the cumulated squared autocorrelations with the corresponding
              critical value. We provide the distribution theory needed to
              conduct inference, propose some model free upper bound critical
              values, and apply our methods to S\&P500 stock index return data.
              The empirical results suggest some directional predictability in
              returns. The evidence is strongest in mid range quantiles like
              5--10\% and for daily data. The evidence for predictability at
              the median is of comparable strength to the evidence around the
              mean, and is strongest at the daily frequency.",
  journal  = "J. Econom.",
  volume   =  141,
  number   =  1,
  pages    = "250--282",
  month    =  nov,
  year     =  2007,
  keywords = "Correlogram; Dependence; Efficient markets; Empirical process;
              Portmanteau; Quantiles"
}

@ARTICLE{Chung2007-bk,
  title     = "Model-free evaluation of directional predictability in foreign
               exchange markets",
  author    = "Chung, Jaehun and Hong, Yongmiao",
  journal   = "J. Appl. Econometrics",
  publisher = "Wiley Online Library",
  volume    =  22,
  number    =  5,
  pages     = "855--889",
  year      =  2007
}

@ARTICLE{Han2016-ri,
  title    = "The cross-quantilogram: Measuring quantile dependence and testing
              directional predictability between time series",
  author   = "Han, Heejoon and Linton, Oliver and Oka, Tatsushi and Whang,
              Yoon-Jae",
  abstract = "Abstract This paper proposes the cross-quantilogram to measure
              the quantile dependence between two time series. We apply it to
              test the hypothesis that one time series has no directional
              predictability to another time series. We establish the
              asymptotic distribution of the cross-quantilogram and the
              corresponding test statistic. The limiting distributions depend
              on nuisance parameters. To construct consistent confidence
              intervals we employ a stationary bootstrap procedure; we
              establish consistency of this bootstrap. Also, we consider a
              self-normalized approach, which yields an asymptotically pivotal
              statistic under the null hypothesis of no predictability. We
              provide simulation studies and two empirical applications. First,
              we use the cross-quantilogram to detect predictability from stock
              variance to excess stock return. Compared to existing tools used
              in the literature of stock return predictability, our method
              provides a more complete relationship between a predictor and
              stock return. Second, we investigate the systemic risk of
              individual financial institutions, such as JP Morgan Chase,
              Morgan Stanley and AIG.",
  journal  = "J. Econom.",
  volume   =  193,
  number   =  1,
  pages    = "251--270",
  month    =  jul,
  year     =  2016,
  keywords = "Quantile; Correlogram; Dependence; Predictability; Systemic risk"
}

@ARTICLE{Baltas2013-za,
  title   = "Momentum strategies in futures markets and trend-following funds",
  author  = "Baltas, Nick and Kosowski, Robert",
  journal = "Working Paper, Imperial College Business School",
  year    =  2013
}

@ARTICLE{Bekiros2008-wv,
  title     = "Direction-of-change forecasting using a volatility-based
               recurrent neural network",
  author    = "Bekiros, Stelios D and Georgoutsos, Dimitris A",
  journal   = "J. Forecast.",
  publisher = "Wiley Online Library",
  volume    =  27,
  number    =  5,
  pages     = "407--417",
  year      =  2008
}

@ARTICLE{Berk1999-fs,
  title     = "Optimal investment, growth options, and security returns",
  author    = "Berk, Jonathan B and Green, Richard C and Naik, Vasant",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  54,
  number    =  5,
  pages     = "1553--1607",
  year      =  1999
}

@ARTICLE{Brooks2001-ym,
  title     = "A trading strategy based on the lead--lag relationship between
               the spot index and futures contract for the {FTSE} 100",
  author    = "Brooks, Chris and Rew, Alistair G and Ritson, Stuart",
  journal   = "Int. J. Forecast.",
  publisher = "Elsevier",
  volume    =  17,
  number    =  1,
  pages     = "31--44",
  year      =  2001
}

@ARTICLE{Buhlmann2000-iq,
  title     = "Model selection for variable length Markov chains and tuning the
               context algorithm",
  author    = "B{\"u}hlmann, Peter",
  journal   = "Ann. Inst. Stat. Math.",
  publisher = "Springer",
  volume    =  52,
  number    =  2,
  pages     = "287--315",
  year      =  2000
}

@ARTICLE{Chordia2002-yn,
  title     = "Momentum, business cycle, and time-varying expected returns",
  author    = "Chordia, Tarun and Shivakumar, Lakshmanan",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  57,
  number    =  2,
  pages     = "985--1019",
  year      =  2002
}

@ARTICLE{Daniel1998-kl,
  title     = "Investor psychology and security market under-and overreactions",
  author    = "Daniel, Kent and Hirshleifer, David and Subrahmanyam, Avanidhar",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  53,
  number    =  6,
  pages     = "1839--1885",
  year      =  1998
}

@ARTICLE{Estrella1998-ef,
  title     = "A new measure of fit for equations with dichotomous dependent
               variables",
  author    = "Estrella, Arturo",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  16,
  number    =  2,
  pages     = "198--205",
  year      =  1998
}

@ARTICLE{Hurst2012-jp,
  title   = "A century of evidence on trend-following investing",
  author  = "Hurst, Brian and Ooi, Yao Hua and Pedersen, Lasse H",
  journal = "AQR Capital Management",
  year    =  2012
}

@ARTICLE{Hurst2013-zs,
  title   = "Demystifying managed futures",
  author  = "Hurst, Brian and Ooi, Yao Hua and Pedersen, Lasse Heje",
  journal = "Journal of Investment Management",
  volume  =  11,
  number  =  3,
  pages   = "42--58",
  year    =  2013
}

@ARTICLE{Larsen1995-dz,
  title     = "Market Timing Can Work in the Real World",
  author    = "Larsen, Jr, Glen A and Wozniak, Gregory D",
  journal   = "Portfolio Management",
  publisher = "Institutional Investor Journals",
  volume    =  21,
  number    =  3,
  pages     = "74--81",
  month     =  jan,
  year      =  1995
}

@ARTICLE{Moskowitz1999-cy,
  title     = "Do industries explain momentum?",
  author    = "Moskowitz, Tobias J and Grinblatt, Mark",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  54,
  number    =  4,
  pages     = "1249--1290",
  year      =  1999
}

@MISC{Newey1986-qi,
  title     = "A simple, positive semi-definite, heteroskedasticity and
               autocorrelationconsistent covariance matrix",
  author    = "Newey, Whitney K and West, Kenneth D",
  journal   = "Econometrica",
  publisher = "National Bureau of Economic Research Cambridge, Mass., USA",
  volume    =  55,
  pages     = "703--708",
  year      =  1986
}

@ARTICLE{Nyberg2011-il,
  title     = "Forecasting the direction of the {US} stock market with dynamic
               binary probit models",
  author    = "Nyberg, Henri",
  journal   = "Int. J. Forecast.",
  publisher = "Elsevier",
  volume    =  27,
  number    =  2,
  pages     = "561--578",
  year      =  2011
}

@ARTICLE{Sewell2011-hg,
  title     = "Characterization of financial time series",
  author    = "Sewell, Martin",
  journal   = "RN",
  publisher = "Citeseer",
  volume    =  11,
  number    =  01,
  pages     = "01",
  year      =  2011
}

@ARTICLE{Thomakos2008-ac,
  title   = "Optimal linear filtering, smoothing and trend extraction for
             processes with unit roots and cointegration",
  author  = "Thomakos, Dimitrios D",
  journal = "Available at SSRN: https://ssrn. com/abstract=1113331 or
             http://dx. doi. org/10. 2139/ssrn. 1113331",
  year    =  2008
}

@ARTICLE{Zhou2013-np,
  title   = "An Equilibrium Model of {Moving-Average} Predictability and
             {Time-Series} Momentum",
  author  = "Zhou, Guofu and Zhu, Yingzi",
  journal = "Unpublished working paper, Washington University in St. Louis",
  year    =  2013
}

@ARTICLE{Yang2009-zm,
  title     = "Strong law of large numbers for countable nonhomogeneous Markov
               chains",
  author    = "Yang, Weiguo",
  journal   = "Linear Algebra Appl.",
  publisher = "Elsevier",
  volume    =  430,
  number    = "11-12",
  pages     = "3008--3018",
  year      =  2009
}

@ARTICLE{Schwarz1978-sv,
  title     = "Estimating the dimension of a model",
  author    = "Schwarz, Gideon and {Others}",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  6,
  number    =  2,
  pages     = "461--464",
  year      =  1978
}

@ARTICLE{Specht1990-ju,
  title     = "Probabilistic neural networks",
  author    = "Specht, Donald F",
  journal   = "Neural Netw.",
  publisher = "Elsevier",
  volume    =  3,
  number    =  1,
  pages     = "109--118",
  year      =  1990
}

@ARTICLE{Rouwenhorst1999-fd,
  title     = "Local return factors and turnover in emerging stock markets",
  author    = "Rouwenhorst, K Geert",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  54,
  number    =  4,
  pages     = "1439--1464",
  year      =  1999
}

@ARTICLE{Ponka2017-ae,
  title     = "Predicting the direction of {US} stock markets using industry
               returns",
  author    = "P{\"o}nk{\"a}, Harri",
  journal   = "Empir. Econ.",
  publisher = "Springer",
  volume    =  52,
  number    =  4,
  pages     = "1451--1480",
  year      =  2017
}

@ARTICLE{Lewellen2002-pb,
  title     = "Momentum and autocorrelation in stock returns",
  author    = "Lewellen, Jonathan",
  journal   = "Rev. Financ. Stud.",
  publisher = "Soc Financial Studies",
  volume    =  15,
  number    =  2,
  pages     = "533--564",
  year      =  2002
}

@ARTICLE{Hvidkjaer2006-tm,
  title     = "A trade-based analysis of momentum",
  author    = "Hvidkjaer, Soeren",
  journal   = "Rev. Financ. Stud.",
  publisher = "Soc Financial Studies",
  volume    =  19,
  number    =  2,
  pages     = "457--491",
  year      =  2006
}

@ARTICLE{Hong2003-eh,
  title   = "Are the directions of stock price changes predictable? Statistical
             theory and evidence",
  author  = "Hong, Yongmiao and Chung, Jaehun",
  journal = "manuscript, Cornell University",
  year    =  2003
}

@ARTICLE{Fama1998-fp,
  title     = "Market efficiency, long-term returns, and behavioral finance",
  author    = "Fama, Eugene F",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  49,
  number    =  3,
  pages     = "283--306",
  year      =  1998
}

@ARTICLE{Brier1950-ts,
  title     = "Verification of forecasts expressed in terms of probability",
  author    = "Brier, Glenn W",
  journal   = "Mon. Weather Rev.",
  publisher = "American Meteorological Society",
  volume    =  75,
  pages     = "1--3",
  year      =  1950
}

@ARTICLE{Yao2012-kp,
  title     = "Momentum, contrarian, and the January seasonality",
  author    = "Yao, Yaqiong",
  abstract  = "Abstract This paper reexamines the apparent success of two
               prominent stock trading strategies: long-term contrarian and
               intermediate-term momentum. The paper demonstrates that
               long-term contrarian is entirely attributable to the classic
               January size effect, rather than to investor overreaction, as
               argued by De Bondt and Thaler (1985). Further, the paper also
               resolves the Novy-Marx (2011) concern about whether return
               autocorrelation ``is really momentum'' by demonstrating that the
               superior performance of intermediate-term momentum is due to
               strong January seasonality in the cross-section of returns. The
               implications are that long-term contrarian must be considered
               largely illusory, and intermediate-term momentum must take
               account of annual seasonalities in returns.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  36,
  number    =  10,
  pages     = "2757--2769",
  month     =  oct,
  year      =  2012,
  keywords  = "Momentum; Contrarian; Seasonality; January"
}

@ARTICLE{Welch2008-vk,
  title     = "A Comprehensive Look at The Empirical Performance of Equity
               Premium Prediction",
  author    = "Welch, Ivo and Goyal, Amit",
  abstract  = "Our article comprehensively reexamines the performance of
               variables that have been suggested by the academic literature to
               be good predictors of the equity premium. We find that by and
               large, these models have predicted poorly both in-sample (IS)
               and out-of-sample (OOS) for 30 years now; these models seem
               unstable, as diagnosed by their out-of-sample predictions and
               other statistics; and these models would not have helped an
               investor with access only to available information to profitably
               time the market.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  4,
  pages     = "1455--1508",
  month     =  jul,
  year      =  2008
}

@ARTICLE{Wang2015-qs,
  title     = "Hedging with Futures: Does Anything Beat the Na{\"\i}ve Hedging
               Strategy?",
  author    = "Wang, Yudong and Wu, Chongfeng and Yang, Li",
  abstract  = "This paper investigates out-of-sample performance of the
               na{\"\i}ve hedging strategy relative to that of the minimum
               variance hedging strategy, in which the covariance parameters
               are estimated from 18 econometric models. Hedging performance is
               compared across 24 futures markets. Our main findings suggest
               that it is difficult to find a strategy under the minimum
               variance framework that outperforms the na{\"\i}ve hedging
               strategy both consistently and significantly. Our findings are
               robust to different sample periods, estimation windows, and
               hedging horizons and can be partly explained by the effects of
               estimation error and model misspecification. Data, as
               supplemental material, are available at
               http://dx.doi.org/10.1287/mnsc.2014.2028. This paper was
               accepted by Itay Goldstein, finance.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  61,
  number    =  12,
  pages     = "2870--2889",
  month     =  jan,
  year      =  2015
}

@ARTICLE{Vayanos2013-ij,
  title     = "An Institutional Theory of Momentum and Reversal",
  author    = "Vayanos, Dimitri and Woolley, Paul",
  abstract  = "We propose a theory of momentum and reversal based on flows
               between investment funds. Flows are triggered by changes in fund
               managers' efficiency, which investors either observe directly or
               infer from past performance. Momentum arises if flows exhibit
               inertia, and because rational prices underreact to expected
               future flows. Reversal arises because flows push prices away
               from fundamental values. Besides momentum and reversal, flows
               generate comovement, lead-lag effects, and amplification, with
               these being larger for high idiosyncratic risk assets. A
               calibration of our model using evidence on mutual fund returns
               and flows generates sizeable Sharpe ratios for momentum and
               value strategies.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  26,
  number    =  5,
  pages     = "1087--1145",
  month     =  may,
  year      =  2013
}

@ARTICLE{Szakmary2010-oe,
  title     = "Trend-following trading strategies in commodity futures: A
               re-examination",
  author    = "Szakmary, Andrew C and Shen, Qian and Sharma, Subhash C",
  abstract  = "Abstract This paper examines the performance of trend-following
               trading strategies in commodity futures markets using a monthly
               dataset spanning 48years and 28 markets. We find that all
               parameterizations of the dual moving average crossover and
               channel strategies that we implement yield positive mean excess
               returns net of transactions costs in at least 22 of the 28
               markets. When we pool our results across markets, we show that
               all of the trading rules earn hugely significant positive
               returns that prevail over most subperiods of the data as well.
               These results are robust with respect to the set of commodities
               the trading rules are implemented with, distributional
               assumptions, data-mining adjustments and transactions costs, and
               help resolve divergent evidence in the extant literature
               regarding the performance of momentum and pure trend-following
               strategies that is otherwise difficult to explain.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  34,
  number    =  2,
  pages     = "409--426",
  month     =  feb,
  year      =  2010,
  keywords  = "Trend-following; Trading rules; Momentum; Commodity futures"
}

@INPROCEEDINGS{Specht1988-tf,
  title     = "Probabilistic neural networks for classification, mapping, or
               associative memory",
  booktitle = "{IEEE} 1988 International Conference on Neural Networks",
  author    = "Specht, D F",
  abstract  = "It can be shown that by replacing the sigmoid activation
               function often used in neural networks with an exponential
               function, a neural network can be formed which computes
               nonlinear decision boundaries. This technique yields decision
               surfaces which approach the Bayes optimal under certain
               conditions. There is a continuous control of the linearity of
               the decision boundaries, from linear for small training sets to
               any degree of nonlinearity justified by larger training sets. A
               four-layer neural network of the type proposed can map any input
               pattern to any number of classifications. The input variables
               can be either continuous or binary. Modification of the decision
               boundaries based on new data can be accomplished in real time
               simply by defining a set of weights equal to the new training
               vector. The decision boundaries can be implemented using analog
               'neurons', which operate entirely in parallel. The organization
               proposed takes into account the projected pin limitations of
               neural-net chips of the near future. By a change in
               architecture, these same components could be used as associative
               memories, to compute nonlinear multivariate regression surfaces,
               or to compute a posteriori probabilities of an event.>",
  volume    =  1,
  pages     = "525--532 vol.1",
  month     =  jul,
  year      =  1988,
  keywords  = "Bayes methods;artificial intelligence;content-addressable
               storage;neural nets;pattern recognition;Bayes
               optimal;architecture;artificial intelligence;associative
               memory;classification;exponential function;four-layer neural
               network;mapping;nonlinear decision boundaries;probabilistic
               neural networks;training sets;Artificial
               intelligence;Associative memories;Bayes procedures;Neural
               networks;Pattern recognition"
}

@ARTICLE{Shen2007-jm,
  title     = "An examination of momentum strategies in commodity futures
               markets",
  author    = "Shen, Qian and Szakmary, Andrew C and Sharma, Subhash C",
  abstract  = "Commodity futures and equity markets differ in several important
               respects. Nevertheless, it was found that momentum profits in
               commodities are highly significant for holding periods as long
               as 9 months, and returns to momentum strategies are roughly
               equal in magnitude to those that have been reported in stocks.
               The profits documented are too large to be subsumed by
               transactions costs. Although the momentum strategies appear to
               be quite risky, their profitability cannot be fully accounted
               for in the context of a market factor model. Further, it is
               shown that momentum profits eventually reverse if positions are
               maintained long enough after portfolio formation. \copyright{}
               2007 Wiley Periodicals, Inc. Jrl Fut Mark 27:227--256, 2007",
  journal   = "J. Futures Mark.",
  publisher = "Wiley Subscription Services, Inc., A Wiley Company",
  volume    =  27,
  number    =  3,
  pages     = "227--256",
  month     =  mar,
  year      =  2007
}

@ARTICLE{Sharpe1966-bd,
  title     = "Mutual Fund Performance",
  author    = "Sharpe, William F",
  journal   = "The Journal of Business",
  publisher = "University of Chicago Press",
  volume    =  39,
  number    =  1,
  pages     = "119--138",
  year      =  1966
}

@ARTICLE{Sagi2007-vp,
  title     = "Firm-specific attributes and the cross-section of momentum",
  author    = "Sagi, Jacob S and Seasholes, Mark S",
  abstract  = "Abstract This paper identifies observable firm-specific
               attributes that drive momentum. We find that a firm's revenues,
               costs, and growth options combine to determine the dynamics of
               its return autocorrelation. We use these insights to implement
               momentum strategies (buying winners and selling losers) with
               both numerically simulated returns and CRSP/Compustat data. In
               both sets of data, momentum strategies that use firms with high
               revenue growth volatility, low costs, or valuable growth options
               outperform traditional momentum strategies by approximately 5\%
               per year.",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  84,
  number    =  2,
  pages     = "389--434",
  month     =  may,
  year      =  2007,
  keywords  = "Asset pricing; Expected returns; Momentum; Real options"
}

@ARTICLE{Rydberg2003-id,
  title     = "Dynamics of {Trade-by-Trade} Price Movements: Decomposition and
               Models",
  author    = "Rydberg, Tina Hviid and Shephard, Neil",
  abstract  = "In this article we introduce a decomposition of the joint
               distribution of price changes of assets recorded trade-by-trade.
               Our decomposition means that we can model the dynamics of price
               changes using quite simple and interpretable models which are
               easily extended in a great number of directions, including using
               durations and volume as explanatory variables. Thus we provide
               an econometric basis for empirical work on market microstructure
               using time series of transaction data. We use maximum likelihood
               estimation and testing methods to assess the fit of the model to
               one year of IBM stock price data taken from the New York Stock
               Exchange.",
  journal   = "Journal of Financial Econometrics",
  publisher = "Oxford University Press",
  volume    =  1,
  number    =  1,
  pages     = "2--25",
  month     =  mar,
  year      =  2003
}

@ARTICLE{Pesaran1995-gd,
  title     = "Predictability of Stock Returns: Robustness and Economic
               Significance",
  author    = "Pesaran, M Hashem and Timmermann, Allan",
  abstract  = "This article examines the robustness of the evidence on
               predictability of U.S. stock returns, and addresses the issue of
               whether this predictability could have been historically
               exploited by investors to earn profits in excess of a
               buy-and-hold strategy in the market index. We find that the
               predictive power of various economic factors over stock returns
               changes through time and tends to vary with the volatility of
               returns. The degree to which stock returns were predictable
               seemed quite low during the relatively calm markets in the
               1960s, but increased to a level where, net of transaction costs,
               it could have been exploited by investors in the volatile
               markets of the 1970s.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  50,
  number    =  4,
  pages     = "1201--1228",
  month     =  sep,
  year      =  1995
}

@ARTICLE{Papailias2017-nd,
  title    = "Returns Signal Momentum",
  author   = "Papailias, Fotis and Liu, Jiadong and Thomakos, Dimitrios",
  abstract = "A new type of momentum based on past return signs is introduced,
              called Returns Signal Momentum. This is mainly driven by sign
              dependence, which is positively r",
  journal  = "Available at SSRN: https://ssrn. com/abstract=2971444",
  month    =  may,
  year     =  2017,
  keywords = "Return Sign, Trading Strategies, Market Timing, Time Series
              Momentum"
}

@ARTICLE{Newcombe1998-mr,
  title    = "Two-sided confidence intervals for the single proportion:
              comparison of seven methods",
  author   = "Newcombe, R G",
  abstract = "Simple interval estimate methods for proportions exhibit poor
              coverage and can produce evidently inappropriate intervals.
              Criteria appropriate to the evaluation of various proposed
              methods include: closeness of the achieved coverage probability
              to its nominal value; whether intervals are located too close to
              or too distant from the middle of the scale; expected interval
              width; avoidance of aberrations such as limits outside [0,1] or
              zero width intervals; and ease of use, whether by tables,
              software or formulae. Seven methods for the single proportion are
              evaluated on 96,000 parameter space points. Intervals based on
              tail areas and the simpler score methods are recommended for use.
              In each case, methods are available that aim to align either the
              minimum or the mean coverage with the nominal 1 -alpha.",
  journal  = "Stat. Med.",
  volume   =  17,
  number   =  8,
  pages    = "857--872",
  month    =  apr,
  year     =  1998,
  language = "en"
}

@ARTICLE{Newcombe1998-ks,
  title    = "Interval estimation for the difference between independent
              proportions: comparison of eleven methods",
  author   = "Newcombe, R G",
  abstract = "Several existing unconditional methods for setting confidence
              intervals for the difference between binomial proportions are
              evaluated. Computationally simpler methods are prone to a variety
              of aberrations and poor coverage properties. The closely
              interrelated methods of Mee and Miettinen and Nurminen perform
              well but require a computer program. Two new approaches which
              also avoid aberrations are developed and evaluated. A tail area
              profile likelihood based method produces the best coverage
              properties, but is difficult to calculate for large denominators.
              A method combining Wilson score intervals for the two proportions
              to be compared also performs well, and is readily implemented
              irrespective of sample size.",
  journal  = "Stat. Med.",
  volume   =  17,
  number   =  8,
  pages    = "873--890",
  month    =  apr,
  year     =  1998,
  language = "en"
}

@ARTICLE{Neumann2013-fa,
  title     = "Predictable Dynamics in {Higher-Order} {Risk-Neutral} Moments:
               Evidence from the {S\&P} 500 Options",
  author    = "Neumann, Michael and Skiadopoulos, George",
  abstract  = "Predictable Dynamics in Higher-Order Risk-Neutral Moments:
               Evidence from the S\&P 500 Options - Volume 48 Issue 3 - Michael
               Neumann, George Skiadopoulos",
  journal   = "Journal of Financial and Quantitative Analysis",
  publisher = "Cambridge University Press",
  volume    =  48,
  number    =  3,
  pages     = "947--977",
  month     =  jun,
  year      =  2013
}

@ARTICLE{Miffre2007-lp,
  title     = "Momentum strategies in commodity futures markets",
  author    = "Miffre, Jo{\"e}lle and Rallis, Georgios",
  abstract  = "Abstract The article tests for the presence of short-term
               continuation and long-term reversal in commodity futures prices.
               While contrarian strategies do not work, the article identifies
               13 profitable momentum strategies that generate 9.38\% average
               return a year. A closer analysis of the constituents of the
               long--short portfolios reveals that the momentum strategies buy
               backwardated contracts and sell contangoed contracts. The
               correlation between the momentum returns and the returns of
               traditional asset classes is also found to be low, making the
               commodity-based relative-strength portfolios excellent
               candidates for inclusion in well-diversified portfolios.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  31,
  number    =  6,
  pages     = "1863--1886",
  month     =  jun,
  year      =  2007,
  keywords  = "Commodity futures; Momentum; Backwardation; Contango;
               Diversification"
}

@ARTICLE{Meese1983-no,
  title     = "Empirical exchange rate models of the seventies: Do they fit out
               of sample?",
  author    = "Meese, Richard A and Rogoff, Kenneth",
  abstract  = "Abstract This study compares the out-of-sample forecasting
               accuracy of various structural and time series exchange rate
               models. We find that a random walk model performs as well as any
               estimated model at one to twelve month horizons for the
               dollar/pound, dollar/mark, dollar/yen and trade-weighted dollar
               exchange rates. The candidate structural models include the
               flexible-price (Frenkel-Bilson) and sticky-price
               (Dornbusch-Frankel) monetary models, and a sticky-price model
               which incorporates the current account (Hooper-Morton). The
               structural models perform poorly despite the fact that we base
               their forecasts on actual realized values of future explanatory
               variables.",
  journal   = "J. Int. Econ.",
  publisher = "Elsevier",
  volume    =  14,
  number    =  1,
  pages     = "3--24",
  month     =  feb,
  year      =  1983
}

@ARTICLE{Lo1990-mh,
  title     = "When Are Contrarian Profits Due to Stock Market Overreaction?",
  author    = "Lo, Andrew W and MacKinlay, A Craig",
  abstract  = "If returns on some stocks systematically lead or lag those of
               others, a portfolio strategy that sells ``winners'' and buys
               ``losers'' can produce positive expected returns, even if no
               stock's returns are negatively autocorrelated as virtually all
               models of overreaction imply. Using a particular contrarian
               strategy we show that, despite negative autocorrelation in
               individual stock returns, weekly portfolio returns are strongly
               positively autocorrelated and are the result of important
               cross-autocorrelations. We find that the returns of large stocks
               lead those of smaller stocks, and we present evidence against
               overreaction as the only source of contrarian profits.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  3,
  number    =  2,
  pages     = "175--205",
  month     =  apr,
  year      =  1990
}

@ARTICLE{Lo1988-mz,
  title     = "Stock Market Prices Do Not Follow Random Walks: Evidence from a
               Simple Specification Test",
  author    = "Lo, Andrew W and MacKinlay, A Craig",
  abstract  = "In this article we test the random walk hypothesis for weekly
               stock market returns by comparing variance estimators derived
               from data sampled at different frequencies. The random walk
               model is strongly rejected for the entire sample period
               (1962--1985) and for all subperiods for a variety of aggregate
               returns indexes and size-sorted portfolios. Although the
               rejections are due largely to the behavior of small stocks, they
               cannot be attributed completely to the effects of infrequent
               trading or time-varying volatilities. Moreover, the rejection of
               the random walk for weekly returns does not support a
               mean-reverting model of asset prices.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  1,
  number    =  1,
  pages     = "41--66",
  month     =  jan,
  year      =  1988
}

@ARTICLE{Lee2000-ve,
  title     = "Price Momentum and Trading Volume",
  author    = "Lee, Charles M C and Swaminathan, Bhaskaran",
  abstract  = "This study shows that past trading volume provides an important
               link between ``momentum'' and ``value'' strategies.
               Specifically, we find that firms with high (low) past turnover
               ratios exhibit many glamour (value) characteristics, earn lower
               (higher) future returns, and have consistently more negative
               (positive) earnings surprises over the next eight quarters. Past
               trading volume also predicts both the magnitude and persistence
               of price momentum. Specifically, price momentum effects reverse
               over the next five years, and high (low) volume winners (losers)
               experience faster reversals. Collectively, our findings show
               that past volume helps to reconcile intermediate-horizon
               ``underreaction'' and long-horizon ``overreaction'' effects.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishers, Inc.",
  volume    =  55,
  number    =  5,
  pages     = "2017--2069",
  month     =  oct,
  year      =  2000
}

@ARTICLE{Kim2016-ga,
  title     = "Time series momentum and volatility scaling",
  author    = "Kim, Abby Y and Tse, Yiuman and Wald, John K",
  abstract  = "Abstract Moskowitz, Ooi, and Pedersen (2012) show that time
               series momentum delivers a large and significant alpha for a
               diversified portfolio of international futures contracts. We
               find that their results are largely driven by volatility-scaling
               returns (or the so-called risk parity approach to asset
               allocation) rather than by time series momentum. Without scaling
               by volatility, time series momentum and a buy-and-hold strategy
               offer similar cumulative returns, and their alphas are not
               significantly different. This similarity holds for most sectors
               and for a combined portfolio of futures contracts.
               Cross-sectional momentum also offers a higher (similar) alpha
               than unscaled (scaled) time series momentum.",
  journal   = "Journal of Financial Markets",
  publisher = "Elsevier",
  volume    =  30,
  number    = "Supplement C",
  pages     = "103--124",
  month     =  sep,
  year      =  2016,
  keywords  = "Momentum; Futures pricing; International asset allocation"
}

@ARTICLE{Kim1998-xc,
  title     = "Graded forecasting using an array of bipolar predictions:
               application of probabilistic neural networks to a stock market
               index",
  author    = "Kim, Steven H and Hak Chun, Se",
  abstract  = "Abstract To an increasing extent over the past decade, software
               learning methods including neural networks have been used for
               prediction in financial markets and other areas. By far the most
               popular type of neural network has been backpropagation.
               However, the advantages of other learning techniques such as the
               swift response of the probabilistic neural network (PNN) suggest
               the desirability of adapting other models to the predictive
               function. Unfortunately, the conventional architecture for
               probabilistic neural networks yields only a bipolar output
               corresponding to Yes or No; Up or Down. This limitation may be
               circumvented in part by using a graded forecast of multiple
               discrete values. More specifically, the approach involves an
               architecture comprising an array of elementary PNNs with bipolar
               output. This paper explores a number of interrelated topics: (1)
               presentation of a new architecture for graded forecasting using
               an arrayed probabilistic network (APN); (2) use of a ``mistake
               chart'' to compare the accuracy of learning systems against
               default performance based on a constant prediction; and (3)
               evaluation of several backpropagation models against a recurrent
               neural network (RNN) as well as PNN, APN, and case based
               reasoning. These concepts are investigated against the backdrop
               of a practical application involving the prediction of a stock
               market index.",
  journal   = "Int. J. Forecast.",
  publisher = "Elsevier",
  volume    =  14,
  number    =  3,
  pages     = "323--337",
  month     =  sep,
  year      =  1998,
  keywords  = "Forecasting system; Artificial intelligence; Financial market
               forecasting"
}

@ARTICLE{Kauppi2008-ke,
  title     = "Predicting {U.S}. Recessions with Dynamic Binary Response Models",
  author    = "Kauppi, Heikki and Saikkonen, Pentti",
  abstract  = "Abstract We develop dynamic binary probit models and apply them
               for predicting U.S. recessions using the interest rate spread as
               the driving predictor. The new models use lags of the binary
               response (a recession dummy) to forecast its future values and
               allow for the potential forecast power of lags of the underlying
               conditional probability. We show how multiperiod-ahead forecasts
               are computed iteratively using the same one-period-ahead model.
               Iterated forecasts that apply specific lags supported by
               statistical model selection procedures turn out to be more
               accurate than previously used direct forecasts based on
               horizon-specific model specifications.",
  journal   = "Rev. Econ. Stat.",
  publisher = "MIT Press",
  volume    =  90,
  number    =  4,
  pages     = "777--791",
  month     =  oct,
  year      =  2008
}

@ARTICLE{Jegadeesh2002-jh,
  title     = "{Cross-Sectional} and {Time-Series} Determinants of Momentum
               Returns",
  author    = "Jegadeesh, Narasimhan and Titman, Sheridan",
  abstract  = "Portfolio strategies that buy stocks with high returns over the
               previous 3--12 months and sell stocks with low returns over this
               same time period perform well over the following 12 months. A
               recent article by Conrad and Kaul (1998) presents striking
               evidence suggesting that the momentum profits are attributable
               to cross-sectional differences in expected returns rather than
               to any time-series dependence in returns. This article shows
               that Conrad and Kaul reach this conclusion because they do not
               take into account the small sample biases in their tests and
               bootstrap experiments. Our unbiased empirical tests indicate
               that cross-sectional differences in expected returns explain
               very little, if any, of the momentum profits.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  15,
  number    =  1,
  pages     = "143--157",
  month     =  jan,
  year      =  2002
}

@ARTICLE{Jegadeesh1995-ry,
  title     = "Overreaction, Delayed Reaction, and Contrarian Profits",
  author    = "Jegadeesh, Narasimhan and Titman, Sheridan",
  abstract  = "This article examines the contribution of stock price
               overreaction and delayed reaction to the profitability of
               contrarian strategies. The evidence indicates that stock prices
               overreact to firm-specific information, but react with a delay
               to common factors. Delayed reactions to common factors give rise
               to a size-related lead-lag effect in stock returns. In sharp
               contrast with the conclusions in the extant literature, however,
               this article finds that most of the contrarian profit is due to
               stock price overreaction and a very small fraction of the profit
               can be attributed to the lead-lag effect.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  8,
  number    =  4,
  pages     = "973--993",
  month     =  oct,
  year      =  1995
}

@ARTICLE{Hutchinson2015-bi,
  title    = "Time Series Momentum and Macroeconomic Risk",
  author   = "Hutchinson, Mark and O'Brien, John",
  abstract = "The time series momentum strategy has been shown to deliver
              consistent profitability over a long time horizon. Funds pursuing
              these strategies are now a compone",
  journal  = "Available at SSRN: https://ssrn. com/abstract=2550718 or
              http://dx. doi. org/10. 2139/ssrn. 2550718",
  month    =  aug,
  year     =  2015,
  keywords = "Trend Following, Time Series Momentum, Macroeconomic Risk, CTA"
}

@ARTICLE{Huang2005-ee,
  title     = "Forecasting stock market movement direction with support vector
               machine",
  author    = "Huang, Wei and Nakamori, Yoshiteru and Wang, Shou-Yang",
  abstract  = "Abstract Support vector machine (SVM) is a very specific type of
               learning algorithms characterized by the capacity control of the
               decision function, the use of the kernel functions and the
               sparsity of the solution. In this paper, we investigate the
               predictability of financial movement direction with SVM by
               forecasting the weekly movement direction of NIKKEI 225 index.
               To evaluate the forecasting ability of SVM, we compare its
               performance with those of Linear Discriminant Analysis,
               Quadratic Discriminant Analysis and Elman Backpropagation Neural
               Networks. The experiment results show that SVM outperforms the
               other classification methods. Further, we propose a combining
               model by integrating SVM with the other classification methods.
               The combining model performs best among all the forecasting
               methods.",
  journal   = "Comput. Oper. Res.",
  publisher = "Elsevier",
  volume    =  32,
  number    =  10,
  pages     = "2513--2522",
  month     =  oct,
  year      =  2005,
  keywords  = "Support vector machine; Forecasting; Multivariate classification"
}

@ARTICLE{Hillert2014-jr,
  title     = "Media Makes Momentum",
  author    = "Hillert, Alexander and Jacobs, Heiko and M{\"u}ller, Sebastian",
  abstract  = "Relying on 2.2 million articles from forty-five national and
               local U.S. newspapers between 1989 and 2010, we find that firms
               particularly covered by the media exhibit, ceteris paribus,
               significantly stronger momentum. The effect depends on article
               tone, reverses in the long run, is more pronounced for stocks
               with high uncertainty, and is stronger in states with high
               investor individualism. Our findings suggest that media coverage
               can exacerbate investor biases, leading return predictability to
               be strongest for firms in the spotlight of public attention.
               These results collectively lend credibility to an
               overreaction-based explanation for the momentum effect.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  27,
  number    =  12,
  pages     = "3467--3501",
  month     =  dec,
  year      =  2014
}

@INPROCEEDINGS{Hassan2005-ij,
  title       = "Stock market forecasting using hidden Markov model: a new
                 approach",
  booktitle   = "5th International Conference on Intelligent Systems Design and
                 Applications ({ISDA'05})",
  author      = "Hassan, M R and Nath, B",
  abstract    = "This paper presents hidden Markov models (HMM) approach for
                 forecasting stock price for interrelated markets. We apply HMM
                 to forecast some of the airlines stock. HMMs have been
                 extensively used for pattern recognition and classification
                 problems because of its proven suitability for modelling
                 dynamic systems. However, using HMM for predicting future
                 events is not straightforward. Here we use only one HMM that
                 is trained on the past dataset of the chosen airlines. The
                 trained HMM is used to search for the variable of interest
                 behavioural data pattern from the past dataset. By
                 interpolating the neighbouring values of these datasets
                 forecasts are prepared. The results obtained using HMM are
                 encouraging and HMM offers a new paradigm for stock market
                 forecasting, an area that has been of much research interest
                 lately.",
  pages       = "192--196",
  institution = "IEEE",
  year        =  2005,
  keywords    = "forecasting theory;hidden Markov models;pattern
                 classification;stock markets;airlines stock;hidden Markov
                 model;pattern classification;pattern recognition;stock market
                 forecasting;stock price forecasting;Artificial
                 intelligence;Artificial neural networks;Economic
                 forecasting;Fluctuations;Fuzzy systems;Hidden Markov
                 models;Humans;Intelligent systems;Predictive models;Stock
                 markets;HMM;feature selection;financial time series;stock
                 market forecasting"
}

@ARTICLE{Gregoir2000-mq,
  title     = "Measuring the probability of a business cycle turning point by
               using a multivariate qualitative hidden Markov model",
  author    = "Gregoir, St{\'e}phane and Lenglart, Fabrice",
  abstract  = "A two-step procedure to produce a statistical measure of the
               probability of being in an accelerating or decelerating phase of
               economic activity is proposed. It consists of, first, an
               extraction of the individual linear innovations of a set of
               relevant macroeconomic variables whose signs are accumulated
               into a qualitative vector process and, second, of a factor
               analysis applied to this vector. The factor process is a
               two-state Markov process of order one whose states are described
               as favourable and unfavourable. Estimated on French business
               surveys, this measure appears to be a competitive coincident
               indicator. Copyright \copyright{} 2000 John Wiley \& Sons, Ltd.",
  journal   = "J. Forecast.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  19,
  number    =  2,
  pages     = "81--102",
  month     =  mar,
  year      =  2000,
  keywords  = "business cycle; turning point; business survey; Hidden Markov
               Model"
}

@ARTICLE{Gorton2012-lt,
  title     = "The Fundamentals of Commodity Futures Returns",
  author    = "Gorton, Gary B and Hayashi, Fumio and Rouwenhorst, K Geert",
  abstract  = "Commodity futures risk premiums vary across commodities and over
               time depending on the level of physical inventories. The
               convenience yield is a decreasing, nonlinear function of
               inventories. Price measures, such as the futures basis, prior
               futures returns, prior spot returns, and spot price volatilities
               reflect the state of inventories and are informative about
               commodity futures risk premiums. We verify these theoretical
               predictions using a comprehensive data set on 31 commodity
               futures and physical inventories between 1971 and 2010. We find
               no evidence that the positions of participants in futures
               markets predict risk premiums on commodity futures.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  17,
  number    =  1,
  pages     = "35--105",
  month     =  aug,
  year      =  2012
}

@ARTICLE{Giraitis2014-rm,
  title     = "Inference on stochastic time-varying coefficient models",
  author    = "Giraitis, L and Kapetanios, G and Yates, T",
  abstract  = "Abstract Recently, there has been considerable work on
               stochastic time-varying coefficient models as vehicles for
               modelling structural change in the macroeconomy with a focus on
               the estimation of the unobserved paths of random coefficient
               processes. The dominant estimation methods, in this context, are
               based on various filters, such as the Kalman filter, that are
               applicable when the models are cast in state space
               representations. This paper introduces a new class of
               autoregressive bounded processes that decompose a time series
               into a persistent random attractor, a time varying
               autoregressive component, and martingale difference errors. The
               paper examines, rigorously, alternative kernel based,
               nonparametric estimation approaches for such models and derives
               their basic properties. These estimators have long been studied
               in the context of deterministic structural change, but their use
               in the presence of stochastic time variation is novel. The
               proposed inference methods have desirable properties such as
               consistency and asymptotic normality and allow a tractable
               studentization. In extensive Monte Carlo and empirical studies,
               we find that the methods exhibit very good small sample
               properties and can shed light on important empirical issues such
               as the evolution of inflation persistence and the purchasing
               power parity (PPP) hypothesis.",
  journal   = "J. Econom.",
  publisher = "Elsevier",
  volume    =  179,
  number    =  1,
  pages     = "46--65",
  month     =  mar,
  year      =  2014,
  keywords  = "Time-varying coefficient models; Random coefficient models;
               Nonparametric estimation; Kernel estimation; Autoregressive
               processes"
}

@ARTICLE{Fuertes2010-ss,
  title     = "Tactical allocation in commodity futures markets: Combining
               momentum and term structure signals",
  author    = "Fuertes, Ana-Maria and Miffre, Jo{\"e}lle and Rallis, Georgios",
  abstract  = "Abstract This paper examines the combined role of momentum and
               term structure signals for the design of profitable trading
               strategies in commodity futures markets. With significant
               annualized alphas of 10.14\% and 12.66\%, respectively, the
               momentum and term structure strategies appear profitable when
               implemented individually. With an abnormal return of 21.02\%,
               our double-sort strategy that exploits both momentum and term
               structure signals clearly outperforms the single-sort
               strategies. This double-sort strategy can additionally be
               utilized as a portfolio diversification tool. The abnormal
               performance of the combined portfolios cannot be explained by a
               lack of liquidity, data mining or transaction costs.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  34,
  number    =  10,
  pages     = "2530--2548",
  month     =  oct,
  year      =  2010,
  keywords  = "Commodity futures; Momentum; Term structure; Double-sort
               strategy"
}

@ARTICLE{Fama1993-rx,
  title     = "Common risk factors in the returns on stocks and bonds",
  author    = "Fama, Eugene F and French, Kenneth R",
  abstract  = "Abstract This paper identifies five common risk factors in the
               returns on stocks and bonds. There are three stock-market
               factors: an overall market factor and factors related to firm
               size and book-to-market equity. There are two bond-market
               factors, related to maturity and default risks. Stock returns
               have shared variation due to the stock-market factors, and they
               are linked to bond returns through shared variation in the
               bond-market factors. Except for low-grade corporates, the
               bond-market factors capture the common variation in bond
               returns. Most important, the five factors seem to explain
               average returns on stocks and bonds.",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  33,
  number    =  1,
  pages     = "3--56",
  month     =  feb,
  year      =  1993
}

@ARTICLE{DeMiguel2014-ro,
  title     = "Stock Return Serial Dependence and {Out-of-Sample} Portfolio
               Performance",
  author    = "DeMiguel, Victor and Nogales, Francisco J and Uppal, Raman",
  abstract  = "We study whether investors can exploit serial dependence in
               stock returns to improve out-of-sample portfolio performance. We
               show that a vector-autoregressive (VAR) model captures stock
               return serial dependence in a statistically significant manner.
               Analytically, we demonstrate that, unlike contrarian and
               momentum portfolios, an arbitrage portfolio based on the VAR
               model attains positive expected returns regardless of the sign
               of asset return cross-covariances and autocovariances.
               Empirically, we show, however, that both the arbitrage and
               mean-variance portfolios based on the VAR model outperform the
               traditional unconditional portfolios only for transaction costs
               below ten basis points.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  27,
  number    =  4,
  pages     = "1031--1073",
  month     =  apr,
  year      =  2014
}

@ARTICLE{De_Groot2014-zi,
  title     = "Exploiting commodity momentum along the futures curves",
  author    = "de Groot, Wilma and Karstanje, Dennis and Zhou, Weili",
  abstract  = "Abstract This study examines novel momentum strategies in
               commodities futures markets that incorporate term-structure
               information. We show that momentum strategies that invest in
               contracts on the futures curve with the largest expected
               roll-yield or the strongest momentum earn significantly higher
               risk-adjusted returns than a traditional momentum strategy,
               which only invests in the nearest contracts. Moreover, when
               incorporating conservative transaction costs we observe that our
               low-turnover momentum strategy more than doubles the net return
               compared to a traditional momentum strategy.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  48,
  number    = "Supplement C",
  pages     = "79--93",
  month     =  nov,
  year      =  2014,
  keywords  = "Commodity futures; Momentum; Term structure; Futures curve; Roll
               yield; Transaction costs"
}

@ARTICLE{Conrad2017-dh,
  title     = "Momentum and Reversal: Does What Goes Up Always Come Down?",
  author    = "Conrad, Jennifer and Yavuz, M Deniz",
  abstract  = "The stocks in a momentum portfolio, which contribute to momentum
               profits, do not experience significant subsequent reversals.
               Conversely, stocks that do not contribute to momentum profits
               over the intermediate horizon exhibit subsequent reversals.
               Merging these separate securities into a single portfolio causes
               momentum and reversal patterns to appear linked. Stocks with
               momentum can be separated from those that exhibit reversal by
               sorting on size and book-to-market equity ratio. Controlling for
               proxies for behavioral biases, market illiquidity, and
               macroeconomic factors does not affect our results.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  2,
  pages     = "555--581",
  month     =  mar,
  year      =  2017
}

@ARTICLE{Conrad1998-ib,
  title     = "An Anatomy of Trading Strategies",
  author    = "Conrad, Jennifer and Kaul, Gautam",
  abstract  = "In this article we use a single unifying framework to analyze
               the sources of profits to a wide spectrum of return-based
               trading strategies implemented in the literature. We show that
               less than 50\% of the 120 strategies implemented in the article
               yield statistically significant profits and, unconditionally,
               momentum and contrarian strategies are equally likely to be
               successful. However, when we condition on the return horizon
               (short, medium, or long) of the strategy, or the time period
               during which it is implemented, two patterns emerge. A momentum
               strategy is usually profitable at the medium (3- to 12-months)
               horizon, while a contrarian strategy nets statistically
               significant profits at long horizons, but only during the
               1926--1947 subperiod. More importantly, our results show that
               the cross-sectional variation in the mean returns of individual
               securities included in these strategies play an important role
               in their profitability. The cross-sectional variation can
               potentially account for the profitability of momentum strategies
               and it is also responsible for attenuating the profits from
               price reversals to long-horizon contrarian strategies.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  11,
  number    =  3,
  pages     = "489--519",
  month     =  jul,
  year      =  1998
}

@ARTICLE{Clare2016-tx,
  title     = "The trend is our friend: Risk parity, momentum and trend
               following in global asset allocation",
  author    = "Clare, Andrew and Seaton, James and Smith, Peter N and Thomas,
               Stephen",
  abstract  = "Abstract We examine applying a trend following methodology to
               global asset allocation between equities, bonds, commodities and
               real estate. This strategy offers substantial improvement in
               risk-adjusted performance compared to buy-and-hold portfolios
               and a superior method of asset allocation than risk parity. We
               believe the discipline of trend following overcomes many of the
               behavioural biases investors succumb to, such as regret and
               herding, and offers a solution to the inappropriate sequence of
               returns which can be problematic for decumulation portfolios.
               The other side of behavioural biases is that they may be
               exploited by investors: an example is momentum investing where
               herding leads to continuation of returns and has been identified
               across many assets. Momentum and trend following differ as the
               former is a relative concept and the latter absolute. Combining
               both can achieve the higher return levels associated with
               momentum portfolios with much reduced volatility and drawdowns
               due to trend following. Measures based on utility of a
               representative investor reinforce the superiority of combining
               trend following with momentum strategies. These techniques help
               address the sequencing of returns issue which can be a serious
               issue for financial planning.",
  journal   = "Journal of Behavioral and Experimental Finance",
  publisher = "Elsevier",
  volume    =  9,
  number    = "Supplement C",
  pages     = "63--80",
  month     =  mar,
  year      =  2016,
  keywords  = "Behavioural biases; Trend following; Asset allocation"
}

@ARTICLE{Clare2014-cr,
  title     = "Trend following, risk parity and momentum in commodity futures",
  author    = "Clare, Andrew and Seaton, James and Smith, Peter N and Thomas,
               Stephen",
  abstract  = "Abstract We show that combining momentum and trend following
               strategies for individual commodity futures can lead to
               portfolios which offer attractive risk adjusted returns which
               are superior to simple momentum strategies; when we expose these
               returns to a wide array of sources of systematic risk we find
               that robust alpha survives. Experimenting with risk parity
               portfolio weightings has limited impact on our results though in
               particular is beneficial to long--short strategies; the marginal
               impact of applying trend following methods far outweighs
               momentum and risk parity adjustments in terms of risk-adjusted
               returns and limiting downside risk. Overall this leads to an
               attractive strategy for investing in commodity futures and
               emphasises the importance of trend following as an investment
               strategy in the commodity futures context.",
  journal   = "International Review of Financial Analysis",
  publisher = "Elsevier",
  volume    =  31,
  number    = "Supplement C",
  pages     = "1--12",
  month     =  jan,
  year      =  2014,
  keywords  = "Trend following; Momentum; Risk parity; Equally-weighted;
               Portfolios; Commodity futures"
}

@ARTICLE{Christoffersen2006-pk,
  title    = "{Direction-of-Change} Forecasts Based on Conditional Variance,
              Skewness and Kurtosis Dynamics: International Evidence",
  author   = "Christoffersen, Peter and Diebold, Francis and Mariano, Roberto
              and Tay, Anthony and Tse, Yiu",
  abstract = "Recent theoretical work has revealed a direct connection between
              asset return volatility forecastability and asset return sign
              forecastability. This suggests th",
  journal  = "PIER Working Paper",
  volume   = "No. 06-016",
  month    =  feb,
  year     =  2006,
  keywords = "Volatility, variance, skewness, kurtosis, market timing, asset
              management, asset allocation, portfolio management"
}

@ARTICLE{Christoffersen2006-gf,
  title     = "Financial Asset Returns, {Direction-of-Change} Forecasting, and
               Volatility Dynamics",
  author    = "Christoffersen, Peter F and Diebold, Francis X",
  abstract  = "We consider three sets of phenomena that feature prominently in
               the financial economics literature: (1) conditional mean
               dependence (or lack thereof) in asset returns, (2) dependence
               (and hence forecastability) in asset return signs, and (3)
               dependence (and hence forecastability) in asset return
               volatilities. We show that they are very much interrelated and
               explore the relationships in detail. Among other things, we show
               that (1) volatility dependence produces sign dependence, so long
               as expected returns are nonzero, so that one should expect sign
               dependence, given the overwhelming evidence of volatility
               dependence; (2) it is statistically possible to have sign
               dependence without conditional mean dependence; (3) sign
               dependence is not likely to be found via analysis of sign
               autocorrelations, runs tests, or traditional market timing tests
               because of the special nonlinear nature of sign dependence, so
               that traditional market timing tests are best viewed as tests
               for sign dependence arising from variation in expected returns
               rather than from variation in volatility or higher moments; (4)
               sign dependence is not likely to be found in very high-frequency
               (e.g., daily) or very low-frequency (e.g., annual) returns;
               instead, it is more likely to be found at intermediate return
               horizons; and (5) the link between volatility dependence and
               sign dependence remains intact in conditionally non-Gaussian
               environments, for example, with time-varying conditional
               skewness and/or kurtosis.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  52,
  number    =  8,
  pages     = "1273--1287",
  month     =  aug,
  year      =  2006
}

@ARTICLE{Chevapatrakul2013-jf,
  title     = "Return sign forecasts based on conditional risk: Evidence from
               the {UK} stock market index",
  author    = "Chevapatrakul, Thanaset",
  abstract  = "Abstract Recent theoretical works have found a link between
               return sign forecastability and conditional volatility. This
               paper compares the predictive performance of the conditional
               country risk and the conditional residual risk in forecasting
               the direction of change in the return on the UK stock market
               index. The conditional country risk and the conditional residual
               risk are estimated using the bivariate BEKK-GARCH technique and
               the direction of change in the UK stock market index is modelled
               using the binary logit approach. Both the in-sample and the
               out-of-sample predictions suggest that, as a predictor, the
               conditional residual risk is superior to the conditional country
               risk. Our findings support the residual risk model while
               contradicting the traditional capital asset pricing model
               (CAPM). Moreover, our tactical asset allocation simulations show
               that when the conditional residual risk is used in conjunction
               with multiple-threshold trading strategies to guide the
               investment decisions, the actively managed portfolio achieves
               greater returns than the return on a buy and hold portfolio.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  37,
  number    =  7,
  pages     = "2342--2353",
  month     =  jul,
  year      =  2013,
  keywords  = "Asset pricing; Asset price volatility; Multivariate GARCH;
               Limited-dependent variable approach"
}

@BOOK{Campbell1997-fa,
  title     = "The Econometrics of Financial Markets",
  author    = "Campbell, John Y and Lo, Andrew Wen-Chuan and MacKinlay, Archie
               Craig",
  abstract  = "The past twenty years have seen an extraordinary growth in the
               use of quantitative methods in financial markets. Finance
               professionals now routinely use sophisticated statistical
               techniques in portfolio management, proprietary trading, risk
               management, financial consulting, and securities regulation.
               This graduate-level textbook is intended for PhD students,
               advanced MBA students, and industry professionals interested in
               the econometrics of financial modeling. The book covers the
               entire spectrum of empirical finance, including: the
               predictability of asset returns, tests of the Random Walk
               Hypothesis, the microstructure of securities markets, event
               analysis, the Capital Asset Pricing Model and the Arbitrage
               Pricing Theory, the term structure of interest rates, dynamic
               models of economic equilibrium, and nonlinear financial models
               such as ARCH, neural networks, statistical fractals, and chaos
               theory. Each chapter develops statistical techniques within the
               context of a particular financial application. This exciting new
               text contains a unique and accessible combination of theory and
               practice, bringing state-of-the-art statistical techniques to
               the forefront of financial applications. Each chapter also
               includes a discussion of recent empirical evidence, for example,
               the rejection of the Random Walk Hypothesis, as well as problems
               designed to help readers incorporate what they have read into
               their own applications",
  publisher = "Princeton University Press",
  year      =  1997,
  language  = "en"
}

@ARTICLE{Brock1992-gd,
  title     = "Simple Technical Trading Rules and the Stochastic Properties of
               Stock Returns",
  author    = "Brock, William and Lakonishok, Josef and LeBARON, Blake",
  abstract  = "This paper tests two of the simplest and most popular trading
               rules---moving average and trading range break---by utilizing
               the Dow Jones Index from 1897 to 1986. Standard statistical
               analysis is extended through the use of bootstrap techniques.
               Overall, our results provide strong support for the technical
               strategies. The returns obtained from these strategies are not
               consistent with four popular null models: the random walk, the
               AR(1), the GARCH-M, and the Exponential GARCH. Buy signals
               consistently generate higher returns than sell signals, and
               further, the returns following buy signals are less volatile
               than returns following sell signals, and further, the returns
               following buy signals are less volatile than returns following
               sell signals. Moreover, returns following sell signals are
               negative, which is not easily explained by any of the currently
               existing equilibrium models.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  47,
  number    =  5,
  pages     = "1731--1764",
  month     =  dec,
  year      =  1992
}

@ARTICLE{Bejerano2004-yq,
  title     = "Algorithms for variable length Markov chain modeling",
  author    = "Bejerano, Gill",
  abstract  = "UNLABELLED: We present a general purpose implementation of
               variable length Markov models. Contrary to fixed order Markov
               models, these models are not restricted to a predefined uniform
               depth. Rather, by examining the training data, a model is
               constructed that fits higher order Markov dependencies where
               such contexts exist, while using lower order Markov dependencies
               elsewhere. As both theoretical and experimental results show,
               these models are capable of capturing rich signals from a modest
               amount of training data, without the use of hidden states.
               AVAILABILITY: The source code is freely available at
               http://www.soe.ucsc.edu/~jill/src/",
  journal   = "Bioinformatics",
  publisher = "Oxford University Press",
  volume    =  20,
  number    =  5,
  pages     = "788--789",
  month     =  mar,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Fama1998-ta,
  title     = "Value versus Growth: The International Evidence",
  author    = "Fama, Eugene F and French, Kenneth R",
  abstract  = "Value stocks have higher returns than growth stocks in markets
               around the world. For the period 1975 through 1995, the
               difference between the average returns on global portfolios of
               high and low book-to-market stocks is 7.68 percent per year, and
               value stocks outperform growth stocks in twelve of thirteen
               major markets. An international capital asset pricing model
               cannot explain the value premium, but a two-factor model that
               includes a risk factor for relative distress captures the value
               premium in international returns.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishers, Inc.",
  volume    =  53,
  number    =  6,
  pages     = "1975--1999",
  month     =  dec,
  year      =  1998
}

@ARTICLE{Fama1996-vi,
  title     = "Multifactor Explanations of Asset Pricing Anomalies",
  author    = "Fama, Eugene F and French, Kenneth R",
  abstract  = "Previous work shows that average returns on common stocks are
               related to firm characteristics like size, earnings/price, cash
               flow/price, book-to-market equity, past sales growth, long-term
               past return, and short-term past return. Because these patterns
               in average returns apparently are not explained by the CAPM,
               they are called anomalies. We find that, except for the
               continuation of short-term returns, the anomalies largely
               disappear in a three-factor model. Our results are consistent
               with rational ICAPM or APT asset pricing, but we also consider
               irrational pricing and data problems as possible explanations.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  51,
  number    =  1,
  pages     = "55--84",
  month     =  mar,
  year      =  1996
}

@ARTICLE{Breen1989-fc,
  title     = "Economic Significance of Predictable Variations in Stock Index
               Returns",
  author    = "Breen, William and Glosten, Lawrence R and Jagannathan, Ravi",
  abstract  = "Knowledge of the one-month interest rate is useful in
               forecasting the sign as well as the variance of the excess
               return on stocks. The services of a portfolio manager who makes
               use of the forecasting model to shift funds between bills and
               stocks would be worth an annual management fee of 2\% of the
               value of the assets managed. During 1954:4 to 1986:12, the
               variance of monthly returns on the managed portfolio was about
               60\% of the variance of the returns on the value weighted index,
               whereas the average return was two basis points higher.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Ltd",
  volume    =  44,
  number    =  5,
  pages     = "1177--1189",
  month     =  dec,
  year      =  1989
}

@ARTICLE{Boons2015-pc,
  title    = "{Basis-Momentum} in the Futures Curve and Volatility Risk",
  author   = "Boons, Martijn and Prado, Melissa Porras",
  abstract = "We introduce a commodity-return predictor related to slope and
              curvature of the futures curve: basis-momentum. Basis-momentum
              strongly outperforms benchmark cha",
  journal  = "Available at SSRN: https://ssrn. com/abstract=2587784",
  month    =  apr,
  year     =  2015,
  keywords = "Basis-momentum, term structure of commodity futures returns,
              maturity-specific price pressure, commodity factor pricing model,
              volatility risk"
}

@ARTICLE{Bondt1985-yw,
  title     = "Does the stock market overreact?",
  author    = "Bondt, Wfm and Thaler - The Journal of finance, R and {1985}",
  abstract  = "ABSTRACT Research in experimental psychology suggests that, in
               violation of Bayes' rule, most people tend to ``overreact'' to
               unexpected and dramatic news events. This study of market
               efficiency investigates whether such behavior affects stock
               prices. The empirical",
  journal   = "Wiley Online Library",
  publisher = "Wiley Online Library",
  volume    =  40,
  number    =  3,
  pages     = "793--805",
  year      =  1985
}

@ARTICLE{Bird2016-mi,
  title     = "Time-series and cross-sectional momentum strategies under
               alternative implementation strategies",
  author    = "Bird, Ron and Gao, Xiaojun and Yeung, Danny",
  abstract  = "The study compares the performance of alternative
               implementations of both time-series and cross-sectional momentum
               strategies across 24 markets. We find that over our sample
               period, both types of momentum strategies generate positive
               returns under the majority of implementations evaluated but that
               time-series momentum is clearly superior. An important
               difference between the two momentum strategies is that with
               time-series momentum, the number of stocks included in the
               winner and loser portfolios vary with the state of the market.
               As a consequence, cross-sectional momentum digs deeper to select
               winning stocks when markets are weak and deeper to select losing
               stocks when markets are strong. As the information in the
               momentum signals is concentrated in the tails of the return
               distribution, it is not that surprising that momentum is best
               implemented using time-series momentum.",
  journal   = "Australian Journal of Management",
  publisher = "SAGE Publications Ltd",
  volume    =  42,
  number    =  2,
  pages     = "230--251",
  month     =  mar,
  year      =  2016
}

@ARTICLE{Bianchi2015-uy,
  title     = "Combining momentum with reversal in commodity futures",
  author    = "Bianchi, Robert J and Drew, Michael E and Fan, John Hua",
  abstract  = "Abstract This paper examines profitable trading strategies that
               jointly exploit momentum and reversal signals in commodity
               futures. While the single-sort momentum strategies returns
               11.14\% per annum, on average, a consistent reversal pattern of
               momentum profits is pronounced from 12 to 30months after
               portfolio formation. Combining the observed reversal pattern
               with the momentum signal, our double-sort strategy returns
               20.24\% per annum, which significantly outperforms single-sort
               strategies. The proposed strategy is robust to seasonality
               effects and sample adjustments in commodity futures. The
               profitability of the double-sort strategy cannot be explained by
               standard risk factors, term structure, market volatility,
               investor sentiment, data-mining or transaction costs, but
               appears to be related to global funding liquidity. As a
               consequence, the double-sort strategy in commodity futures may
               be employed as a portfolio diversification tool.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  59,
  number    = "Supplement C",
  pages     = "423--444",
  month     =  oct,
  year      =  2015,
  keywords  = "Commodity futures; Momentum; Reversal; Double-sort strategy;
               Seasonality; Funding liquidity"
}

@ARTICLE{Bessembinder1998-dj,
  title     = "Market Efficiency and the Returns to Technical Analysis",
  author    = "Bessembinder, Hendrik and Chan, Kalok",
  abstract  = "We further investigate and provide interpretation for the
               intriguing Brock, Lakonishok, and LeBaron (1992) finding that
               simple forms of technical analysis contain significant forecast
               power for US equity index returns. We document that the forecast
               ability is partially, but not solely, attributable to return
               measurement errors arising from nonsynchronous trading. We argue
               that the evidence supporting technical forecast power need not
               be inconsistent with market efficiency. ``Break-even'' one-way
               trading costs are computed to be 0.39\% for the full sample and
               0.22\% since 1975, which are small compared to recent estimates
               of actual trading costs.",
  journal   = "Financial Management",
  publisher = "[Financial Management Association International, Wiley]",
  volume    =  27,
  number    =  2,
  pages     = "5--17",
  year      =  1998
}

@ARTICLE{Barberis1998-yu,
  title     = "A model of investor sentiment",
  author    = "Barberis, N and Shleifer, A and Vishny - Journal of financial
               economics, R and {1998}",
  abstract  = "Recent empirical research in finance has uncovered two families
               of pervasive regularities: underreaction of stock prices to news
               such as earnings announcements, and overreaction of stock prices
               to a series of good or bad news. In this paper, we present a
               parsimonious model",
  journal   = "Elsevier Oceanogr. Ser.",
  publisher = "Elsevier",
  volume    =  49,
  number    =  3,
  pages     = "307--343",
  year      =  1998
}

@ARTICLE{Baltas2015-xk,
  title     = "Demystifying time-series momentum strategies: volatility
               estimators, trading rules and pairwise correlations",
  author    = "Baltas, N and Kosowski, R",
  abstract  = "Abstract: Motivated by studies of the impact of frictions on
               asset prices, we examine the effect of key components of
               time-series momentum strategies on their turnover and
               performance from 1974 until 2013. We show that more efficient
               volatility estimation and price trend detection significantly
               reduce portfolio turnover and therefore rebalancing costs. The
               poor performance of time-series momentum strategies during the
               post-2008 period is explained ...",
  journal   = "Working Paper, Imperial College Business School",
  publisher = "papers.ssrn.com",
  year      =  2015
}

@ARTICLE{Avramov2006-mz,
  title     = "Asset Pricing Models and Financial Market Anomalies",
  author    = "Avramov, Doron and Chordia, Tarun",
  abstract  = "This article develops a framework that applies to single
               securities to test whether asset pricing models can explain the
               size, value, and momentum anomalies. Stock level beta is allowed
               to vary with firm-level size and book-to-market as well as with
               macroeconomic variables. With constant beta, none of the models
               examined capture any of the market anomalies. When beta is
               allowed to vary, the size and value effects are often explained,
               but the explanatory power of past return remains robust. The
               past return effect is captured by model mispricing that varies
               with macroeconomic variables.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  19,
  number    =  3,
  pages     = "1001--1040",
  month     =  oct,
  year      =  2006
}

@ARTICLE{Asness1997-fv,
  title     = "Parallels Between the {Cross-Sectional} Predictability of Stock
               and Country Returns",
  author    = "Asness, Clifford S and Liew, John M and Stevens, Ross L",
  journal   = "Portfolio Management",
  publisher = "Institutional Investor Journals",
  volume    =  23,
  number    =  3,
  pages     = "79--87",
  month     =  jan,
  year      =  1997
}

@ARTICLE{Anily1987-jt,
  title     = "Simulated annealing methods with general acceptance
               probabilities",
  author    = "Anily, S and Federgruen, A",
  abstract  = "Simulated annealing methods with general acceptance
               probabilities - Volume 24 Issue 3 - S. Anily, A. Federgruen",
  journal   = "J. Appl. Probab.",
  publisher = "Cambridge University Press",
  volume    =  24,
  number    =  3,
  pages     = "657--667",
  month     =  sep,
  year      =  1987,
  keywords  = "PROBABILISTIC PERFORMANCE ANALYSIS; CONVERGENCE CONDITIONS;
               GENERAL ACCEPTANCE PROBABILITIES"
}

@ARTICLE{Anatolyev2010-vi,
  title     = "Modeling Financial Return Dynamics via Decomposition",
  author    = "Anatolyev, Stanislav and Gospodinov, Nikolay",
  abstract  = "While the predictability of excess stock returns is detected by
               traditional predictive regressions as statistically small, the
               direction-of-change and volatility of returns exhibit a
               substantially larger degree of dependence over time. We
               capitalize on this observation and decompose the returns into a
               product of sign and absolute value components whose joint
               distribution is obtained by combining a multiplicative error
               model for absolute values, a dynamic binary choice model for
               signs, and a copula for their interaction. Our decomposition
               model is able to incorporate important nonlinearities in excess
               return dynamics that cannot be captured in the standard
               predictive regression setup. The empirical analysis of U.S.
               stock return data shows statistically and economically
               significant forecasting gains of the decomposition model over
               the conventional predictive regression.",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  28,
  number    =  2,
  pages     = "232--245",
  month     =  apr,
  year      =  2010
}

@ARTICLE{Altman1994-il,
  title     = "Corporate distress diagnosis: Comparisons using linear
               discriminant analysis and neural networks (the Italian
               experience)",
  author    = "Altman, Edward I and Marco, Giancarlo and Varetto, Franco",
  abstract  = "Abstract This study analyzes the comparison between traditional
               statistical methodologies for distress classification and
               prediction, i.e., linear discriminant (LDA) or logit analyses,
               with an artificial intelligence algorithm known as neural
               networks (NN). Analyzing well over 1,000 healthy, vulnerable and
               unsound industrial Italian firms from 1982--1992, this study was
               carried out at the Centrale dei Bilanci in Turin, Italy and is
               now being tested in actual diagnostic situations. The results
               are part of a larger effort involving separate models for
               industrial, retailing/trading and construction firms. The
               results indicate a balanced degree of accuracy and other
               beneficial characteristics between LDA and NN. We are
               particularly careful to point out the problems of the
               `black-box' NN systems, including illogical weightings of the
               indicators and overfitting in the training stage both of which
               negatively impacts predictive accuracy. Both types of diagnoslic
               techniques displayed acceptable, over 90\%, classificalion and
               holdoul sample accuracy and the study concludes that there
               certainly should be further studies and tests using the two
               lechniques and suggests a combined approach for predictive
               reinforcement.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  18,
  number    =  3,
  pages     = "505--529",
  month     =  may,
  year      =  1994,
  keywords  = "Distress diagnosis; Discriminant models; Neural nelworks;
               Corporate bankruptcy risk; Financial ratio analysis"
}

@ARTICLE{Ahn2003-ti,
  title     = "Risk Adjustment and Trading Strategies",
  author    = "Ahn, Dong-Hyun and Conrad, Jennifer and Dittmar, Robert F",
  abstract  = "We assess the profitability of momentum strategies using a
               stochastic discount factor approach. In unconditional tests,
               approximately half of the strategies' profitability is
               explained. In conditional tests we see a further slight decline
               in profits. We argue that the risk of these strategies should be
               increasing in the market risk premium. Empirically, while their
               risk measures estimated relative to the stochastic discount
               factor behave as predicted, market betas do not; thus capital
               asset pricing model (CAPM)-like benchmarks may lead to incorrect
               inferences. Given that our nonparametric risk adjustment
               explains roughly half of momentum strategy profits, we cannot
               rule out the possibility of residual mispricing.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  16,
  number    =  2,
  pages     = "459--485",
  month     =  apr,
  year      =  2003
}

@ARTICLE{Leuz2016-eg,
  title    = "The Economics of Disclosure and Financial Reporting Regulation:
              Evidence and Suggestions for Future Research",
  author   = "Leuz, Christian and Wysocki, Peter D",
  abstract = "This paper discusses the empirical literature on the economic
              consequences of disclosure and financial reporting regulation,
              drawing on U.S. and international evidence. Given the policy
              relevance of research on regulation, we highlight the challenges
              with (1) quantifying regulatory costs and benefits, (2) measuring
              disclosure and reporting outcomes, and (3) drawing causal
              inferences from regulatory studies. Next, we discuss empirical
              studies that link disclosure and reporting activities to
              firm-specific and market-wide economic outcomes. Understanding
              these links is important when evaluating regulation. We then
              synthesize the empirical evidence on the economic effects of
              disclosure regulation and reporting standards, including the
              evidence on International Financial Reporting Standards (IFRS)
              adoption. Several important conclusions emerge. We generally lack
              evidence on market-wide effects and externalities from
              regulation, yet such evidence is central to the economic
              justification of regulation. Moreover, evidence on causal effects
              of disclosure and reporting regulation is still relatively rare.
              We also lack evidence on the real effects of such regulation.
              These limitations provide many research opportunities. We
              conclude with several specific suggestions for future research.",
  journal  = "Journal of Accounting Research",
  volume   =  54,
  number   =  2,
  pages    = "525--622",
  month    =  may,
  year     =  2016,
  keywords = "D78; D82; G14; G18; G30; G38; K22; K42; M41; M42; transparency;
              regulation; accounting standards; capital markets; institutional
              economics; international accounting; disclosure; IFRS; political
              economy; cost--benefit analysis; real effects"
}

@ARTICLE{Berger1997-uo,
  title    = "Efficiency of financial institutions: International survey and
              directions for future research",
  author   = "Berger, Allen N and Humphrey, David B",
  abstract = "Abstract This paper surveys 130 studies that apply frontier
              efficiency analysis to financial institutions in 21 countries.
              The primary goals are to summarize and critically review
              empirical estimates of financial institution efficiency and to
              attempt to arrive at a consensus view. We find that the various
              efficiency methods do not necessarily yield consistent results
              and suggest some ways that these methods might be improved to
              bring about findings that are more consistent, accurate, and
              useful. Secondary goals are to address the implications of
              efficiency results for financial institutions in the areas of
              government policy, research, and managerial performance. Areas
              needing additional research are also outlined.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  98,
  number   =  2,
  pages    = "175--212",
  month    =  apr,
  year     =  1997
}

@BOOK{Fare2006-uw,
  title     = "New Directions: Efficiency and Productivity",
  author    = "F{\"a}re, Rolf and Grosskopf, Shawna",
  abstract  = "The format of this monograph is three essays, which we arrived
               at after spending a year writing over one hundred pages of what
               we even tually realized was a tedious reworking of old material.
               So we started over determined to write something new. At first
               we thought this approach might not work as a coherent mono
               graph, which is why we chose the essay format rather than
               chapters. As it turns out, there is a common thread---namely the
               directional distance function, which also gave us our title. As
               you shall see, the directional distance function includes
               traditional distance functions and efficiency measures as
               special cases providing a unifying framework for existing
               productivity and efficiency measures. It is also flexible enough
               to open up new areas in productivity and efficiency analysis
               such as environmen tal and aggregation issues. That we did not
               see this earlier is humbling; a student at a recent conference
               raised his hand and asked 'Why didn't you start with the
               directional distance function in the first place? In deed. This
               manuscript is intended to make up for our earlier oversights.
               This monograph contains papers coauthored with Wen-Fu Lee and
               Osman Zaim and one paper written by two former students,
               Hiroyuki Fukuyama and Bill Weber. We thank them for their
               contributions. An other former student, Jim Logan (Logi) read
               and critiqued the manu script for which we are grateful.",
  publisher = "Springer Science \& Business Media",
  month     =  mar,
  year      =  2006,
  language  = "en"
}

@ARTICLE{Liu2017-sx,
  title     = "Time series reversal of financial assets",
  author    = "Liu, J and Papailias, F",
  abstract  = "Abstract A reversal pattern in the time series context from 12
               to 24 months after the formation of trend following signals is
               observed. By decomposing the trend following strategy returns
               according to their performance, we find that instruments with
               sell signals in the trend following portfolio (ie`` losers'')
               contribute to this type of reversal, even if their profits are
               not realised. The instruments with buy signals in the trend
               following portfolio (ie`` winners'') ...",
  journal   = "Available at SSRN: https://ssrn. com/abstract=2971875",
  publisher = "papers.ssrn.com",
  year      =  2017
}

@ARTICLE{Bernardi2015-ii,
  title    = "The Model Confidence Set Package for {R}",
  author   = "Bernardi, Mauro and Catania, Leopoldo",
  abstract = "This paper presents the R package MCS which implements the Model
              Confidence Set (MCS) procedure recently developed by Hansen,
              Lunde, and Nason (2011). The Hanse",
  journal  = "Available at SSRN: https://ssrn. com/abstract=2692118",
  month    =  nov,
  year     =  2015,
  keywords = "Hypothesis testing, Model Confidence Set, Value-at-Risk, VaR
              combination, ARCH-Models, R-CRAN"
}

@ARTICLE{Shleifer2005-fc,
  title     = "Understanding Regulation",
  author    = "Shleifer, Andrei",
  journal   = "European Financial Management",
  publisher = "Blackwell Publishing Ltd/Inc.",
  volume    =  11,
  number    =  4,
  pages     = "439--451",
  month     =  sep,
  year      =  2005
}

@BOOK{Van_Buuren2012-wd,
  title     = "Flexible Imputation of Missing Data",
  author    = "van Buuren, Stef",
  abstract  = "Missing data form a problem in every scientific discipline, yet
               the techniques required to handle them are complicated and often
               lacking. One of the great ideas in statistical
               science---multiple imputation---fills gaps in the data with
               plausible values, the uncertainty of which is coded in the data
               itself. It also solves other problems, many of which are missing
               data problems in disguise. Flexible Imputation of Missing Data
               is supported by many examples using real data taken from the
               author's vast experience of collaborative research, and presents
               a practical guide for handling missing data under the framework
               of multiple imputation. Furthermore, detailed guidance of
               implementation in R using the author's package MICE is included
               throughout the book. Assuming familiarity with basic statistical
               concepts and multivariate methods, Flexible Imputation of
               Missing Data is intended for two audiences: (Bio)statisticians,
               epidemiologists, and methodologists in the social and health
               sciences Substantive researchers who do not call themselves
               statisticians, but who possess the necessary skills to
               understand the principles and to follow the recipes This
               graduate-tested book avoids mathematical and technical details
               as much as possible: formulas are accompanied by a verbal
               statement that explains the formula in layperson terms. Readers
               less concerned with the theoretical underpinnings will be able
               to pick up the general idea, and technical material is available
               for those who desire deeper understanding. The analyses can be
               replicated in R using a dedicated package developed by the
               author.",
  publisher = "CRC Press",
  month     =  mar,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Fare2005-cc,
  title    = "Characteristics of a polluting technology: theory and practice",
  author   = "F{\"a}re, Rolf and Grosskopf, Shawna and Noh, Dong-Woon and
              Weber, William",
  abstract = "Abstract We use a quadratic directional output distance function
              to measure the technical efficiency of 209 electric utilities
              that produce electricity and a polluting byproduct, SO2 before
              (1993) and after (1997) implementation of Phase I regulations of
              the acid rain program. We also estimate the shadow price of SO2
              and the output elasticity of substitution between electricity and
              SO2 and find that electric utilities can reduce emissions by
              4000--6000 tons by reducing inefficiency, the shadow price of SO2
              increases from 1993 to 1997, and that the elasticity of
              substitution indicates that the ability to trade reductions in
              electricity production for reductions in SO2 emissions became
              more difficult between 1993 and 1997.",
  journal  = "J. Econom.",
  volume   =  126,
  number   =  2,
  pages    = "469--492",
  month    =  jun,
  year     =  2005,
  keywords = "Shadowpricing; Directional distance function"
}

@TECHREPORT{Forsund2008-mk,
  title       = "Good modelling of bad outputs: pollution and multiple-output
                 production",
  author      = "F{\o}rsund, Finn R",
  institution = "Memorandum//Department of Economics, University of Oslo",
  year        =  2008
}

@ARTICLE{Picazo-Tadeo2009-eh,
  title    = "Environmental externalities and efficiency measurement",
  author   = "Picazo-Tadeo, Andr{\'e}s J and Prior, Diego",
  abstract = "Production of desirable outputs often produces by-products that
              have harmful effects on the environment. This paper investigates
              technologies where the biggest good output producer is not the
              greatest polluter, i.e. technologies located on the
              downward-sloping segment of the frontier depicted in F{\"a}re et
              al. (1989). Directional distance functions and Data Envelopment
              Analysis techniques are used to define an algorithm that allows
              them to be identified empirically. Furthermore, we show that in
              such situations producers can contribute social goods, i.e.
              reducing polluting wastes, without limiting their capacity to
              maximise production of marketable output. Finally, we illustrate
              our methodology with an empirical application to a sample of
              Spanish ceramic tile producers.",
  journal  = "J. Environ. Manage.",
  volume   =  90,
  number   =  11,
  pages    = "3332--3339",
  month    =  aug,
  year     =  2009,
  language = "en"
}

@ARTICLE{Picazo-Tadeo2005-at,
  title    = "Directional distance functions and environmental regulation",
  author   = "Picazo-Tadeo, Andr{\'e}s J and Reig-Mart{\'\i}nez, Ernest and
              Hern{\'a}ndez-Sancho, Francesc",
  abstract = "Abstract In this paper we use directional technology distance
              functions to evaluate the impact of environmental regulations on
              firms' performance. Following F{\"a}re et al. [F{\"a}re, R.,
              Grosskopf, S., Lovell, C.A.K., Pasurka, C., 1989. Multilateral
              productivity comparisons when some outputs are undesirable: a
              nonparametric approach. The Review of Economics and Statistics
              71, 90--98.], we construct an index that measures opportunity
              costs for individual firms arising from regulations that prevent
              free disposal of wastes. The methodology is applied to a sample
              of Spanish producers of ceramic pavements. We assume that firms
              maximise desirable output simultaneously reducing inputs, with no
              change in the production of bad outputs. Our results show that
              when firms face environmental rules preventing free disposal of
              bads, their potential to increase desirable output by behaving
              efficiently is largely affected. We also find that large firms
              show smaller regulation costs.",
  journal  = "Res. Energy Econ.",
  volume   =  27,
  number   =  2,
  pages    = "131--142",
  month    =  jun,
  year     =  2005,
  keywords = "Directional technology distance functions; Environmental
              regulation; Efficiency"
}

@MISC{Faure_undated-vm,
  title   = "Environmental Regulation",
  author  = "Faure, Michael G",
  journal = "Regulation and Economics"
}

@ARTICLE{Malikov2015-ye,
  title     = "Bayesian Approach to Disentangling Technical and Environmental
               Productivity",
  author    = "Malikov, Emir and Kumbhakar, Subal C and Tsionas, Efthymios G",
  abstract  = "This paper models the firm's production process as a system of
               simultaneous technologies for desirable and undesirable outputs.
               Desirable outputs are produced by transforming inputs via the
               conventional transformation function, whereas (consistent with
               the material balance condition) undesirable outputs are
               by-produced via the so-called ``residual generation
               technology''. By separating the production of undesirable
               outputs from that of desirable outputs, not only do we ensure
               that undesirable outputs are not modeled as inputs and thus
               satisfy costly disposability, but we are also able to
               differentiate between the traditional
               (desirable-output-oriented) technical productivity and the
               undesirable-output-oriented environmental, or so-called
               ``green'', productivity. To measure the latter, we derive a
               Solow-type Divisia environmental productivity index which,
               unlike conventional productivity indices, allows crediting the
               ceteris paribus reduction in undesirable outputs. Our index also
               provides a meaningful way to decompose environmental
               productivity into environmental technological and efficiency
               changes.",
  journal   = "Econometrics",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  3,
  number    =  2,
  pages     = "443--465",
  month     =  jun,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Tsionas2015-nr,
  title     = "Estimation of Input Distance Functions: A System Approach",
  author    = "Tsionas, Efthymios G and Kumbhakar, Subal C and Malikov, Emir",
  abstract  = "This article offers a methodology to address the endogeneity of
               inputs in the input distance function (IDF) formulation of the
               production processes. We propose to tackle endogenous input
               ratios appearing in the normalized IDF by considering a flexible
               (simultaneous) system of the IDF and the first-order conditions
               from the firm's cost minimization problem. Our model can
               accommodate both technical and (input) allocative inefficiencies
               among firms. We also present the algorithm for quantifying the
               cost of allocative inefficiency. We showcase our
               cost-system-based model by applying it to study the production
               of Norwegian dairy farms during the 1991--2008 period. Among
               other things, we find both an economically and statistically
               significant improvement in the levels of technical efficiency
               among dairy farms associated with the 1997 quota scheme change,
               which a more conventional single-equation stochastic frontier
               model appears to be unable to detect.",
  journal   = "Am. J. Agric. Econ.",
  publisher = "Oxford University Press",
  volume    =  97,
  number    =  5,
  pages     = "1478--1493",
  month     =  oct,
  year      =  2015
}

@ARTICLE{Wanke2015-to,
  title    = "Financial distress drivers in Brazilian banks: A dynamic slacks
              approach",
  author   = "Wanke, Peter and Barros, Carlos P and Faria, Jo{\~a}o R",
  abstract = "Abstract This study applies the Dynamic Slacks Based Model (DSBM)
              developed by Tone and Tsutsui (2010) in order to assess the
              evolution of input saving/output increasing potentials in major
              Brazilian Banks from 1996 to 2011. We propose that these
              potentials or slacks can be used as proxies for an eventual
              financial distress situation in the future. The main research
              objective is to determine whether or not different
              characteristics of bank type -- related to ownership, size, and
              merger and acquisition processes -- are significantly related to
              inefficiency levels and, by extension, to an eventual financial
              distress situation, since higher inefficiency levels also imply
              lower input saving/output decreasing potentials. Based on a
              balanced panel model, secondary data from Economatica were
              collected and analyzed. Results indicate higher inefficiency
              levels and slacks in small public and national banks. Policy
              implications are also addressed.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  240,
  number   =  1,
  pages    = "258--268",
  month    =  jan,
  year     =  2015,
  keywords = "Efficiency; Banks; Brazil; Dynamic DEA; Financial distress"
}

@ARTICLE{Weill2009-pb,
  title    = "Convergence in banking efficiency across European countries",
  author   = "Weill, Laurent",
  abstract = "Abstract Our paper aims to check whether financial integration
              has taken place on the EU banking markets, by investigating the
              convergence in banking efficiency for European countries between
              1994 and 2005. We provide evidence of cross-country differences
              in cost efficiency and of an improvement in cost efficiency for
              all EU countries. $\beta$ and $\sigma$ convergence tests for
              panel data show a process in convergence in cost efficiency
              between EU countries. Robustness checks with alternative
              specifications confirm these findings. These results support the
              view that financial integration has taken place on the EU banking
              markets in the recent years.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  19,
  number   =  5,
  pages    = "818--833",
  month    =  dec,
  year     =  2009,
  keywords = "Banking; Convergence; Efficiency; European integration;
              Stochastic frontier approach"
}

@ARTICLE{Casu2010-ja,
  title    = "Integration and efficiency convergence in {EU} banking markets",
  author   = "Casu, Barbara and Girardone, Claudia",
  abstract = "Abstract Evidence of financial integration and convergence are
              considered of importance in assessing the outcome of EU
              deregulation policies aimed at improving the efficiency and
              performance of banking sectors. This paper evaluates the recent
              dynamics of bank cost efficiency by means of data envelopment
              analysis (DEA). Borrowing from the growth literature, we apply
              dynamic panel data models (GMM) to the concepts of
              $\beta$-convergence and $\sigma$-convergence to assess the speed
              at which banking markets are integrating. We also employ a
              partial adjustment model to evaluate convergence towards best
              practice. Results seem to provide supporting evidence of
              convergence of efficiency levels towards an EU average.
              Nevertheless, there is no evidence of an overall improvement of
              efficiency levels towards best practice.",
  journal  = "Omega",
  volume   =  38,
  number   =  5,
  pages    = "260--267",
  month    =  oct,
  year     =  2010,
  keywords = "Efficiency; Convergence; Banking; Data envelopment analysis"
}

@ARTICLE{Bauer2014-ho,
  title    = "Are hazard models superior to traditional bankruptcy prediction
              approaches? A comprehensive test",
  author   = "Bauer, J and Agarwal, V",
  abstract = "In recent years hazard models, using both market and accounting
              information, have become state of the art in predicting firm
              bankruptcies. However, a comprehensive test comparing their
              performance against the traditional accounting-based approach or
              the contingent claims approach is missing in the literature.
              Using a complete database of UK Main listed firms between 1979
              and 2009, our Receiver Operating Characteristics (ROC) curve
              analysis shows that the hazard models are superior to the
              alternatives. Further, our information content tests demonstrate
              that the hazard models subsume all bankruptcy related information
              in the Taffler (1983) z-score model as well as in Bharath and
              Shumway (2008) contingent claims-based model. Finally, using a
              mixed regime competitive loan market with different costs of
              misclassification, the economic benefit of using the Shumway
              (2001) hazard model is clear, particularly when the performance
              is judged with return on risk weighted assets computed under
              Basel III. \copyright{} 2014 .",
  journal  = "Journal of Banking and Finance",
  volume   =  40,
  number   =  1,
  pages    = "432--442",
  year     =  2014,
  keywords = "Basel III; Credit risk; Distress risk; Hazard models; Option
              pricing"
}

@ARTICLE{King2013-gi,
  title    = "The Basel {III} Net Stable Funding Ratio and bank net interest
              margins",
  author   = "King, M R",
  abstract = "The Net Stable Funding Ratio (NSFR) is a new Basel III liquidity
              requirement designed to limit funding risk arising from maturity
              mismatches between bank assets and liabilities. This study
              explains the NSFR and estimates this ratio for banks in 15
              countries. Banks below the ratio need to increase stable sources
              of funding and to reduce assets requiring funding. The most
              cost-effective strategies to meet the NSFR are to increase
              holdings of higher-rated securities and to extend the maturity of
              wholesale funding. These changes reduce net interest margins by
              70-88 basis points on average, or around 40\% of their year-end
              2009 values. Universal banks with diversified funding sources and
              high trading assets are penalized most by the NSFR. \copyright{}
              2013 Elsevier B.V.",
  journal  = "Journal of Banking and Finance",
  volume   =  37,
  number   =  11,
  pages    = "4144--4156",
  year     =  2013,
  keywords = "Banks; Basel III; Funding risk; Liquidity; Net interest margins;
              Regulation"
}

@ARTICLE{Rossignolo2013-ng,
  title    = "Market crises and Basel capital requirements: Could Basel {III}
              have been different? Evidence from Portugal, Ireland, Greece and
              Spain ({PIGS})",
  author   = "Rossignolo, A F and {Fethi} and Shaban, M",
  abstract = "Basel III represents a crucial step in strengthening the capital
              rules underlying banking operations, aimed at reducing the
              probability and severity of a systemic crisis. Alongside two
              supplementary capital buffers, the Basel Committee of Banking
              Supervision imposed severe pressure on the Value-at-Risk based
              Internal Models Approach in order to increase. This is to
              increase the capital base by adding the stressed Value-at-Risk
              component in an effort to reduce reliance on internal models
              while keeping the Standardized Approach avenue open. However,
              even though those measures might appear theoretically correct,
              evidence gathered for long and short exposures in Portugal,
              Italy, Greece and Spain highlights several defects in Basel III.
              We emphasize that leptokurtic models, primarily those derived
              from Extreme Value Theory, should be enforced in the regulations
              given their superior performance in market crises, and that Basel
              II could have shielded against 2008 mayhem provided that
              heavy-tailed techniques had been employed. \copyright{} 2012
              Elsevier B.V.",
  journal  = "Journal of Banking and Finance",
  volume   =  37,
  number   =  5,
  pages    = "1323--1339",
  year     =  2013,
  keywords = "Basel III; Extreme value theory; Internal models approach; PIGS;
              Standardized approach; Value-at-risk"
}

@ARTICLE{Meeks2017-ip,
  title     = "Capital regulation and the macroeconomy: Empirical evidence and
               macroprudential policy",
  author    = "Meeks, R",
  abstract  = "We present new evidence on the macroeconomic effects of changes
               in microprudential bank capital requirements, using confidential
               regulatory data from the Basel I and II regimes in the United
               Kingdom. Our central result is that an increase in capital
               requirements lowered lending to firms and households, reduced
               aggregate expenditure and raised credit spreads. A financial
               accelerator effect is found to have amplified the macroeconomic
               responses to shifts in bank credit supply. Results from a
               counterfactual experiment that links capital requirements to
               house prices and mortgage spreads indicate that tighter
               macroprudential policy would have had a moderating effect on
               house price and mortgage lending growth in the early 2000s, with
               easier monetary policy acting to offset its contractionary
               effects on output. \copyright{} 2017",
  journal   = "Eur. Econ. Rev.",
  publisher = "Elsevier B.V.",
  volume    =  95,
  pages     = "125--141",
  year      =  2017,
  keywords  = "Bank capital regulation; Bank lending and the macroeconomy;
               Basel III; Housing market; Macroprudential policy; banking;
               capital market; economic policy; financial system; housing
               market; macroeconomics; mortgage lending; regulatory framework;
               United Kingdom"
}

@ARTICLE{Chiaramonte2017-qg,
  title     = "Capital and liquidity ratios and financial distress. Evidence
               from the European banking industry",
  author    = "Chiaramonte, L and Casu, B",
  abstract  = "Using a large bank-level dataset, we test the relevance of both
               structural liquidity and capital ratios, as defined in Basel
               III, on banks' probability of failure. To include all relevant
               episodes of bank failure and distress (F\&D) occurring in the
               EU-28 member states over the past decade, we develop a broad
               indicator that includes information not only on bankruptcies,
               liquidations, under receivership and dissolved banks, but also
               accounts for state interventions, mergers in distress and EBA
               stress test results. Estimates from several versions of the
               logistic probability model indicate that the likelihood of
               failure and distress decreases with increased liquidity
               holdings, while capital ratios are significant only for large
               banks. Our results provide support for Basel III's initiatives
               on structural liquidity and for the increased regulatory focus
               on large and systemically important banks. \copyright{} 2016
               Elsevier Ltd",
  journal   = "British Accounting Review",
  publisher = "Academic Press",
  volume    =  49,
  number    =  2,
  pages     = "138--161",
  year      =  2017,
  keywords  = "Bank capital; Bank failure and distress; Basel III; Financial
               crises; Structural liquidity"
}

@ARTICLE{Ippolito2016-ov,
  title     = "Double bank runs and liquidity risk management",
  author    = "Ippolito, F and Peydr{\'o}, J-L and Polo, A and Sette, E",
  abstract  = "By providing liquidity to depositors and credit-line borrowers,
               banks can be exposed to double-runs on assets and liabilities.
               For identification, we exploit the 2007 freeze of the European
               interbank market and the Italian Credit Register. After the
               shock, there are sizeable, aggregate double-runs. In the
               cross-section, credit-line drawdowns are not larger for banks
               more exposed to the interbank market; however, they are larger
               when we condition on the same firms with multiple credit lines.
               We show that, ex-ante, more exposed banks actively manage their
               liquidity risk by granting fewer credit lines to firms that run
               more during crises. \copyright{} 2016 Elsevier Ltd",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  122,
  number    =  1,
  pages     = "135--154",
  year      =  2016,
  keywords  = "Basel III; Credit lines; Financial crisis; Liquidity risk; Runs"
}

@ARTICLE{Yan2012-ma,
  title    = "A cost-benefit analysis of Basel {III}: Some evidence from the
              {UK}",
  author   = "Yan, M and Hall, M J B and Turner, P",
  abstract = "This paper provides a long-term cost-benefit analysis for the
              United Kingdom of the Basel III capital and liquidity
              requirements proposed by the Basel Committee on Banking
              Supervision (BCBS, 2010a). We provide evidence that the Basel III
              reforms will have a significant net positive long-term effect on
              the United Kingdom economy. The estimated optimal tangible common
              equity capital ratio is 10\% of risk-weighted assets, which is
              larger than the Basel III target of 7\%. We also estimate the
              maximum net benefit when banks meet the Basel III long-term
              liquidity requirements. Our estimated permanent net benefit is
              larger than the average estimates of the BCBS. This significant
              marginal benefit suggests that UK banks need to increase their
              reliance on common equity in their capital base beyond the level
              required by Basel III as well as boosting customer deposits as a
              funding source. \copyright{} 2012 Elsevier Inc.",
  journal  = "International Review of Financial Analysis",
  volume   =  25,
  pages    = "73--82",
  year     =  2012,
  keywords = "Basel III; Cost-benefit analysis; Liquidity; Tangible common
              equity capital"
}

@ARTICLE{Awrey2013-gq,
  title    = "Toward a supply-side theory of financial innovation",
  author   = "Awrey, D",
  abstract = "Innovation. The word is evocative of ideas, products and
              processes which have somehow made the world a better place. Prior
              to the global financial crisis, many viewed financial innovation
              as unequivocally falling into this category. Underpinning this
              view was a pervasive belief in the self-correcting nature of
              markets and their consequent optimality as mechanisms for
              allocating society's resources. This belief exerted a profound
              influence on how we regulated financial markets and
              institutions.This paper examines the influence of this market
              fundamentalist thinking on the regulation of OTC derivatives
              markets in the US during the pivotal period between the enactment
              of the Commodity Futures Trading Commission Act (1974) and the
              Dodd-. Frank Wall Street Reform and Consumer Protection Act
              (2010). More specifically, it traces how the conventional
              'demand-side' view of financial innovation played an important
              role in blinding policymakers to a host of pressing regulatory
              challenges. The objective of this paper is to start us down the
              path toward a more complete theoretical account of the nature,
              sources and potential private and social welfare implications of
              financial innovation. It also aspires to move us incrementally
              toward a more constructive equilibrium between the important
              insights of financial theory and how we conceptualize and pursue
              the objectives of financial regulation. \copyright{} 2013
              Association for Comparative Economic Studies.",
  journal  = "J. Comp. Econ.",
  volume   =  41,
  number   =  2,
  pages    = "401--419",
  year     =  2013,
  keywords = "Collateral swaps; Dodd-Frank Act; ETFs; Financial innovation;
              Modigliani and Miller; OTC derivatives; Securitization;
              Structured finance; Swaps; Synthetic exchange-traded funds"
}

@MISC{Watson2007-wi,
  title    = "Gender differences in risk aversion and expected retirement
              benefits",
  author   = "Watson, John and McNaughton, Mark",
  abstract = "Women are generally considered more risk averse than men.
              Controlling for age, income, and education, this study examined
              the impact of gender on the superannuation (retirement) fund risk
              preferences of staff in the Australian university sector. The
              findings suggest that women choose more conservative investment
              strategies than men and that lower income (which affects the
              amount members contribute to their superannuation funds) is the
              primary contributor to the lower projected retirement benefits of
              women. Providing members with a choice among risk levels in their
              retirement investments should significantly benefit male and
              female retirees.",
  journal  = "Financial Analysts Journal",
  volume   =  63,
  pages    = "52--62",
  year     =  2007
}

@ARTICLE{Kumbhakar2016-bg,
  title    = "The good, the bad and the technology: Endogeneity in
              environmental production models",
  author   = "Kumbhakar, Subal C and Tsionas, Efthymios G",
  abstract = "Abstract In this paper we consider an environmental production
              process in which firms intend to produce outputs (which we label
              as desirable/good) but the production process is such that it
              automatically produces some other unintentional but inevitable
              undesirable (bad) outputs as by-products (emission of
              pollutants). Like stochastic production frontier, by-production
              technology specifies that there is a minimal amount of the
              by-product that is produced, given the quantities of inputs and
              desirable outputs. The presence of (environmental) inefficiency
              in by-production therefore means that more than this minimal
              amount of the undesirable output is produced. Similarly, the
              presence of technical inefficiency implies that, given inputs,
              less than the maximal possible amount of desirable outputs is
              produced. Alternatively, it means that more than the minimal
              amounts of inputs are used to produce a given level of desirable
              output. We use the ``by-production technology'' approach which is
              a composition of production technology of desirable outputs and
              the technology of by-products, and estimate both technical and
              environmental efficiency. Given that electricity, the good output
              in our application, is demand determined, we treat it as
              exogenous and address the endogeneity of inputs by using the
              first-order conditions of cost minimization. Some of our models
              automatically take endogeneity of bad outputs into account. We
              use an efficient Bayesian MCMC technique to estimate both good
              and bad output technologies and both types of inefficiency. We
              also compare results with some alternative models with and
              without endogeneity corrections.",
  journal  = "J. Econom.",
  volume   =  190,
  number   =  2,
  pages    = "315--327",
  month    =  feb,
  year     =  2016,
  keywords = "By-production; Endogeneity; MCMC; Technical and environmental
              efficiency"
}

@ARTICLE{Berg1992-jm,
  title     = "Malmquist Indices of Productivity Growth during the Deregulation
               of Norwegian Banking, 1980-89",
  author    = "Berg, Sigbj{\o}rn Atle and F{\o}rsund, Finn R and Jansen, Eilev
               S",
  abstract  = "Productivity growth during the deregulation of the Norwegian
               banking industry is studied within the framework of Data
               Envelopment Analysis, which explicitly allows for multiple
               outputs. Introducing Malmquist indices for productivity growth,
               total growth can be decomposed into frontier growth and change
               in each bank's distance to the frontier. Both the total growth
               index and its components can be consistently chained over time.
               We find productivity regress at the average bank prior to the
               deregulation, but rapid growth when deregulation took place.
               Deregulation also led to less dispersion of productivity levels
               within the industry.",
  journal   = "Scand. J. Econ.",
  publisher = "[Wiley, Scandinavian Journal of Economics]",
  volume    =  94,
  pages     = "S211--S228",
  year      =  1992
}

@TECHREPORT{Alogoskoufis2018-lr,
  title       = "Regulating the doom loop",
  author      = "Alogoskoufis, Spyros and Langfield, Sam",
  number      =  74,
  institution = "European Systemic Risk Board",
  year        =  2018
}

@ARTICLE{Brown1974-xb,
  title     = "Robust Tests for the Equality of Variances",
  author    = "Brown, Morton B and Forsythe, Alan B",
  abstract  = "Abstract Alternative formulations of Levene's test statistic for
               equality of variances are found to be robust under nonnormality.
               These statistics use more robust estimators of central location
               in place of the mean. They are compared with the unmodified
               Levene's statistic, a jackknife procedure, and a ?2 test
               suggested by Layard which are all found to be less robust under
               nonnormality.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  69,
  number    =  346,
  pages     = "364--367",
  month     =  jun,
  year      =  1974
}

@MISC{Valencia2012-ry,
  title   = "Systemic Banking Crises Database: An Update",
  author  = "Valencia, Fabian and Laeven, Luc",
  journal = "IMF Working Papers",
  volume  =  12,
  number  =  163,
  pages   = "1",
  year    =  2012
}

@ARTICLE{Begenau2017-su,
  title    = "Financial Regulation in a Quantitative Model of the Modern
              Banking System",
  author   = "Begenau, Juliane and Landvoigt, Tim",
  abstract = "How does the shadow banking system respond to changes in the
              capital regulation of commercial banks? We propose a tractable,
              quantitative general equilibrium mo",
  month    =  apr,
  year     =  2017,
  keywords = "Shadow Banks, Liquidity Demand, Capital Requirement, Bank
              Regulation"
}

@ARTICLE{Almamy2016-im,
  title     = "An evaluation of Altman's Z-score using cash flow ratio to
               predict corporate failure amid the recent financial crisis:
               Evidence from the {UK}",
  author    = "Almamy, J and Aston, J and Ngwa, L N",
  abstract  = "This paper investigates the extension of the Z-score model in
               predicting the health of UK companies; using discriminant
               analysis, and performance ratios to test which ratios are
               statistically significant in predicting the health of UK
               companies from 2000 to 2013. The purpose of this study is to
               contribute towards Altman's (1968) original Z-score model by
               adding a new variable. We found that, cash flow when combined
               with the original Z-score variable is highly significant in
               predicting the health of UK companies. A J-UK model was
               developed to test the health of UK companies. When compared to
               the Z-score model, the predictive power of the model was 82.9\%,
               which is consistent with Taffler's (1982) UK model. Furthermore,
               to test the predictive power of the model before, during and
               after the financial crisis period; results show that J-UK model
               had higher accuracy to predict the health of UK companies than
               the Z-score UK model. Thus, the extension of Altman's Z score
               model leads to better results and assists users such as
               researchers, managers, regulators and other practitioners to
               manage their risk profile more effectively. \copyright{} 2015
               Elsevier B.V.",
  journal   = "Journal of Corporate Finance",
  publisher = "Elsevier",
  volume    =  36,
  pages     = "278--285",
  year      =  2016,
  keywords  = "Cash flow ratio; Corporate failure; Prediction models; UK
               companies; Univariate analysis; Z-score"
}

@ARTICLE{Ling2015-bh,
  title    = "Frontiers in Time Series and Financial Econometrics: An overview",
  author   = "Ling, Shiqing and McAleer, Michael and Tong, Howell",
  abstract = "Abstract Two of the fastest growing frontiers in econometrics and
              quantitative finance are time series and financial econometrics.
              Significant theoretical contributions to financial econometrics
              have been made by experts in statistics, econometrics,
              mathematics, and time series analysis. The purpose of this
              special issue of the journal on ``Frontiers in Time Series and
              Financial Econometrics'' is to highlight several areas of
              research by leading academics in which novel methods have
              contributed significantly to time series and financial
              econometrics, including forecasting co-volatilities via factor
              models with asymmetry and long memory in realized covariance,
              prediction of L{\'e}vy-driven CARMA processes, functional index
              coefficient models with variable selection, LASSO estimation of
              threshold autoregressive models, high dimensional stochastic
              regression with latent factors, endogeneity and nonlinearity,
              sign-based portmanteau test for ARCH-type models with
              heavy-tailed innovations, toward optimal model averaging in
              regression models with time series errors, high dimensional
              dynamic stochastic copula models, a misspecification test for
              multiplicative error models of non-negative time series
              processes, sample quantile analysis for long-memory stochastic
              volatility models, testing for independence between functional
              time series, statistical inference for panel dynamic simultaneous
              equations models, specification tests of calibrated option
              pricing models, asymptotic inference in multiple-threshold double
              autoregressive models, a new hyperbolic GARCH model, intraday
              value-at-risk: an asymmetric autoregressive conditional duration
              approach, refinements in maximum likelihood inference on spatial
              autocorrelation in panel data, statistical inference of
              conditional quantiles in nonlinear time series models,
              quasi-likelihood estimation of a threshold diffusion process,
              threshold models in time series analysis --- some reflections,
              and generalized ARMA models with martingale difference errors.",
  journal  = "J. Econom.",
  volume   =  189,
  number   =  2,
  pages    = "245--250",
  month    =  dec,
  year     =  2015,
  keywords = "Time series; Financial econometrics; Threshold models;
              Conditional volatility; Stochastic volatility; Copulas;
              Conditional duration"
}

@ARTICLE{Ernkvist2015-fe,
  title    = "The double knot of technology and business-model innovation in
              the era of ferment of digital exchanges: The case of {OM}, a
              pioneer in electronic options exchanges",
  author   = "Ernkvist, Mirko",
  abstract = "Abstract The discontinuous shift to an electronic exchange system
              has been a catalyst for extensive transformation of financial
              markets during the last decades. Previous research has proposed
              that there are prohibitive regulatory barriers for de novo
              entrants with limited resources to introduce discontinuous
              technological innovations and novel business models in finance.
              We challenge this notion by studying the techno-social
              transformative process of the first decade of OM, a Swedish de
              novo entrant that introduced the world's first commercially
              successful electronic options exchange and the first for-profit
              exchange. Our analysis reveals that this entrant had a key role
              in both the technological and regulatory construction of the new
              market through a bricolage technological development process and
              behaved as a proactive corporate political entrepreneur (CPE).
              The first phase of the era of ferment in electronic exchange
              technology was characterized by an adaptive techno-social
              development context that was open for bricolage technological and
              business-model experimentation. In the second phase, these
              experimental lessons were connected to a deliberative regulatory
              process that enabled the entrant to influence the regulatory
              process. We conclude by suggesting that an entrant's dual roles
              as a bricolage technological entrepreneur and as a CPE provide an
              alternative path for resource-constrained entrants to transform
              capital markets.",
  journal  = "Technol. Forecast. Soc. Change",
  volume   =  99,
  number   = "Supplement C",
  pages    = "285--299",
  month    =  oct,
  year     =  2015,
  keywords = "Exchange; Options; Discontinuous technology; Bricolage;
              Technological entrepreneurship; Corporate political
              entrepreneurship"
}

@ARTICLE{Essendorfer2015-op,
  title    = "Creative destruction in Wall Street's technological arms race:
              Evidence from patent data",
  author   = "Essendorfer, Stephan and Diaz-Rainey, Ivan and Falta, Michael",
  abstract = "Abstract Technology and policy have transformed the market
              infrastructure of trading in capital markets and have helped
              financialize other markets, such as commodities trading. The
              associated `technological arms race' has created a new market
              ecology which has made trading cheaper and faster but more
              volatile and fragmented. This paper charts the technological
              roots of this transformation from a conventional measurement of
              innovation perspective. We do so by employing content analysis
              techniques and extracting market infrastructure patent counts
              from the USPTO (United States Patent and Trademark Office)
              database for the period January 1976 to October 2013. From the
              resulting time series and a qualitative examination of patents we
              find that (1) the number of market infrastructure patents has
              dramatically increased since 1999, as confirmed by an associated
              structural break; (2) the new market ecology has, in true
              Schumpeterian style, been associated with a new breed of firms,
              most notably software firms and historically smaller brokerage
              firms that have invested heavily in technology internally and
              through strategic acquisitions; and (3) some incumbent firms have
              responded aggressively to the new market ecology, most notably
              the Chicago Mercantile Exchange and Goldman Sachs. We conclude
              that policymakers, regulators and academics wishing to further
              investigate the technological roots of recent changes in capital
              should refer to patent data. Our principal contribution is to
              highlight that Wall Street has been actively patenting market
              infrastructure innovations in a pattern consistent with claims
              that an associated `technological arms race' started in the late
              1990s.",
  journal  = "Technol. Forecast. Soc. Change",
  volume   =  99,
  number   = "Supplement C",
  pages    = "300--316",
  month    =  oct,
  year     =  2015,
  keywords = "Capital markets; Patents; High frequency trading; Financial
              innovation"
}

@ARTICLE{Panourgias2015-ob,
  title    = "Capital markets integration: A sociotechnical study of the
              development of a cross-border securities settlement system",
  author   = "Panourgias, Nikiforos S",
  abstract = "Abstract Digital information and communications technologies
              (ICTs) are transforming capital markets. The integration of
              capital markets is seen as one such area of transformation. The
              research presented in this article studies one integration
              initiative that took shape around the proposed combination of a
              number of key European securities marketplaces through the
              development of a cross-border settlement system. Taking a
              sociotechnical approach, the research presents the positions of
              the key actants identified in relation to key controversies
              regarding the development of the settlement system and shows how
              the relations between the controversies and the positions of the
              actants involved in them evolve. By examining the role of ICTs in
              the evolution of these relations, the study seeks to illuminate
              the complex causalities between the social and technical aspects
              of cross-border capital market integration. The article argues
              that in addition to enabling the interconnecting of an expanded
              set of transacting parties, ICTs bring important cognitive
              dimensions that enable the inspiration, planning, and foresight
              necessary for both developers and market participants to
              formulate their plans, strategies, and positions vis-{\`a}-vis
              the expanded and transformed marketplace arrangements.",
  journal  = "Technol. Forecast. Soc. Change",
  volume   =  99,
  number   = "Supplement C",
  pages    = "317--338",
  month    =  oct,
  year     =  2015,
  keywords = "Securities; Markets; Marketplace; Technology; Integration;
              Globalisation; Settlement; CSDs"
}

@ARTICLE{Gerardi2010-bo,
  title     = "The Impact of Deregulation and Financial Innovation on
               Consumers: The Case of the Mortgage Market",
  author    = "Gerardi, Kristopher S and Rosen, Harvey S and Willen, Paul S",
  abstract  = "We develop a technique to assess the impact of changes in
               mortgage markets on households, exploiting an implication of the
               permanent income hypothesis: The higher a household's expected
               future income, the higher its desired consumption, ceteris
               paribus. With perfect credit markets, desired consumption
               matches actual consumption and current spending forecasts future
               income. Because credit market imperfections mute this effect,
               the extent to which house spending predicts future income
               measures the ``imperfectness'' of mortgage markets. Using
               micro-data, we find that since the early 1980s, mortgage markets
               have become less imperfect in this sense, and securitization has
               played an important role.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  65,
  number    =  1,
  pages     = "333--360",
  month     =  feb,
  year      =  2010
}

@ARTICLE{Miles2008-ch,
  title     = "Financial innovation and European housing and mortgage markets",
  author    = "Miles, David and Pillonca, Vladimir",
  abstract  = "In this paper we assess the recent history of house prices and
               of mortgage lending across Europe. We develop a simple economic
               framework to estimate the likely contributions of fundamental
               factors, such as changes in real incomes and population growth,
               to house price appreciation. We also try to quantify how much of
               price rises might have been driven by rising expectations of
               future capital gains. We estimate that this might have played a
               significant role in several countries, including Spain, Sweden,
               Belgium, and the UK. We then consider what different types of
               mortgage arrangement might become attractive in a world of
               higher house prices, analysing types of indexed mortgage that
               have advantages where prices are higher relative to incomes and
               where house prices may be volatile and cannot be assumed to
               carry on rising.",
  journal   = "Oxf Rev Econ Policy",
  publisher = "Oxford University Press",
  volume    =  24,
  number    =  1,
  pages     = "145--175",
  month     =  mar,
  year      =  2008
}

@UNPUBLISHED{Gerardi2007-yz,
  title       = "Do Households Benefit from Financial Deregulation and
                 Innovation? The Case of the Mortgage Market",
  author      = "Gerardi, Kristopher and Rosen, Harvey S and Willen, Paul",
  abstract    = "The U.S. mortgage market has experienced phenomenal change
                 over the last 35 years. This paper develops and implements a
                 technique for assessing the impact of changes in the mortgage
                 market on households. Our framework, which is based on the
                 permanent income hypothesis, that allows us to gauge the
                 importance of borrowing constraints by estimating the
                 empirical relationship between the value of a household's home
                 purchase and its future income. We find that over the past
                 several decades, housing markets have become less imperfect in
                 the sense that households are now more able to buy homes whose
                 values are consistent with their long-term income prospects.
                 One issue that has received particular attention is the role
                 that the housing Government Sponsored Enterprises (GSEs),
                 Fannie Mae and Freddie Mac, have played in improving the
                 market for housing finance. We find no evidence that the GSEs'
                 activities have contributed to this phenomenon. This is true
                 whether we look at all homebuyers, or at subsamples of the
                 population whom we might expect to benefit particularly from
                 GSE activity, such as low-income households and first-time
                 homebuyers.",
  number      =  12967,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  mar,
  year        =  2007
}

@ARTICLE{Campbell2015-zc,
  title    = "A Model of Mortgage Default",
  author   = "Campbell, John Y and Cocco, Jo{\~a}o F",
  abstract = "In this paper, we solve a dynamic model of households' mortgage
              decisions incorporating labor income, house price, inflation, and
              interest rate risk. Using a zero-profit condition for mortgage
              lenders, we solve for equilibrium mortgage rates given borrower
              characteristics and optimal decisions. The model quantifies the
              effects of adjustable versus fixed mortgage rates, loan-to-value
              ratios, and mortgage affordability measures on mortgage premia
              and default. Mortgage selection by heterogeneous borrowers helps
              explain the higher default rates on adjustable-rate mortgages
              during the recent U.S. housing downturn, and the variation in
              mortgage premia with the level of interest rates.",
  journal  = "J. Finance",
  volume   =  70,
  number   =  4,
  pages    = "1495--1554",
  month    =  aug,
  year     =  2015
}

@ARTICLE{Beck2016-lu,
  title    = "Financial innovation: The bright and the dark sides",
  author   = "Beck, Thorsten and Chen, Tao and Lin, Chen and Song, Frank M",
  abstract = "Abstract Based on data from 32 countries over the period
              1996--2010, this paper is the first to assess the relationship
              between financial innovation, on the one hand, and bank growth
              and fragility, as well as economic growth, on the other hand. We
              find that different measures of financial innovation, capturing
              both a broad concept and specific innovations, are associated
              with faster bank growth, but also higher bank fragility and worse
              bank performance during the recent crisis. These effects are
              stronger in countries with larger securities markets and more
              restrictive regulatory frameworks. In spite of these seemingly
              ambiguous findings, our evidence points to a positive net effect
              of financial innovation on economic growth: financial innovation
              is associated with higher growth in countries and industries with
              better growth opportunities.",
  journal  = "Journal of Banking \& Finance",
  volume   =  72,
  number   = "Supplement C",
  pages    = "28--51",
  month    =  nov,
  year     =  2016,
  keywords = "Financial innovation; Securitization; Bank risk taking; Finance
              and growth"
}

@ARTICLE{Iacoviello2013-gm,
  title    = "Housing and debt over the life cycle and over the business cycle",
  author   = "Iacoviello, Matteo and Pavan, Marina",
  abstract = "Abstract Housing and mortgage debt are studied in a quantitative
              general equilibrium model. The model matches wealth distribution,
              age profiles of homeownership and debt, and frequency of housing
              adjustment. Over the cycle, the model matches the cyclicality and
              volatility of housing investment, and the procyclicality of debt.
              Higher individual income risk and lower downpayments can explain
              the reduced volatility of housing investment, the reduced
              procyclicality of debt, and part of the reduced volatility of
              GDP. In an experiment that mimics the Great Recession,
              countercyclical financial conditions can account for large drops
              in housing activity and debt following large negative shocks.",
  journal  = "J. Monet. Econ.",
  volume   =  60,
  number   =  2,
  pages    = "221--238",
  month    =  mar,
  year     =  2013
}

@ARTICLE{Campbell2012-hf,
  title     = "Mortgage market design",
  author    = "Campbell, John Y",
  journal   = "Review of finance",
  publisher = "Oxford University Press",
  volume    =  17,
  number    =  1,
  pages     = "1--33",
  year      =  2012
}

@UNPUBLISHED{Justiniano2015-lj,
  title       = "Credit Supply and the Housing Boom",
  author      = "Justiniano, Alejandro and Primiceri, Giorgio E and Tambalotti,
                 Andrea",
  abstract    = "The housing boom that preceded the Great Recession was due to
                 an increase in credit supply driven by looser lending
                 constraints in the mortgage market. This view on the
                 fundamental drivers of the boom is consistent with four
                 empirical observations: the unprecedented rise in home prices
                 and household debt, the stability of debt relative to house
                 values, and the fall in mortgage rates. These facts are
                 difficult to reconcile with the popular view that attributes
                 the housing boom to looser borrowing constraints associated
                 with lower collateral requirements. In fact, a slackening of
                 collateral constraints at the peak of the lending cycle
                 triggers a fall in home prices in our framework, providing a
                 novel perspective on the possible origins of the bust.",
  number      =  20874,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  jan,
  year        =  2015
}

@ARTICLE{Fisher1936-au,
  title     = "The use of multiple measurements in taxonomic problems",
  author    = "Fisher, Ronald A",
  journal   = "Ann. Eugen.",
  publisher = "Wiley Online Library",
  volume    =  7,
  number    =  2,
  pages     = "179--188",
  year      =  1936
}

@ARTICLE{Allen2016-xe,
  title    = "Take it to the limit: Innovative {CVaR} applications to extreme
              credit risk measurement",
  author   = "Allen, D E and Powell, R J and Singh, A K",
  abstract = "The Global Financial Crisis (GFC) demonstrated the devastating
              impact of extreme credit risk on global economic stability. We
              develop four credit models to better measure credit risk in
              extreme economic circumstances, by applying innovative
              Conditional Value at Risk (CVaR) techniques to structural models
              (called Xtreme-S), transition models (Xtreme-T), quantile
              regression models (Xtreme-Q), and the author's unique iTransition
              model (Xtreme-i) which incorporates industry factors into
              transition matrices. We find the Xtreme-S and Xtreme-Q models to
              be the most responsive to changing market conditions. The paper
              also demonstrates how the models can be used to determine capital
              buffers required to deal with extreme credit risk.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  249,
  number   =  2,
  pages    = "465--475",
  month    =  mar,
  year     =  2016,
  keywords = "Uncertainty modeling; Credit risk; Conditional Value at Risk;
              Conditional probability of default; Capital buffers"
}

@ARTICLE{Dakpo2016-jq,
  title    = "Modelling pollution-generating technologies in performance
              benchmarking: Recent developments, limits and future prospects in
              the nonparametric framework",
  author   = "Dakpo, K Herv{\'e} and Jeanneaux, Philippe and Latruffe, Laure",
  abstract = "Abstract This article is a critical review of methods integrating
              environmental aspects into productive efficiency. We describe the
              classic modelling approach relying on the weak disposability
              assumption, and explain the major recent developments around the
              inclusion of undesirable outputs in production technology
              modelling, namely the materials balance principles and the weak
              G-disposability, the by-production modelling and the cost
              disposability assumption, and the unified model under natural and
              managerial disposability concepts. We discuss the limits inherent
              in each methodology and suggest future research perspectives.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  250,
  number   =  2,
  pages    = "347--359",
  month    =  apr,
  year     =  2016,
  keywords = "Data envelopment analysis; Bad outputs; Weak disposability;
              Weak-G disposability; By-production"
}

@ARTICLE{Ho_undated-vh,
  title  = "{MatchIt}: Nonparametric Preprocessing for Parametric Causal
            Inference1",
  author = "Ho, Daniel E and Imai, Kosuke and King, Gary and Stuart, Elizabeth
            A"
}

@ARTICLE{Williamson1993-mm,
  title   = "Calculativeness, Trust, and Economic Organization",
  author  = "Williamson, Oliver E",
  journal = "The Journal of Law and Economics",
  volume  =  36,
  number  = "1, Part 2",
  pages   = "453--486",
  year    =  1993
}

@ARTICLE{Acharya2007-fp,
  title    = "Too many to fail---An analysis of time-inconsistency in bank
              closure policies",
  author   = "Acharya, Viral V and Yorulmazer, Tanju",
  abstract = "Abstract While the too-big-to-fail guarantee is explicitly a part
              of bank regulation in many countries, this paper shows that bank
              closure policies also suffer from an implicit
              ``too-many-to-fail'' problem: when the number of bank failures is
              large, the regulator finds it ex-post optimal to bail out some or
              all failed banks, whereas when the number of bank failures is
              small, failed banks can be acquired by the surviving banks. This
              gives banks incentives to herd and increases the risk that many
              banks may fail together. The ex-post optimal regulation may thus
              be time-inconsistent or sub-optimal from an ex-ante standpoint.
              In contrast to the too-big-to-fail problem which mainly affects
              large banks, we show that the too-many-to-fail problem affects
              small banks more by giving them stronger incentives to herd.",
  journal  = "Journal of Financial Intermediation",
  volume   =  16,
  number   =  1,
  pages    = "1--31",
  month    =  jan,
  year     =  2007,
  keywords = "Bank regulation; Systemic risk; Bailout; Herding; Too big to fail"
}

@ARTICLE{Nijskens2011-bn,
  title    = "Credit risk transfer activities and systemic risk: How banks
              became less risky individually but posed greater risks to the
              financial system at the same time",
  author   = "Nijskens, Rob and Wagner, Wolf",
  abstract = "Abstract A main cause of the crisis of 2007--2009 is the various
              ways through which banks have transferred credit risk in the
              financial system. We study the systematic risk of banks before
              the crisis, using two samples of banks respectively trading
              Credit Default Swaps (CDS) and issuing Collateralized Loan
              Obligations (CLOs). After their first usage of either risk
              transfer method, the share price beta of these banks increases
              significantly. This suggests the market anticipated the risks
              arising from these methods, long before the crisis. We
              additionally separate this beta effect into a volatility and a
              market correlation component. Quite strikingly, this
              decomposition shows that the increase in the beta is solely due
              to an increase in banks' correlations. Thus, while banks may have
              shed their individual credit risk, they actually posed greater
              systemic risk. This creates a challenge for financial regulation,
              which has typically focused on individual institutions.",
  journal  = "Journal of Banking \& Finance",
  volume   =  35,
  number   =  6,
  pages    = "1391--1398",
  month    =  jun,
  year     =  2011,
  keywords = "Securitization; Credit derivatives; Systemic risk; Subprime
              crisis"
}

@ARTICLE{Allen2012-nx,
  title    = "Asset commonality, debt maturity and systemic risk",
  author   = "Allen, Franklin and Babus, Ana and Carletti, Elena",
  abstract = "Abstract We develop a model in which asset commonality and
              short-term debt of banks interact to generate excessive systemic
              risk. Banks swap assets to diversify their individual risk. Two
              asset structures arise. In a clustered structure, groups of banks
              hold common asset portfolios and default together. In an
              unclustered structure, defaults are more dispersed. Portfolio
              quality of individual banks is opaque but can be inferred by
              creditors from aggregate signals about bank solvency. When bank
              debt is short-term, creditors do not roll over in response to
              adverse signals and all banks are inefficiently liquidated. This
              information contagion is more likely under clustered asset
              structures. In contrast, when bank debt is long-term, welfare is
              the same under both asset structures.",
  journal  = "J. financ. econ.",
  volume   =  104,
  number   =  3,
  pages    = "519--534",
  month    =  jun,
  year     =  2012,
  keywords = "Contagion; Clustered and unclustered networks Interim information"
}

@ARTICLE{Stein2012-ey,
  title     = "Monetary Policy as Financial Stability Regulation",
  author    = "Stein, Jeremy C",
  abstract  = "This article develops a model that speaks to the goals and
               methods of financial stability policies. There are three main
               points. First, from a normative perspective, the model defines
               the fundamental market failure to be addressed, namely, that
               unregulated private money creation can lead to an externality in
               which intermediaries issue too much short-term debt and leave
               the system excessively vulnerable to costly financial crises.
               Second, it shows how in a simple economy where commercial banks
               are the only lenders, conventional monetary policy tools such as
               open-market operations can be used to regulate this externality,
               whereas in more advanced economies it may be helpful to
               supplement monetary policy with other measures. Third, from a
               positive perspective, the model provides an account of how
               monetary policy can influence bank lending and real activity,
               even in a world where prices adjust frictionlessly and there are
               other transactions media besides bank-created money that are
               outside the control of the central bank.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  127,
  number    =  1,
  pages     = "57--95",
  month     =  feb,
  year      =  2012
}

@ARTICLE{Nesvetailova2015-eh,
  title     = "A Crisis of the Overcrowded Future: Shadow Banking and the
               Political Economy of Financial Innovation",
  author    = "Nesvetailova, Anastasia",
  abstract  = "This article focuses on the role the shadow banking system
               played in the financial crisis of 2007?9. Engaging with emergent
               theories of shadow banking, I inquire into its structural role
               in contemporary capitalism. My main premise here is that the
               crisis of 2007?9 is distinct in financial history because it did
               not centre on any organised market. Rather, it was crisis of the
               overcrowded financial channels bridging the present and the
               future, which have become congested because of the massive
               concentration of financial values generated, yet not sustained,
               through the shadow banking network. My analysis suggests that
               shadow banking has determined the nature of financial crisis of
               2007?9 and continues to play a necessary role in financial
               capitalism based on futurity. Drawing on scholarship in
               financial Keynesianism, contemporary legal studies and early
               evolutionary political economy, I argue that shadow banking is
               best seen as the organic institutional infrastructure of
               financialised capitalism based on debt and geared towards
               futurity, a concept originally developed by John Commons.",
  journal   = "New Political Economy",
  publisher = "Routledge",
  volume    =  20,
  number    =  3,
  pages     = "431--453",
  month     =  may,
  year      =  2015
}

@ARTICLE{Diaz-Rainey2015-cm,
  title    = "The technological transformation of capital markets",
  author   = "Diaz-Rainey, Ivan and Ibikunle, Gbenga and Mention, Anne-Laure",
  abstract = "Abstract Technology has dramatically altered capital markets over
              the past few decades. Technologically induced innovations such as
              electronic exchanges, high frequency trading (HFT) and exchange
              traded funds (ETFs) have made trading in capital markets faster,
              cheaper and more integrated, yet at the same time market
              liquidity has become more fragmented and opaque. Further, there
              are concerns that this new paradigm leads to greater volatility
              and myopia in the core function of finance (raising capital for
              entrepreneurial activity). Capital markets are clearly complex
              adaptive techno-social systems that are undergoing dramatic
              changes yet they are rarely researched from an innovation
              research or technological change perspective. In this editorial,
              we introduce the themes and issues highlighted by the papers in
              this Special Issue that addresses this gap in the literature. The
              contributions illuminate the technologies and related innovations
              that are changing the nature of capital markets. However,
              technology cannot be seen in isolation from other forces, most
              notably regulation, organisational innovation and new entrants.
              Moreover, technology is not only changing existing markets, it is
              expanding the scope of markets. Thus we conclude that
              financialization (the process by which financial markets become
              increasingly important in the economy and society) is a
              technology enabled phenomenon --- something hitherto largely
              overlooked by the financialization literature.",
  journal  = "Technol. Forecast. Soc. Change",
  volume   =  99,
  number   = "Supplement C",
  pages    = "277--284",
  month    =  oct,
  year     =  2015,
  keywords = "Financial markets; Information technology; High frequency
              trading; Financialization; Electronic exchanges; Financial
              innovation"
}

@ARTICLE{Lechman2015-ar,
  title    = "{ICT} technologies and financial innovations: The case of
              exchange traded funds in Brazil, Japan, Mexico, South Korea and
              the United States",
  author   = "Lechman, Ewa and Marszk, Adam",
  abstract = "Abstract Exchange traded funds (ETFs), funds structured in order
              to mimic the performance of selected financial assets, are one of
              the most significant innovative financial instruments recently
              introduced. They have gained considerable popularity among
              investors due to their advantages in comparison with conventional
              mutual funds, investment vehicles with a significantly longer
              history. This paper explores empirically links between
              information and communications technology (ICT) penetration and
              the introduction of financial innovations in emerging economies.
              It examines the impact of increasing ICT penetration on the
              assets of exchange traded funds in Brazil, Mexico, Japan and
              South Korea and the United States over 2002--2012. The
              methodological framework includes descriptive statistics,
              logistic growth models applied to estimate ETF market development
              patterns, and panel data analysis used to test the hypothesized
              relationship between increasing ICT penetration and ETF market
              development. The empirical findings collectively indicate that in
              all countries increases in ICT penetration have been pervasive
              and accompanied by a rapid development of ETF markets.
              Furthermore, the relationship between increasing ICT penetration
              and ETF market development is found to be strong, positive and
              statistically significant in Japan, Mexico, the United States and
              South Korea; while in Brazil the analogous relationship is
              relatively weak, although still positive.",
  journal  = "Technol. Forecast. Soc. Change",
  volume   =  99,
  number   = "Supplement C",
  pages    = "355--376",
  month    =  oct,
  year     =  2015,
  keywords = "Emerging markets; ICT; ETFs; Financial innovations"
}

@ARTICLE{Carruthers2015-hv,
  title     = "Financialization and the institutional foundations of the new
               capitalism",
  author    = "Carruthers, Bruce G",
  abstract  = "\copyright{} The Author 2015. Published by Oxford University
               Press and the Society for the Advancement of Socio-Economics.
               All rights reserved. For Permissions, please email:
               journals.permissions@oup.comOne of key features of capitalism as
               a form of economic organization concerns its ability to change.
               Innovation often occurs by using old things in new ways, or by
               taking pre-existing elements and rearranging them into novel
               configurations [termed `conversion' by Streeck and Thelen (2005,
               p. 26)]. Change can also happen when old activities are simply
               discontinued, or when new activities are added [what Mahoney and
               Thelen (2010, p. 16) call `layering']. Capitalist innovation
               does not arise ex nihilo, nor does it involve wholesale
               rejection of the past. As even casual students of contemporary
               capitalism realize, much of today's capitalism resembles the
               old-fashioned kind studied by nineteenth-century social
               theorists like Marx, Durkheim and Weber. Heavy industry still
               exists, tangible goods are still...",
  journal   = "Socioecon Rev",
  publisher = "Oxford University Press",
  volume    =  13,
  number    =  2,
  pages     = "379--398",
  month     =  apr,
  year      =  2015
}

@ARTICLE{Soderlund2017-qk,
  title    = "Liminality in Management and Organization Studies: Process,
              Position and Place",
  author   = "S{\"o}derlund, Jonas and Borg, Elisabeth",
  abstract = "This paper explores liminality, a concept receiving increased
              attention in management and organization studies and gaining
              prominence because of its capacity to capture the interstitial
              and temporary elements of organizing and work. The authors
              present a systematic review of the literature on liminality,
              covering 61 published papers, and undertake a critical analysis
              of how the concept of liminality has been used in prior research.
              This review reveals associations with three main themes: process;
              position; and place. For each theme, the authors identify the
              central research questions posed, while comparing individual and
              collective levels of analysis. During this process, the authors
              revisit several ideas central to the original, anthropological
              research on liminality, a perspective from which they suggest a
              rejuvenation of liminality research in management and
              organization studies. This paper argues for a greater focus on
              the liminal experience itself -- especially its ritual and
              temporal dimensions -- and for improving the comparative analysis
              of liminality following the three themes identified in this
              paper. The authors suggest that revising the agenda for
              liminality research along these lines could facilitate more
              informed responses to the challenges of an increasingly temporary
              and dynamic work life.",
  journal  = "International Journal of Management Reviews",
  year     =  2017
}

@ARTICLE{Caccioli2014-ee,
  title    = "Stability analysis of financial contagion due to overlapping
              portfolios",
  author   = "Caccioli, Fabio and Shrestha, Munik and Moore, Cristopher and
              Farmer, J Doyne",
  abstract = "Abstract Common asset holdings are widely believed to have been
              the primary vector of contagion in the recent financial crisis.
              We develop a network approach to the amplification of financial
              contagion due to the combination of overlapping portfolios and
              leverage, and we show how it can be understood in terms of a
              generalized branching process. This can be used to compute the
              stability for any particular configuration of portfolios. By
              studying a stylized model we estimate the circumstances under
              which systemic instabilities are likely to occur as a function of
              parameters such as leverage, market crowding, diversification,
              and market impact. Although diversification may be good for
              individual institutions, it can create dangerous systemic
              effects, and as a result financial contagion gets worse with too
              much diversification. There is a critical threshold for leverage;
              below it financial networks are always stable, and above it the
              unstable region grows as leverage increases. Note that our model
              assumes passive portfolio management during a crisis; however, we
              show that dynamic deleveraging during a crisis can amplify
              instabilities. The financial system exhibits ``robust yet
              fragile'' behavior, with regions of the parameter space where
              contagion is rare but catastrophic whenever it occurs. Our model
              and methods of analysis can be calibrated to real data and
              provide simple yet powerful tools for macroprudential stress
              testing.",
  journal  = "Journal of Banking \& Finance",
  volume   =  46,
  number   = "Supplement C",
  pages    = "233--245",
  month    =  sep,
  year     =  2014,
  keywords = "Systemic risk; Network models; Contagion"
}

@BOOK{Foucault2013-if,
  title     = "Market Liquidity: Theory, Evidence, and Policy",
  author    = "Foucault, Thierry and Pagano, Marco and Roell, Ailsa and
               R{\"o}ell, Ailsa",
  abstract  = "The way in which securities are traded is very different from
               the idealized picture of a frictionless and self-equilibrating
               market offered by the typical finance textbook. Market Liquidity
               offers a more accurate and authoritative take on liquidity and
               price discovery. The authors start from the assumption that not
               everyone is present at all times simultaneously on the market,
               and that even the limited number of participants who are have
               quite diverse information about the security's fundamentals. As
               a result, the order flow is a complex mix of information and
               noise, and a consensus price only emerges gradually over time as
               the trading process evolves and the participants interpret the
               actions of other traders. Thus a security's actual transaction
               price may deviate from its fundamental value, as it would be
               assessed by a fully informed set of investors. This book takes
               these deviations seriously, and explains why and how they emerge
               in the trading process and are eventually eliminated. The
               authors draw on a vast body of theoretical insights and
               empirical findings on security price formation that have
               accumulated in the last thirty years, and have come to form a
               well-defined field within financial economics known as 'market
               microstructure.' Focusing on liquidity and price discovery, they
               analyze the tension between the two, pointing out that when
               price-relevant information reaches the market through trading
               pressure rather than through a public announcement, liquidity
               suffers. The book also confronts many puzzling phenomena in
               securities markets and uses the analytical tools and empirical
               methods of market microstructure to understand them. These
               include issues such as why liquidity changes over time, why
               large trades move prices up or down, and why these price changes
               are subsequently reversed, why we see concentration of
               securities trading, why some traders willingly disclose their
               intended trades while others hide them, and why we observe
               temporary deviations from arbitrage prices.",
  publisher = "OUP USA",
  month     =  apr,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Raper2017-ms,
  title    = "The shock of the mean",
  author   = "Raper, Simon",
  abstract = "Simon Raper recounts the history of the arithmetic mean, why
              scientists of the past rejected the idea, and why their concerns
              are still relevant in the ongoing struggle to communicate
              statistical concepts",
  journal  = "Significance",
  volume   =  14,
  number   =  6,
  pages    = "12--17",
  month    =  dec,
  year     =  2017
}

@ARTICLE{Frame2014-gi,
  title    = "Technological Change, Financial Innovation, and Diffusion in
              Banking",
  author   = "Frame, W and White, Lawrence",
  abstract = "The commercial banking business has changed dramatically over the
              past 30 years, due in large part to technological change. The
              paper first describes the role o",
  month    =  jan,
  year     =  2014,
  keywords = "technological change; financial innovation, diffusion; banking"
}

@ARTICLE{Leow2016-zb,
  title    = "The stability of survival model parameter estimates for
              predicting the probability of default: Empirical evidence over
              the credit crisis",
  author   = "Leow, Mindy and Crook, Jonathan",
  abstract = "Using a large portfolio of credit card loans observed between
              2002 and 2011 provided by a major UK bank, we investigate the
              stability of the parameter estimates of discrete survival models,
              especially since the start of the credit crisis of 2008. Two
              survival models are developed for accounts that were accepted
              before and since the crisis. We find that the two sets of
              parameter estimates are statistically different from each other.
              By applying the estimated parameters onto a common test set, we
              also show that they give different predictions of probabilities
              of default. The changes in the predicted probability
              distributions are then investigated. We theorise them to be due
              to the quality of the cohort accepted under different economic
              conditions, or due to the drastically different economic
              conditions that was seen in the UK economy, or a combination of
              both. We test for each effect.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  249,
  number   =  2,
  pages    = "457--464",
  month    =  mar,
  year     =  2016,
  keywords = "Forecasting; Robustness and sensitivity analysis; Macroeconomic
              variables; Structural change; Probability forecasting"
}

@UNPUBLISHED{Baker2007-cf,
  title       = "Investor Sentiment in the Stock Market",
  author      = "Baker, Malcolm and Wurgler, Jeffrey",
  abstract    = "Real investors and markets are too complicated to be neatly
                 summarized by a few selected biases and trading frictions. The
                 ``top down'' approach to behavioral finance focuses on the
                 measurement of reduced form, aggregate sentiment and traces
                 its effects to stock returns. It builds on the two broader and
                 more irrefutable assumptions of behavioral finance --
                 sentiment and the limits to arbitrage -- to explain which
                 stocks are likely to be most affected by sentiment. In
                 particular, stocks of low capitalization, younger,
                 unprofitable, high volatility, non-dividend paying, growth
                 companies, or stocks of firms in financial distress, are
                 likely to be disproportionately sensitive to broad waves of
                 investor sentiment. We review the theoretical and empirical
                 evidence for these predictions.",
  journal     = "www.nber.org/papers/w13189",
  number      =  13189,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  jun,
  year        =  2007
}

@ARTICLE{Stambaugh2012-yo,
  title     = "The short of it: Investor sentiment and anomalies",
  author    = "Stambaugh, Robert F and Yu, Jianfeng and Yuan, Yu",
  abstract  = "Abstract This study explores the role of investor sentiment in a
               broad set of anomalies in cross-sectional stock returns. We
               consider a setting in which the presence of market-wide
               sentiment is combined with the argument that overpricing should
               be more prevalent than underpricing, due to short-sale
               impediments. Long-short strategies that exploit the anomalies
               exhibit profits consistent with this setting. First, each
               anomaly is stronger (its long-short strategy is more profitable)
               following high levels of sentiment. Second, the short leg of
               each strategy is more profitable following high sentiment.
               Finally, sentiment exhibits no relation to returns on the long
               legs of the strategies.",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  104,
  number    =  2,
  pages     = "288--302",
  month     =  may,
  year      =  2012,
  keywords  = "Investor sentiment; Anomalies"
}

@ARTICLE{Da2011-cw,
  title     = "In Search of Attention",
  author    = "Da, Zhi and Engelberg, Joseph and Gao, Pengjie",
  abstract  = "We propose a new and direct measure of investor attention using
               search frequency in Google (Search Volume Index (SVI)). In a
               sample of Russell 3000 stocks from 2004 to 2008, we find that
               SVI (1) is correlated with but different from existing proxies
               of investor attention; (2) captures investor attention in a more
               timely fashion and (3) likely measures the attention of retail
               investors. An increase in SVI predicts higher stock prices in
               the next 2 weeks and an eventual price reversal within the year.
               It also contributes to the large first-day return and long-run
               underperformance of IPO stocks.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  66,
  number    =  5,
  pages     = "1461--1499",
  month     =  oct,
  year      =  2011
}

@ARTICLE{Baker2010-sx,
  title     = "Benchmarks as Limits to Arbitrage: Understanding the
               {Low-Volatility} Anomaly",
  author    = "Baker, Malcolm and Bradley, Brendan and Wurgler, Jeffrey",
  abstract  = "Contrary to basic finance principles, high-beta and
               high-volatility stocks have long underperformed low-beta and
               low-volatility stocks. This anomaly may be partly explained by
               the fact that the typical institutional investor?s mandate to
               beat a fixed benchmark discourages arbitrage activity in both
               high-alpha, low-beta stocks and low-alpha, high-beta stocks.",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  67,
  number    =  1,
  pages     = "40--54",
  month     =  dec,
  year      =  2010
}

@ARTICLE{Houston2012-ux,
  title     = "Regulatory Arbitrage and International Bank Flows",
  author    = "Houston, Joel F and Lin, Chen and Ma, Yue",
  abstract  = "We study whether cross-country differences in regulations have
               affected international bank flows. We find strong evidence that
               banks have transferred funds to markets with fewer regulations.
               This form of regulatory arbitrage suggests there may be a
               destructive ``race to the bottom'' in global regulations, which
               restricts domestic regulators' ability to limit bank
               risk-taking. However, we also find that the links between
               regulation differences and bank flows are significantly stronger
               if the recipient country is a developed country with strong
               property rights and creditor rights. This suggests that, while
               differences in regulations have important influences, without a
               strong institutional environment, lax regulations are not enough
               to encourage massive capital flows.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  67,
  number    =  5,
  pages     = "1845--1895",
  month     =  oct,
  year      =  2012
}

@ARTICLE{Carbo-Valverde2012-xk,
  title     = "Regulatory Arbitrage in {Cross-Border} Banking Mergers within
               the {EU}",
  author    = "Carbo-Valverde, Santiago and Kane, Edward J and
               Rodriguez-Fernandez, Francisco",
  abstract  = "Abstract Expanding the cross-country footprint of an
               organization's profit-making activities changes the geographic
               pattern of its exposure to loss in ways that are hard for
               regulators and supervisors to observe. This paper tests and
               confirms the hypothesis that differences in",
  journal   = "J. Money Credit Bank.",
  publisher = "Wiley Online Library",
  volume    =  44,
  number    =  8,
  pages     = "1609--1629",
  year      =  2012
}

@ARTICLE{Karolyi2015-eg,
  title     = "Regulatory arbitrage and cross-border bank acquisitions",
  author    = "Karolyi, G Andrew and Taboada, Alvaro G",
  abstract  = "ABSTRACT We study how differences in bank regulation influence
               cross-border bank acquisition flows and share price reactions to
               cross-border deal announcements. Using a sample of 7,297
               domestic and 916 majority cross-border deals announced between
               1995",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  70,
  number    =  6,
  pages     = "2395--2450",
  year      =  2015
}

@ARTICLE{Kisin2016-di,
  title     = "The Shadow Cost of Bank Capital Requirements",
  author    = "Kisin, Roni and Manela, Asaf",
  abstract  = "We estimate the shadow cost of capital requirements using data
               on a costly loophole that allowed banks to relax these
               constraints. This loophole---liquidity guarantees to
               asset-backed commercial paper conduits---was exploited by the
               largest banks before the crisis of 2008. We show theoretically
               that a bank's use of the loophole reveals its private compliance
               cost, which takes into account both the costs of issuing equity
               and the effectiveness of capital regulation. We find that
               increasing capital requirements would impose a modest
               cost---$220 million a year for all participating banks combined
               per one-percentage-point increase, and $14 million on
               average.Received June 5, 2015; accepted February 23, 2016 by
               Editor Philip Strahan.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  29,
  number    =  7,
  pages     = "1780--1820",
  month     =  jul,
  year      =  2016
}

@ARTICLE{Karolyi2016-nb,
  title     = "The gravity of culture for finance",
  author    = "Karolyi, G Andrew",
  abstract  = "Abstract Scholarship in finance has paid relatively little
               attention to the role of culture in financial decision-making
               compared to other business disciplines and economics. This paper
               will review what research has been done to date including a
               critical assessment of the key databases used to measure
               differences in cultural values. Notwithstanding the concerns
               with the measures of cultural values, I conduct an empirical
               analysis of the role of cultural distance for explaining the
               foreign bias in international portfolio holdings using
               traditional gravity models in international economics. I affirm
               the statistical explanatory power of culture for these
               investment biases and outline several new potential directions
               for research.",
  journal   = "Journal of Corporate Finance",
  publisher = "Elsevier",
  volume    =  41,
  number    = "Supplement C",
  pages     = "610--625",
  month     =  dec,
  year      =  2016,
  keywords  = "Culture; International portfolio choices; Gravity models;
               Research trends in finance"
}

@ARTICLE{Altunbas2018-zf,
  title     = "Macroprudential policy and bank risk",
  author    = "Altunbas, Yener and Binici, Mahir and Gambacorta, Leonardo",
  abstract  = "Abstract This paper investigates the effects of macroprudential
               policies on bank risk through a large panel of banks operating
               in 61 advanced and emerging market economies. There are three
               main findings. First, there is evidence suggesting that
               macroprudential tools have a significant impact on bank risk.
               Second, the responses to changes in macroprudential tools differ
               among banks, depending on their specific balance sheet
               characteristics. In particular, banks that are small, weakly
               capitalised and with a higher share of wholesale funding react
               more strongly to changes in macroprudential tools. Third,
               controlling for bank-specific characteristics, macroprudential
               policies are more effective in a tightening than in an easing
               episode.",
  journal   = "J. Int. Money Finance",
  publisher = "Elsevier",
  volume    =  81,
  number    = "Supplement C",
  pages     = "203--220",
  month     =  mar,
  year      =  2018,
  keywords  = "Macroprudential policies; Effectiveness; Bank risk"
}

@ARTICLE{De_Gooijer2006-fz,
  title    = "25 years of time series forecasting",
  author   = "De Gooijer, Jan G and Hyndman, Rob J",
  abstract = "Abstract We review the past 25 years of research into time series
              forecasting. In this silver jubilee issue, we naturally highlight
              results published in journals managed by the International
              Institute of Forecasters (Journal of Forecasting 1982--1985 and
              International Journal of Forecasting 1985--2005). During this
              period, over one third of all papers published in these journals
              concerned time series forecasting. We also review highly
              influential works on time series forecasting that have been
              published elsewhere during this period. Enormous progress has
              been made in many areas, but we find that there are a large
              number of topics in need of further development. We conclude with
              comments on possible future research directions in this field.",
  journal  = "Int. J. Forecast.",
  volume   =  22,
  number   =  3,
  pages    = "443--473",
  month    =  jan,
  year     =  2006,
  keywords = "Accuracy measures; ARCH; ARIMA; Combining; Count data; Densities;
              Exponential smoothing; Kalman filter; Long memory; Multivariate;
              Neural nets; Nonlinearity; Prediction intervals;
              Regime-switching; Robustness; Seasonality; State space;
              Structural models; Transfer function; Univariate; VAR"
}

@ARTICLE{Martinez2017-fw,
  title     = "A methodology for applying k-nearest neighbor to time series
               forecasting",
  author    = "Mart{\'\i}nez, Francisco and Fr{\'\i}as, Mar{\'\i}a Pilar and
               P{\'e}rez, Mar{\'\i}a Dolores and Rivera, Antonio Jes{\'u}s",
  abstract  = "In this paper a methodology for applying k-nearest neighbor
               regression on a time series forecasting context is developed.
               The goal is to devise an automatic tool, i.e., a tool that can
               work without human intervention; furthermore, the methodology
               should be effective and efficient, so that it can be applied to
               accurately forecast a great number of time series. In order to
               be incorporated into our methodology, several modeling and
               preprocessing techniques are analyzed and assessed using the N3
               competition data set. One interesting feature of the proposed
               methodology is that it resolves the selection of important
               modeling parameters, such as k or the input variables, combining
               several models with different parameters. In spite of the
               simplicity of k-NN regression, our methodology seems to be quite
               effective.",
  journal   = "Artif Intell Rev",
  publisher = "Springer Netherlands",
  pages     = "1--19",
  month     =  nov,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Gokalp_Yavuz2017-fh,
  title     = "Fostering Undergraduate Data Science",
  author    = "Gokalp Yavuz, Fulya and Ward, Mark Daniel",
  abstract  = "AbstractData Science is one of the newest interdisciplinary
               areas. It is transforming our lives unexpectedly fast. This
               transformation is also happening in our learning styles and
               practicing habits. We advocate an approach to data science
               training that utilizes several types of computational tools,
               including R, bash, awk, regular expressions, SQL, and XPath,
               often used in tandem. We discuss ways for undergraduate mentees
               to learn about data science topics, at an early point in their
               training. We give some intuition for researchers, professors,
               and practitioners about how to effectively embed real-life
               examples into data science learning environments. As a result,
               we have a unified program built on a foundation of
               team-oriented, data-driven projects.",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  pages     = "0--0",
  month     =  dec,
  year      =  2017
}

@ARTICLE{Hyndman2006-az,
  title    = "Another look at measures of forecast accuracy",
  author   = "Hyndman, Rob J and Koehler, Anne B",
  abstract = "Abstract We discuss and compare measures of accuracy of
              univariate time series forecasts. The methods used in the
              M-competition as well as the M3-competition, and many of the
              measures recommended by previous authors on this topic, are found
              to be degenerate in commonly occurring situations. Instead, we
              propose that the mean absolute scaled error become the standard
              measure for comparing forecast accuracy across multiple time
              series.",
  journal  = "Int. J. Forecast.",
  volume   =  22,
  number   =  4,
  pages    = "679--688",
  month    =  oct,
  year     =  2006,
  keywords = "Forecast accuracy; Forecast evaluation; Forecast error measures;
              M-competition; Mean absolute scaled error"
}

@MISC{noauthor_undated-ae,
  title = "oxfordhb-9780199559084-e-3.pdf"
}

@MISC{noauthor_undated-ca,
  title = "oxfordhb-9780199559084-e-6.pdf"
}

@ARTICLE{Halkos2014-bp,
  title     = "Measuring the effect of Kyoto protocol agreement on countries'
               environmental efficiency in {CO2} emissions: an application of
               conditional full frontiers",
  author    = "Halkos, George E and Tzeremes, Nickolaos G",
  abstract  = "This paper applies the probabilistic approach developed by
               Daraio and Simar (J Prod Anal 24:93--121, 2005, Advanced robust
               and nonparametric methods in efficiency analysis. Springer
               Science, New York, 2007a, J Prod Anal 28:13--32, 2007b) in order
               to develop conditional and unconditional data envelopment
               analysis (DEA) models for the measurement of countries'
               environmental efficiency levels for a sample of 110 countries in
               2007. In order to capture the effect of countries compliance
               with the Kyoto protocol agreement (KPA) policies, we condition
               first the years since a country has signed the KPA until 2007
               and secondly the obliged percentage level of countries' emission
               reductions. Particularly, various DEA models have been applied
               alongside with bootstrap techniques in order to determine the
               effect of KPA on countries' environmental efficiencies. The
               study illustrates how the recent developments in efficiency
               analysis and statistical inference can be applied when
               evaluating environmental performance issues. The results
               indicate a nonlinear relationship between countries' obliged
               percentage levels of emission reductions and their environmental
               efficiency levels. Finally, a similar nonlinear relationship is
               also recorded between the duration which a country has signed
               the KPA and its environmental efficiency levels.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  41,
  number    =  3,
  pages     = "367--382",
  month     =  jun,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Vallascas2013-ln,
  title     = "The risk sensitivity of capital requirements: Evidence from an
               international sample of large banks",
  author    = "Vallascas, Francesco and Hagendorff, Jens",
  journal   = "Review of Finance",
  publisher = "Oxford University Press",
  volume    =  17,
  number    =  6,
  pages     = "1947--1988",
  year      =  2013
}

@ARTICLE{Unknoew2018-gv,
  title   = "Failure of credit unions and of commercial banks",
  author  = "{Unknoew}",
  journal = "Journal of financial stability (Submission)",
  year    =  2018
}

@ARTICLE{Grinblatt2009-dn,
  title     = "Sensation Seeking, Overconfidence, and Trading Activity",
  author    = "Grinblatt, Mark and Keloharju, Matti",
  abstract  = "This study analyzes the role that two psychological
               attributes---sensation seeking and overconfidence---play in the
               tendency of investors to trade stocks. Equity trading data from
               Finland are combined with data from investor tax filings,
               driving records, and mandatory psychological profiles. We use
               these data, obtained from a large population, to construct
               measures of overconfidence and sensation seeking tendencies.
               Controlling for a host of variables, including wealth, income,
               age, number of stocks owned, marital status, and occupation, we
               find that overconfident investors and those investors most prone
               to sensation seeking trade more frequently.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  64,
  number    =  2,
  pages     = "549--578",
  month     =  apr,
  year      =  2009
}

@ARTICLE{Chui2010-tz,
  title     = "Individualism and Momentum around the World",
  author    = "Chui, Andy C W and Titman, Sheridan and Wei, K C John",
  abstract  = "This paper examines how cultural differences influence the
               returns of momentum strategies. Cross-country cultural
               differences are measured with an individualism index developed
               by Hofstede (2001), which is related to overconfidence and
               self-attribution bias. We find that individualism is positively
               associated with trading volume and volatility, as well as to the
               magnitude of momentum profits. Momentum profits are also
               positively related to analyst forecast dispersion, transaction
               costs, and the familiarity of the market to foreigners, and
               negatively related to firm size and volatility. However, the
               addition of these and other variables does not dampen the
               relation between individualism and momentum profits.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishing Inc",
  volume    =  65,
  number    =  1,
  pages     = "361--392",
  month     =  feb,
  year      =  2010
}

@ARTICLE{Graham2009-mr,
  title     = "Investor Competence, Trading Frequency, and Home Bias",
  author    = "Graham, John R and Harvey, Campbell R and Huang, Hai",
  abstract  = "People are more willing to bet on their own judgments when they
               feel skillful or knowledgeable. We investigate whether this
               ?competence effect? influences trading frequency and home bias.
               We find that investors who feel competent trade more often and
               have more internationally diversified portfolios. We also find
               that male investors, and investors with larger portfolios or
               more education, are more likely to perceive themselves as
               competent than are female investors, and investors with smaller
               portfolios or less education. Our paper also contributes to
               understanding the theoretical link between overconfidence and
               trading frequency. Existing theories on trading frequency have
               focused on one aspect of overconfidence, i.e., miscalibration.
               Our paper offers a potential mechanism for the
               ?better-than-average? aspect of overconfidence to influence
               trading frequency. In the context of our paper, overconfident
               investors tend to perceive themselves to be more competent, and
               thus are more willing to act on their beliefs, leading to higher
               trading frequency.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  55,
  number    =  7,
  pages     = "1094--1106",
  month     =  apr,
  year      =  2009
}

@ARTICLE{Deaves2009-kc,
  title     = "An Experimental Test of the Impact of Overconfidence and Gender
               on Trading Activity",
  author    = "Deaves, Richard and L{\"u}ders, Erik and Luo, Guo Ying",
  abstract  = "We perform an asset market experiment in order to investigate
               whether overconfidence induces trading. We investigate three
               manifestations of overconfidence: calibration-based
               overconfidence, the better-than-average effect and illusion of
               control. Novelly, the measure employed for calibration-based
               overconfidence is task-specific in that it is designed to
               influence behavior. We find that calibration-based
               overconfidence does engender additional trade, though the
               better-than-average also appears to play a role. This is true
               both at the level of the individual and also at the level of the
               market. There is little evidence that gender influences trading
               activity.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  13,
  number    =  3,
  pages     = "555--575",
  month     =  jul,
  year      =  2009
}

@ARTICLE{Charness2010-zo,
  title     = "{PORTFOLIO} {CHOICE} {AND} {RISK} {ATTITUDES}: {AN} {EXPERIMENT}",
  author    = "Charness, Gary and Gneezy, Uri",
  abstract  = "Using financial incentives, we study how portfolio choice (how
               much to invest in a risky asset) depends on three well-known
               behavioral phenomena: ambiguity aversion, the illusion of
               control, and myopic loss aversion. We find evidence that these
               phenomena are present and test how the level of investment is
               affected by these motivations; at the same time, we investigate
               whether participants are willing to explicitly pay a small sum
               of money to indulge preferences for less ambiguity, more
               control, or more frequent feedback/opportunities to choose the
               investment level. First, the observed preference for ``control''
               did not affect investment behavior and in fact disappeared when
               participants were asked to actually pay to gain more control.
               Second, while people were indeed willing to pay for less
               ambiguity, the level of ambiguity did not influence investment
               levels. Finally, participants were willing to pay to have more
               frequent feedback opportunities to change their portfolio, even
               though prior research has shown that people invest less in risky
               assets (and earn less) in this case. (JEL B49, C91, D81, G11,
               G19)",
  journal   = "Econ. Inq.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  48,
  number    =  1,
  pages     = "133--146",
  month     =  jan,
  year      =  2010
}

@ARTICLE{Fudenberg2006-eu,
  title   = "Advancing beyond advances in behavioral economics",
  author  = "Fudenberg, Drew",
  journal = "J. Econ. Lit.",
  volume  =  44,
  number  =  3,
  pages   = "694--711",
  year    =  2006
}

@ARTICLE{Witmer2017-jt,
  title     = "Bayes and {MCMC} for Undergraduates",
  author    = "Witmer, J",
  abstract  = "Students of statistics should be taught the ideas and methods
               that are widely used in practice and that will help them
               understand the world of statistics. Today, this means teaching
               them about Bayesian methods. In this article, I present ideas on
               teaching an undergraduate Bayesian course that uses Markov chain
               Monte Carlo and that can be a second course or, for strong
               students, a first course in statistics. \copyright{} 2017
               American Statistical Association.",
  journal   = "Am. Stat.",
  publisher = "American Statistical Association",
  volume    =  71,
  number    =  3,
  pages     = "259--264",
  year      =  2017,
  keywords  = "Bayesian inference; Education; Markov chain Monte Carlo"
}

@ARTICLE{Pawitan2017-or,
  title     = "Wallet Game: Probability, Likelihood, and Extended Likelihood",
  author    = "Pawitan, Y and Lee, Y",
  abstract  = "We propose a likelihood explanation to the two-person wallet
               game, a probability-related paradox, where an obviously fair
               game may appear favorable to both players. Yet a small variation
               of the game, without changing its fairness, turns it to seem
               unfavorable. The extended likelihood concept seems logically
               necessary if we want to allow the sense of uncertainty
               associated with a realized but still unobserved random outcome,
               while at the same time avoid potential probability-related
               paradoxes. \copyright{} 2017 American Statistical Association.",
  journal   = "Am. Stat.",
  publisher = "American Statistical Association",
  volume    =  71,
  number    =  2,
  pages     = "120--122",
  year      =  2017,
  keywords  = "Exchange paradox; Fisher; Hierarchical generalized linear
               models; Random effects"
}

@ARTICLE{Stine2017-pn,
  title     = "Explaining Normal {Quantile-Quantile} Plots Through Animation:
               The {Water-Filling} Analogy",
  author    = "Stine, R A",
  abstract  = "A normal quantile-quantile (QQ) plot is an important diagnostic
               for checking the assumption of normality. Though useful, these
               plots confuse students in my introductory statistics classes. A
               water-filling analogy, however, intuitively conveys the
               underlying concept. This analogy characterizes a QQ plot as a
               parametric plot of the water levels in two gradually filling
               vases. Each vase takes its shape from a probability distribution
               or sample. If the vases share a common shape, then the water
               levels match throughout the filling, and the QQ plot traces a
               diagonal line. An R package qqvases provides an interactive
               animation of this process and is suitable for classroom use.
               \copyright{} 2017 American Statistical Association.",
  journal   = "Am. Stat.",
  publisher = "American Statistical Association",
  volume    =  71,
  number    =  2,
  pages     = "145--147",
  year      =  2017,
  keywords  = "Diagnostic; Education; Simulation"
}

@ARTICLE{Kaplan2017-uk,
  title     = "Teaching Stats for Data Science",
  author    = "Kaplan, Daniel",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  pages     = "0--0",
  month     =  nov,
  year      =  2017
}

@ARTICLE{Pesaran2004-py,
  title     = "How costly is it to ignore breaks when forecasting the direction
               of a time series?",
  author    = "Pesaran, M Hashem and Timmermann, Allan",
  journal   = "Int. J. Forecast.",
  publisher = "Elsevier",
  volume    =  20,
  number    =  3,
  pages     = "411--425",
  year      =  2004
}

@ARTICLE{Lee2001-ek,
  title    = "{STOCK} {RETURNS} {AND} {VOLATILITY} {ON} {CHINA'S} {STOCK}
              {MARKETS}",
  author   = "Lee, Cheng F and Chen, Gong-Meng and Rui, Oliver M",
  abstract = "Abstract We examine time-series features of stock returns and
              volatility, as well as the relation between return and volatility
              in four of China's stock exchanges. Variance ratio tests reject
              the hypothesis that stock returns follow a random walk. We find
              evidence of long memory of returns. Application of GARCH and
              EGARCH models provides strong evidence of time-varying volatility
              and shows volatility is highly persistent and predictable. The
              results of GARCH-M do not show any relation between expected
              returns and expected risk. Daily trading volume used as a proxy
              for information arrival time has no significant explanatory power
              for the conditional volatility of daily returns. JEL
              classification: G15",
  journal  = "J. Financ. Res.",
  volume   =  24,
  number   =  4,
  pages    = "523--543",
  month    =  dec,
  year     =  2001
}

@ARTICLE{Khachatryan2015-gh,
  title     = "Incorporating Statistical Consulting Case Studies in
               Introductory Time Series Courses",
  author    = "Khachatryan, Davit",
  abstract  = "Established as a rigorous pedagogical device at Harvard
               University, the case method has grown into an indispensable mode
               of instruction at many business schools. Its effectiveness has
               been praised for increasing student participation during
               in-class discussions, providing hands-on engagement in
               real-world business problems, and increasing long-term retention
               rates. This article illustrates how novel case studies that
               mimic real-life statistical consulting engagements can be
               incorporated in the curriculum of an undergraduate, introductory
               time series course. The assessment of learning objectives as
               well as pedagogical implications when teaching using statistical
               consulting case studies is elucidated. The article also lays out
               guidelines for adopting statistical consulting case studies
               should the readers choose to incorporate the case method into
               the curricula of courses that they teach. A sample case study
               which the author has successfully used in his classroom
               instruction is provided in this article.Received July 2014.
               Revised January 2015",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  69,
  number    =  4,
  pages     = "387--396",
  month     =  oct,
  year      =  2015
}

@ARTICLE{Biais2005-od,
  title     = "Judgemental Overconfidence, {Self-Monitoring}, and Trading
               Performance in an Experimental Financial Market",
  author    = "Biais, Bruno and Hilton, Denis and Mazurier, Karine and Pouget,
               S{\'e}bastien",
  abstract  = "We measure the degree of overconfidence in judgement (in the
               form of miscalibration, i.e. the tendency to overestimate the
               precision of one's information) and self-monitoring (a form of
               attentiveness to social cues) of 245 participants and also
               observe their behaviour in an experimental financial market
               under asymmetric information. Miscalibrated traders,
               underestimating the conditional uncertainty about the asset
               value, are expected to be especially vulnerable to the winner's
               curse. High self-monitors are expected to behave strategically
               and achieve superior results. Our empirical results show that
               miscalibration reduces and self-monitoring enhances trading
               performance. The effect of the psychological variables is strong
               for men but non-existent for women.",
  journal   = "Rev. Econ. Stud.",
  publisher = "Oxford University Press",
  volume    =  72,
  number    =  2,
  pages     = "287--312",
  month     =  apr,
  year      =  2005
}

@ARTICLE{Heaton2017-en,
  title    = "Deep learning for finance: deep portfolios",
  author   = "Heaton, J B and Polson, N G and Witte, J H",
  abstract = "We explore the use of deep learning hierarchical models for
              problems in financial prediction and classification. Financial
              prediction problems -- such as those presented in designing and
              pricing securities, constructing portfolios, and risk management
              -- often involve large data sets with complex data interactions
              that currently are difficult or impossible to specify in a full
              economic model. Applying deep learning methods to these problems
              can produce more useful results than standard methods in finance.
              In particular, deep learning can detect and exploit interactions
              in the data that are, at least currently, invisible to any
              existing financial economic theory. Copyright \copyright{} 2016
              John Wiley \& Sons, Ltd.",
  journal  = "Appl. Stoch. Mod. Data Anal.",
  volume   =  33,
  number   =  1,
  pages    = "3--12",
  month    =  jan,
  year     =  2017,
  keywords = "deep learning; machine learning; big data; artificial
              intelligence; finance; asset pricing; volatility; deep frontier"
}

@ARTICLE{Golez2018-dx,
  title    = "Four centuries of return predictability",
  author   = "Golez, Benjamin and Koudijs, Peter",
  abstract = "We combine annual stock market data for the most important equity
              markets of the last four centuries: the Netherlands and UK
              (1629--1812), UK (1813--1870), and US (1871--2015). We show that
              dividend yields are stationary and consistently forecast returns.
              The documented predictability holds for annual and multi-annual
              horizons and works both in- and out-of-sample, providing strong
              evidence that expected returns in stock markets are time-varying.
              In part, this variation is related to the business cycle, with
              expected returns increasing in recessions. We also find that,
              except for the period after 1945, dividend yields predict
              dividend growth rates.",
  journal  = "J. financ. econ.",
  volume   =  127,
  number   =  2,
  pages    = "248--263",
  month    =  feb,
  year     =  2018,
  keywords = "Dividend-to-price ratio; Return predictability; Dividend growth
              predictability"
}

@ARTICLE{Twala2010-fb,
  title     = "Multiple classifier application to credit risk assessment",
  author    = "Twala, Bhekisipho",
  abstract  = "Credit risk prediction models seek to predict quality factors
               such as whether an individual will default (bad applicant) on a
               loan or not (good applicant). This can be treated as a kind of
               machine learning (ML) problem. Recently, the use of ML
               algorithms has proven to be of great practical value in solving
               a variety of risk problems including credit risk prediction. One
               of the most active areas of recent research in ML has been the
               use of ensemble (combining) classifiers. Research indicates that
               ensemble individual classifiers lead to a significant
               improvement in classification performance by having them vote
               for the most popular class. This paper explores the predicted
               behaviour of five classifiers for different types of noise in
               terms of credit risk prediction accuracy, and how such accuracy
               could be improved by using classifier ensembles. Benchmarking
               results on four credit datasets and comparison with the
               performance of each individual classifier on predictive accuracy
               at various attribute noise levels are presented. The
               experimental evaluation shows that the ensemble of classifiers
               technique has the potential to improve prediction accuracy.",
  journal   = "Expert Syst. Appl.",
  publisher = "Elsevier",
  volume    =  37,
  number    =  4,
  pages     = "3326--3336",
  month     =  apr,
  year      =  2010,
  keywords  = "Machine learning; Supervised learning; Statistical pattern
               recognition; Ensemble; Credit risk prediction; Noise"
}

@ARTICLE{Galindo2000-cw,
  title     = "Credit Risk Assessment Using Statistical and Machine Learning:
               Basic Methodology and Risk Modeling Applications",
  author    = "Galindo, J and Tamayo, P",
  abstract  = "Risk assessment of financialintermediaries is an area of renewed
               interest due tothe financial crises of the 1980's and 90's.
               Anaccurate estimation of risk, and its use in corporateor global
               financial risk models, could be translatedinto a more efficient
               use of resources. One importantingredient to accomplish this
               goal is to find accuratepredictors of individual risk in the
               credit portfoliosof institutions. In this context we make a
               comparativeanalysis of different statistical and machine
               learningmodeling methods of classification on a mortgage
               loandata set with the motivation to understand theirlimitations
               and potential. We introduced a specificmodeling methodology
               based on the study of errorcurves. Using state-of-the-art
               modeling techniques webuilt more than 9,000 models as part of
               the study. Theresults show that CART decision-tree models
               providethe best estimation for default with an average
               8.31\%error rate for a training sample of 2,000 records. Asa
               result of the error curve analysis for this model weconclude
               that if more data were available,approximately 22,000 records, a
               potential 7.32\% errorrate could be achieved. Neural Networks
               provided thesecond best results with an average error of
               11.00\%.The K-Nearest Neighbor algorithm had an averageerror
               rate of 14.95\%. These results outperformed thestandard Probit
               algorithm which attained an averageerror rate of 15.13\%.
               Finally we discuss thepossibilities to use this type of accurate
               predictivemodel as ingredients of institutional and global
               riskmodels.",
  journal   = "Comput. Econ.",
  publisher = "Kluwer Academic Publishers",
  volume    =  15,
  number    = "1-2",
  pages     = "107--143",
  month     =  apr,
  year      =  2000,
  language  = "en"
}

@ARTICLE{Hinton2006-qd,
  title     = "A fast learning algorithm for deep belief nets",
  author    = "Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye",
  abstract  = "We show how to use ``complementary priors'' to eliminate the
               explaining-away effects that make inference difficult in densely
               connected belief nets that have many hidden layers. Using
               complementary priors, we derive a fast, greedy algorithm that
               can learn deep, directed belief networks one layer at a time,
               provided the top two layers form an undirected associative
               memory. The fast, greedy algorithm is used to initialize a
               slower learning procedure that fine-tunes the weights using a
               contrastive version of the wake-sleep algorithm. After
               fine-tuning, a network with three hidden layers forms a very
               good generative model of the joint distribution of handwritten
               digit images and their labels. This generative model gives
               better digit classification than the best discriminative
               learning algorithms. The low-dimensional manifolds on which the
               digits lie are modeled by long ravines in the free-energy
               landscape of the top-level associative memory, and it is easy to
               explore these ravines by using the directed connections to
               display what the associative memory has in mind.",
  journal   = "Neural Comput.",
  publisher = "MIT Press",
  volume    =  18,
  number    =  7,
  pages     = "1527--1554",
  month     =  jul,
  year      =  2006,
  language  = "en"
}

@ARTICLE{Jordan2015-uh,
  title     = "Machine learning: Trends, perspectives, and prospects",
  author    = "Jordan, M I and Mitchell, T M",
  abstract  = "Machine learning addresses the question of how to build
               computers that improve automatically through experience. It is
               one of today's most rapidly growing technical fields, lying at
               the intersection of computer science and statistics, and at the
               core of artificial intelligence and data science. Recent
               progress in machine learning has been driven both by the
               development of new learning algorithms and theory and by the
               ongoing explosion in the availability of online data and
               low-cost computation. The adoption of data-intensive
               machine-learning methods can be found throughout science,
               technology and commerce, leading to more evidence-based
               decision-making across many walks of life, including health
               care, manufacturing, education, financial modeling, policing,
               and marketing.",
  journal   = "Science",
  publisher = "science.sciencemag.org",
  volume    =  349,
  number    =  6245,
  pages     = "255--260",
  month     =  jul,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Barth2018-zn,
  title    = "Capital regulation with heterogeneous banks -- Unintended
              consequences of a too strict leverage ratio",
  author   = "Barth, Andreas and Seckinger, Christian",
  abstract = "Abstract We provide an equilibrium analysis of potential
              consequences from the introduction of a binding leverage ratio,
              as proposed in Basel III. If banks differ in their monitoring
              skills and their ability to successfully complete a risky
              investment project, a tighter leverage ratio does not only
              mitigate moral hazard arising from limited liability, but also
              carries an unintended consequence: high-quality banks are not
              allowed to absorb the entire supply of debt if it is too costly
              to issue new equity. This increases the market share of
              low-skilled bankers and decreases the average ability of
              operating banks. We further show that rising heterogeneity in the
              banking sector increases this negative effect.",
  journal  = "Journal of Banking \& Finance",
  volume   =  88,
  pages    = "455--465",
  year     =  2018,
  keywords = "Leverage ratio; Bank regulation; Risk-taking; Financial stability"
}

@ARTICLE{Kirilenko2013-oe,
  title    = "Moore's Law versus Murphy's Law: Algorithmic Trading and Its
              Discontents",
  author   = "Kirilenko, Andrei A and Lo, Andrew W",
  journal  = "J. Econ. Perspect.",
  volume   =  27,
  number   =  2,
  pages    = "51--72",
  month    =  may,
  year     =  2013
}

@ARTICLE{Kondratyev2017-we,
  title    = "{MVA} Optimisation with Machine Learning Algorithms",
  author   = "Kondratyev, Alexei and Giorgidze, George",
  abstract = "MVA is becoming a dominant XVA component in interdealer
              derivatives trading in the post Margin Reform environment. Unlike
              FVA which can be either a funding cost",
  month    =  oct,
  year     =  2017,
  keywords = "MVA, Machine Learning, Genetic Algorithm, Particle Swarm
              Optimisation, MVA Optimisation"
}

@ARTICLE{Kim2018-yu,
  title    = "Do players perform for pay? An empirical examination via {NFL}
              players' compensation contracts",
  author   = "Kim, Seoyoung and Sarin, Atulya and Sarin, Saagar",
  abstract = "How to properly compensate and incentivize players is an
              important question in the realm of professional sports, and more
              broadly, is a central question in contract design. With the
              increasing use of performance-based compensation packages and tax
              law favoring such compensation design, a natural question arises
              as to whether workers do indeed perform for pay. We examine this
              question in a setting that is not fraught with the typical
              measurement and identification problems found in many
              pay-performance settings. Specifically, we examine changes in a
              NFL player's Win Probability Added (WPA) and Expected Points
              Added (EPA) in response to his compensation-contract design.
              Overall, our paper provides evidence that players do indeed
              perform for (properly designed) pay, and has important
              implications for future work on compensation and incentive-based
              contract design.",
  journal  = "Journal of Banking \& Finance",
  volume   =  88,
  pages    = "330--346",
  month    =  mar,
  year     =  2018,
  keywords = "Pay-performance sensitivity; Incentive-based compensation; NFL /
              sports compensation; Win Probability Added (WPA); Expected Points
              Added (EPA); Sports analytics"
}

@ARTICLE{Fengler2009-uw,
  title     = "Arbitrage-free smoothing of the implied volatility surface",
  author    = "Fengler, Matthias R",
  abstract  = "The pricing accuracy and pricing performance of local volatility
               models depends on the absence of arbitrage in the implied
               volatility surface. An input implied volatility surface that is
               not arbitrage-free can result in negative transition
               probabilities and consequently mispricings and false greeks. We
               propose an approach for smoothing the implied volatility smile
               in an arbitrage-free way. The method is simple to implement,
               computationally cheap and builds on the well-founded theory of
               natural smoothing splines under suitable shape constraints.",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  volume    =  9,
  number    =  4,
  pages     = "417--428",
  month     =  jun,
  year      =  2009
}

@ARTICLE{Guidolin2003-ho,
  title     = "Option prices under Bayesian learning: implied volatility
               dynamics and predictive densities",
  author    = "Guidolin, Massimo and Timmermann, Allan",
  abstract  = "This paper shows that many of the empirical biases of the Black
               and Scholes option pricing model can be explained by Bayesian
               learning effects. In the context of an equilibrium model where
               dividend news evolve on a binomial lattice with unknown but
               recursively updated probabilities we derive closed-form pricing
               formulas for European options. Learning is found to generate
               asymmetric skews in the implied volatility surface and
               systematic patterns in the term structure of option prices. Data
               on S\&P 500 index option prices is used to back out the
               parameters of the underlying learning process and to predict the
               evolution in the cross-section of option prices. The proposed
               model leads to lower out-of-sample forecast errors and smaller
               hedging errors than a variety of alternative option pricing
               models, including Black--Scholes and a GARCH model.",
  journal   = "J. Econ. Dyn. Control",
  publisher = "Elsevier",
  volume    =  27,
  number    =  5,
  pages     = "717--769",
  month     =  mar,
  year      =  2003
}

@ARTICLE{Audrino2010-ph,
  title     = "Semi-parametric forecasts of the implied volatility surface
               using regression trees",
  author    = "Audrino, Francesco and Colangelo, Dominik",
  abstract  = "We present a new semi-parametric model for the prediction of
               implied volatility surfaces that can be estimated using machine
               learning algorithms. Given a reasonable starting model, a
               boosting algorithm based on regression trees sequentially
               minimizes generalized residuals computed as differences between
               observed and estimated implied volatilities. To overcome the
               poor predictive power of existing models, we include a grid in
               the region of interest, and implement a cross-validation
               strategy to find an optimal stopping value for the boosting
               procedure. Back testing the out-of-sample performance on a large
               data set of implied volatilities from S\&P 500 options, we
               provide empirical evidence of the strong predictive power of our
               model.",
  journal   = "Stat. Comput.",
  publisher = "Springer US",
  volume    =  20,
  number    =  4,
  pages     = "421--434",
  month     =  oct,
  year      =  2010,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Goncalves2006-kg,
  title     = "Predictable Dynamics in the {S\&P} 500 Index Options Implied
               Volatility Surface",
  author    = "Gon{\c c}alves, S{\'\i}lvia and Guidolin, Massimo",
  abstract  = "Recent evidence suggests that the parameters characterizing the
               implied volatility surface (IVS) in option prices are unstable.
               We study whether the resulting predictability patterns may be
               exploited. In a first stage we model the surface along
               cross‚Äêsectional moneyness and maturity dimensions. In a second
               stage we model the dynamics of the first‚Äêstage coefficients. We
               find that the movements of the S\&P 500 IVS are highly
               predictable. Whereas profitable delta‚Äêhedged positions can be
               set up under selective trading rules, profits disappear when we
               increase transaction costs and trade on wide segments of the
               IVS.",
  journal   = "The Journal of Business",
  publisher = "The University of Chicago Press",
  volume    =  79,
  number    =  3,
  pages     = "1591--1635",
  year      =  2006
}

@ARTICLE{Bernales2015-vr,
  title     = "Learning to smile: Can rational learning explain predictable
               dynamics in the implied volatility surface?",
  author    = "Bernales, Alejandro and Guidolin, Massimo",
  abstract  = "We develop a general equilibrium asset pricing model under
               incomplete information and rational learning in order to
               understand the unexplained predictability of option prices. In
               our model, the fundamental dividend growth rate is unknown and
               subject to breaks. Immediately after a break, there is
               insufficient information to price option contracts accurately.
               However, as new information arrives, a representative Bayesian
               agent recursively learns about the parameters of the process
               followed by fundamentals. We show that learning makes beliefs
               time-varying and generates predictability patterns across option
               contracts with different strike prices and maturities; as a
               result, the implied movements in the implied volatility surface
               resemble those observed empirically.",
  journal   = "Journal of Financial Markets",
  publisher = "Elsevier",
  volume    =  26,
  pages     = "1--37",
  month     =  nov,
  year      =  2015,
  keywords  = "Option pricing; Rational learning; Bayesian updating; Implied
               volatility; Predictability"
}

@ARTICLE{Ai2015-an,
  title     = "Volatility Risks and Growth Options",
  author    = "Ai, Hengjie and Kiku, Dana",
  abstract  = "We propose to measure growth opportunities by firms? exposure to
               idiosyncratic volatility news. Theoretically, we show that the
               value of a growth option increases in idiosyncratic volatility
               but its response to volatility of aggregate shocks can be either
               positive or negative depending on option moneyness. Empirically,
               we show that price sensitivity to variation in idiosyncratic
               volatility carries significant information about firms? future
               investment and growth even after controlling for conventional
               proxies of growth options such as book-to-market and other
               relevant firm characteristics. Consistent with our theoretical
               arguments, we also find that firm? exposure to aggregate
               volatility, while priced, does not help predict their future
               growth. Option-intensive firms identified using our
               idiosyncratic volatility-based measure earn a lower premium than
               do firms that rely more heavily on assets in place. This paper
               was accepted by Jerome Detemple, finance.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  62,
  number    =  3,
  pages     = "741--763",
  month     =  jun,
  year      =  2015
}

@ARTICLE{Sullivan2001-ke,
  title     = "Dangers of data mining: The case of calendar effects in stock
               returns",
  author    = "Sullivan, Ryan and Timmermann, Allan and White, Halbert",
  abstract  = "Economics is primarily a non-experimental science. Typically, we
               cannot generate new data sets on which to test hypotheses
               independently of the data that may have led to a particular
               theory. The common practice of using the same data set to
               formulate and test hypotheses introduces data-mining biases
               that, if not accounted for, invalidate the assumptions
               underlying classical statistical inference. A striking example
               of a data-driven discovery is the presence of calendar effects
               in stock returns. There appears to be very substantial evidence
               of systematic abnormal stock returns related to the day of the
               week, the week of the month, the month of the year, the turn of
               the month, holidays, and so forth. However, this evidence has
               largely been considered without accounting for the intensive
               search preceding it. In this paper we use 100 years of daily
               data and a new bootstrap procedure that allows us to explicitly
               measure the distortions in statistical inference induced by data
               mining. We find that although nominal p-values for individual
               calendar rules are extremely significant, once evaluated in the
               context of the full universe from which such rules were drawn,
               calendar effects no longer remain significant.",
  journal   = "J. Econom.",
  publisher = "Elsevier",
  volume    =  105,
  number    =  1,
  pages     = "249--286",
  month     =  nov,
  year      =  2001,
  keywords  = "Data mining; Market efficiency; Bootstrap testing; Calendar
               effects"
}

@ARTICLE{Hansen2005-mh,
  title     = "Testing the Significance of Calendar Effects",
  author    = "Hansen, Peter Reinhard and Lunde, Asger and Nason, James M",
  abstract  = "This paper studies tests of calendar effects in equity returns.
               It is necessary to control for all possible calendar effects to
               avoid spurious results. The auth",
  publisher = "papers.ssrn.com",
  month     =  jan,
  year      =  2005,
  keywords  = "Calendar effects, data mining, significance test"
}

@ARTICLE{Hentschel2003-cc,
  title     = "Errors in Implied Volatility Estimation",
  author    = "Hentschel, Ludger",
  abstract  = "Errors in Implied Volatility Estimation - Volume 38 Issue 4 -
               Ludger Hentschel",
  journal   = "Journal of Financial and Quantitative Analysis",
  publisher = "Cambridge University Press",
  volume    =  38,
  number    =  4,
  pages     = "779--810",
  month     =  dec,
  year      =  2003
}

@ARTICLE{Ait-Sahalia1998-hd,
  title     = "Nonparametric estimation of state-price densities implicit in
               financial asset prices",
  author    = "A{\"\i}t-Sahalia, Yacine and Lo, Andrew W",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  53,
  number    =  2,
  pages     = "499--547",
  year      =  1998
}

@BOOK{Cootner1964-hf,
  title     = "The Random Character of Stock Market Prices",
  author    = "Cootner, P H",
  publisher = "Cambridge, Mass. MIT Press",
  year      =  1964,
  keywords  = "bibtex-import"
}

@ARTICLE{West1996-pa,
  title     = "Asymptotic inference about predictive ability",
  author    = "West, Kenneth D",
  journal   = "Econometrica",
  publisher = "JSTOR",
  pages     = "1067--1084",
  year      =  1996
}

@MISC{Imbens_undated-ax,
  title        = "Causal Inference for Statistics, Social, and Biomedical
                  Sciences by Guido W. Imbens",
  booktitle    = "Cambridge Core",
  author       = "Imbens, Guido W and Rubin, Donald B",
  abstract     = "Cambridge Core - Econometrics and Mathematical Methods -
                  Causal Inference for Statistics, Social, and Biomedical
                  Sciences - by Guido W. Imbens",
  howpublished = "\url{https://www.cambridge.org/core/books/causal-inference-for-statistics-social-and-biomedical-sciences/71126BE90C58F1A431FE9B2DD07938AB}",
  note         = "Accessed: 2018-2-18"
}

@ARTICLE{Dudik2011-xv,
  title         = "Doubly Robust Policy Evaluation and Learning",
  author        = "Dudik, Miroslav and Langford, John and Li, Lihong",
  abstract      = "We study decision making in environments where the reward is
                   only partially observed, but can be modeled as a function of
                   an action and an observed context. This setting, known as
                   contextual bandits, encompasses a wide variety of
                   applications including health-care policy and Internet
                   advertising. A central task is evaluation of a new policy
                   given historic data consisting of contexts, actions and
                   received rewards. The key challenge is that the past data
                   typically does not faithfully represent proportions of
                   actions taken by a new policy. Previous approaches rely
                   either on models of rewards or models of the past policy.
                   The former are plagued by a large bias whereas the latter
                   have a large variance. In this work, we leverage the
                   strength and overcome the weaknesses of the two approaches
                   by applying the doubly robust technique to the problems of
                   policy evaluation and optimization. We prove that this
                   approach yields accurate value estimates when we have either
                   a good (but not necessarily consistent) model of rewards or
                   a good (but not necessarily consistent) model of past
                   policy. Extensive empirical comparison demonstrates that the
                   doubly robust approach uniformly improves over existing
                   techniques, achieving both lower variance in value
                   estimation and better policies. As such, we expect the
                   doubly robust approach to become common practice.",
  month         =  mar,
  year          =  2011,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1103.4601"
}

@ARTICLE{Athey2016-vb,
  title         = "Approximate Residual Balancing: {De-Biased} Inference of
                   Average Treatment Effects in High Dimensions",
  author        = "Athey, Susan and Imbens, Guido W and Wager, Stefan",
  abstract      = "There are many settings where researchers are interested in
                   estimating average treatment effects and are willing to rely
                   on the unconfoundedness assumption, which requires that the
                   treatment assignment be as good as random conditional on
                   pre-treatment variables. The unconfoundedness assumption is
                   often more plausible if a large number of pre-treatment
                   variables are included in the analysis, but this can worsen
                   the performance of standard approaches to treatment effect
                   estimation. In this paper, we develop a method for
                   de-biasing penalized regression adjustments to allow sparse
                   regression methods like the lasso to be used for
                   sqrt\{n\}-consistent inference of average treatment effects
                   in high-dimensional linear models. Given linearity, we do
                   not need to assume that the treatment propensities are
                   estimable, or that the average treatment effect is a sparse
                   contrast of the outcome model parameters. Rather, in
                   addition standard assumptions used to make lasso regression
                   on the outcome model consistent under 1-norm error, we only
                   require overlap, i.e., that the propensity score be
                   uniformly bounded away from 0 and 1. Procedurally, our
                   method combines balancing weights with a regularized
                   regression adjustment.",
  month         =  apr,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1604.07125"
}

@ARTICLE{Agarwal2016-wt,
  title         = "Making Contextual Decisions with Low Technical Debt",
  author        = "Agarwal, Alekh and Bird, Sarah and Cozowicz, Markus and
                   Hoang, Luong and Langford, John and Lee, Stephen and Li,
                   Jiaji and Melamed, Dan and Oshri, Gal and Ribas, Oswaldo and
                   Sen, Siddhartha and Slivkins, Alex",
  abstract      = "Applications and systems are constantly faced with decisions
                   that require picking from a set of actions based on
                   contextual information. Reinforcement-based learning
                   algorithms such as contextual bandits can be very effective
                   in these settings, but applying them in practice is fraught
                   with technical debt, and no general system exists that
                   supports them completely. We address this and create the
                   first general system for contextual learning, called the
                   Decision Service. Existing systems often suffer from
                   technical debt that arises from issues like incorrect data
                   collection and weak debuggability, issues we systematically
                   address through our ML methodology and system abstractions.
                   The Decision Service enables all aspects of contextual
                   bandit learning using four system abstractions which connect
                   together in a loop: explore (the decision space), log,
                   learn, and deploy. Notably, our new explore and log
                   abstractions ensure the system produces correct, unbiased
                   data, which our learner uses for online learning and to
                   enable real-time safeguards, all in a fully reproducible
                   manner. The Decision Service has a simple user interface and
                   works with a variety of applications: we present two live
                   production deployments for content recommendation that
                   achieved click-through improvements of 25-30\%, another with
                   18\% revenue lift in the landing page, and ongoing
                   applications in tech support and machine failure handling.
                   The service makes real-time decisions and learns
                   continuously and scalably, while significantly lowering
                   technical debt.",
  month         =  jun,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1606.03966"
}

@ARTICLE{Athey2016-iv,
  title         = "Generalized Random Forests",
  author        = "Athey, Susan and Tibshirani, Julie and Wager, Stefan",
  abstract      = "We propose generalized random forests, a method for
                   non-parametric statistical estimation based on random
                   forests (Breiman, 2001) that can be used to fit any quantity
                   of interest identified as the solution to a set of local
                   moment equations. Following the literature on local maximum
                   likelihood estimation, our method operates at a particular
                   point in covariate space by considering a weighted set of
                   nearby training examples; however, instead of using
                   classical kernel weighting functions that are prone to a
                   strong curse of dimensionality, we use an adaptive weighting
                   function derived from a forest designed to express
                   heterogeneity in the specified quantity of interest. We
                   propose a flexible, computationally efficient algorithm for
                   growing generalized random forests, develop a large sample
                   theory for our method showing that our estimates are
                   consistent and asymptotically Gaussian, and provide an
                   estimator for their asymptotic variance that enables valid
                   confidence intervals. We use our approach to develop new
                   methods for three statistical tasks: non-parametric quantile
                   regression, conditional average partial effect estimation,
                   and heterogeneous treatment effect estimation via
                   instrumental variables. A software implementation, grf for R
                   and C++, is available from CRAN.",
  month         =  oct,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1610.01271"
}

@INCOLLECTION{Brooks2010-yk,
  title     = "The way forward for real estate modelling and forecasting",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "434--440",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-ie,
  title     = "Real estate forecasting in practice",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "414--433",
  year      =  2010,
  address   = "Cambridge"
}

@ARTICLE{Brooks1999-te,
  title     = "The impact of economic and financial factors on {UK} property
               performance",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  abstract  = "This paper employs a vector autoregressive model to investigate
               the impact of macroeconomic and financial variables on a UK real
               estate return series. The results indicate that unexpected
               inflation, and the interest rate term spread have explanatory
               powers for the property market. However, the most significant
               influence on the real estate series are the lagged values of the
               real estate series themselves. We conclude that identifying the
               factors that have determined UK property returns over the past
               twelve years remains a difficult task.",
  journal   = "Journal of Property Research",
  publisher = "Routledge",
  volume    =  16,
  number    =  2,
  pages     = "139--152",
  month     =  jan,
  year      =  1999
}

@INCOLLECTION{Brooks2010-dc,
  title     = "Applications of regression analysis",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "194--224",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-qs,
  title     = "Cointegration in real estate markets",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "369--413",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-zn,
  title     = "Vector autoregressive models",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "337--368",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-du,
  title     = "Multi-equation structural models",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "303--336",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-if,
  title     = "Forecast evaluation",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "268--302",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-bi,
  title     = "Further issues in regression analysis",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "108--134",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-jy,
  title     = "Statistical tools for real estate analysis",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "41--71",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-zz,
  title     = "Time series models",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "225--267",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-mh,
  title     = "Diagnostic testing",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "135--193",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-hi,
  title     = "An overview of regression analysis",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "72--107",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-el,
  title     = "Introduction",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "1--20",
  year      =  2010,
  address   = "Cambridge"
}

@INCOLLECTION{Brooks2010-xy,
  title     = "Preface",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  editor    = "Brooks, Chris and Tsolacos, Sotiris",
  publisher = "Cambridge University Press",
  pages     = "xv--xviii",
  year      =  2010,
  address   = "Cambridge"
}

@BOOK{Brooks2010-gr,
  title     = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  abstract  = "Cambridge Core - Finance and Accountancy - Real Estate Modelling
               and Forecasting - by Chris Brooks",
  publisher = "Cambridge University Press",
  month     =  apr,
  year      =  2010
}

@INCOLLECTION{Brooks_undated-xu,
  title     = "References",
  booktitle = "Real Estate Modelling and Forecasting",
  author    = "Brooks, Chris and Tsolacos, Sotiris",
  pages     = "441--447"
}

@ARTICLE{Mills1991-sn,
  title     = "{THE} {INTERNATIONAL} {TRANSMISSION} {OF} {BOND} {MARKET}
               {MOVEMENTS}",
  author    = "Mills, Terence C and Mills, Alessandra G",
  abstract  = "The benefits from international portfolio diversification have
               been argued by various economists over the past 20 years (see,
               for example, Grubel (1968) and Levy and Sarnat (1970)), with the
               many findings that national stock markets were not highly
               interdependent being seen as evidence in favour of such
               diversification. In recent years, however, the much talked about
               'globalization'of financial markets has suggested that greater
               interdependence may now exist: in particular, unexpected
               developments in international financial markets",
  journal   = "Bull. Econ. Res.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  43,
  number    =  3,
  pages     = "273--281",
  month     =  jul,
  year      =  1991
}

@ARTICLE{Madhavan2012-no,
  title     = "{Exchange-Traded} Funds, Market Structure, and the Flash Crash",
  author    = "Madhavan, Ananth",
  abstract  = "The author analyzes the relationship between market structure
               and the flash crash. The proliferation of trading venues has
               resulted in a market that is more fragmented than ever. The
               author constructs measures to capture fragmentation and shows
               that they are important in explaining extreme price movements.
               New market structure reforms should help mitigate such market
               disruptions in the future but have not eliminated the
               possibility of another flash crash, albeit with a different
               catalyst.",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  68,
  number    =  4,
  pages     = "20--35",
  month     =  jul,
  year      =  2012
}

@UNPUBLISHED{Ben-David2014-yp,
  title       = "Do {ETFs} Increase Volatility?",
  author      = "Ben-David, Itzhak and Franzoni, Francesco and Moussawi, Rabih",
  abstract    = "We study whether exchange traded funds (ETFs)--an asset of
                 increasing importance--impact the volatility of their
                 underlying stocks. Using identification strategies based on
                 the mechanical variation in ETF ownership, we present evidence
                 that stocks owned by ETFs exhibit significantly higher
                 intraday and daily volatility. We estimate that an increase of
                 one standard deviation in ETF ownership is associated with an
                 increase of 16\% in daily stock volatility. The driving
                 channel appears to be arbitrage activity between ETFs and the
                 underlying stocks. Consistent with this view, the effects are
                 stronger for stocks with lower bid-ask spread and lending
                 fees. Finally, the evidence that ETF ownership increases stock
                 turnover suggests that ETF arbitrage adds a new layer of
                 trading to the underlying securities.",
  number      =  20071,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  apr,
  year        =  2014
}

@ARTICLE{Israeli2017-ii,
  title     = "Is there a dark side to exchange traded funds? An information
               perspective",
  author    = "Israeli, Doron and Lee, Charles M C and Sridharan, Suhas A",
  abstract  = "We examine whether an increase in ETF ownership is accompanied
               by a decline in pricing efficiency for the underlying component
               securities. Our tests show an increase in ETF ownership is
               associated with (1) higher trading costs (bid-ask spreads and
               market liquidity), (2) an increase in ``stock return
               synchronicity,'' (3) a decline in ``future earnings response
               coefficients,'' and (4) a decline in the number of analysts
               covering the firm. Collectively, our findings support the view
               that increased ETF ownership can lead to higher trading costs
               and lower benefits from information acquisition. This
               combination results in less informative security prices for the
               underlying firms.",
  journal   = "Rev Account Stud",
  publisher = "Springer US",
  volume    =  22,
  number    =  3,
  pages     = "1048--1083",
  month     =  sep,
  year      =  2017,
  language  = "en"
}

@INCOLLECTION{Aggarwal_undated-xa,
  title     = "The Growth of Global {ETFs} and Regulatory Challenges",
  booktitle = "Advances in Financial Economics",
  author    = "Aggarwal, Reena and Schofield, Laura",
  abstract  = "Abstract Purpose Exchange traded funds (ETFs) are one of the
               most innovative financial products listed on exchanges. As
               reflected by the size of the market, they have become popular
               among both retail and institutional investors. The original ETFs
               were simple and easy to understand; however, recent products,
               such as leveraged, inverse, and synthetic ETFs, are more complex
               and have additional dimensions of risk. The additional risks,
               complexity, and reduced transparency have resulted in heightened
               attention by regulators. This chapter aims to increase
               understanding of how ETFs function in the market and can
               potentially impact financial stability and market volatility.
               Design/methodology/approach We discuss the evolution of ETFs,
               growing regulatory concerns, and the various responses to these
               concerns. Findings We find that concerns related to systemic
               risk and excess volatility, suitability for retail investors,
               lack of transparency and liquidity, securities lending and
               counterparty exposure are being addressed by both market
               participants and policy makers. There has been a shift toward
               multiple counterparties, overcollateralization, disclosure of
               collateral holdings and index holdings. Originality/value The
               analysis contained in this chapter provides an understanding of
               the role of ETFs in the financial markets and the global economy
               that should be valuable to market participants, investors, and
               policy makers.",
  pages     = "77--102",
  chapter   =  3
}

@ARTICLE{Madhavan2015-qi,
  title    = "Price Dynamics and Liquidity of {Exchange-Traded} Funds",
  author   = "Madhavan, Ananth and Sobczyk, Aleksander",
  abstract = "Exchange traded funds (ETFs) have grown substantially in
              diversity and size in recent years, reflecting a broader shift
              towards passive, index investing. As a",
  month    =  apr,
  year     =  2015,
  keywords = "ETFs, premiums, discounts, price discovery"
}

@ARTICLE{Foucher2014-sx,
  title     = "Exchange-traded funds: Evolution of benefits, vulnerabilities
               and risks",
  author    = "Foucher, Ian and Gray, Kyle",
  journal   = "Bank of Canada Financial System Review",
  publisher = "Citeseer",
  pages     = "37--46",
  year      =  2014
}

@ARTICLE{Naumenko2015-ej,
  title    = "An Empirical Study on the Differences between Synthetic and
              Physical {ETFs}",
  author   = "Naumenko, Klym and Chystiakova, Olena",
  abstract = "This research focuses on the differences between synthetic and
              traditional exchange-traded funds in benchmark replication
              process. It extends previous literature by empirical examining
              the tracking ability of traditional and synthetic ETFs traded at
              the Swiss Stock Exchange offered by the leading providers in
              Europe. 35 equity ETFs are used in the sample. For both types of
              funds the average tracking error is estimated in four ways. The
              research demonstrates that both traditional and synthetic ETFs
              have significant tracking errors. The findings also show that,
              contrary to popular opinion, synthetic ETFs have higher tracking
              errors than physical. These facts are of a significant importance
              to private investors and portfolio managers, especially if the
              performance of portfolio manager is based on a comparison to the
              performance of a benchmark. Additionally, factors that influence
              tracking errors are estimated. The results suggest that tracking
              errors are influenced by expense ratio, number of securities in
              the benchmark and type of replication for all estimation methods.
              Average daily trading volume doesn't influence dependent
              variables except for one tracking error estimation method.",
  journal  = "International Journal of Economics and Finance",
  volume   =  7,
  number   =  3,
  pages    = "24",
  month    =  feb,
  year     =  2015,
  language = "en"
}

@ARTICLE{Shum2016-ba,
  title     = "Intraday Share Price Volatility and Leveraged {ETF} Rebalancing",
  author    = "Shum, Pauline and Hejazi, Walid and Haryanto, Edgar and Rodier,
               Arthur",
  abstract  = "Regulators and market participants are concerned about leveraged
               exchange-traded funds (ETFs)' role in driving up end-of-day
               volatility through hedging activities near the market's close.
               Leveraged ETF providers counter that the funds are too small to
               make a meaningful impact on volatility. For the period
               surrounding the financial crisis, 2006--11, we show that
               end-of-day volatility was positively and statistically
               significantly correlated with the ratio of potential rebalancing
               trades to total trading volume. The impacts were not all
               economically significant, but largest during the most volatile
               days. Given the predictable pattern of leveraged ETF hedging
               demands, implications for predatory trading are explored.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  20,
  number    =  6,
  pages     = "2379--2409",
  month     =  oct,
  year      =  2016
}

@ARTICLE{Bhattacharya2017-nk,
  title     = "Abusing {ETFs}",
  author    = "Bhattacharya, Utpal and Loos, Benjamin and Meyer, Steffen and
               Hackethal, Andreas",
  abstract  = "Using data from a large German brokerage, we find that
               individuals investing in passive exchange-traded funds (ETFs) do
               not improve their portfolio performance, even before transaction
               costs. Further analysis suggests that this is because of poor
               ETF timing as well as poor ETF selection (relative to the choice
               of low-cost, well-diversified ETFs). An exploration of investor
               heterogeneity shows that though investors who trade more have
               worse ETF timing, no groups of investors benefit by using ETFs,
               and no groups will lose by investing in low-cost,
               well-diversified ETFs.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  3,
  pages     = "1217--1250",
  month     =  may,
  year      =  2017
}

@ARTICLE{Benoit2017-gr,
  title     = "Where the Risks Lie: A Survey on Systemic Risk",
  author    = "Benoit, Sylvain and Colliard, Jean-Edouard and Hurlin,
               Christophe and P{\'e}rignon, Christophe",
  abstract  = "We review the extensive literature on systemic risk and connect
               it to the current regulatory debate. While we take stock of the
               achievements of this rapidly growing field, we identify a gap
               between two main approaches. The first one studies different
               sources of systemic risk in isolation, uses confidential data,
               and inspires targeted but complex regulatory tools. The second
               approach uses market data to produce global measures which are
               not directly connected to any particular theory, but could
               support a more efficient regulation. Bridging this gap will
               require encompassing theoretical models and improved data
               disclosure.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  1,
  pages     = "109--152",
  month     =  mar,
  year      =  2017
}

@INCOLLECTION{Skousen_undated-qi,
  title     = "Detecting and predicting financial statement fraud: The
               effectiveness of the fraud triangle and {SAS} No. 99",
  booktitle = "Corporate Governance and Firm Performance",
  author    = "Skousen, Christopher J and Smith, Kevin R and Wright, Charlotte
               J",
  abstract  = "This study empirically examines the effectiveness of Cressey's
               (1953) fraud risk factor framework adopted in SAS No. 99 in
               detection of financial statement fraud. According to Cressey's
               theory pressure, opportunity and rationalization are always
               present in fraud situations.We develop variables which serve as
               proxy measures for pressure, opportunity, and rationalization
               and test these variables using publicly available information
               relating to a set of fraud firms and a matched sample of
               no-fraud firms. We identify five pressure proxies and two
               opportunity proxies that are significantly related to financial
               statement fraud. We find that rapid asset growth, increased cash
               needs, and external financing are positively related to the
               likelihood of fraud. Internal versus external ownership of
               shares and control of the board of directors are also linked to
               increased incidence of financial statement fraud. Expansion in
               the number of independent members on the audit committee,
               however, is negatively related to the occurrence of fraud.
               Further testing indicates that the significant variables are
               also effective at predicting which of the sample firms were in
               the fraud versus no-fraud groups.",
  pages     = "53--81"
}

@ARTICLE{Qin2001-tv,
  title     = "The Error Term in the History of Time Series Econometrics",
  author    = "Qin, Duo and Gilbert, Christopher L",
  abstract  = "THE ERROR TERM IN THE HISTORY OF TIME SERIES ECONOMETRICS -
               Volume 17 Issue 2 - Duo Qin, Christopher L. Gilbert",
  journal   = "Econometric Theory",
  publisher = "Cambridge University Press",
  volume    =  17,
  number    =  2,
  pages     = "424--450",
  month     =  apr,
  year      =  2001
}

@ARTICLE{Hendry2004-hg,
  title     = "The nobel memorial prize for clive wj granger",
  author    = "Hendry, David F",
  journal   = "Scand. J. Econ.",
  publisher = "Wiley Online Library",
  volume    =  106,
  number    =  2,
  pages     = "187--213",
  year      =  2004
}

@ARTICLE{Qin2011-mz,
  title     = "Rise of {VAR} modelling approach",
  author    = "Qin, Duo",
  journal   = "J. Econ. Surv.",
  publisher = "Wiley Online Library",
  volume    =  25,
  number    =  1,
  pages     = "156--174",
  year      =  2011
}

@ARTICLE{Mills2009-qe,
  title     = "Modelling trends and cycles in economic time series: historical
               perspective and future developments",
  author    = "Mills, Terence C",
  abstract  = "This paper provides a retrospective on the modelling of trends
               and cycles in economic time series and considers where the
               research agenda currently stands and where future developments
               might lie. A brief survey of the early empirical research on
               trends and cycles is first provided before attention is focused
               on four papers published in 1961---our `annus mirabilis' of
               trend and cycle modelling---which we argue have been `prime
               movers' in various aspects of research in this area. The links
               from these papers to current research issues are then teased out
               before the likely future directions of research in both
               theoretical and applied aspects of the modelling of trends and
               cycles are considered.",
  journal   = "Cliometrica",
  publisher = "Springer-Verlag",
  volume    =  3,
  number    =  3,
  pages     = "221--244",
  month     =  oct,
  year      =  2009,
  language  = "en"
}

@ARTICLE{noauthor_undated-yt,
  title = "[{PDF]BOOTSTRAPPING} {THE} {ILLIQUIDITY} 1. Introduction Pricing
           complex .."
}

@ARTICLE{Nerlove1998-bj,
  title   = "Properties of Alternative Estimators of Dynamic Panel Models",
  author  = "Nerlove, Marc",
  journal = "Panel Data and Limited Dependent Variables, Essays in Honor of GS
             Maddala, Cambridge University Press, Cambridge",
  year    =  1998
}

@ARTICLE{Aven2016-ux,
  title    = "Risk assessment and risk management: Review of recent advances on
              their foundation",
  author   = "Aven, Terje",
  abstract = "Risk assessment and management was established as a scientific
              field some 30--40 years ago. Principles and methods were
              developed for how to conceptualise, assess and manage risk. These
              principles and methods still represent to a large extent the
              foundation of this field today, but many advances have been made,
              linked to both the theoretical platform and practical models and
              procedures. The purpose of the present invited paper is to
              perform a review of these advances, with a special focus on the
              fundamental ideas and thinking on which these are based. We have
              looked for trends in perspectives and approaches, and we also
              reflect on where further development of the risk field is needed
              and should be encouraged. The paper is written for readers with
              different types of background, not only for experts on risk.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  253,
  number   =  1,
  pages    = "1--13",
  month    =  aug,
  year     =  2016,
  keywords = "Risk assessment; Risk management; Foundational issues; Review"
}

@ARTICLE{Goodman2016-dl,
  title         = "European Union regulations on algorithmic decision-making
                   and a ``right to explanation''",
  author        = "Goodman, Bryce and Flaxman, Seth",
  abstract      = "We summarize the potential impact that the European Union's
                   new General Data Protection Regulation will have on the
                   routine use of machine learning algorithms. Slated to take
                   effect as law across the EU in 2018, it will restrict
                   automated individual decision-making (that is, algorithms
                   that make decisions based on user-level predictors) which
                   ``significantly affect'' users. The law will also
                   effectively create a ``right to explanation,'' whereby a
                   user can ask for an explanation of an algorithmic decision
                   that was made about them. We argue that while this law will
                   pose large challenges for industry, it highlights
                   opportunities for computer scientists to take the lead in
                   designing algorithms and evaluation frameworks which avoid
                   discrimination and enable explanation.",
  month         =  jun,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1606.08813"
}

@TECHREPORT{Sohnke_m_bartram_jurgen_branke_mehrshad_motahari2020-ze,
  title       = "Artificial Intelligence in Asset Management",
  author      = "{S{\"O}HNKE M. BARTRAM, J{\"U}RGEN BRANKE, MEHRSHAD MOTAHARI}",
  institution = "CFA Institute Research Foundation",
  year        =  2020
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Corwin2012-bx,
  title     = "A simple way to estimate bid‚Äêask spreads from daily high and low
               prices",
  author    = "Corwin, S A and Schultz, P",
  abstract  = "ABSTRACT We develop a bid-ask spread estimator from daily high
               and low prices. Daily high (low) prices are almost always buy
               (sell) trades. Hence, the high--low ratio reflects both the
               stock's variance and its bid-ask spread. Although the variance
               component of the high-- low ratio is proportional to the return
               interval, the spread component is not. This allows us to derive
               a spread estimator as a function of high--low ratios over 1-day
               and 2-day intervals. The estimator is easy to calculate, can be
               applied in a variety of research areas, and",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  year      =  2012
}

@ARTICLE{Amihud2002-ej,
  title    = "Illiquidity and stock returns: cross-section and time-series
              effects",
  author   = "Amihud, Yakov",
  abstract = "This paper shows that over time, expected market illiquidity
              positively affects ex ante stock excess return, suggesting that
              expected stock excess return partly represents an illiquidity
              premium. This complements the cross-sectional positive
              return--illiquidity relationship. Also, stock returns are
              negatively related over time to contemporaneous unexpected
              illiquidity. The illiquidity measure here is the average across
              stocks of the daily ratio of absolute stock return to dollar
              volume, which is easily obtained from daily stock data for long
              time series in most stock markets. Illiquidity affects more
              strongly small firm stocks, thus explaining time series
              variations in their premiums over time.",
  journal  = "Journal of Financial Markets",
  volume   =  5,
  number   =  1,
  pages    = "31--56",
  month    =  jan,
  year     =  2002,
  keywords = "Liquidity and asset pricing; Liquidity premium"
}

@ARTICLE{Roll1984-dm,
  title     = "A simple implicit measure of the effective bid-ask spread in an
               efficient market",
  author    = "Roll, Richard",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  39,
  number    =  4,
  pages     = "1127--1139",
  year      =  1984
}

@MISC{Abner2010-op,
  title     = "The {ETF} handbook",
  author    = "Abner, David",
  publisher = "John Wiley and Sons, Inc., Hoboken, New Jersey",
  year      =  2010
}

@ARTICLE{Gallagher2015-hm,
  title     = "Northern Ireland's property market bubble: a preliminary
               analysis",
  author    = "Gallagher, E and Bond, D and Ramsey, E",
  abstract  = "This article applies a recursive regression technique developed
               by Phillips and Yu (2011) to examine recent property market
               movements in both the Republic of Ireland and Northern Ireland
               in the context of an asset market bubble. This technique, which
               interprets explosiveness in the price series as evidence of the
               existence of a bubble, provides a method for identifying not
               only bubble behaviour but also a dating mechanism. Statistically
               significant bubble characteristics are identified in both
               series. For the Republic of Ireland, a prolonged period of
               explosiveness is detected ranging from 1996Q2 to 2007Q2.
               Interestingly, the explosiveness is stronger in the late 1990s
               and early 2000s. For Northern Ireland, the analysis identifies a
               short yet intense bubble when the market in the Republic had
               matured, extending from 2005Q3 to 2009Q1 and reaching a peak in
               2007Q2. These results taken together indicate the possibility of
               a spillover effect between the two regions; however, further
               research to investigate this possibility is necessary.",
  journal   = "Appl. Econ. Lett.",
  publisher = "Routledge",
  volume    =  22,
  number    =  1,
  pages     = "61--65",
  month     =  jan,
  year      =  2015
}

@INCOLLECTION{Wu2015-xi,
  title     = "The Future of Prediction: How Google Searches Foreshadow Housing
               Prices and Sales",
  booktitle = "Economic Analysis of the Digital Economy",
  author    = "Wu, Lynn and Brynjolfsson, Erik",
  publisher = "University of Chicago Press",
  pages     = "89--118",
  month     =  apr,
  year      =  2015
}

@ARTICLE{Quinn2018-he,
  title    = "Picking up the pennies in front of the bulldozer: The
              profitability of gilt based trading strategies",
  author   = "Quinn, Barry and Hanna, Alan and MacDonald, Fred",
  abstract = "We develop a simple cointegrated pairs trading strategy,
              including automatic risk control and adjustment for short-selling
              costs. We applied the strategy to the previously untested and
              highly liquid market for gilt futures. Profitability is exploited
              through the mean reversion in the relationship between long and
              medium gilt futures, and between medium and short gilt futures.
              Results show the potential for arbitrage profits exists, even
              using a relatively unsophisticated model, particularly between
              long and medium gilt futures.",
  journal  = "Finance Research Letters",
  volume   =  27,
  pages    = "214--222",
  month    =  dec,
  year     =  2018,
  keywords = "Arbitrage trading; Fixed income market; Market efficiency; UK
              gilt futures"
}

@ARTICLE{Chacko2016-iw,
  title    = "An index-based measure of liquidity",
  author   = "Chacko, George and Das, Sanjiv and Fan, Rong",
  abstract = "The liquidity shocks of '08--'09 revealed that measures of
              liquidity risk being used in most financial institutions turned
              out to be woefully inadequate. The construction of long-short
              portfolios based on liquidity proxies introduces errors such as
              extraneous risk factors and hedging error. We develop a new
              measure for liquidity risk using exchange-traded funds (ETFs)
              that attempts to minimize this error. We form a
              theoretically-supported measure that is long ETFs and short the
              underlying components of that ETF, i.e., long and short a similar
              set of underlying securities with the same weights. Pricing
              discrepancies between the long and short positions are driven by
              liquidity differences between the ETF and its underlying
              components. Constructing liquidity risk factors in a number of
              markets, we undertake several tests to validate our new liquidity
              metric. The results show that our illiquidity measure is strongly
              related to other measures of illiquidity, explains bond index
              returns, and reveals a systematic illiquidity component across
              fixed-income markets.",
  journal  = "Journal of Banking \& Finance",
  volume   =  68,
  pages    = "162--178",
  month    =  jul,
  year     =  2016,
  keywords = "ETFs; Liquidity; Immediacy"
}

@ARTICLE{Li2017-xe,
  title    = "Dynamic prediction of financial distress using Malmquist {DEA}",
  author   = "Li, Zhiyong and Crook, Jonathan and Andreeva, Galina",
  abstract = "Creditors such as banks frequently use expert systems to support
              their decisions when issuing loans and credit assessment has been
              an important area of application of machine learning techniques
              for decades. In practice, banks are often required to provide the
              rationale behind their decisions in addition to being able to
              predict the performance of companies when assessing corporate
              applicants for loans. One solution is to use Data Envelopment
              Analysis (DEA) to evaluate multiple decision-making units (DMUs
              or companies) which are ranked according to the best practice in
              their industrial sector. A linear programming algorithm is
              employed to calculate corporate efficiency as a measure to
              distinguish healthy companies from those in financial distress.
              This paper extends the cross-sectional DEA models to time-varying
              Malmquist DEA, since dynamic predictive models allow one to
              incorporate changes over time. This decision-support system can
              adjust the efficiency frontier intelligently over time and make
              robust predictions. Results based on a sample of 742 Chinese
              listed companies observed over 10 years suggest that Malmquist
              DEA offers insights into the competitive position of a company in
              addition to accurate financial distress predictions based on the
              DEA efficiency measures.",
  journal  = "Expert Syst. Appl.",
  volume   =  80,
  pages    = "94--106",
  month    =  sep,
  year     =  2017,
  keywords = "Malmquist DEA; Bankruptcy prediction; Financial distress;
              Efficiency; Dynamic model"
}

@MISC{noauthor_undated-ji,
  title = "treasury\_analysis\_economic\_impact\_of\_eu\_membership\_2016.pdf"
}

@ARTICLE{Duan2015-ut,
  title    = "{Non-Gaussian} Bridge Sampling with an Application",
  author   = "Duan, Jin-Chuan and Zhang, Changhao",
  abstract = "This paper provides a new bridge sampler that can efficiently
              generate sample paths, subject to some endpoint condition, for
              non-Gaussian dynamic models. This b",
  month    =  oct,
  year     =  2015,
  keywords = "sequential Monte Carlo, density tempering, Metropolis-Hastings,
              GARCH, systemic risk, infill estimation"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Altunbas2017-ly,
  title    = "Realized bank risk during the great recession",
  author   = "Altunbas, Yener and Manganelli, Simone and Marques-Ibanez, David",
  abstract = "We find that certain bank characteristics---aggressive credit
              growth, less reliance on deposit funding, and size---prior to the
              2007‚àí2009 crisis are consistently related to the systemic
              dimensions of bank risk during the crisis. Exposures to real
              estate play a major role explaining this relationship: Banks with
              larger real estate betas exhibited higher levels of systemic risk
              during the crisis. The impact of real estate betas on systemic
              risk increases for larger banks, following aggressive credit
              growth policies in the presence of housing bubbles. We show that
              the relationship between bank characteristics and risk could also
              be detected using measures of systemic risk calculated prior to
              the financial crisis.",
  journal  = "Journal of Financial Intermediation",
  volume   =  32,
  pages    = "29--44",
  month    =  oct,
  year     =  2017,
  keywords = "Bank risk; Bank characteristics; Real estate; Loan growth; Great
              recession"
}

@ARTICLE{Chu2015-qq,
  title     = "Bank consolidation and stability: The Canadian experience,
               1867-1935",
  author    = "Chu, K H",
  abstract  = "Transfer-function estimation results for bank M\&As in Canada
               during 1867-1935 support the concentration-stability hypothesis.
               The systemic stability is attributed to risk reduction through
               geographic diversification as 2/3 of the M\&As were
               cross-province acquisitions. Furthermore, our empirical findings
               together with a probabilistic theoretical model support the
               efficiency hypothesis rather than the imminent failure
               hypothesis. They not only shed light on the debate in the
               literature but also have policy lessons for M\&As today. More
               specifically, two mega-mergers would have been denied according
               to the concentration ratio or HHI criteria commonly used in
               merger guidelines today, thus hindering banks' risk reduction
               through consolidation. \copyright{} 2015 Elsevier B.V.",
  journal   = "Journal of Financial Stability",
  publisher = "Elsevier",
  volume    =  21,
  pages     = "46--60",
  year      =  2015,
  keywords  = "Bank mergers and acquisitions; Banking stability; Canadian
               banking; Geographic diversification; Z-score"
}

@INCOLLECTION{Chapman2015-hz,
  title     = "International banking and liquidity risk transmission: Evidence
               from Canada",
  booktitle = "{IMF} Economic Review",
  author    = "Chapman, J and Damar, H E",
  abstract  = "This paper investigates how liquidity conditions in Canada may
               affect domestic and/or foreign lending of globally active
               Canadian banks, and whether this transmission is influenced by
               individual bank characteristics. It finds that Canadian banks
               expanded their foreign lending during the recent financial
               crisis, often through acquisitions of foreign banks. It also
               finds evidence that internal capital markets play a role in the
               lending activities of globally active Canadian banks during
               times of heightened liquidity risk. \copyright{} 2015
               International Monetary Fund.",
  publisher = "Palgrave Macmillan Ltd.",
  volume    =  63,
  pages     = "455--478",
  year      =  2015
}

@ARTICLE{Di_Matteo2015-xb,
  title     = "The evolution of financial intermediation: Evidence from
               19th-century Ontario microdata",
  author    = "Di Matteo, L and Redish, A",
  abstract  = "Microdata for Ontario decedents in 1892 and 1902 is analyzed to
               help explain the dramatic growth of the Canadian banking system
               in the late 19th century. Combining data from probate
               inventories with census data at the township level, we find that
               the expansion of deposit banking happened at the extensive
               rather than intensive margin and was correlated with the
               expansion of the branch network of the banking system, although
               we cannot assign causation. Wealth and urbanization help to
               explain the growth of deposit banking but the significance of a
               dummy variable for 1902 points to other time-correlated factors
               such as innovations in transportation and financial innovations
               that lowered costs and facilitated access to banking services.
               \copyright{} 2015 Canadian Economics Association.",
  journal   = "Canadian Journal of Economics",
  publisher = "Blackwell Publishing Ltd",
  volume    =  48,
  number    =  3,
  pages     = "963--987",
  year      =  2015,
  keywords  = "banking; census; economic history; financial system; innovation;
               nineteenth century; Canada; Ontario [Canada]"
}

@ARTICLE{Bordo2015-bf,
  title     = "Why didn't Canada have a banking crisis in 2008 (or in 1930, or
               1907, or...)?",
  author    = "{Bordo} and Redish, A and Rockoff, H",
  abstract  = "The financial crisis of 2008 engulfed the banking system of the
               US and many large European countries. Canada was a notable
               exception. In this article we argue that the structure of
               financial systems is path-dependent. The relative stability of
               the Canadian banks in the recent crisis compared to the US in
               our view reflected the original institutional foundations laid
               in place in the early nineteenth century in the two countries.
               The Canadian concentrated banking system that had evolved by the
               end of the twentieth century had absorbed the key sources of
               systemic risk-the mortgage market and investment banking-and was
               tightly regulated by one overarching regulator. In contrast, the
               relatively weak, fragmented, and crisis-prone US banking system
               that had evolved since the early nineteenth century led to the
               rise of securities markets, investment banks, and money market
               mutual funds (the shadow banking system) combined with multiple
               competing regulatory authorities. The consequence was that the
               systemic risk that led to the crisis of 2007-8 was not
               contained. \copyright{} Economic History Society 2014.",
  journal   = "Econ. Hist. Rev.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  68,
  number    =  1,
  pages     = "218--243",
  year      =  2015
}

@ARTICLE{Beaton2014-wz,
  title     = "The propagation of {U.S}. shocks to Canada: Understanding the
               role of real financial linkages",
  author    = "Beaton, K and Lalonde, R and Snudden, S",
  abstract  = "This paper examines the role of financial frictions in affecting
               the transmission of U.S. real and financial shocks to Canada
               using a dynamic stochastic general-equilibrium model with an
               active banking sector and financial frictions. We find that the
               U.S. banking and interbank markets can be a potentially
               important source of variability of Canadian output and
               inflation-consistent with the financial crisis. The presence of
               both the demand and the real supply sides of credit in the model
               help to capture the stylized facts of both the domestic and the
               international business cycles. \copyright{} Canadian Economics
               Association.",
  journal   = "Canadian Journal of Economics",
  publisher = "Blackwell Publishing Ltd",
  volume    =  47,
  number    =  2,
  pages     = "466--493",
  year      =  2014,
  keywords  = "banking; business cycle; demand analysis; financial crisis;
               financial market; general equilibrium analysis; inflation;
               stochasticity; Canada; United States"
}

@ARTICLE{Calmes2009-rm,
  title    = "Financial structure change and banking income: A {Canada-U.S}.
              comparison",
  author   = "Calm{\`e}s, C and Liu, Y",
  abstract = "Data suggest that the Canadian financial structure, and
              particularly indirect finance (e.g., banking), have become more
              market-oriented. We associate this financial trend in part with
              the regulatory changes that have occurred in Canada since the
              1980s. Financial intermediaries are increasingly involved with
              financial market activities-e.g. off-balance sheet (OBS)
              activities such as underwriting securities. In this article we
              analyze the noninterest income attributable to these financial
              market activities. We find that the variance of Canadian banks'
              aggregate operating-income growth is rising because of the
              increased contribution of noninterest income. Overall, our
              analysis corroborates the U.S. findings of Stiroh and Rumble
              (Stiroh, K., 2006. A portfolio view of banking with interest and
              noninterest assets. Jounal of Money, Credit, and Banking 38,
              1351-1361; Stiroh, K., Rumble, A., 2006. The darkside of
              diversification: the case of U.S. financial holding companies.
              Journal of Banking and Finance 30, 2131-2161): by contributing to
              banking income volatility, market-oriented activities do not
              necessarily yield straightforward diversification benefits to
              Canadian banks. \copyright{} 2007 Elsevier B.V. All rights
              reserved.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  19,
  number   =  1,
  pages    = "128--139",
  year     =  2009,
  keywords = "Diversification; Noninterest income; Regulatory changes"
}

@INCOLLECTION{Ivaschenko2007-hn,
  title     = "Competition in the banking system",
  booktitle = "{IMF} Occasional Papers",
  author    = "Ivaschenko, I V",
  editor    = "{Bayoumi T.} and {Klyuev V.} and {Muehleisen M.}",
  abstract  = "There has been some concern about the merger of the largest
               banks in Canada which might result in affecting the competition
               and consumer interests. Another is that if mergers were not
               encouraged, it may constrain a banks' ability to compete
               internationally, given the size of the Canadian market.
               Meanwhile, an industrial organization (IO) approach is used to
               measure the degree of competition among the largest banks in
               Canada compared with other nations. It is suggested by standard
               IO methods applied o the banking industry hat perfect
               competition fosters allocative efficiency by channeling credit
               to its most productive use. They also suggest that perfect
               competition achieves productive efficiency because it maximizes
               the quantity of credit supplied at the lowest interest rate.
               Using a Panzar-Rosse approach to measure market power and
               assuming a perfect competition, it was showed that Canadian
               banks have grown slowly than their major foreign competitors.
               For instance, they lag in profitability compared with their US
               counterparts. A side discovery has it that the number of large
               banks in a country is not as important for the level of
               competition as their combine market share. Nevertheless, the
               Canadian banking system remains to be strongly competitive.",
  pages     = "116--119",
  year      =  2007,
  keywords  = "banking; competition (economics); interest rate; merger;
               profitability; Canada; North America"
}

@ARTICLE{Allen2007-mi,
  title    = "Efficiency and economies of scale of large Canadian banks",
  author   = "Allen, J and Liu, Y",
  abstract = "This paper measures the economies of scale of Canada's six
              largest banks and their cost-efficiency over time. Using a unique
              panel data set from 1983 to 2003, we estimate pooled cost
              functions and derive measures of relative efficiency and
              economies of scale. The disaggregation of the data allows us to
              include non-traditional outputs as well as time-varying,
              bank-specific effects. Our model leads us to reject constant
              returns to scale. These findings suggest there are potential
              scale benefits in the Canadian banking industry. We also find
              that technological and regulatory changes have had significant
              positive effects on the banks' cost structure. \copyright{} 2007
              The Canadian Economics Association.",
  journal  = "Canadian Journal of Economics",
  volume   =  40,
  number   =  1,
  pages    = "225--244",
  year     =  2007,
  keywords = "banking; data set; economic structure; economy of scale;
              efficiency measurement; panel data; returns to scale; Canada;
              North America"
}

@ARTICLE{Bordo1994-ba,
  title    = "The {U.S}. Banking System From a Northern Exposure: Stability
              versus Efficiency",
  author   = "{Bordo} and Rockoff, H and Redish, A",
  abstract = "This article asks whether the vaunted comparative stability of
              the Canadian banking system has been purchased at the cost of
              creating an oligopoly. We assembled a data set that compares bank
              failures, lending rates, interest paid on deposits, and related
              variables over the period 1920 to 1980. Our principal findings
              are (1) interest rates paid on deposits were generally higher in
              Canada; (2) interest income received on securities was generally
              slightly higher in Canada; (3) interest rates charged on loans
              were generally quite similar; and (4) net rates of return to
              equity were generally higher in Canada than in the United States.
              \copyright{} 1994, The Economic History Association. All rights
              reserved.",
  journal  = "J. Econ. Hist.",
  volume   =  54,
  number   =  2,
  pages    = "325--341",
  year     =  1994
}

@ARTICLE{Nathan1989-kn,
  title    = "Competition and contestability in Canada's financial system:
              empirical results",
  author   = "Nathan, A and Neave, E H",
  abstract = "This paper presents empirical assessments of the competitiveness
              in the Canadian banking, trust company, and mortgage company
              industries, which support the view that parts of Canada's
              financial system exhibit characteristics of contestability. Since
              asset concentrations in Canada's financial system do not seem to
              be decreasing, and since it has long been believed that financial
              industry concentration impairs competitiveness, making such
              assessments seems worthwhile. We use a non-structural estimation
              technique to evaluate the elasticity of total revenues with
              respect to changes in input prices. The significantly positive
              values of the elasticity measure indicate that Canada's financial
              system does not exhibit monopoly power. -Authors",
  journal  = "Canadian Journal of Economics",
  volume   =  22,
  number   =  3,
  pages    = "576--594",
  year     =  1989,
  keywords = "competitiveness; estimation technique; financial service; price
              elasticity; Canada"
}

@ARTICLE{Ametrano2009-hl,
  title    = "Bootstrapping the Illiquidity: Multiple Yield Curves Construction
              for Market Coherent Forward Rates Estimation",
  author   = "Ametrano, Ferdinando M and Bianchetti, Marco",
  abstract = "The large basis spreads observed on the interest rate market
              since the liquidity crisis of summer 2007 imply that different
              yield curves are required for market",
  month    =  may,
  year     =  2009,
  keywords = "Liquidity crisis, credit crunch, interest rates, yield curve,
              forward curve, discount curve, bootstrapping, pricing, hedging,
              interest rate derivatives, Deposit, FRA, Futures, Swap, Basis
              Swap, turn of year, spline, QuantLib"
}

@ARTICLE{Ametrano2009-ib,
  title     = "Bootstrapping the illiquidity",
  author    = "Ametrano, F and Bianchetti, Marco",
  journal   = "Modelling Interest Rates: Advances for Derivatives Pricing",
  publisher = "Risk Books",
  year      =  2009
}

@ARTICLE{Delis2011-cw,
  title     = "Supervisory Effectiveness and Bank Risk",
  author    = "Delis, Manthos D and Staikouras, Panagiotis K",
  abstract  = "This paper investigates the role of banking supervision in
               controlling bank risk. Banking supervision is measured in terms
               of enforcement outputs (i.e., on-site audits and sanctions). Our
               results show an inverted U-shaped relationship between on-site
               audits and bank risk, while the relationship between sanctions
               and risk appears to be linear and negative. We also consider the
               combined effect of effective supervision and banking regulation
               (in the form of capital and market discipline requirements) on
               bank risk. We find that effective supervision and market
               discipline requirements are important and complementary
               mechanisms in reducing bank fragility. This is in contrast to
               capital requirements, which prove to be rather futile in
               controlling bank risk, even when supplemented with a higher
               volume of on-site audits and sanctions.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  15,
  number    =  3,
  pages     = "511--543",
  month     =  jul,
  year      =  2011
}

@ARTICLE{Delis2017-no,
  title     = "Bank Market Power and Firm Performance",
  author    = "Delis, Manthos D and Kokas, Sotirios and Ongena, Steven",
  abstract  = "Does market power of banks affect firm performance? To answer
               this question we examine 25,236 syndicated loan facilities
               granted between 2000 and 2010 by 296 banks to 9,029 US
               non-financial firms. Accounting for both observed and unobserved
               bank and firm heterogeneity, we find that firms that were
               recently poorly performing obtain loans from banks with more
               market power. However, in the year after loan origination market
               power positively affects firm performance, but only if it is not
               too high. Our estimates thus suggest that bank market power can
               facilitate access to credit by poorly performing firms, yet at
               the same time also boosts the performance of the firms that
               obtain credit.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  1,
  pages     = "299--326",
  month     =  mar,
  year      =  2017
}

@ARTICLE{Caiazza2014-mo,
  title     = "Bank Stability and Enforcement Actions in Banking",
  author    = "Caiazza, Stefano and Cotugno, Matteo and Fiordelisi, Franco and
               Stefanelli, Valeria",
  abstract  = "This paper analyzes the causes and consequences of the
               enforcement actions (sanctions) imposed by supervisory
               authorities for banks. Focusing on a sample of Ita",
  publisher = "papers.ssrn.com",
  month     =  aug,
  year      =  2014,
  keywords  = "Enforcement actions, Supervisory, Credit Risk"
}

@ARTICLE{Delis2013-tn,
  title     = "Enforcement actions and bank behavior",
  author    = "Delis, Manthos D and Staikouras, Panagiotis and Tsoumas, Chris",
  abstract  = "Employing a unique data set for the period 2000-2010, this paper
               examines the impact of enforcement actions (sanctions) on bank
               capital, risk, and performance. We find that high risk weighted
               asset ratios tend to attract supervisory intervention. Sanctions
               whose cause lies at the core of bank safety and soundness
               curtail the risk-weighted asset ratio, but amplify the risk of
               insolvency and returns volatility, which implies that these
               sanctions do not improve the risk profile of the involved banks,
               possibly because they come too late. Sanctions targeting
               internal control and risk management weaknesses appear to be
               well-timed and to restrain further increases in the
               risk-weighted assets ratio without impairing bank fundamentals.
               Sanctions against institution-affiliated parties do not seem to
               affect bank behavior. We suggest that supervisory attention
               should be placed on the timely uncovering of internal control
               and risk management deficiencies as this would allow the early
               tackling of the origins of financial distress.",
  journal   = "MPRA working paper",
  publisher = "mpra.ub.uni-muenchen.de",
  month     =  jan,
  year      =  2013,
  keywords  = "Enforcement actions; banking supervision; capital; bank risk;
               bank performance",
  language  = "en"
}

@ARTICLE{Delis2014-gd,
  title     = "The risk of financial intermediaries",
  author    = "Delis, Manthos D and Hasan, Iftekhar and Tsionas, Efthymios G",
  abstract  = "This paper reconsiders the formal estimation of bank risk using
               the variability of the profit function. In our model, point
               estimates of the variability of profits are derived from a model
               where this variability is endogenous to other bank
               characteristics, such as capital and liquidity. We estimate the
               new model on the entire panel of US banks, spanning the period
               1985q1--2012q4. The findings show that bank risk was fairly
               stable up to 2001 and accelerated quickly thereafter up to 2007.
               We also establish that the risk of the relatively large banks
               and banks that failed in the subprime crisis is higher than the
               industry's average. Thus, we provide a new leading indicator,
               which is able to forecast future solvency problems of banks.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  44,
  pages     = "1--12",
  month     =  jul,
  year      =  2014,
  keywords  = "Estimation of risk; Profit function; Financial institutions;
               Banks; Endogenous risk; US banking sector"
}

@ARTICLE{Willcocks2009-pc,
  title     = "{UK} Housing Market: Time Series Processes with Independent and
               Identically Distributed Residuals",
  author    = "Willcocks, Geoff",
  abstract  = "The paper examines whether a univariate data generating process
               can be identified which explains the data by having residuals
               that are independent and identically distributed, as verified by
               the BDS test. The stationary first differenced natural log
               quarterly house price index is regressed, initially with a
               constant variance and then with a conditional variance. The only
               regression function that produces independent and identically
               distributed standardised residuals is a mean process based on a
               pure random walk format with Exponential GARCH in mean for the
               conditional variance. There is an indication of an asymmetric
               volatility feedback effect but higher frequency data is required
               to confirm this. There could be scope for forecasting the index
               but this is tempered by the reduction in the power of the BDS
               test if there is a non-linear conditional variance process.",
  journal   = "J. Real Estate Fin. Econ.",
  publisher = "Springer US",
  volume    =  39,
  number    =  4,
  pages     = "403",
  month     =  nov,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Clements2018-pk,
  title    = "Do Macroforecasters Herd?",
  author   = "Clements, Michael P",
  abstract = "Abstract We show that typical tests of whether forecasters herd
              will falsely indicate herding behavior for a variety of types of
              behavior and forecasting environments that give rise to
              disagreement among forecasters. We establish that forecasters
              will appear to herd if differences between them reflect noise as
              opposed to private information, or if they arise from
              informational rigidities. Noise can have a behavioral
              interpretation and if so will depend on the behavioral model
              under consideration. An application of the herding tests to U.S.
              quarterly survey forecasts of inflation and output growth data
              1981?2013 does not support herding behavior.",
  journal  = "J. Money Credit Bank.",
  volume   =  50,
  number   = "2-3",
  pages    = "265--292",
  month    =  mar,
  year     =  2018
}

@ARTICLE{White2000-tb,
  title     = "A reality check for data snooping",
  author    = "White, Halbert",
  journal   = "Econometrica",
  publisher = "Wiley Online Library",
  volume    =  68,
  number    =  5,
  pages     = "1097--1126",
  year      =  2000
}

@ARTICLE{Alam2014-wi,
  title     = "Banking mergers--an application of matching strategy",
  author    = "Alam, Nafis and Lee Ng, Seok",
  journal   = "Review of Accounting and Finance",
  publisher = "Emerald Group Publishing Limited",
  volume    =  13,
  number    =  1,
  pages     = "2--23",
  year      =  2014
}

@ARTICLE{Molony2010-az,
  title    = "Charity, Truth and Corporate Governance",
  author   = "Molony, Thomas",
  abstract = "Charity and truth seem very foreign to corporate governance. Yet,
              both are absolutely necessary to answer fundamental questions --
              even those that arise in corp",
  month    =  may,
  year     =  2010
}

@ARTICLE{Fisher2000-iu,
  title     = "Investor Sentiment and Stock Returns",
  author    = "Fisher, Kenneth L and Statman, Meir",
  abstract  = "Investors are not all alike, and neither are their sentiments.
               We show that the sentiment of Wall Street strategists is
               unrelated to the sentiment of individual investors or that of
               newsletter writers, although the sentiment of the last two
               groups is closely related. Sentiment can be useful for tactical
               asset allocation. We found a negative relationship between the
               sentiment of each of these three groups and future stock
               returns, and the relationship is statistically significant for
               Wall Street strategists and individual investors.",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  56,
  number    =  2,
  pages     = "16--23",
  month     =  mar,
  year      =  2000
}

@ARTICLE{Schmeling2009-bc,
  title    = "Investor sentiment and stock returns: Some international evidence",
  author   = "Schmeling, Maik",
  abstract = "We examine whether consumer confidence -- as a proxy for
              individual investor sentiment -- affects expected stock returns
              internationally in 18 industrialized countries. In line with
              recent evidence for the U.S., we find that sentiment negatively
              forecasts aggregate stock market returns on average across
              countries. When sentiment is high, future stock returns tend to
              be lower and vice versa. This relation also holds for returns of
              value stocks, growth stocks, small stocks, and for different
              forecasting horizons. Finally, we employ a cross-sectional
              perspective and provide evidence that the impact of sentiment on
              stock returns is higher for countries which have less market
              integrity and which are culturally more prone to herd-like
              behavior and overreaction.",
  journal  = "Journal of Empirical Finance",
  volume   =  16,
  number   =  3,
  pages    = "394--408",
  month    =  jun,
  year     =  2009,
  keywords = "Consumer confidence; Growth stocks; Investor sentiment; Noise
              trader; Predictive regressions; Value stocks"
}

@ARTICLE{Bogoev2017-ye,
  title    = "Detection of algorithmic trading",
  author   = "Bogoev, Dimitar and Karam, Arz{\'e}",
  abstract = "We develop a new approach to reflect the behavior of algorithmic
              traders. Specifically, we provide an analytical and tractable way
              to infer patterns of quote volatility and price momentum
              consistent with different types of strategies employed by
              algorithmic traders, and we propose two ratios to quantify these
              patterns. Quote volatility ratio is based on the rate of
              oscillation of the best ask and best bid quotes over an extremely
              short period of time; whereas price momentum ratio is based on
              identifying patterns of rapid upward or downward movement in
              prices. The two ratios are evaluated across several asset
              classes. We further run a two-stage Artificial Neural Network
              experiment on the quote volatility ratio; the first stage is used
              to detect the quote volatility patterns resulting from
              algorithmic activity, while the second is used to validate the
              quality of signal detection provided by our measure.",
  journal  = "Physica A: Statistical Mechanics and its Applications",
  volume   =  484,
  pages    = "168--181",
  month    =  oct,
  year     =  2017,
  keywords = "Algorithmic trading patterns; Quote volatility; Price momentum;
              Artificial Neural Network"
}

@ARTICLE{Chen2017-uo,
  title     = "Empirical Investigation of an Equity Pairs Trading Strategy",
  author    = "Chen, Huafeng and Chen, Shaojun and Chen, Zhuo and Li, Feng",
  abstract  = "We show that an equity pairs trading strategy generates large
               and significant abnormal returns. We find that two components of
               the trading signal (i.e., short-term reversal and pairs
               momentum) have different dynamic and cross-sectional properties.
               The pairs momentum is largely explained by the one-month version
               of the industry momentum. Therefore, the pairs trading profits
               are largely explained by the short-term reversal and a version
               of the industry momentum. The online appendix is available at
               https://doi.org/10.1287/mnsc.2017.2825.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  month     =  sep,
  year      =  2017
}

@ARTICLE{Mester2017-qh,
  title    = "The nexus of macroprudential supervision, monetary policy, and
              financial stability",
  author   = "Mester, Loretta J",
  abstract = "I discuss changes to bank supervision and regulation since the
              financial crisis. Microprudential supervision promotes the safety
              and soundness of individual institutions, while macroprudential
              supervision focuses on emerging risks to financial system
              stability. I highlight tools for implementing this
              macroprudential approach to promoting financial stability, and
              discuss the interactions and proper relationship between monetary
              policy and financial stability. While macroprudential tools
              should be the first line of defense against emerging financial
              imbalances, in cases where those tools proved to be inadequate to
              limit risks to financial stability, monetary policy should be
              considered as a possible defense.",
  journal  = "Journal of Financial Stability",
  volume   =  30,
  pages    = "177--180",
  month    =  jun,
  year     =  2017,
  keywords = "Banking supervision and regulation; Financial stability; Monetary
              policy; Macroprudential supervision"
}

@BOOK{Box2005-wm,
  title     = "Statistics for experimenters: design, innovation, and discovery",
  author    = "Box, George E P and Hunter, J Stuart and Hunter, William Gordon",
  publisher = "Wiley-Interscience New York",
  volume    =  2,
  year      =  2005
}

@ARTICLE{Cookson2018-wy,
  title    = "When saving is gambling",
  author   = "Cookson, J Anthony",
  abstract = "Prize-linked savings (PLS) accounts, which allocate interest
              using lottery payments rather than fixed interest, encourage
              savings by appealing to households' gambling preferences. I
              introduce new data on casino cash withdrawals to measure
              gambling, and examine how individual gambling expenditures
              respond to the introduction of PLS in Nebraska using a
              difference-in-differences design. After PLS is introduced,
              individuals who live in counties that offer PLS reduce gambling
              by at least 3\% more than unaffected individuals. The
              substitution effect is stronger in low-frills gambling
              environments, which most resemble PLS, indicating that these
              accounts fulfill the desire to gamble.",
  journal  = "J. financ. econ.",
  month    =  apr,
  year     =  2018,
  keywords = "Prize linked savings; Gambling; Risk aversion; Financial
              literacy; Credit unions"
}

@ARTICLE{Cleary2016-ab,
  title    = "An efficient and functional model for predicting bank distress:
              In and out of sample evidence",
  author   = "Cleary, Sean and Hebb, Greg",
  abstract = "We examine the failures of 132 U.S. banks over the 2002--2009
              period using discriminant analysis and successfully distinguish
              between banks that failed and those that didn't 92\% of the time
              using in-sample quarterly data. Our two most important variables
              are related to bank capital and loan quality, as one might
              expect; although bank profitability is also important. The
              resulting model is then used out-of-sample to examine the failure
              of 191 banks during 2010--11, with predictive accuracy in the
              90--95\% range. Our results demonstrate that our model can also
              easily be applied to a large number of firms (even those that
              don't fail) and does an excellent job of distinguishing healthy
              from distressed banks. Combining this effectiveness with its ease
              of implementation makes it very functional. Such a model should
              be of obvious interest to regulators, analysts, and all those
              with a direct interest in assessing bank financial health.",
  journal  = "Journal of Banking \& Finance",
  volume   =  64,
  pages    = "101--111",
  month    =  mar,
  year     =  2016,
  keywords = "Bank failures; Financial distress; Financial crisis"
}

@ARTICLE{Loipersberger2018-ec,
  title    = "The effect of supranational banking supervision on the financial
              sector: Event study evidence from Europe",
  author   = "Loipersberger, Florian",
  abstract = "This paper investigates how the introduction of the Single
              Supervisory Mechanism, the European Union's implementation of
              harmonized banking supervision, has affected the banking sector
              in Europe. I perform an event study on banks' stock returns and
              find evidence for small but significant positive effects. A
              potential hypothesis for this result is the fact that a single
              supervisory authority can take spillover effects between
              countries into account and is therefore able to stabilize the
              European banking sector. Splitting the sample by an indicator for
              supervisory power, an indicator for corruption, and by debt/GDP
              reveals that the positive impact of the SSM was stronger for
              banks in countries that perform poorly with respect to these
              measures.",
  journal  = "Journal of Banking \& Finance",
  volume   =  91,
  pages    = "34--48",
  month    =  jun,
  year     =  2018,
  keywords = "Banks; Event study; Supervision; Single Supervisory Mechanism;
              Harmonization"
}

@ARTICLE{Brunnermeier2009-lt,
  title    = "Deciphering the Liquidity and Credit Crunch 2007-2008",
  author   = "Brunnermeier, Markus K",
  abstract = "Deciphering the Liquidity and Credit Crunch 2007-2008 by Markus
              K. Brunnermeier. Published in volume 23, issue 1, pages 77-100 of
              Journal of Economic Perspectives, Winter 2009, Abstract: The
              financial market turmoil in 2007 and 2008 has led to the most
              severe financial crisis since the Great Depress...",
  journal  = "J. Econ. Perspect.",
  volume   =  23,
  number   =  1,
  pages    = "77--100",
  month    =  mar,
  year     =  2009
}

@ARTICLE{Banker1993-vw,
  title     = "Maximum Likelihood, Consistency and Data Envelopment Analysis: A
               Statistical Foundation",
  author    = "Banker, Rajiv D",
  abstract  = "This paper provides a formal statistical basis for the
               efficiency evaluation techniques of data envelopment analysis
               (DEA). DEA estimators of the best practice monotone increasing
               and concave production function are shown to be also maximum
               likelihood estimators if the deviation of actual output from the
               efficient output is regarded as a stochastic variable with a
               monotone decreasing probability density function. While the best
               practice frontier estimator is biased below the theoretical
               frontier for a finite sample size, the bias approaches zero for
               large samples. The DEA estimators exhibit the desirable
               asymptotic property of consistency, and the asymptotic
               distribution of the DEA estimators of inefficiency deviations is
               identical to the true distribution of these deviations. This
               result is then employed to suggest possible statistical tests of
               hypotheses based on asymptotic distributions.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  39,
  number    =  10,
  pages     = "1265--1273",
  month     =  oct,
  year      =  1993
}

@INCOLLECTION{Banker2011-kz,
  title     = "Statistical Tests Based on {DEA} Efficiency Scores",
  booktitle = "Handbook on Data Envelopment Analysis",
  author    = "Banker, Rajiv D and Natarajan, Ram",
  editor    = "Cooper, William W and Seiford, Lawrence M and Zhu, Joe",
  abstract  = "This chapter is written for analysts and researchers who may use
               data envelopment analysis (DEA) to statistically evaluate
               hypotheses about characteristics of production correspondences
               and factors affecting productivity. Contrary to some
               characterizations, it is shown that DEA is a full-fledged
               statistical methodology, based on the characterization of DMU
               efficiency as a stochastic variable. The DEA estimator of the
               production frontier has desirable statistical properties, and
               provides a basis for the construction of a wide range of formal
               statistical tests (Banker RD Mgmt Sci. 1993;39(10):1265--73).
               Specific tests described here address issues such as comparisons
               of efficiency of groups of DMUs, existence of scale economies,
               existence of allocative inefficiency, separability and
               substitutability of inputs in production systems, analysis of
               technical change and productivity change, impact of contextual
               variables on productivity, and the adequacy of parametric
               functional forms in estimating monotone and concave production
               functions.",
  publisher = "Springer US",
  pages     = "273--295",
  year      =  2011,
  address   = "Boston, MA"
}

@ARTICLE{Akgul1984-wv,
  title    = "A Note on Shadow Prices in Linear Programming",
  author   = "Akg{\"u}l, Mustafa",
  abstract = "Using convex analysis and a characterization of the entire family
              of optimal solutions to an L.P., we show that in order to obtain
              shadow prices, one has to solve a much smaller L.P. derived from
              any optimal tableau. We then show that positive as well as
              negative shadow prices for any constraint or for any combination
              of constraints can easily be computed by parametric linear
              programming. Some examples exhibiting the method are also
              included.",
  journal  = "J. Oper. Res. Soc.",
  volume   =  35,
  number   =  5,
  pages    = "425--431",
  month    =  may,
  year     =  1984
}

@ARTICLE{Bhagwati1978-zp,
  title     = "Value Subtracted, Negative Shadow Prices of Factors in Project
               Evaluation, and Immiserising Growth: Three Paradoxes in the
               Presence of Trade Distortions",
  author    = "Bhagwati, Jagdish N and Srinivasan, T N and Wan, Henry",
  journal   = "Econ. J. Nepal",
  publisher = "[Royal Economic Society, Wiley]",
  volume    =  88,
  number    =  349,
  pages     = "121--125",
  year      =  1978
}

@ARTICLE{Fukushima1985-is,
  title    = "On negative shadow factor prices in the presence of factor market
              distortions",
  author   = "Fukushima, Takashi",
  abstract = "It has been believed that dynamic stability of factor markets
              rules out negative shadow factor prices in a two-factor,
              two-commodity model of production in the presence of factor
              market distortions. This paper derives exact conditions to have
              negative shadow factor prices, and proves that negative shadow
              factor prices are consistent with stability of factor markets.",
  journal  = "J. Int. Econ.",
  volume   =  18,
  number   =  3,
  pages    = "365--371",
  month    =  may,
  year     =  1985
}

@ARTICLE{Lucas1987-le,
  title   = "Resolution Ex ante versus ex post {DRC's} and the possibility of
             negative shadow prices",
  author  = "Lucas, Robert E B and Pursell, Garry and Tower, Edward",
  journal = "J. Dev. Econ.",
  volume  =  26,
  number  =  1,
  pages   = "173--174",
  month   =  jun,
  year    =  1987
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Koch2014-zi,
  title    = "Causes of the {EU} {ETS} price drop: Recession, {CDM}, renewable
              policies or a bit of everything?---New evidence",
  author   = "Koch, Nicolas and Fuss, Sabine and Grosjean, Godefroy and
              Edenhofer, Ottmar",
  abstract = "The price of EU allowances (EUAs) in the EU Emissions Trading
              Scheme (EU ETS) fell from almost 30‚Ç¨/tCO2 in mid-2008 to less
              than 5‚Ç¨/tCO2 in mid-2013. The sharp and persistent price decline
              has sparked intense debates both in academia and among
              policy-makers about the decisive allowance price drivers. In this
              paper we examine whether and to what extent the EUA price drop
              can be justified by three commonly identified explanatory
              factors: the economic recession, renewable policies and the use
              of international credits. Capitalizing on marginal abatement cost
              theory and a broadly extended data set, we find that only
              variations in economic activity and the growth of wind and solar
              electricity production are robustly explaining EUA price
              dynamics. Contrary to simulation-based analyses, our results
              point to moderate interaction effects between the overlapping EU
              ETS and renewable policies. The bottom line, however, is that
              90\% of the variations of EUA price changes remains unexplained
              by the abatement-related fundamentals. Together, our findings do
              not support the widely-held view that negative demand shocks are
              the main cause of the weak carbon price signal. In view of the
              new evidence, we evaluate the EU ETS reform options which are
              currently discussed.",
  journal  = "Energy Policy",
  volume   =  73,
  pages    = "676--685",
  month    =  oct,
  year     =  2014,
  keywords = "EU ETS; Carbon price; Renewables"
}

@ARTICLE{Delis2017-nn,
  title    = "Endogenous bank risk and efficiency",
  author   = "Delis, Manthos and Iosifidi, Maria and Tsionas, Mike G",
  abstract = "We develop a framework to incorporate bank risk, as measured from
              the variance of profits or returns, within a model of frontier
              efficiency. Our framework follows the premise that risk is
              endogenously related to efficiency. We estimate our model using
              panel data for U.S. banks and Bayesian techniques. We show that
              excluding risk from the efficiency model significantly biases the
              efficiency estimates and the ranking of banks according to their
              competitive advantage. We also demonstrate that there is a
              negative risk-efficiency nexus with causality running both ways,
              while our estimates of risk are fully consistent with the
              developments in the banking industry over the period 1976--2014.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  260,
  number   =  1,
  pages    = "376--387",
  month    =  jul,
  year     =  2017,
  keywords = "OR in banking; Stochastic frontier; Endogenous risk;
              Risk-efficiency relationship; Bayesian methods"
}

@TECHREPORT{Bruno2017-np,
  title       = "Bad Loans and Resource Allocation in Crisis Years: Evidence
                 from European Banks",
  author      = "Bruno, Brunella and Marino, Immacolata",
  abstract    = "The aim of this study is to explore the relation between loan
                 portfolio quality and lending in European banks over
                 2005-2014. We focus on lending behavior of ba",
  number      = "2017- 52",
  institution = "Bocconi University Centre for Applied Research on
                 International Markets, Banking, Finance and Regulation",
  month       =  feb,
  year        =  2017,
  keywords    = "bad loans, NPLs, non-performing loans, bank lending, crisis"
}

@ARTICLE{Baltagi2017-wt,
  title   = "Replication of unconditional Quantile Regressions by Firpo, Fortin
             and Lemieux (2009): {REPLICATION} {OF} {UNCONDITIONAL} {QUANTILE}
             {REGRESSIONS} {BY} {FIRPO} {ET} {AL}. (2009)",
  author  = "Baltagi, Badi H and Ghosh, Pallab Kumar",
  journal = "J. Appl. Econ.",
  volume  =  32,
  number  =  1,
  pages   = "218--223",
  month   =  jan,
  year    =  2017
}

@ARTICLE{Pina2017-rj,
  title   = "Mergers Between Savings Banks. The Solution for Improving Risk in
             the Spanish Banking Sector?",
  author  = "Pina, Vicente and Torres, Lourdes and Bachiller, Patricia",
  journal = "International Review of Entrepreneurship",
  volume  =  15,
  number  =  1,
  year    =  2017
}

@ARTICLE{Loderer1985-jh,
  title     = "A test of the {OPEC} cartel hypothesis: 1974--1983",
  author    = "Loderer, Claudio",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  40,
  number    =  3,
  pages     = "991--1006",
  year      =  1985
}

@BOOK{Cremer2013-gn,
  title     = "Models of the Oil Market",
  author    = "Cr{\'e}mer, J and Salehi-Isfahani, D",
  abstract  = "Economists have proposed a large variety of models of the oil
               market and this survey integrates them in a coherent framework.",
  publisher = "Taylor \& Francis",
  month     =  jul,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Demirer2010-wj,
  title    = "The behavior of crude oil spot and futures prices around {OPEC}
              and {SPR} announcements: An event study perspective",
  author   = "Demirer, R{\i}za and Kutan, Ali M",
  abstract = "This paper examines the informational efficiency of crude oil
              spot and futures markets with respect to OPEC conference and U.S.
              Strategic Petroleum Reserve (SPR) announcements. We employ the
              event study methodology to examine the abnormal returns in crude
              oil spot and futures markets around OPEC conference and SPR
              announcement dates between 1983 and 2008. Our findings regarding
              OPEC announcements indicate an asymmetry in that only OPEC
              production cut announcements yield a statistically significant
              impact with the impact diminishing for longer maturities. We also
              find that the persistence of returns following OPEC production
              cut announcements creates substantial excess returns to investors
              who take long positions on the day following the end of OPEC
              conferences. In the case of SPR announcements, we find that the
              government's use of this program initiates a short-run market
              reaction following the announcement date. Furthermore, our tests
              of cumulative abnormal returns suggest that the market reacts
              efficiently to SPR announcements providing support for the use of
              the strategic reserves as a tool to stabilize the oil market. Our
              findings have significant policy implications for investors and
              are useful in designing effective energy policy strategies.",
  journal  = "Energy Econ.",
  volume   =  32,
  number   =  6,
  pages    = "1467--1476",
  month    =  nov,
  year     =  2010,
  keywords = "Crude oil; OPEC; Strategic petroleum reserves; Futures markets;
              Event study; Energy policy"
}

@ARTICLE{Lin2010-hc,
  title    = "{OPEC} announcements and their effects on crude oil prices",
  author   = "Lin, Sharon Xiaowen and Tamvakis, Michael",
  abstract = "We investigate evidence on the effects of OPEC announcements on
              world oil prices by examining announcements from both official
              conferences and ministerial meetings on major international
              crudes, including the key benchmarks and several other heavy and
              light grades. With data from 1982 to 2008, we use event study
              methodology and find differentiation in the magnitude and
              significance of market responses to OPEC quota decisions under
              different price bands. We also find some (weak) evidence of
              differentiation between light and heavy crude grades.",
  journal  = "Energy Policy",
  volume   =  38,
  number   =  2,
  pages    = "1010--1016",
  month    =  feb,
  year     =  2010,
  keywords = "OPEC; Event study; Price bands"
}

@ARTICLE{Mensi2014-de,
  title    = "How do {OPEC} news and structural breaks impact returns and
              volatility in crude oil markets? Further evidence from a long
              memory process",
  author   = "Mensi, Walid and Hammoudeh, Shawkat and Yoon, Seong-Min",
  abstract = "Since its formation, OPEC through its conference decisions has
              been a major player in the world oil markets. The purpose of this
              paper is to examine the impacts of OPEC's different news
              announcements on the conditional expectations and volatility of
              crude oil markets in the presence of long memory and structural
              changes. To do so, we first discern OPEC's oil production
              behavior in response to its ``cut'', ``maintain'', and
              ``increase'' decisions. Then by applying the ARMA--GARCH class
              models to the two global benchmarks WTI and Brent over the period
              May 1987 through December 2012, we find strong evidence of long
              memory. The empirical evidence also shows that OPEC's
              announcements especially the ``cut'' and the ``maintain''
              decisions have a significant effect on both returns and
              volatility of the crude oil markets, particularly that of the
              WTI. Moreover, we explore the possibility of structural breaks in
              the crude oil prices and detect five (six) breakpoints for the
              WTI (Brent) oil markets. The presence of structural breaks
              reduces the persistence of volatility. Accounting for OPEC's
              scheduled news announcements in the presence of structural
              changes reduces the degree of volatility persistence and enhances
              the understanding of this volatility in the oil markets. These
              results have several implications for policy makers, oil traders
              and other participants in the crude oil markets.",
  journal  = "Energy Econ.",
  volume   =  42,
  pages    = "343--354",
  month    =  mar,
  year     =  2014,
  keywords = "Crude oil prices; OPEC announcements; Volatility; Structural
              breaks; Long memory"
}

@ARTICLE{Ji2015-go,
  title    = "Oil price volatility and oil-related events: An Internet concern
              study perspective",
  author   = "Ji, Qiang and Guo, Jian-Feng",
  abstract = "Oil-related events have increased the uncertainty and complexity
              of the worldwide oil market. This paper investigates the effects
              of four types of oil-related events on world oil prices, using an
              event study methodology and an AR-GARCH model. The Internet
              information concerning these events, which is derived from search
              query volumes in Google, is introduced in an analytical framework
              to identify the magnitude and significance of the market response
              to oil-related events. The results indicate that world oil prices
              responding to different oil-related events display obvious
              differentiation. The cumulative abnormal returns, which reflect
              the influence of the global financial crisis, tend to drop first
              and then reverse and rise, while the cumulative abnormal returns
              induced by other oil-related events present a stronger persistent
              effect. The impact of the global financial crisis on oil price
              returns is significantly negative, while the impact of the Libyan
              war and hurricanes is significantly positive. However, the
              reactions of oil price returns to different OPEC production
              announcements are inconsistent.",
  journal  = "Appl. Energy",
  volume   =  137,
  pages    = "256--264",
  month    =  jan,
  year     =  2015,
  keywords = "Oil price volatility; Event analysis; Information transmission;
              Internet"
}

@UNPUBLISHED{Brad_Jones_University_of_California_Davis_undated-lx,
  title  = "Complications in Event History I: Frailty Models What is it?",
  author = "{Brad Jones (University of California, Davis)}"
}

@MISC{Ben_Cheikh2018-yr,
  title        = "Oil Prices and {GCC} Stock Markets: New Evidence from Smooth
                  Transition Models",
  booktitle    = "{IMF}",
  author       = "Ben Cheikh, Nidhaleddine and Ben Naceur, Sami and Kanaan,
                  Oussama and Rault, Christophe",
  month        =  may,
  year         =  2018,
  howpublished = "\url{http://www.imf.org/en/Publications/WP/Issues/2018/05/09/Oil-Prices-and-GCC-Stock-Markets-New-Evidence-from-Smooth-Transition-Models-45819}",
  note         = "Accessed: 2018-6-13"
}

@ARTICLE{Cazalet2013-cn,
  title    = "The Smart Beta Indexing Puzzle",
  author   = "Cazalet, Z{\'e}lia and Grison, Pierre and Roncalli, Thierry",
  abstract = "In this article, we consider smart beta indexing, which is an
              alternative to capitalization-weighted (CW) indexing. In
              particular, we focus on risk-based (RB) i",
  month    =  jul,
  year     =  2013,
  keywords = "Smart beta, risk-based indexing, minimum variance portfolio, risk
              parity, equally weighted portfolio, equal risk contribution
              portfolio, diversification, low beta anomaly, low volatility
              anomaly, tracking error, liquidity"
}

@ARTICLE{Healy1992-av,
  title    = "Does corporate performance improve after mergers?",
  author   = "Healy, Paul M and Palepu, Krishna G and Ruback, Richard S",
  abstract = "We examine post-acquisition performance for the 50 largest U.S.
              mergers between 1979 and mid-1984. Merged firms show significant
              improvements in asset productivity relative to their industries,
              leading to higher operating cash flow returns. This performance
              improvement is particularly strong for firms with highly
              overlapping businesses. Mergers do not lead to cuts in long-term
              capital and R\&D investments. There is a strong positive relation
              between postmerger increases in operating cash flows and abnormal
              stock returns at merger announcements, indicating that
              expectations of economic improvements underlie the equity
              revaluations of the merging firms.",
  journal  = "J. financ. econ.",
  volume   =  31,
  number   =  2,
  pages    = "135--175",
  month    =  apr,
  year     =  1992
}

@ARTICLE{Kahn2016-ad,
  title     = "The asset manager's dilemma: How smart beta is disrupting the
               investment management industry",
  author    = "Kahn, Ronald N and Lemmon, Michael",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  72,
  number    =  1,
  pages     = "15--20",
  year      =  2016
}

@ARTICLE{Gambacorta2016-ac,
  title    = "Why bank capital matters for monetary policy",
  author   = "Gambacorta, Leonardo and Shin, Hyun Song",
  abstract = "One aim of post-crisis monetary policy has been to ease credit
              conditions for borrowers by unlocking bank lending. We find that
              bank equity is an important determinant of both the bank's
              funding cost and its lending growth. In a cross-country
              bank-level study, we find that a 1 percentage point increase in
              the equity-to-total assets ratio is associated with a four basis
              point reduction in the cost of debt financing and with a 0.6
              percentage point increase in annual loan growth. These findings
              suggest that greater retention of bank earnings and hence higher
              bank capital would have aided the transmission of accommodative
              monetary policy to ease financial conditions faced by ultimate
              borrowers. In particular, we find that the effects of a monetary
              tightening are smaller for banks with higher capitalization,
              which have easier access to uninsured financing. These results
              suggest that if the banking system as a whole is weakly
              capitalized, there may be some tension between the monetary
              policy imperative of unlocking bank lending (i.e., expanding
              credit) and the supervisory objective of ensuring the soundness
              of individual banks (i.e., shrinking credit).",
  journal  = "Journal of Financial Intermediation",
  month    =  oct,
  year     =  2016,
  keywords = "Bank capital; Book equity; Monetary transmission mechanisms;
              Funding; Bank lending"
}

@ARTICLE{Yermack2017-en,
  title     = "Corporate Governance and Blockchains",
  author    = "Yermack, David",
  abstract  = "Blockchains represent a novel application of cryptography and
               information technology to age-old problems of financial
               record-keeping, and they may lead to far-reaching changes in
               corporate governance. Many major players in the financial
               industry have began to invest in this new technology, and stock
               exchanges have proposed using blockchains as a new method for
               trading corporate equities and tracking their ownership. This
               essay evaluates the potential implications of these changes for
               managers, institutional investors, small shareholders, auditors,
               and other parties involved in corporate governance. The lower
               cost, greater liquidity, more accurate record-keeping, and
               transparency of ownership offered by blockchains may
               significantly upend the balance of power among these cohorts.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  1,
  pages     = "7--31",
  month     =  mar,
  year      =  2017
}

@ARTICLE{Corgnet2018-wl,
  title    = "What Makes a Good Trader? On the Role of Intuition and Reflection
              on Trader Performance",
  author   = "Corgnet, Brice and Desantis, Mark and Porter, David",
  abstract = "Using laboratory experiments, we provide evidence on three
              factors influencing trader performance: fluid intelligence,
              cognitive reflection, and theory of mind (ToM). Fluid
              intelligence provides traders with computational skills necessary
              to draw a statistical inference. Cognitive reflection helps
              traders avoid behavioral biases and thereby extract signals from
              market orders and update their prior beliefs accordingly. ToM
              describes the degree to which traders correctly assess the
              informational content of orders. We show that cognitive
              reflection and ToM are complementary because traders benefit from
              understanding signals' quality only if they are capable of
              processing these signals.",
  journal  = "J. Finance",
  volume   =  73,
  number   =  3,
  pages    = "1113--1137",
  month    =  jun,
  year     =  2018
}

@ARTICLE{Duarte2007-pe,
  title     = "Risk and Return in {Fixed-Income} Arbitrage: Nickels in Front of
               a Steamroller?",
  author    = "Duarte, Jefferson and Longstaff, Francis A and Yu, Fan",
  abstract  = "We conduct an analysis of the risk and return characteristics of
               a number of widely used fixed-income arbitrage strategies. We
               find that the strategies requiring more ``intellectual capital''
               to implement tend to produce significant alphas after
               controlling for bond and equity market risk factors. These
               positive alphas remain significant even after taking into
               account typical hedge fund fees. In contrast with other hedge
               fund strategies, many of the fixed-income arbitrage strategies
               produce positively skewed returns. These results suggest that
               there may be more economic substance to fixed-income arbitrage
               than simply ``picking up nickels in front of a steamroller.''",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  20,
  number    =  3,
  pages     = "769--811",
  month     =  may,
  year      =  2007
}

@ARTICLE{Mitchell2012-tg,
  title    = "Arbitrage crashes and the speed of capital",
  author   = "Mitchell, Mark and Pulvino, Todd",
  abstract = "The imminent failure of prime brokers during the 2008 financial
              crisis caused a sudden decrease in the leverage afforded hedge
              funds. This decrease resulted from the asymmetrical payoff to
              rehypothecation lenders---the ultimate financiers, through prime
              brokers, to hedge funds. Seemingly long-term debt capital became
              short-term capital creating a duration mismatch between left-hand
              side arbitrage opportunities and right-hand side liabilities.
              Consequently, arbitrageurs became unable to maintain similar
              prices of similar assets. Mispricing magnitudes, and the time
              required to correct them, reflect the role of arbitrageurs in
              maintaining accurate prices during normal times and offer an
              estimate of discounts at which assets transact during crises.",
  journal  = "J. financ. econ.",
  volume   =  104,
  number   =  3,
  pages    = "469--490",
  month    =  jun,
  year     =  2012,
  keywords = "Arbitrage; Financial crisis; Hedge funds"
}

@ARTICLE{Cao2010-qn,
  title    = "The information content of option-implied volatility for credit
              default swap valuation",
  author   = "Cao, Charles and Yu, Fan and Zhong, Zhaodong",
  abstract = "Credit default swaps (CDS) are similar to out-of-the-money put
              options in that both offer a low cost and effective protection
              against downside risk. This study investigates whether put
              option-implied volatility is an important determinant of CDS
              spreads. Using a large sample of firms with both CDS and options
              data, we find that individual firms' put option-implied
              volatility dominates historical volatility in explaining the
              time-series variation in CDS spreads. To understand this result,
              we show that implied volatility is a more efficient forecast for
              future realized volatility than historical volatility. More
              importantly, the volatility risk premium embedded in option
              prices covaries with the CDS spread. These findings complement
              existing empirical evidence based on market-level data.",
  journal  = "Journal of Financial Markets",
  volume   =  13,
  number   =  3,
  pages    = "321--343",
  month    =  aug,
  year     =  2010,
  keywords = "Credit default swaps; Option implied volatility; Historical
              volatility; Price discovery; Volatility risk premium"
}

@ARTICLE{Hudson2015-ye,
  title    = "Is investor sentiment contagious? International sentiment and
              {UK} equity returns",
  author   = "Hudson, Yawen and Green, Christopher J",
  abstract = "This paper contributes to a growing body of literature studying
              investor sentiment. Separate sentiment measures for UK investors
              and UK institutional investors are constructed from commonly
              cited sentiment indicators using the first principle component
              method. We then examine if the sentiment measures can help
              predict UK equity returns, distinguishing between ``turbulent''
              and ``tranquil'' periods in the financial markets. We find that
              sentiment tends to be a more important determinant of returns in
              the run-up to a crisis than at other times. We also examine if US
              investor sentiment can help predict UK equity returns, and find
              that US investor sentiment is highly significant in explaining
              the UK equity returns.",
  journal  = "Journal of Behavioral and Experimental Finance",
  volume   =  5,
  pages    = "46--59",
  month    =  mar,
  year     =  2015,
  keywords = "Investor sentiment; Contagion; Institutional investors; Equity
              returns"
}

@ARTICLE{Freestone2017-nf,
  title    = "Financial fair play and competitive balance in the Premier League",
  author   = "Freestone, Christopher John and Manoli, Argyro Elisavet",
  abstract = "Purpose The introduction of financial fair play (FFP) regulations
              in 2011 was accompanied by criticism that they would have an
              adverse effect on competitive balance in European football.
              Counter-points were also expressed, suggesting that the opposite
              would occur; that they would actually increase competitive
              balance through reducing the importance of financial power. The
              lack of clarity and cohesion on this issue prompted this paper.
              The purpose of this paper is to examine the effect FFP has had on
              competitive balance in the English Premier League.
              Design/methodology/approach The analysis conducted uses the
              Herfindahl Index of Competitive Balance as the primary method,
              and is supported by standard deviation of points analysis and a
              Scully-Noll ratio analysis, which together provide an indication
              of the level of competitive balance for each of the past 21
              seasons, from 1995/1996 to 2015/2016. This examination allows for
              the trends in competitive balance to be identified, with emphasis
              drawn on the seasons after the introduction of the regulations.
              Findings The results provide no indication that FFP regulations
              have resulted in a decline in competitive balance in the EPL,
              instead hinting that a positive effect may have been caused. This
              positive effect exceeds the primary aim of the regulations and
              underlines their importance in the future stability of club
              football. Originality/value While underlining the need for
              further research on the topic, this study provides the first
              insights into the effects of FFP regulations on competitive
              balance in the EPL. These insights would support the view that
              FFP initiatives have begun to shift the focus of sporting
              competition away from financial strength towards more natural
              means of competition such as efficiency, innovation and good
              management.",
  journal  = "Sport, Business and Management: An International Journal",
  volume   =  7,
  number   =  2,
  pages    = "175--196",
  year     =  2017
}

@ARTICLE{Skinner2009-cd,
  title     = "Soccer matches as experiments: how often does the `best' team
               win?",
  author    = "Skinner, G K and Freeman, G H",
  abstract  = "Models in which the number of goals scored by a team in a soccer
               match follow a Poisson distribution, or a closely related one,
               have been widely discussed. We here consider a soccer match as
               an experiment to assess which of two teams is superior and
               examine the probability that the outcome of the experiment
               (match) truly represents the relative abilities of the two
               teams. Given a final score, it is possible by using a Bayesian
               approach to quantify the probability that it was or was not the
               case that ?the best team won?. For typical scores, the
               probability of a misleading result is significant. Modifying the
               rules of the game to increase the typical number of goals scored
               would improve the situation, but a level of confidence that
               would normally be regarded as satisfactory could not be obtained
               unless the character of the game was radically changed.",
  journal   = "J. Appl. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  36,
  number    =  10,
  pages     = "1087--1095",
  month     =  oct,
  year      =  2009
}

@ARTICLE{Gao2018-fe,
  title    = "Market intraday momentum",
  author   = "Gao, Lei and Han, Yufeng and Zhengzi Li, Sophia and Zhou, Guofu",
  abstract = "Based on high frequency S \& P 500 exchange-traded fund (ETF)
              data from 1993--2013, we show an intraday momentum pattern: the
              first half-hour return on the market as measured from the
              previous day's market close predicts the last half-hour return.
              This predictability, which is both statistically and economically
              significant, is stronger on more volatile days, on higher volume
              days, on recession days, and on major macroeconomic news release
              days. Intraday momentum also exists for ten other most actively
              traded domestic and international ETFs. Theoretically, the
              intraday momentum is consistent not only with Bogousslavsky's
              (2016) model of infrequent portfolio rebalancing but also with a
              model of late-informed trading near the market close.",
  journal  = "J. financ. econ.",
  volume   =  129,
  number   =  2,
  pages    = "394--414",
  month    =  aug,
  year     =  2018,
  keywords = "High frequency trading; Overnight return; Intraday;
              Predictability; Momentum"
}

@ARTICLE{Barber1996-ng,
  title    = "Detecting abnormal operating performance: The empirical power and
              specification of test statistics",
  author   = "Barber, Brad M and Lyon, John D",
  abstract = "This research evaluates methods used in event studies that employ
              accounting-based measures of operating performance. We examine
              the choice of an accounting-based performance measure, a
              statistical test, and a model of expected operating performance.
              We document the impact of these choices on the test statistics
              designed to detect abnormal operating performance. We find that
              commonly used research designs yield test statistics that are
              misspecified in cases where sample firms have performed either
              unusually well or poorly. In this sampling situation, the test
              statistics are only well specified when sample firms are matched
              to control firms of similar pre-event performance.",
  journal  = "J. financ. econ.",
  volume   =  41,
  number   =  3,
  pages    = "359--399",
  month    =  jul,
  year     =  1996,
  keywords = "Operating performance; Event studies; Return on assets; Return on
              sales"
}

@ARTICLE{Barber1997-nn,
  title    = "Detecting long-run abnormal stock returns: The empirical power
              and specification of test statistics",
  author   = "Barber, Brad M and Lyon, John D",
  abstract = "We analyze the empirical power and specification of test
              statistics in event studies designed to detect long-run (one- to
              five-year) abnormal stock returns. We document that test
              statistics based on abnormal returns calculated using a reference
              portfolio, such as a market index, are misspecified (empirical
              rejection rates exceed theoretical rejection rates) and identify
              three reasons for this misspecification. We correct for the three
              identified sources of misspecification by matching sample firms
              to control firms of similar sizes and book-to-market ratios. This
              control firm approach yields well-specified test statistics in
              virtually all sampling situations considered.",
  journal  = "J. financ. econ.",
  volume   =  43,
  number   =  3,
  pages    = "341--372",
  month    =  mar,
  year     =  1997,
  keywords = "Event studies; Firm size; Book-to-market ratios"
}

@ARTICLE{Gleeson2009-ti,
  title   = "More regulation means more risk",
  author  = "Gleeson, S",
  journal = "International Financial Law Review",
  volume  =  28,
  number  =  1,
  pages   = "14",
  year    =  2009
}

@ARTICLE{Hillegeist2004-tr,
  title     = "Assessing the Probability of Bankruptcy",
  author    = "Hillegeist, Stephen A and Keating, Elizabeth K and Cram, Donald
               P and Lundstedt, Kyle G",
  abstract  = "We assess whether two popular accounting-based measures,
               Altman's (1968) Z-Score and Ohlson's (1980) O-Score, effectively
               summarize publicly-available information about the probability
               of bankruptcy. We compare the relative information content of
               these Scores to a market-based measure of the probability of
               bankruptcy that we develop based on the Black--Scholes--Merton
               option-pricing model, BSM-Prob. Our tests show that BSM-Prob
               provides significantly more information than either of the two
               accounting-based measures. This finding is robust to various
               modifications of Z-Score and O-Score, including updating the
               coefficients, making industry adjustments, and decomposing them
               into their lagged levels and changes. We recommend that
               researchers use BSM-Prob instead of Z-Score and O-Score in their
               studies and provide the SAS code to calculate BSM-Prob.",
  journal   = "Rev. Acct. Stud.",
  publisher = "Kluwer Academic Publishers",
  volume    =  9,
  number    =  1,
  pages     = "5--34",
  month     =  mar,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Wager2017-ps,
  title     = "Estimation and Inference of Heterogeneous Treatment Effects
               using Random Forests",
  author    = "Wager, Stefan and Athey, Susan",
  abstract  = "ABSTRACTMany scientific and engineering challenges?ranging from
               personalized medicine to customized marketing
               recommendations?require an understanding of treatment effect
               heterogeneity. In this article, we develop a nonparametric
               causal forest for estimating heterogeneous treatment effects
               that extends Breiman?s widely used random forest algorithm. In
               the potential outcomes framework with unconfoundedness, we show
               that causal forests are pointwise consistent for the true
               treatment effect and have an asymptotically Gaussian and
               centered sampling distribution. We also discuss a practical
               method for constructing asymptotic confidence intervals for the
               true treatment effect that are centered at the causal forest
               estimates. Our theoretical results rely on a generic Gaussian
               theory for a large family of random forest algorithms. To our
               knowledge, this is the first set of results that allows any type
               of random forest, including classification and regression
               forests, to be used for provably valid statistical inference. In
               experiments, we find causal forests to be substantially more
               powerful than classical methods based on nearest-neighbor
               matching, especially in the presence of irrelevant covariates.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  pages     = "1--15",
  month     =  apr,
  year      =  2017
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Athey2017-su,
  title     = "The State of Applied Econometrics: Causality and Policy
               Evaluation",
  author    = "Athey, Susan and Imbens, Guido W",
  abstract  = "In this paper, we discuss recent developments in econometrics
               that we view as important for empirical researchers working on
               policy evaluation questions. We focus on three main areas, in
               each case, highlighting recommendations for applied work. First,
               we discuss new ‚Ä¶",
  journal   = "J. Econ. Perspect.",
  publisher = "aeaweb.org",
  volume    =  31,
  number    =  2,
  pages     = "3--32",
  month     =  may,
  year      =  2017
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Athey2015-tu,
  title     = "Machine Learning and Causal Inference for Policy Evaluation",
  booktitle = "Proceedings of the 21th {ACM} {SIGKDD} International Conference
               on Knowledge Discovery and Data Mining",
  author    = "Athey, Susan",
  abstract  = "A large literature on causal inference in statistics,
               econometrics, biostatistics, and epidemiology (see, eg, Imbens
               and Rubin [2015] for a recent survey) has focused on methods for
               statistical estimation and inference in a setting where the
               researcher wishes to ‚Ä¶",
  publisher = "ACM",
  pages     = "5--6",
  series    = "KDD '15",
  year      =  2015,
  address   = "New York, NY, USA",
  keywords  = "a/b tests, causal inference, counterfactual prediction,
               cross-validation, model robustness, policy evaluation,
               randomized experiments, supervised machine learning, treatment
               effects",
  location  = "Sydney, NSW, Australia"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Athey2015-dv,
  title     = "Machine learning methods for estimating heterogeneous causal
               effects",
  author    = "Athey, Susan and Imbens, Guido W",
  abstract  = "In this paper we study the problems of estimating heterogeneity
               in causal effects in experimental or observational studies and
               conducting inference about the magnitude of the differences in
               treatment effects across subsets of the population. In
               applications, our method ‚Ä¶",
  journal   = "Stat",
  publisher = "researchgate.net",
  volume    =  1050,
  number    =  5,
  year      =  2015
}

@ARTICLE{Athey2016-ek,
  title     = "Recursive partitioning for heterogeneous causal effects",
  author    = "Athey, Susan and Imbens, Guido",
  abstract  = "In this paper we propose methods for estimating heterogeneity in
               causal effects in experimental and observational studies and for
               conducting hypothesis tests about the magnitude of differences
               in treatment effects across subsets of the population. We
               provide a data-driven approach to partition the data into
               subpopulations that differ in the magnitude of their treatment
               effects. The approach enables the construction of valid
               confidence intervals for treatment effects, even with many
               covariates relative to the sample size, and without ``sparsity''
               assumptions. We propose an ``honest'' approach to estimation,
               whereby one sample is used to construct the partition and
               another to estimate treatment effects for each subpopulation.
               Our approach builds on regression tree methods, modified to
               optimize for goodness of fit in treatment effects and to account
               for honest estimation. Our model selection criterion anticipates
               that bias will be eliminated by honest estimation and also
               accounts for the effect of making additional splits on the
               variance of treatment effect estimates within each
               subpopulation. We address the challenge that the ``ground
               truth'' for a causal effect is not observed for any individual
               unit, so that standard approaches to cross-validation must be
               modified. Through a simulation study, we show that for our
               preferred method honest estimation results in nominal coverage
               for 90\% confidence intervals, whereas coverage ranges between
               74\% and 84\% for nonhonest approaches. Honest estimation
               requires estimating the model with a smaller sample size; the
               cost in terms of mean squared error of treatment effects for our
               preferred method ranges between 7-22\%.",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  113,
  number    =  27,
  pages     = "7353--7360",
  month     =  jul,
  year      =  2016,
  keywords  = "causal inference; cross-validation; heterogeneous treatment
               effects; potential outcomes; supervised machine learning",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Mullainathan2017-xo,
  title     = "Machine Learning: An Applied Econometric Approach",
  author    = "Mullainathan, Sendhil and Spiess, Jann",
  abstract  = "‚Ä¶ Athey (2015) provides a brief overview of how machine learning
               relates to causal inference ‚Ä¶ a prediction task (what added
               value will a given teacher have?), one that is intimately tied
               to the causal question of ‚Ä¶ As a result, inference on predictive
               performance of a fixed predictive ‚Ä¶",
  journal   = "J. Econ. Perspect.",
  publisher = "aeaweb.org",
  volume    =  31,
  number    =  2,
  pages     = "87--106",
  month     =  may,
  year      =  2017
}

@ARTICLE{Sheu2011-gz,
  title    = "Effective options trading strategies based on volatility
              forecasting recruiting investor sentiment",
  author   = "Sheu, Her-Jiun and Wei, Yu-Chen",
  abstract = "This study investigates an algorithm for an effective option
              trading strategy based on superior volatility forecasts using
              actual option price data for the Taiwan stock market. The
              forecast evaluation supports the significant incremental
              explanatory power of investor sentiment in the fitting and
              forecasting of future volatility in relation to its adversarial
              multiple-factor model, especially the market turnover and
              volatility index which are referred to as the investors' mood
              gauge and proxy for overreaction. After taking into consideration
              the margin-based transaction cost, the simulated trading
              indicates that a long or short straddle 15days before the
              options' final settlement day based on the 60-day
              in-sample-period volatility forecasting recruiting market
              turnover achieves the best average monthly return of 15.84\%.
              This study bridges the gap between option trading, market
              volatility, and the signal of the investors' overreaction through
              the simulation of the option trading strategy. The trading
              algorithm based on the volatility forecasting recruiting investor
              sentiment could be further applied in electronic trading and
              other artificial intelligence decision support systems.",
  journal  = "Expert Syst. Appl.",
  volume   =  38,
  number   =  1,
  pages    = "585--596",
  month    =  jan,
  year     =  2011,
  keywords = "Volatility forecasting; Investor sentiment; Options trading
              strategy; Decision support; Market turnover"
}

@UNPUBLISHED{noauthor_2018-ks,
  title = "How and Why Do Banks Differ in Their Practices for Implementing
           External Regulation? The Role of Top Management's Attitudes",
  year  =  2018
}

@ARTICLE{Gu2020-si,
  title    = "Empirical Asset Pricing via Machine Learning",
  author   = "Gu, Shihao and Kelly, Bryan and Xiu, Dacheng",
  abstract = "Abstract. We perform a comparative analysis of machine learning
              methods for the canonical problem of empirical asset pricing:
              measuring asset risk premiums. We",
  journal  = "Rev. Financ. Stud.",
  series   = "Working Paper Series",
  month    =  feb,
  year     =  2020
}

@ARTICLE{Cucinelli2018-ob,
  title    = "Credit risk in European banks: The bright side of the internal
              ratings based approach",
  author   = "Cucinelli, Doriana and Battista, Maria Luisa Di and Marchese,
              Malvina and Nieri, Laura",
  abstract = "This paper investigates the accuracy of internal rating based
              (IRB) models in measuring credit risk. We contribute to the
              growing debate on the current prudential regulatory framework by
              investigating the use of validated IRB models in promoting
              efficient risk management practices. Our empirical analysis is
              based on a novel panel data set of 177 Western European banks
              observed from 2008 to 2015, in the aftermath of the financial and
              economic crisis. We find that IRB banks were able to curb the
              increase in credit risk driven by the macroeconomic slowdown
              better than banks under the standardized approach. This suggests
              that the introduction of the internal ratings based approach by
              Basel II has promoted the adoption of stronger risk management
              practices among banks, as meant by the regulators.",
  journal  = "Journal of Banking \& Finance",
  volume   =  93,
  pages    = "213--229",
  month    =  aug,
  year     =  2018,
  keywords = "Internal ratings based approach; Credit risk; Prudential
              regulation; Dynamic panels; State dependent endogenous dummy;
              System GMM"
}

@ARTICLE{McKee1966-zc,
  title     = "An Economic Analysis of Credit Unions in Michigan",
  author    = "McKee, David L",
  journal   = "J. Finance",
  publisher = "[American Finance Association, Wiley]",
  volume    =  21,
  number    =  4,
  pages     = "752--752",
  year      =  1966
}

@ARTICLE{Taylor1977-nm,
  title     = "Credit Unions and {Economic-Efficiency}",
  author    = "Taylor, Ryland A",
  journal   = "Rivista Internazionale Di Scienze Economiche E Commerciali",
  publisher = "Casa Editrice Dott Antonio Milani Via Jappelli 5, 35121 Padua,
               Italy",
  volume    =  24,
  number    =  3,
  pages     = "239--247",
  year      =  1977
}

@TECHREPORT{Benos2017-pt,
  title       = "The Economics of Distributed Ledger Technology for Securities
                 Settlement",
  author      = "Benos, Evangelos and Garratt, Rod and Gurrola-Perez, Pedro",
  number      = "Staff Working Paper No. 670",
  institution = "Bank of England",
  month       =  aug,
  year        =  2017,
  keywords    = "distributed ledger technology, securities settlement,
                 innovation, market power"
}

@ARTICLE{Violon2017-sc,
  title  = "The impact of the identification of {GSIBs} on their business model",
  author = "Violon, Aur{\'e}lien and Durant, Dominique and Toader, Oana",
  year   =  2017
}

@ARTICLE{Jones2013-mk,
  title     = "Valuation and systemic risk consequences of bank opacity",
  author    = "Jones, Jeffrey S and Lee, Wayne Y and Yeager, Timothy J",
  abstract  = "We examine the effects of opacity on bank valuation and
               synchronicity in bank equity returns over the years 2000--2006
               prior to the 2007 financial crisis. As expected, investments in
               opaque assets are more profitable than investments in
               transparent assets, and taking profitability into account, have
               larger valuation discounts relative to transparent assets. The
               valuation discounts on opaque asset investments decline over the
               2000--2006 period only to be followed by a sharp reversal in
               2007. The decline is coincident with a rise in bank equity share
               prices, decrease in transparent asset holdings by banks, and
               greater return synchronicity -- evidence consistent with a
               feedback effect.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  37,
  number    =  3,
  pages     = "693--706",
  month     =  mar,
  year      =  2013,
  keywords  = "Banks; Opacity; Systemic risk; Market discipline; Price
               synchronicity"
}

@UNPUBLISHED{Cetorelli2016-nr,
  title       = "Organizational Complexity and Balance Sheet Management in
                 Global Banks",
  author      = "Cetorelli, Nicola and Goldberg, Linda S",
  abstract    = "Banks have progressively evolved from being standalone
                 institutions to being subsidiaries of increasingly complex
                 financial conglomerates. We conjecture and provide evidence
                 that the organizational complexity of the family of a bank is
                 a fundamental driver of the business model of the bank itself,
                 as reflected in the management of the bank's own balance
                 sheet. Using micro-data on global banks with branch operations
                 in the United States, we show that branches of conglomerates
                 in more complex families have a markedly lower lending
                 sensitivity to funding shocks. The balance sheet management
                 strategies of banks are very much determined by the structure
                 of the organizations the banks belong to. The complexity of
                 the conglomerate can change the scale of the lending channel
                 for a large global bank by more than 30 percent.",
  number      =  22169,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  apr,
  year        =  2016
}

@ARTICLE{Carmassi2016-ir,
  title     = "The Corporate Complexity of Global Systemically Important Banks",
  author    = "Carmassi, Jacopo and Herring, Richard",
  abstract  = "The financial crisis of 2007-2009 revealed that the corporate
               complexity of most of the Global Systemically Important Banks
               (G-SIBs) presented a formidable obstacle to any plausible
               orderly resolution of these institutions. This paper documents
               the extent of this complexity making use of an historical time
               series, developed by the authors, that shows the evolution of
               the number of majority-owned subsidiaries of G-SIBs over time.
               After a very significant increase in complexity before the
               crisis and until 2011, this trend may be reversing, possibly in
               response to regulatory and market pressures on banks since then.
               Nonetheless the reduction in complexity has been uneven across
               institutions and may not persist. The econometric analysis of
               this new set of panel data produces two key results with
               relevant policy implications: first, the relationship found in
               previous studies between the number of subsidiaries and bank
               size loses significance when time effects are introduced;
               second, large mergers and acquisitions are a key driver of
               complexity and their effect remains significant even when time
               effects are considered.",
  journal   = "J Financ Serv Res",
  publisher = "Springer US",
  volume    =  49,
  number    = "2-3",
  pages     = "175--201",
  month     =  jun,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Philippon2016-ud,
  title     = "The {FinTech} Opportunity",
  author    = "Philippon, Thomas",
  abstract  = "This paper assesses the potential impact of FinTech on the
               finance industry, focusing on financial stability and access to
               services. I document first that financial services remain
               surprisingly expensive, which explains the emergence of new
               entrants. I then argue that the current regulatory approach is
               subject to significant political economy and coordination costs,
               and therefore unlikely to deliver much structural change.
               FinTech, on the other hand, can bring deep changes but is likely
               to create significant regulatory challenges.",
  journal   = "NBER Work. Pap. Ser.",
  publisher = "nber.org",
  number    =  22476,
  series    = "Working Paper Series",
  month     =  aug,
  year      =  2016
}

@ARTICLE{Krause2017-lp,
  title     = "Complexity and bank risk during the financial crisis",
  author    = "Krause, Thomas and Sondershaus, Talina and Tonzer, Lena",
  abstract  = "We construct a novel dataset to measure banks' complexity and
               relate it to banks' riskiness. The sample covers stock listed
               Euro area banks from 2007 to 2014. Bank stability is
               significantly affected by complexity, whereas the direction of
               the effect differs across complexity measures.",
  journal   = "Econ. Lett.",
  publisher = "Elsevier",
  volume    =  150,
  pages     = "118--121",
  month     =  jan,
  year      =  2017,
  keywords  = "Bank risk; Complexity; Globalization"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Cetorelli2014-hr,
  title     = "Measures of global bank complexity",
  author    = "Cetorelli, Nicola and Goldberg, Linda S and {Others}",
  abstract  = "The increasing size and complexity of financial institutions has
               received renewed attention in recent years---prompted in part by
               the debate over the issue of too-big-to-fail entities. How the
               size of failing institutions might contribute to systemic
               disruption is well understood ‚Ä¶",
  journal   = "FRBNY Economic Policy Review",
  publisher = "newyorkfed.org",
  volume    =  20,
  number    =  2,
  pages     = "107--126",
  year      =  2014
}

@INCOLLECTION{Ashcraft2014-vr,
  title     = "Shadow Bank Monitoring",
  booktitle = "The Oxford Handbook of Banking, Second Edition (2 ed.)",
  author    = "Ashcraft, Adam and Adrian, Tobias and Cetorelli, Nicola",
  editor    = "Berger, Allen N and Molyneux, Philip and Wilson, John O S",
  abstract  = "We provide a framework for monitoring the shadow banking system.
               The shadow banking system consists of a web of specialized
               financial institutions that conduct credit, maturity, and
               liquidity transformation without direct, explicit access to
               public backstops. The lack of such access to sources of
               government liquidity and credit backstops makes shadow banks
               inherently fragile. Shadow banking activities are often
               intertwined with core regulated institutions such as bank
               holding companies, security brokers and dealers, and insurance
               companies. These interconnections of shadow banks with other
               financial institutions create sources of systemic risk for the
               broader financial system. We provide elements of monitoring
               risks in the shadow banking system, including recent efforts by
               the Financial Stability Board.",
  publisher = "Oxford University Press",
  month     =  nov,
  year      =  2014,
  keywords  = "shadow banking; financial stability monitoring; financial
               intermediation"
}

@ARTICLE{Carhart1997-pq,
  title     = "On persistence in mutual fund performance",
  author    = "Carhart, Mark M",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  52,
  number    =  1,
  pages     = "57--82",
  year      =  1997
}

@ARTICLE{Lambert2018-ac,
  title     = "Lobbying on Regulatory Enforcement Actions: Evidence from {U.S}.
               Commercial and Savings Banks",
  author    = "Lambert, Thomas",
  abstract  = "This paper analyzes the relationship between bank lobbying and
               supervisory decisions of regulators and documents its moral
               hazard implications. Exploiting bank-level information on the
               universe of commercial and savings banks in the United States, I
               find that regulators are 44.7\% less likely to initiate
               enforcement actions against lobbying banks. This result is
               robust across measures of lobbying and accounts for endogeneity
               concerns by employing instrumental variables strategies. In
               addition, I show that lobbying banks are riskier and reliably
               underperform their nonlobbying peers. Overall, these results
               appear rather inconsistent with an information-based explanation
               of bank lobbying, but consistent with the theory of regulatory
               capture. This paper was accepted by Amit Seru, finance.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  month     =  jan,
  year      =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Navarro_undated-aq,
  title  = "Shock diffusion in large regular networks: the role of transitive
            cycles‚àó",
  author = "Navarro, N and Dan Tran, H"
}

@ARTICLE{Shafir1997-us,
  title     = "Money Illusion",
  author    = "Shafir, Eldar and Diamond, Peter and Tversky, Amos",
  abstract  = "The term ``money illusion'' refers to a tendency to think in
               terms of nominal rather than real monetary values. Money
               illusion has significant implications for economic theory, yet
               it implies a lack of rationality that is alien to economists.
               This paper reviews survey questions regarding people's reactions
               to variations in inflation and prices, designed to shed light on
               the psychology that underlies money illusion. We propose that
               people often think about economic transactions in both nominal
               and real terms, and that money illusion arises from an
               interaction between these representations, which results in a
               bias toward a nominal evaluation.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  112,
  number    =  2,
  pages     = "341--374",
  month     =  may,
  year      =  1997
}

@ARTICLE{Brunnermeier2008-xv,
  title     = "Money Illusion and Housing Frenzies",
  author    = "Brunnermeier, Markus K and Julliard, Christian",
  abstract  = "A reduction in inflation can fuel run-ups in housing prices if
               people suffer from money illusion. For example, investors who
               decide whether to rent or buy a house by simply comparing
               monthly rent and mortgage payments do not take into account the
               fact that inflation lowers future real mortgage costs. We
               decompose the price--rent ratio into a rational
               component---meant to capture the ``proxy effect'' and risk
               premia---and an implied mispricing. We find that inflation and
               nominal interest rates explain a large share of the time series
               variation of the mispricing, and that the tilt effect is very
               unlikely to rationalize this finding.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  1,
  pages     = "135--180",
  month     =  jan,
  year      =  2008
}

@ARTICLE{Tyran2007-lp,
  title    = "Economics. Money illusion and the market",
  author   = "Tyran, Jean-Robert",
  journal  = "Science",
  volume   =  317,
  number   =  5841,
  pages    = "1042--1043",
  month    =  aug,
  year     =  2007,
  language = "en"
}

@ARTICLE{Cetorelli2014-cz,
  title     = "Evolution in Bank Complexity",
  author    = "Cetorelli, Nicola and McAndrews, James and Traina, James",
  abstract  = "The authors present an analytical framework for bank complexity
               through the hypothesis that complexity is necessary for banks to
               stay viable in the evolving ind",
  publisher = "papers.ssrn.com",
  month     =  mar,
  year      =  2014,
  keywords  = "organizational complexity, financial intermediation"
}

@ARTICLE{Taddy2016-vq,
  title     = "A Nonparametric Bayesian Analysis of Heterogenous Treatment
               Effects in Digital Experimentation",
  author    = "Taddy, Matt and Gardner, Matt and Chen, Liyun and Draper, David",
  abstract  = "Randomized controlled trials play an important role in how
               Internet companies predict the impact of policy decisions and
               product changes. In these ?digital experiments,? different units
               (people, devices, products) respond differently to the
               treatment. This article presents a fast and scalable Bayesian
               nonparametric analysis of such heterogenous treatment effects
               and their measurement in relation to observable covariates. New
               results and algorithms are provided for quantifying the
               uncertainty associated with treatment effect measurement via
               both linear projections and nonlinear regression trees (CART and
               random forests). For linear projections, our inference strategy
               leads to results that are mostly in agreement with those from
               the frequentist literature. We find that linear regression
               adjustment of treatment effect averages (i.e.,
               post-stratification) can provide some variance reduction, but
               that this reduction will be vanishingly small in the low-signal
               and large-sample setting of digital experiments. For regression
               trees, we provide uncertainty quantification for the machine
               learning algorithms that are commonly applied in tree-fitting.
               We argue that practitioners should look to ensembles of trees
               (forests) rather than individual trees in their analysis. The
               ideas are applied on and illustrated through an example
               experiment involving 21 million unique users of EBay.com.",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  34,
  number    =  4,
  pages     = "661--672",
  month     =  oct,
  year      =  2016
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@TECHREPORT{Ahnert2016-ax,
  title       = "Opaque assets and rollover risk",
  author      = "Ahnert, Toni and Nelson, Benjamin F",
  abstract    = "‚Ä¶ Cetorelli et al. (2014) document the organizational
                 complexity of US bank holding ‚Ä¶ In Wagner (2007), opaque
                 activities constitute an inefficient response by banks to the
                 higher transparency of their traditional activities. By
                 contrast, opacity in our setup is the low precision of ‚Ä¶",
  publisher   = "econstor.eu",
  institution = "Bank of Canada Staff Working Paper",
  year        =  2016
}

@ARTICLE{Besley2012-qj,
  title    = "Estimating the Peace Dividend: The Impact of Violence on House
              Prices in Northern Ireland",
  author   = "Besley, Timothy and Mueller, Hannes",
  journal  = "Am. Econ. Rev.",
  volume   =  102,
  number   =  2,
  pages    = "810--833",
  month    =  apr,
  year     =  2012
}

@ARTICLE{Ward2013-uj,
  title         = "Undefined By Data: A Survey of Big Data Definitions",
  author        = "Ward, Jonathan Stuart and Barker, Adam",
  abstract      = "The term big data has become ubiquitous. Owing to a shared
                   origin between academia, industry and the media there is no
                   single unified definition, and various stakeholders provide
                   diverse and often contradictory definitions. The lack of a
                   consistent definition introduces ambiguity and hampers
                   discourse relating to big data. This short paper attempts to
                   collate the various definitions which have gained some
                   degree of traction and to furnish a clear and concise
                   definition of an otherwise ambiguous term.",
  month         =  sep,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DB",
  eprint        = "1309.5821"
}

@ARTICLE{Gallagher2017-dm,
  title    = "Lurking in the Shadow Prices: A New Shadow Price Statistical
              Significance Test with Applications in Economic Regulation",
  author   = "Gallagher, Ronan and Kuosmanen, Timo and Quinn, Barry",
  abstract = "This paper introduces a novel approach for statistical testing of
              group differences in shadow prices estimated non-parametrically.
              Exploiting trigonometry theor",
  month    =  nov,
  year     =  2017,
  keywords = "data envelopment analysis(DEA), shadow prices statistical
              testing, efficiency analysis, frontier estimation, stochastic
              nonparametric envelopment of data (StoNED), economic regulation"
}

@ARTICLE{Fama1970-sy,
  title     = "Efficient capital markets: A review of theory and empirical work",
  author    = "Fama, Eugene F",
  journal   = "J. Finance",
  publisher = "JSTOR",
  volume    =  25,
  number    =  2,
  pages     = "383--417",
  year      =  1970
}

@ARTICLE{Begg1993-lq,
  title     = "The service sector in regional development",
  author    = "Begg, Iain",
  abstract  = "... question of whether the uneven spatial development of
               producer services , in particular, during ... new starts,
               especially in peripheral regions, that could damage regional
               competitiveness. Functional splits within large companies,
               including financial and busi- ness services (AKSOY ...",
  journal   = "Reg. Stud.",
  publisher = "Taylor \& Francis",
  volume    =  27,
  number    =  8,
  pages     = "817--825",
  year      =  1993
}

@ARTICLE{Walker1977-ve,
  title     = "On the Allocation of the Net Monetary Benefits of Credit Union
               Membership",
  author    = "Walker, Michael C and Chandler, Gary G",
  journal   = "Rev. Soc. Econ.",
  publisher = "Routledge",
  volume    =  35,
  number    =  2,
  pages     = "159--168",
  month     =  oct,
  year      =  1977
}

@ARTICLE{Monaghan2014-aj,
  title     = "``Courting the multinational'': Subnational institutional
               capacity and foreign market insidership",
  author    = "Monaghan, Sin{\'e}ad and Gunnigle, Patrick and Lavelle, Jonathan",
  abstract  = "Significant contemporary challenges face an internationalizing
               firm, including the non-ergodic nature of investment, and the
               liability of outsidership. Recent revisions to the Uppsala
               internationalization process model reflect these challenges,
               whereby ``insidership'' is represented as realized, successful
               foreign market entry. Drawing upon socio-spatial concepts from
               international business and economic geography, this paper
               demonstrates the endogeneity of subnational institutions in
               shaping foreign market insidership within an advanced economy.
               Employing a multi-method research design with almost 60
               subnational actors, the role and interaction of subnational
               institutions within the internationalization process are
               explored. Our findings illustrate how customized coalitions of
               subnational institutions effectively initiate, negotiate and
               accelerate insidership of inward investment within the foreign
               market both prior to and during formal entry. Key aspects of
               this dynamic include communicating tangible and intangible
               locational resources, initiating functional and relevant
               business relationships, and facilitating access to codified and
               tacit knowledge. This paper embellishes the Uppsala
               internationalization process model by demonstrating the capacity
               of subnational institutions to participate actively with foreign
               market insidership, and in so doing advances understanding of
               how the risk and uncertainty associated with foreign market
               entry are currently navigated.",
  journal   = "J Int Bus Stud",
  publisher = "Palgrave Macmillan UK",
  volume    =  45,
  number    =  2,
  pages     = "131--150",
  month     =  feb,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Jiang2017-za,
  title     = "Artificial intelligence in healthcare: past, present and future",
  author    = "Jiang, Fei and Jiang, Yong and Zhi, Hui and Dong, Yi and Li, Hao
               and Ma, Sufeng and Wang, Yilong and Dong, Qiang and Shen,
               Haipeng and Wang, Yongjun",
  abstract  = "Artificial intelligence (AI) aims to mimic human cognitive
               functions. It is bringing a paradigm shift to healthcare,
               powered by increasing availability of healthcare data and rapid
               progress of analytics techniques. We survey the current status
               of AI applications in healthcare and discuss its future. AI can
               be applied to various types of healthcare data (structured and
               unstructured). Popular AI techniques include machine learning
               methods for structured data, such as the classical support
               vector machine and neural network, and the modern deep learning,
               as well as natural language processing for unstructured data.
               Major disease areas that use AI tools include cancer, neurology
               and cardiology. We then review in more details the AI
               applications in stroke, in the three major areas of early
               detection and diagnosis, treatment, as well as outcome
               prediction and prognosis evaluation. We conclude with discussion
               about pioneer AI systems, such as IBM Watson, and hurdles for
               real-life deployment of AI.",
  journal   = "Stroke Vasc Neurol",
  publisher = "BMJ Specialist Journals",
  pages     = "svn--2017--000101",
  month     =  jun,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Qu2007-nc,
  title     = "Estimating and Testing Structural Changes in Multivariate
               Regressions",
  author    = "Qu, Zhongjun and Perron, Pierre",
  abstract  = "This paper considers issues related to estimation, inference,
               and computation with multiple structural changes that occur at
               unknown dates in a system of equations. Changes can occur in the
               regression coefficients and/or the covariance matrix of the
               errors. We also allow arbitrary restrictions on these
               parameters, which permits the analysis of partial structural
               change models, common breaks that occur in all equations, breaks
               that occur in a subset of equations, and so forth. The method of
               estimation is quasi-maximum likelihood based on Normal errors.
               The limiting distributions are obtained under more general
               assumptions than previous studies. For testing, we propose
               likelihood ratio type statistics to test the null hypothesis of
               no structural change and to select the number of changes.
               Structural change tests with restrictions on the parameters can
               be constructed to achieve higher power when prior information is
               present. For computation, an algorithm for an efficient
               procedure is proposed to construct the estimates and test
               statistics. We also introduce a novel locally ordered breaks
               model, which allows the breaks in different equations to be
               related yet not occurring at the same dates.",
  journal   = "Econometrica",
  publisher = "Blackwell Publishing Ltd",
  volume    =  75,
  number    =  2,
  pages     = "459--502",
  month     =  mar,
  year      =  2007,
  keywords  = "Change-point; segmented regressions; break dates; hypothesis
               testing; model selection; system of regressions"
}

@ARTICLE{Martin-Oliver2017-nn,
  title    = "The fall of Spanish cajas: Lessons of ownership and governance
              for banks",
  author   = "Mart{\'\i}n-Oliver, Alfredo and Ruano, Sonia and Salas-Fum{\'a}s,
              Vicente",
  abstract = "Ownership, governance, and institutional diversity among banks
              are a subject of public and regulatory concern. This paper
              addresses this issue by using a case study of Spain, where the
              retail banking market was split evenly between shareholder and
              stakeholder banks before the crisis. We examine how institutional
              diversity mattered in the accumulation of risk in the pre-crisis
              years, in the severity of losses caused by the crisis, and in the
              resilience to recover from the losses. The method of analysis
              consists in linking the risk position of the banks in the
              pre-crisis period and the losses arising during the crisis to the
              decisions of banks to migrate from business models based on
              deposit financing to models based on market-debt financing. We
              find that cajas migrated to more vulnerable business models
              following the strategy of the shareholder banks, but the losses
              in the crisis were much higher in the former than in the latter.
              The paper interprets this result as evidence that what matters
              the most about the ownership of banks is their resilience in bad
              times.",
  journal  = "Journal of Financial Stability",
  month    =  feb,
  year     =  2017,
  keywords = "Ownership of banks; Governance; Banking crisis; Spain; Cajas;
              Business models"
}

@ARTICLE{Lensink2014-qk,
  title     = "Institutions and Bank Performance: A Stochastic Frontier
               Analysis*: Institutions and bank performance",
  author    = "Lensink, Robert and Meesters, Aljar",
  abstract  = "This article investigates the impact of institutions on bank
               efficiency and technology, using a stochastic frontier analysis
               of a data set of 7,959 banks across 136 countries over 10 years.
               The results confirm the importance of well-developed
               institutions for the efficient operation of commercial banks.
               Furthermore, the insights reveal the impact of institutional
               reforms in improving bank efficiency. The results are robust to
               adjustments in country-specific effects, achieved by including
               country dummies, as well as across different risk profiles.
               Moreover, they provide empirical evidence in support of the
               public view of the banking sector. [ABSTRACT FROM AUTHOR]",
  journal   = "Oxf. Bull. Econ. Stat.",
  publisher = "Wiley-Blackwell",
  volume    =  76,
  number    =  1,
  pages     = "67--92",
  month     =  feb,
  year      =  2014,
  keywords  = "ORGANIZATIONAL performance; BANKING industry; STOCHASTIC
               processes; EMPIRICAL research; PUBLIC opinion; FRONTIER
               analysis; E44; G21; G38; K00"
}

@INCOLLECTION{Bernanke1999-lf,
  title     = "Chapter 21 The financial accelerator in a quantitative business
               cycle framework",
  booktitle = "Handbook of Macroeconomics",
  author    = "Bernanke, Ben S and Gertler, Mark and Gilchrist, Simon",
  abstract  = "This chapter develops a dynamic general equilibrium model that
               is intended to help clarify the role of credit market frictions
               in business fluctuations, from both a qualitative and a
               quantitative standpoint. The model is a synthesis of the leading
               approaches in the literature. In particular, the framework
               exhibits a ``financial accelerator'', in that endogenous
               developments in credit markets work to amplify and propagate
               shocks to the macroeconomy. In addition, we add several features
               to the model that are designed to enhance the empirical
               relevance. First, we incorporate money and price stickiness,
               which allows us to study how credit market frictions may
               influence the transmission of monetary policy. In addition, we
               allow for lags in investment which enables the model to generate
               both hump-shaped output dynamics and a lead-lag relation between
               asset prices and investment, as is consistent with the data.
               Finally, we allow for heterogeneity among firms to capture the
               fact that borrowers have differential access to capital markets.
               Under reasonable parametrizations of the model, the financial
               accelerator has a significant influence on business cycle
               dynamics.",
  publisher = "Elsevier",
  volume    =  1,
  pages     = "1341--1393",
  series    = "Handbook of Macroeconomics",
  year      =  1999,
  keywords  = "financial accelerator; business fluctuations; monetary policy"
}

@ARTICLE{Szczepanski2016-ir,
  title    = "Beyond completion rate: evaluating the passing ability of
              footballers",
  author   = "Szczepa{\'n}ski, {\L}ukasz and McHale, Ian",
  abstract = "Passing the ball is one of the key skills of a football player
              yet the metrics commonly used to evaluate passing ability are
              crude and largely limited to various forms of a pass completion
              rate. These metrics can be misleading for two general reasons:
              they do not account for the difficulty of the attempted pass nor
              the various levels of uncertainty involved in empirical
              observations based on different numbers of passes per player. We
              address both these deficiencies by building a statistical model
              in which the success of a pass depends on the skill of the
              executing player as well as other factors including the origin
              and destination of the pass, the skill of his teammates and the
              opponents, and proxies for the defensive pressure put on the
              executing player as well as random chance. We fit the model by
              using data from the 2006--2007 season of the English Premier
              League provided by Opta, estimate each player's passing skill and
              make predictions for the next season. The model predictions
              considerably outperform a naive method of simply using the
              previous season's completion rate as a predictor of the following
              season's completion rate. In particular, we show how a change in
              the difficulty of passes attempted in both seasons explains a
              significant proportion of the shift in the observed performance
              of some players---a fact that is ignored if the raw completion
              rate is used to evaluate player skill.",
  journal  = "J. R. Stat. Soc. A",
  volume   =  179,
  number   =  2,
  pages    = "513--533",
  month    =  feb,
  year     =  2016,
  keywords = "Generalized additive mixed models; Ranking; Rating; Soccer; Sport"
}

@ARTICLE{Halme2014-bv,
  title    = "Non-convex value efficiency analysis and its application to bank
              branch sales evaluation",
  author   = "Halme, Merja and Korhonen, Pekka and Eskelinen, Juha",
  abstract = "We have observed when applying Value Efficiency Analysis [21]
              that decision makers wish to provide preference information
              related to existing rather than virtual (efficient) units. This
              observation motivated us to develop an approach based on the
              preference comparisons of existing units. The Free Disposal Hull
              model provides the requisite framework. We assume that a Decision
              Maker compares units using an implicitly known value function
              that reaches its maximum at his/her most preferred (efficient)
              unit. The unknown value function is assumed to be quasi-concave
              in outputs and quasi-convex in inputs. The main purpose -- as in
              the original Value Efficiency Analysis -- is to approximate the
              distance of each unit from the contour of the value function
              passing through the most preferred unit. We use examples to
              illustrate the approach. Finally, we describe a real application
              in which Value Efficiency Analysis was used to produce
              information for bank managers wishing to evaluate the performance
              of bank branches.",
  journal  = "Omega",
  volume   =  48,
  pages    = "10--18",
  month    =  oct,
  year     =  2014,
  keywords = "Multiple criteria decision making; Data envelopment analysis; FDH
              model; Value efficiency; Quasi-concave; Quasi-convex"
}

@ARTICLE{Mamatzakis2016-vy,
  title     = "What is the impact of bankrupt and restructured loans on
               Japanese bank efficiency?",
  author    = "Mamatzakis, Emmanuel and Matousek, Roman and Vu, Anh Nguyet",
  abstract  = "The Japanese banking system provides a distinctive platform for
               the examination of the long-lasting effect of problem loans on
               efficiency. We measure technical efficiency by modifying a
               translog enhanced hyperbolic distance function with two
               undesirable outputs, identified as problem loans and problem
               other earning assets. Our unique database allows us to
               distinguish between bankrupt and restructured loans to
               investigate the underlying causality between these loans and
               efficiency. From the flexible panel vector autoregression
               specification, primary results reveal that bankrupt loans have a
               positive impact on efficiency related to the ``moral hazard,
               skimping'' hypothesis, with the causality originating from
               bankrupt loans. In contrast, findings for the relationship
               between restructured loans and efficiency support the ``bad
               luck'' hypothesis.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  72,
  pages     = "S187--S202",
  month     =  nov,
  year      =  2016,
  keywords  = "Bank efficiency; Bankrupt loans; Restructured loans; Panel VAR;
               Japan"
}

@ARTICLE{Harris2013-en,
  title     = "The impact of {TARP} on bank efficiency",
  author    = "Harris, Oneil and Huerta, Daniel and Ngo, Thanh",
  abstract  = "This paper examines the impact of the Troubled Asset Relief
               Program (TARP) capital injections on the operational efficiency
               of commercial banks. Using a nonparametric Data Envelopment
               Analysis to measure bank efficiency, we document a deteriorating
               pattern in the operating efficiency for banks that received the
               capital injection from TARP funds that is not evident in
               non-TARP banks. We test the impact of TARP on the change in bank
               efficiency as well as the abnormal change in bank efficiency;
               yet, our results continue to hold. We attribute the decrease in
               the operating efficiency of TARP funded banks to the abated
               incentives of bank managers to adopt best practices that improve
               asset quality, and the moral hazard associated with bailouts.",
  journal   = "Journal of International Financial Markets, Institutions and
               Money",
  publisher = "Elsevier",
  volume    =  24,
  pages     = "85--104",
  month     =  apr,
  year      =  2013,
  keywords  = "Operating efficiency; TARP; Bank bailout"
}

@ARTICLE{Cerasi2014-az,
  title    = "Rethinking the regulatory treatment of securitization",
  author   = "Cerasi, Vittoria and Rochet, Jean-Charles",
  abstract = "In a model where banks play an active role in monitoring
              borrowers, we analyze the impact of securitization on bankers'
              incentives across different macroeconomic scenarios. We show that
              securitization can be part of the optimal financing scheme for
              banks, provided banks retain an equity tranche in the sold loans
              to maintain proper incentives. In economic downturns however
              securitization should be restricted. The implementation of the
              optimal solvency scheme is achieved by setting appropriate
              capital charges through a form of capital insurance, protecting
              the value of bank capital in downturns, while providing
              additional liquidity in upturns.",
  journal  = "Journal of Financial Stability",
  volume   =  10,
  pages    = "20--31",
  month    =  feb,
  year     =  2014,
  keywords = "Solvency regulation; Securitization; Capital insurance;
              Monitoring"
}

@ARTICLE{Karimov2013-qa,
  title    = "Economic Inefficiency and Shadow Prices of Inputs: The Case of
              Vegetable Growing Farms in Uzbekistan",
  author   = "Karimov, Aziz",
  abstract = "This article was devoted to an overall estimation of the economic
              inefficiency of the sample of vegetable growing farms in
              Uzbekistan. Because our analysis is based at the farming system
              level, we have been able to estimate the allocative (price)
              inefficiency of vegetable producers. Another significant finding
              of this study is that we have shown the economic inefficiency of
              each input under consideration. We have also applied a technique
              that calculates shadow prices of land and labor (for which no
              price information was available) using existing price information
              for diesel input under existing production inefficiencies. We
              found relatively large inefficiencies in the production of
              vegetables, both technical and allocative, whereas technical
              inefficiency played a relatively major role in overall
              inefficiency. As the model's results show, seed is economically
              the most inefficiently used input in producing vegetables. We
              found a possible cost savings if farms followed best-practice
              farm groups, which were used for the construction of frontier.",
  journal  = "Procedia Economics and Finance",
  volume   =  5,
  pages    = "403--412",
  month    =  jan,
  year     =  2013,
  keywords = "directional input distance function; shadow prices; economic
              inefficiency; vegetable production"
}

@ARTICLE{Collier2010-rn,
  title     = "Measuring Technical Efficiency in Sports",
  author    = "Collier, Trevor and Johnson, Andrew L and Ruggiero, John",
  abstract  = "Standard economic production theory is the basis for measuring
               technical efficiency in sports. Using programming or regression
               models, efficiency is defined as the distance of a given team
               observation from the technology. In this article, the authors
               show that the standard measures of efficiency using
               deterministic models are biased downward due to serial
               correlation with respect to the efficiency measure. In
               particular, if the number of observed wins for a given team is
               affected by the team's inefficiency, it is necessarily true that
               another team is able to produce outside of the technology. As a
               result, the observed frontier is not feasible if all
               inefficiency is eliminated. In this article, the authors propose
               a correction to this problem and apply new models to estimate
               efficiency in professional football.",
  journal   = "J. Sports Econom.",
  publisher = "SAGE PublicationsSage CA: Los Angeles, CA",
  volume    =  12,
  number    =  6,
  pages     = "579--598",
  month     =  dec,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Abadie2008-yg,
  title     = "Estimation of the Conditional Variance in Paired Experiments",
  author    = "Abadie, Alberto and Imbens, Guido W",
  abstract  = "In paired randomized experiments units are grouped in pairs,
               often based on covariate information, with random assignment
               within the pairs. Average treatment effects are then estimated
               by averaging the within-pair differences in outcomes. Typically
               the variance of the average treatment effect estimator is
               estimated using the sample variance of the within-pair
               differences. However, conditional on the covariates the variance
               of the average treatment effect estimator may be substantially
               smaller. Here we propose a simple way of estimating the
               conditional variance of the average treatment effect estimator
               by forming pairs-of-pairs with similar covariate values and
               estimating the variances within these pairs-of-pairs. Even
               though these within-pairs-of-pairs variance estimators are not
               consistent, their average is consistent for the conditional
               variance of the average treatment effect estimator and leads to
               asymptotically valid confidence intervals. Dans les
               exp{\'e}riences al{\'e}atoires d'appariement les unit{\'e}s sont
               regroup{\'e}es par paires, souvent bas{\'e}es sur des
               caract{\'e}ristiques explicatives, et avec appariement
               al{\'e}atoire. Les effets de traitement moyens sont alors
               estim{\'e}s en faisant la moyenne des diff{\'e}rences
               intra-paires dans les r{\'e}sultats. Typiquement, la variance de
               l'estimateur de l'effet de traitement moyen est estim{\'e}e en
               utilisant la variance des diff{\'e}rences intra-paires dans
               l'{\'e}chantillon. Cependant, conditionnellement aux variables
               explicatives, l'estimateur de l'effet de traitement moyen peut
               {\^e}tre substantiellement plus petit. Nous proposons ici une
               mani{\`e}re simple d'estimer la variance conditionnelle de
               l'estimateur de l'effet de traitement moyen en formant des
               paires de paires avec des valeurs de variables explicatives
               similaires et en estimant les variances entre ces paires de
               paires. M{\^e}me si ces estimateurs fond{\'e}s sur les paires de
               paires ne sont pas convergents, leur moyenne est convergente
               pour la variance conditionnelle de l'estimateur de l'effet de
               traitement moyen et conduit {\`a} des intervalles de confiance
               asymptotiquement valides.",
  journal   = "Annales d'{\'E}conomie et de Statistique",
  publisher = "[GENES, ADRES]",
  volume    = "91/92",
  number    = "91/92",
  pages     = "175--187",
  year      =  2008
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@UNPUBLISHED{Moritz2018-rq,
  title  = "{Tree-Based} Conditional Portfolio Sorts: The Relation Between Past
            and Future Stock Returns‚àó",
  author = "Moritz, Benjamin and Zimmermann, Tom",
  year   =  2018
}

@ARTICLE{Olhede2018-uv,
  title    = "The {AI} spring of 2018",
  author   = "Olhede, Sofia and Wolfe, Patrick",
  abstract = "As nations race for dominance in the field of artificial
              intelligence, Sofia Olhede and Patrick Wolfe consider the
              implications for statistics and statisticians",
  journal  = "Significance",
  volume   =  15,
  number   =  3,
  pages    = "6--7",
  month    =  jun,
  year     =  2018
}

@ARTICLE{Duffie2017-dv,
  title     = "Financial Regulatory Reform After the Crisis: An Assessment",
  author    = "Duffie, Darrell",
  abstract  = "This is a survey of progress with the postcrisis global (G20)
               reform of the financial system, in five key areas of new
               regulation: (1) making financial institutions more resilient;
               (2) ending ?too-big-to-fail?; (3) making derivatives markets
               safer; (4) transforming shadow banking; and (5) improving trade
               competition and market transparency. The resiliency reforms,
               particularly bank capital regulations, have been increasingly
               successful in improving financial stability, but have been
               accompanied by some reduction in secondary-market liquidity. I
               review specific areas where reforms are far from complete, or
               have even been counterproductive. This paper was accepted by
               Gustavo Manso, finance. Conflict of Interest Statement: The
               author's webpage at http://www.stanford.edu/~duffie/ provides
               related research and disclosure of potential conflicts of
               interest.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  month     =  aug,
  year      =  2017
}

@ARTICLE{Manela2017-wc,
  title     = "News implied volatility and disaster concerns",
  author    = "Manela, A and Moreira, A",
  abstract  = "We construct a text-based measure of uncertainty starting in
               1890 using front-page articles of the Wall Street Journal. News
               implied volatility (NVIX) peaks during stock market crashes,
               times of policy-related uncertainty, world wars, and financial
               crises. In US postwar data, periods when NVIX is high are
               followed by periods of above average stock returns, even after
               controlling for contemporaneous and forward-looking measures of
               stock market volatility. News coverage related to wars and
               government policy explains most of the time variation in risk
               premia our measure identifies. Over the longer 1890--2009 sample
               that includes the Great Depression and two world wars, high NVIX
               predicts high future returns in normal times and rises just
               before transitions into economic disasters. The evidence is
               consistent with recent theories emphasizing time variation in
               rare disaster risk as a source of aggregate asset prices
               fluctuations. \copyright{} 2016",
  journal   = "J. financ. econ.",
  publisher = "Elsevier B.V.",
  volume    =  123,
  number    =  1,
  pages     = "137--162",
  year      =  2017,
  keywords  = "Equity premium; Implied volatility; Machine learning; Rare
               disasters; Return predictability; Text-based analysis"
}

@ARTICLE{Nian2018-rj,
  title     = "Learning minimum variance discrete hedging directly from the
               market",
  author    = "Nian, K and Coleman, T F and Li, Y",
  abstract  = "Option hedging is a critical risk management problem in finance.
               In the Black--Scholes model, it has been recognized that
               computing a hedging position from the sensitivity of the
               calibrated model option value function is inadequate in
               minimizing variance of the option hedge risk, as it fails to
               capture the model parameter dependence on the underlying price
               (see e.g. Coleman et al., J. Risk, 2001, 5(6), 63--89; Hull and
               White, J. Bank. Finance, 2017, 82, 180--190). In this paper, we
               demonstrate that this issue can exist generally when determining
               hedging position from the sensitivity of the option function,
               either calibrated from a parametric model from current option
               prices or estimated nonparametricaly from historical option
               prices. Consequently, the sensitivity of the estimated model
               option function typically does not minimize variance of the
               hedge risk, even instantaneously. We propose a data-driven
               approach to directly learn a hedging function from the market
               data by minimizing variance of the local hedge risk. Using the
               S\&P 500 index daily option data for more than a decade ending
               in August 2015, we show that the proposed method outperforms the
               parametric minimum variance hedging method proposed in Hull and
               White [J. Bank. Finance, 2017, 82, 180--190], as well as minimum
               variance hedging corrective techniques based on stochastic
               volatility or local volatility models. Furthermore, we show that
               the proposed approach achieves significant gain over the implied
               BS delta hedging for weekly and monthly hedging. \copyright{}
               2018, \copyright{} 2018 Informa UK Limited, trading as Taylor \&
               Francis Group.",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  volume    =  18,
  number    =  7,
  pages     = "1115--1128",
  year      =  2018,
  keywords  = "Dynamic hedging; Kernels; Machine learning; Regularized network;
               Risk management"
}

@ARTICLE{Creamer2015-lu,
  title     = "Can a corporate network and news sentiment improve portfolio
               optimization using the {Black--Litterman} model?",
  author    = "Creamer, G G",
  abstract  = "The Black--Litterman (BL) model for portfolio optimization
               combines investors' expectations with the Markowitz framework.
               The BL model is designed for investors with private information
               or knowledge of market behaviour. In this paper, I propose a
               method where investors' expectations are based on either news
               sentiment using high-frequency data or on a combination of
               accounting variables; financial analysts' recommendations, and
               corporate social network indicators with quarterly data. The
               results show promise when compared to a market portfolio. I also
               provide recommendations for trading strategies using the results
               of this BL model. \copyright{} 2015 Taylor \& Francis.",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  volume    =  15,
  number    =  8,
  pages     = "1405--1416",
  year      =  2015,
  keywords  = "Black--Litterman model; Boosting; Link mining; Machine learning;
               Portfolio optimization; Social network; Text analysis"
}

@ARTICLE{MacLean2010-om,
  title     = "The Dangers of Decoupling: The Relationship Between Compliance
               Programs, Legitimacy Perceptions, and Institutionalized
               Misconduct",
  author    = "MacLean, Tammy L and Behnam, Michael",
  abstract  = "This theory-building analysis spotlights a dynamic that occurs
               between decoupling, legitimacy, and institutionalized
               misconduct. Using data gathered from a case study of widespread
               deceptive sales practices at a large financial services firm, we
               demonstrate the dangers of decoupling an organizational
               compliance program from the core business activities of an
               organization. We illustrate how decoupling created a ?legitimacy
               facade? that enabled the institutionalization of misconduct and
               precipitated a loss of external legitimacy.",
  journal   = "AMJ",
  publisher = "Academy of Management",
  volume    =  53,
  number    =  6,
  pages     = "1499--1520",
  month     =  dec,
  year      =  2010
}

@ARTICLE{Parker2009-zk,
  title     = "Corporate Compliance Systems: Could They Make Any Difference?",
  author    = "Parker, Christine and Nielsen, Vibeke Lehmann",
  abstract  = "This article critically appraises the potential of corporate
               compliance systems to influence corporate behavior. The authors
               differentiate between the adoption of formal compliance
               management systems and the way compliance is managed in practice
               in business organizations by reference to scholarly literature
               and analysis of survey responses from 999 large Australian
               businesses about their implementation of competition and
               consumer protection law compliance systems. Their analysis shows
               that at least some elements of compliance systems can translate
               into good management of compliance in practice. But management
               commitment to compliance values, managerial oversight and
               planning, and organizational resources are just as important.",
  journal   = "Adm. Soc.",
  publisher = "SAGE Publications Inc",
  volume    =  41,
  number    =  1,
  pages     = "3--37",
  month     =  mar,
  year      =  2009
}

@TECHREPORT{Baranova2016-bp,
  title     = "The role of collateral in supporting liquidity",
  author    = "Baranova, Yuliya and Liu, Zijun and Noss, Joseph",
  abstract  = "Collateral plays an important role in supporting a vast range of
               transactions that help ensure the efficient functioning of the
               financial system. But collateral markets also have the potential
               to exacerbate risks to financial stability, not least given that
               during periods of market stress demand for high-quality
               collateral may increase, whilst collateral availability may
               fall. This paper offers a means to estimate how this potential
               imbalance between collateral supply and demand is likely to vary
               as a function of market stress. In doing so, it offers an
               estimate of the increase in market volatility sufficient to
               cause a dislocation in the market for collateral and a
               subsequent deterioration in market functioning. It suggests that
               --- from the perspective of financial stability --- the
               implications of an imbalance between the supply and demand of
               collateral are likely to be comparatively benign, but that the
               implications of a reduction in the willingness and/or ability of
               market participants to act as intermediaries in collateral
               markets are likely to have more serious consequences for market
               functioning. This work also provides a framework through which
               policymakers might be able to investigate how regulations might
               affect the proximity of these risks.",
  publisher = "Bank of England",
  number    =  609,
  month     =  aug,
  year      =  2016,
  keywords  = "Collateral; securities financing transactions; derivatives;
               regulation; liquidity"
}

@ARTICLE{noauthor_2009-ob,
  title    = "Unconditional Quantile Regressions",
  abstract = "We propose a new regression method to evaluate the impact of
              changes in the distribution of the explanatory variables on
              quantiles of the unconditional (marginal) distribution of an
              outcome variable. The proposed method consists of running a
              regression of the (recentered) influence function (RIF) of the
              unconditional quantile on the explanatory variables. The
              influence function, a widely used tool in robust estimation, is
              easily computed for quantiles, as well as for other
              distributional statistics. Our approach, thus, can be readily
              generalized to other distributional statistics.",
  journal  = "Econometrica",
  volume   =  77,
  number   =  3,
  pages    = "953--973",
  year     =  2009
}

@BOOK{Parker2011-qb,
  title     = "Explaining Compliance: Business Responses to Regulation",
  author    = "Parker, Christine and Nielsen, Vibeke Lehmann",
  abstract  = "Explaining Compliance consists of sixteen specially commissioned
               chapters by the world's leading empirical researchers, examining
               whether and how businesses comply with regulation that is
               designed to affect positive behaviour changes. Each chapter
               consists of reflective summaries on business compliance with
               different state or voluntary regulation, and the theoretical
               lessons to be drawn from it. As a whole, the book develops
               understanding and explanations of how, why and in what
               circumstances, firms come to comply with regulation, and when
               they do not. It also uncovers the complexity, ambiguity and
               transformation of regulation as it is interpreted, implemented
               and negotiated by firms, their stakeholders and internal
               constituencies in everyday business life. This unique and
               detailed resource will appeal to academics, graduate students
               and senior undergraduates in law, political science, sociology,
               criminology, economics, and psychology, as well as business and
               interdisciplinary areas such as law and society, and law and
               economics. Anyone researching business regulation, corporate
               social responsibility, regulation and compliance, enforcement
               and compliance, and public administration, will also find this
               book beneficial.",
  publisher = "Edward Elgar Publishing",
  year      =  2011,
  language  = "en"
}

@ARTICLE{Parker2009-sx,
  title     = "The Challenge of Empirical Research on Business Compliance in
               Regulatory Capitalism",
  author    = "Parker, Christine and Nielsen, Vibeke",
  abstract  = "Regulatory capitalism?a social, political, and economic order
               characterized by a proliferation of both markets and state and
               nonstate attempts to regulate markets and business
               conduct?creates the opportunity for theoretically and
               politically significant research on compliance. The plural and
               decentered nature of regulation, and therefore of compliance, in
               regulatory capitalism creates significant complexity and
               difficulty for social scientists in the conceptual definition
               and operationalization of regulatory compliance, however. We
               survey the different ways in which empirical researchers have
               studied business compliance with regulation, and their strengths
               and weaknesses. In doing so, we review and interrogate the
               literature on regulatory compliance to understand what it is
               that researchers study when we study business compliance with
               regulation, and what we might have been missing or assuming.",
  journal   = "Annu. Rev. Law. Soc. Sci.",
  publisher = "Annual Reviews",
  volume    =  5,
  number    =  1,
  pages     = "45--70",
  month     =  dec,
  year      =  2009
}

@ARTICLE{Krauss2017-kl,
  title    = "Deep neural networks, gradient-boosted trees, random forests:
              Statistical arbitrage on the {S\&P} 500",
  author   = "Krauss, Christopher and Do, Xuan Anh and Huck, Nicolas",
  abstract = "In recent years, machine learning research has gained momentum:
              new developments in the field of deep learning allow for multiple
              levels of abstraction and are starting to supersede well-known
              and powerful tree-based techniques mainly operating on the
              original feature space. All these methods can be applied to
              various fields, including finance. This paper implements and
              analyzes the effectiveness of deep neural networks (DNN),
              gradient-boosted-trees (GBT), random forests (RAF), and several
              ensembles of these methods in the context of statistical
              arbitrage. Each model is trained on lagged returns of all stocks
              in the S\&P 500, after elimination of survivor bias. From 1992 to
              2015, daily one-day-ahead trading signals are generated based on
              the probability forecast of a stock to outperform the general
              market. The highest k probabilities are converted into long and
              the lowest k probabilities into short positions, thus censoring
              the less certain middle part of the ranking. Empirical findings
              are promising. A simple, equal-weighted ensemble (ENS1)
              consisting of one deep neural network, one gradient-boosted tree,
              and one random forest produces out-of-sample returns exceeding
              0.45 percent per day for k=10, prior to transaction costs.
              Irrespective of the fact that profits are declining in recent
              years, our findings pose a severe challenge to the semi-strong
              form of market efficiency.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  259,
  number   =  2,
  pages    = "689--702",
  month    =  jun,
  year     =  2017,
  keywords = "Finance; Deep learning; Gradient-boosting; Random forests;
              Ensemble learning"
}

@ARTICLE{Perezts2015-ql,
  title    = "Compliance or Comfort Zone? The Work of Embedded Ethics in
              Performing Regulation",
  author   = "P{\'e}rezts, Mar and Picard, S{\'e}bastien",
  abstract = "The effective implementation of regulation in organizations is an
              ongoing concern for both research and practice, in order to avoid
              deviant behavior and its consequences. However, the way
              compliance with regulations is actually enacted or ``performed''
              within organizations instead of merely executed, remains largely
              under-characterized. Evidence from an ethnographic study in the
              compliance unit of a French investment bank allows us to develop
              a detailed practice approach to how regulation is actually
              implemented in firms. We characterize the work accomplished by
              compliance analysts who are in fact, ``curving'' the script of
              regulation within what we conceptualize as a ``comfort zone''.
              Beyond agency, ethics appears as a key element in linking the
              ``letter of the law'', which serves as a referential anchor to
              guide action, with the complex nature of specific situations. We
              analyze the way individuals and compliance teams cope with,
              interpret, struggle and in fine, perform regulation within this
              comfort zone. A particular interest is thus given to the work of
              embedded ethics in this process, as an enabler to partly recouple
              compliance with the regulated activity. We find that blind
              execution is not only impossible, but also devoid of meaning both
              from regulatory, risk management, and business perspectives in
              organizations. We highlight and characterize a hermeneutic
              dimension to this work, essential to effectively perform
              regulation in complex environments, and we suggest some
              directions for further research.",
  journal  = "J. Bus. Ethics",
  volume   =  131,
  number   =  4,
  pages    = "833--852",
  month    =  nov,
  year     =  2015
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Edwards2005-oo,
  title    = "Compliance: A review",
  author   = "Edwards, Jonathan and Wolfe, Simon",
  abstract = "Compliance is key to the operation and reputation of the
              financial services sector and is now completely embedded in the
              way financial services organisations carry on investment
              business. It is also fundamental to the Financial Services
              Authority (FSA) in seeking to achieve its regulatory objectives
              as set out in SS. 3‚Äê6 of the Financial Services and Markets Act
              2000. A great deal has been written on the topic of compliance
              and the core objective of this paper is to review and comment on
              the current approach to compliance which has evolved since the
              introduction of the Financial Services Act 1986. It notes the
              change of emphasis by the FSA from individual compliance
              competence to organisational compliance competence. It focuses on
              conduct of business regulation and highlights the importance of
              training and competence to compliance and explains how the
              regulatory approach has been changing from a rules‚Äêbased approach
              to a more flexible ethical one.",
  journal  = "Journal of Financial Regulation and Compliance",
  volume   =  13,
  number   =  1,
  pages    = "48--59",
  year     =  2005
}

@ARTICLE{Lucey2018-gf,
  title    = "Future directions in international financial integration research
              - A crowdsourced perspective",
  author   = "Lucey, Brian M and Vigne, Samuel A and Ballester, Laura and
              Barbopoulos, Leonidas and Brzeszczynski, Janusz and Carchano,
              Oscar and Dimic, Nebojsa and Fernandez, Viviana and Gogolin,
              Fabian and Gonz{\'a}lez-Urteaga, Ana and Goodell, John W and
              Helbing, Pia and Ichev, Riste and Kearney, Fearghal and Laing,
              Elaine and Larkin, Charles J and Lindblad, Annika and Lon{\v
              c}arski, Igor and Ly, Kim Cuong and Marin{\v c}, Matej and McGee,
              Richard J and McGroarty, Frank and Neville, Conor and
              O'Hagan-Luff, Martha and Piljak, Vanja and Sevic, Aleksandar and
              Sheng, Xin and Stafylas, Dimitrios and Urquhart, Andrew and
              Versteeg, Roald and Vu, Anh N and Wolfe, Simon and Yarovaya,
              Larisa and Zaghini, Andrea",
  abstract = "This paper is the result of a crowdsourced effort to surface
              perspectives on the present and future direction of international
              finance. The authors are researchers in financial economics who
              attended the INFINITI 2017 conference in the University of
              Valencia in June 2017 and who participated in the crowdsourcing
              via the Overleaf platform. This paper highlights the actual state
              of scientific knowledge in a multitude of fields in finance and
              proposes different directions for future research.",
  journal  = "International Review of Financial Analysis",
  volume   =  55,
  pages    = "35--49",
  month    =  jan,
  year     =  2018,
  keywords = "Financial economics; Crowdsourcing; Literature review; Financial
              research"
}

@ARTICLE{Greenwood2017-xp,
  title     = "Strengthening and Streamlining Bank Capital Regulation",
  author    = "Greenwood, Robin and Stein, Jeremy C and Hanson, Samuel G and
               Sunderam, Adi",
  abstract  = "ARRAY(0x562642e63a60)",
  journal   = "Brookings Pap. Econ. Act.",
  publisher = "Brookings Institution Press",
  volume    =  2017,
  number    =  2,
  pages     = "479--565",
  year      =  2017
}

@BOOK{Claessens2017-ku,
  title     = "Regulation and structural change in financial systems",
  author    = "Claessens, Stijn and {Others}",
  publisher = "Centre for Economic Policy Research",
  year      =  2017
}

@MISC{Kiminski2016-sr,
  title        = "A best-practice model for bank compliance",
  booktitle    = "{McKinsey} \& Company",
  author       = "Kiminski, Piotr and Kate, Robu",
  abstract     = "Tighter compliance regulations have challenged financial
                  institutions in a variety of ways. Yet those who adapt best
                  may enjoy a distinct competitive advantage.",
  month        =  jan,
  year         =  2016,
  howpublished = "\url{https://www.mckinsey.com/business-functions/risk/our-insights/a-best-practice-model-for-bank-compliance}",
  note         = "Accessed: 2018-8-22"
}

@ARTICLE{Westphal1994-bw,
  title     = "Substance and Symbolism in {CEOs'} {Long-Term} Incentive Plans",
  author    = "Westphal, James D and Zajac, Edward J",
  abstract  = "This study theoretically and empirically addresses the possible
               separation of substance and symbolism in CEO compensation
               contracts by examining political and institutional determinants
               of long-term incentive plan (LTIP) adoption and use among 570 of
               the largest U.S. corporations over two decades. We find that a
               substantial number of firms are likely to adopt but not actually
               use-or only limitedly use-LTIPs, suggesting a potential
               separation of substance and symbol in CEO compensation
               contracts. Analyses suggest that this decoupling of LTIP
               adoption and use is particularly prevalent in firms with
               powerful CEOs and firms with poor prior performance. Further
               analyses show that whereas early adopters are more likely to
               pursue alignment between CEO and shareholder interests
               substantively, later adopters may pursue legitimacy by
               symbolically controlling agency costs. More generally, the study
               highlights how decoupling in organizations can be understood in
               terms of both micro-political and macro-institutional forces.",
  journal   = "Adm. Sci. Q.",
  publisher = "[Sage Publications, Inc., Johnson Graduate School of Management,
               Cornell University]",
  volume    =  39,
  number    =  3,
  pages     = "367",
  month     =  sep,
  year      =  1994
}

@ARTICLE{Westphal1995-sn,
  title     = "Accounting for the Explanations of {CEO} Compensation: Substance
               and Symbolism",
  author    = "Westphal, James D and Zajac, Edward J",
  abstract  = "While current debates about CEO compensation have generally been
               dominated by economic and political perspectives on CEO/board
               relations, we argue in this paper that CEO compensation may be
               driven by symbolic as well as substantive considerations. We
               develop an interdisciplinary theoretical framework to (1)
               explain why alternative explanations rooted in agency and human
               resource logics may be used to reduce ambiguity surrounding the
               adoption of new incentive plans for CEOs and (2) identify the
               possible structural (e.g., institutional, demographic, and
               economic), and interest-based (e.g., political) factors
               influencing the use of such explanations. We generate and test
               hypotheses predicting the alternative explanations for new
               long-term incentive plans using data taken from proxy statements
               over a 15-year period. The findings support the notion that
               explanations for CEO compensation reflect both substance and
               symbolism.",
  journal   = "Adm. Sci. Q.",
  publisher = "[Sage Publications, Inc., Johnson Graduate School of Management,
               Cornell University]",
  volume    =  40,
  number    =  2,
  pages     = "283--308",
  year      =  1995
}

@ARTICLE{Westphal2001-yl,
  title     = "Decoupling Policy from Practice: The Case of Stock Repurchase
               Programs",
  author    = "Westphal, James D and Zajac, Edward J",
  abstract  = "This study examines firms' decoupling of informal practices from
               formally adopted policies through analysis of the implementation
               of stock repurchase programs by large U.S. corporations in the
               late 1980s and early 1990s, when firms were experiencing
               external pressures to adopt policies that demonstrate corporate
               control over managerial behavior. We develop theory to explain
               variation in the responses of firms to such pressures, i.e., why
               some firms acquiesce by actually implementing stock repurchase
               programs, while others decouple formally adopted repurchase
               programs from actual corporate investments, so that the plans
               remain more symbolic than substantive. Results of a longitudinal
               study of stock repurchase programs over a six-year time period
               show that decoupling is more likely to occur when top executives
               have power over boards to avoid institutional pressures for
               change and when social structural or experiential factors
               enhance awareness among powerful actors of the potential for
               organizational decoupling. The study has implications for future
               research on decoupling, organizational learning, and corporate
               governance.",
  journal   = "Adm. Sci. Q.",
  publisher = "SAGE Publications Inc",
  volume    =  46,
  number    =  2,
  pages     = "202--228",
  month     =  jun,
  year      =  2001
}

@ARTICLE{Agoraki2011-wt,
  title    = "Regulations, competition and bank risk-taking in transition
              countries",
  author   = "Agoraki, Maria-Eleni K and Delis, Manthos D and Pasiouras, Fotios",
  abstract = "This study investigates whether regulations have an independent
              effect on bank risk-taking or whether their effect is channeled
              through the market power possessed by banks. Given a
              well-established set of theoretical priors, the regulations
              considered are capital requirements, restrictions on bank
              activities and official supervisory power. We use data from the
              Central and Eastern European banking sectors over the period
              1998--2005. The empirical results suggest that banks with market
              power tend to take on lower credit risk and have a lower
              probability of default. Capital requirements reduce risk in
              general, but for banks with market power this effect
              significantly weakens or can even be reversed. Higher activity
              restrictions in combination with more market power reduce both
              credit risk and the risk of default, while official supervisory
              power has only a direct impact on bank risk.",
  journal  = "Journal of Financial Stability",
  volume   =  7,
  number   =  1,
  pages    = "38--48",
  month    =  jan,
  year     =  2011,
  keywords = "Banking sector reform; Regulations; Competition; Risk-taking; CEE
              banks"
}

@ARTICLE{Pearl2018-wf,
  title    = "Mind over data",
  author   = "Pearl, Judea and Mackenzie, Dana",
  abstract = "In this excerpt from The Book of Why, Judea Pearl and Dana
              Mackenzie explain how the founders of modern statistics
              ?squandered? the chance to establish the science of causal
              inference",
  journal  = "Significance",
  volume   =  15,
  number   =  4,
  pages    = "6--7",
  month    =  aug,
  year     =  2018
}

@ARTICLE{Dimitras1996-ti,
  title    = "A survey of business failures with an emphasis on prediction
              methods and industrial applications",
  author   = "Dimitras, A I and Zanakis, S H and Zopounidis, C",
  abstract = "The considerable interest in the prediction of business failures
              is reflected in the large number of studies presented in the
              literature. Various methods have been used to construct
              prediction models. This paper provides a review of the literature
              and a framework for the presentation of this information.
              Articles can be classified according to the country, industrial
              sector and period of data, as well as the financial ratios and
              models or methods employed. Relationships and research trends in
              the prediction of business failure are discussed.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  90,
  number   =  3,
  pages    = "487--513",
  month    =  may,
  year     =  1996,
  keywords = "Business failures; Statistical analysis"
}

@ARTICLE{Embrechts2014-lf,
  title     = "An Academic Response to Basel 3.5",
  author    = "Embrechts, Paul and Puccetti, Giovanni and R{\"u}schendorf,
               Ludger and Wang, Ruodu and Beleraj, Antonela",
  abstract  = "Recent crises in the financial industry have shown weaknesses in
               the modeling of Risk-Weighted Assets (RWAs). Relatively minor
               model changes may lead to substantial changes in the RWA
               numbers. Similar problems are encountered in the Value-at-Risk
               (VaR)-aggregation of risks. In this article, we highlight some
               of the underlying issues, both methodologically, as well as
               through examples. In particular, we frame this discussion in the
               context of two recent regulatory documents we refer to as Basel
               3.5.",
  journal   = "Risks",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  2,
  number    =  1,
  pages     = "25--48",
  month     =  feb,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Delis2017-jw,
  title     = "Formal Enforcement Actions and Bank Behavior",
  author    = "Delis, Manthos D and Staikouras, Panagiotis K and Tsoumas, Chris",
  abstract  = "Employing a unique data set for the period 2000?2010, this paper
               examines the impact of formal enforcement actions targeting the
               core of the banks? financial safety and soundness in terms of
               bank capital, risk, and performance. We find that, on average,
               these actions reduce both the risk-weighted assets and the
               nonperforming loans ratios of punished banks, but there is no
               increase in the level of regulatory capital. These effects are
               less powerful during the postcrisis period, suggesting that
               banks? scope to improve their safety and soundness condition in
               crisis periods is much more limited. We also find, albeit with
               some limitations, that the timing of formal enforcement actions
               is important: the more the actions are deferred relative to the
               continuous deterioration of the banks? financial condition, the
               more limited their impact on the risk-based capital ratio, while
               actions taken earlier help banks to improve their financial
               soundness. Data, as supplemental material, are available at
               http://dx.doi.org/10.1287/mnsc.2015.2343. This paper was
               accepted by Wei Jiang, finance.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  63,
  number    =  4,
  pages     = "959--987",
  month     =  apr,
  year      =  2017
}

@ARTICLE{Berger2016-fb,
  title    = "Bank liquidity creation following regulatory interventions and
              capital support",
  author   = "Berger, Allen N and Bouwman, Christa H S and Kick, Thomas and
              Schaeck, Klaus",
  abstract = "We study the effects of regulatory interventions and capital
              support (bailouts) on banks' liquidity creation. We rely on
              instrumental variables to deal with possible endogeneity
              concerns. Our key findings, which are based on a unique
              supervisory German dataset, are that regulatory interventions
              robustly trigger decreases in liquidity creation, while capital
              support does not affect liquidity creation. Additional results
              include the effects of these actions on different components of
              liquidity creation, lending, and risk taking. Our findings
              provide new and important insights into the debates about the
              design of regulatory interventions and bailouts.",
  journal  = "Journal of Financial Intermediation",
  volume   =  26,
  pages    = "115--141",
  month    =  apr,
  year     =  2016,
  keywords = "Liquidity creation; Bank distress; Regulatory interventions;
              Capital support; Bank bailouts"
}

@TECHREPORT{Financial_Stability_Board2017-kp,
  title  = "Implementation and Effects of the {G20} Financial Regulatory
            Reforms: Third Annual Report",
  author = "{Financial Stability Board}",
  year   =  2017
}

@ARTICLE{Cohn2014-tw,
  title    = "Business culture and dishonesty in the banking industry",
  author   = "Cohn, Alain and Fehr, Ernst and Mar{\'e}chal, Michel Andr{\'e}",
  abstract = "Trust in others' honesty is a key component of the long-term
              performance of firms, industries, and even whole countries.
              However, in recent years, numerous scandals involving fraud have
              undermined confidence in the financial industry. Contemporary
              commentators have attributed these scandals to the financial
              sector's business culture, but no scientific evidence supports
              this claim. Here we show that employees of a large, international
              bank behave, on average, honestly in a control condition.
              However, when their professional identity as bank employees is
              rendered salient, a significant proportion of them become
              dishonest. This effect is specific to bank employees because
              control experiments with employees from other industries and with
              students show that they do not become more dishonest when their
              professional identity or bank-related items are rendered salient.
              Our results thus suggest that the prevailing business culture in
              the banking industry weakens and undermines the honesty norm,
              implying that measures to re-establish an honest culture are very
              important.",
  journal  = "Nature",
  volume   =  516,
  number   =  7529,
  pages    = "86--89",
  month    =  dec,
  year     =  2014,
  language = "en"
}

@ARTICLE{Maurer2012-sa,
  title   = "Regulation as retrospective ethnography: Mobile money and the arts
             of cash",
  author  = "Maurer, Bill",
  journal = "Banking and Finance Law Review",
  volume  =  27,
  number  =  2,
  pages   = "299",
  year    =  2012
}

@ARTICLE{Watson2011-la,
  title     = "Ethnography, reality, and truth: the vital need for studies of
               `how things work'in organizations and management",
  author    = "Watson, Tony J",
  journal   = "Journal of Management studies",
  publisher = "Wiley Online Library",
  volume    =  48,
  number    =  1,
  pages     = "202--217",
  year      =  2011
}

@ARTICLE{Vom_Lehn2018-it,
  title    = "{Phenomenology-Based} Ethnography for Management Studies and
              Organizational Analysis",
  author   = "vom Lehn, Dirk",
  abstract = "Abstract This paper introduces phenomenology-based ethnography as
              a novel ethnographic approach for research in management studies
              and organizational analysis and describes three methods that have
              been developed from this approach: life-world analytical
              ethnography, focused ethnography and go-along ethnography.
              Phenomenology-based ethnography has emerged from developments in
              sociology that draw on ?social phenomenology? developed by Alfred
              Sch{\"u}tz. These developments involve the use of
              phenomenology-based ethnographic methods that shift the focus of
              research onto participants? subjective experiences of the field
              further than has been required by other ethnographic approaches.
              This paper uses a set of dimensions that allow a comparison of
              these phenomenology-based methods? aims, techniques of data
              collection and analysis, and required effort. These three methods
              are then compared with current ethnographic methods used in
              organizational research and management studies. The paper
              concludes with a discussion that explores and addresses the
              critique of how phenomenology-based ethnography conceives the
              relationship between the researcher and the research subject.",
  journal  = "Brit J Manage",
  volume   =  5,
  pages    = "205",
  month    =  may,
  year     =  2018
}

@INCOLLECTION{Edwards2009-wb,
  title     = "{CHAPTER} 13 - Ethnography and sport management research",
  booktitle = "Qualitative Research in Sport Management",
  author    = "Edwards, Allan and Skinner, James",
  editor    = "Edwards, Allan and Skinner, James",
  publisher = "Butterworth-Heinemann",
  pages     = "257--280",
  month     =  jan,
  year      =  2009,
  address   = "Oxford"
}

@ARTICLE{Barth2008-ut,
  title    = "Bank Regulations are Changing: For Better or Worse?",
  author   = "Barth, James R and Caprio, Gerard and Levine, Ross",
  abstract = "This paper presents new and official survey information on bank
              regulations in 142 countries and makes comparisons with two
              earlier surveys. The data do not suggest that countries have
              primarily reformed their bank regulations for the better over the
              last decade. Following Basel guidelines many countries
              strengthened capital regulations and official supervisory
              agencies, but existing evidence suggests that these reforms will
              not improve bank stability or efficiency. While some countries
              have empowered private monitoring of banks, consistent with the
              third pillar of Basel II, there are many exceptions and reversals
              along this dimension.",
  journal  = "Comp. Econ. Stud.",
  volume   =  50,
  number   =  4,
  pages    = "537--563",
  month    =  dec,
  year     =  2008
}

@ARTICLE{Barth2001-tj,
  title   = "Bank regulation and supervision: a new database",
  author  = "Barth, James and Caprio, Gerard and Levine, Ross",
  journal = "Brookings-Wharton Papers on Financial Services",
  year    =  2001
}

@ARTICLE{Klomp2012-kg,
  title    = "Banking risk and regulation: Does one size fit all?",
  author   = "Klomp, Jeroen and Haan, Jakob de",
  abstract = "Using data for more than 200 banks from 21 OECD countries for the
              period 2002--2008, we examine the impact of bank regulation and
              supervision on banking risk using quantile regressions. In
              contrast to most previous research, we find that banking
              regulation and supervision has an effect on the risks of
              high-risk banks. However, most measures for bank regulation and
              supervision do not have a significant effect on low-risk banks.
              As banking risk and bank regulation and supervision are
              multi-faceted concepts, our measures for both concepts are
              constructed using factor analysis.",
  journal  = "Journal of Banking \& Finance",
  volume   =  36,
  number   =  12,
  pages    = "3197--3212",
  month    =  dec,
  year     =  2012,
  keywords = "Financial soundness; Bank regulation and supervision; Banking
              risk; Quantile regression"
}

@ARTICLE{Zhao2009-xw,
  title    = "Effects of feature construction on classification performance: An
              empirical study in bank failure prediction",
  author   = "Zhao, Huimin and Sinha, Atish P and Ge, Wei",
  abstract = "While extensive research in data mining has been devoted to
              developing better classification algorithms, relatively little
              research has been conducted to examine the effects of feature
              construction, guided by domain knowledge, on classification
              performance. However, in many application domains, domain
              knowledge can be used to construct higher-level features to
              potentially improve performance. For example, past research and
              regulatory practice in early warning of bank failures has
              resulted in various explanatory variables, in the form of
              financial ratios, that are constructed based on bank accounting
              variables and are believed to be more effective than the original
              variables in identifying potential problem banks. In this study,
              we empirically compare the performance of two sets of classifiers
              for bank failure prediction, one built using raw accounting
              variables and the other built using constructed financial ratios.
              Four popular data mining methods are used to learn the
              classifiers: logistic regression, decision tree, neural network,
              and k-nearest neighbor. We evaluate the classifiers on the basis
              of expected misclassification cost under a wide range of possible
              settings. The results of the study strongly indicate that feature
              construction, guided by domain knowledge, significantly improves
              classifier performance and that the degree of improvement varies
              significantly across the methods.",
  journal  = "Expert Syst. Appl.",
  volume   =  36,
  number   = "2, Part 2",
  pages    = "2633--2644",
  month    =  mar,
  year     =  2009,
  keywords = "Data mining; Classification; Feature construction; Bank failure
              prediction; Financial ratios"
}

@ARTICLE{Farhi2012-zi,
  title    = "Collective Moral Hazard, Maturity Mismatch, and Systemic Bailouts",
  author   = "Farhi, Emmanuel and Tirole, Jean",
  abstract = "The article shows that time-consistent, imperfectly targeted sup-
              port to distressed institutions makes private leverage choices
              strategic complements. When everyone engages in maturity
              mismatch, authorities have little choice but intervening,
              creating both current and deferred (sowing the seeds of the next
              crisis) social costs. In turn, it is profitable to adopt a risky
              balance sheet. These insights have important consequences, from
              banks choosing to correlate their risk exposures to the need for
              macro-prudential supervision.",
  journal  = "Am. Econ. Rev.",
  volume   =  102,
  number   =  1,
  pages    = "60--93",
  month    =  feb,
  year     =  2012
}

@ARTICLE{Segura2011-on,
  title    = "Liquidity Shocks, {Roll-Over} Risk and Debt Maturity",
  author   = "Segura, Anatoli and Suarez, Javier",
  abstract = "We develop an infinite horizon model of an economy in which banks
              finance long term assets by placing non-tradable debt among
              savers. Banks choose the overall principal, interest rate, and
              maturity of their debt taking into account two opposite forces:
              (i) investors' preference for short maturities (which stems from
              their exposure to preference shocks) and (ii) banks' exposure to
              systemic liquidity crises (during which debt refinancing becomes
              specially expensive). Importantly, the terms of access to
              refinancing during crises depend endogenously on banks' aggregate
              refinancing needs. Due to pecuniary externalities, the
              unregulated equilibrium exhibits inefficiently short debt
              maturities. We analyze the possibility of restoring efficiency or
              improving welfare by means of limits to debt maturity, Pigovian
              taxes, and liquidity insurance schemes.",
  journal  = "CEPR Discussion Paper No. DP8324",
  month    =  apr,
  year     =  2011,
  keywords = "liquidity premium, liquidity risk regulation, maturity structure,
              pecuniary externalities, systemic crises"
}

@ARTICLE{Ahnert2016-dn,
  title    = "Rollover Risk, Liquidity and Macroprudential Regulation",
  author   = "Ahnert, Toni",
  abstract = "I study rollover risk in wholesale funding markets when
              intermediaries hold liquidity ex ante and fire sales may occur ex
              post. Multiple equilibria exist in a global rollover game:
              intermediate liquidity holdings support equilibria with both
              positive and zero expected liquidation. A simple uniqueness
              refinement pins down the private liquidity choice, which balances
              the forgone expected return on investment with reduced fragility
              and costly liquidation. Due to fire sales, liquidity holdings are
              strategic substitutes. Intermediaries free ride on the holdings
              of other intermediaries, causing excessive liquidation. To
              internalize the systemic nature of liquidity, a macroprudential
              authority imposes liquidity buffers.",
  journal  = "J. Money Credit Bank.",
  volume   =  48,
  number   =  8,
  pages    = "1753--1785",
  month    =  dec,
  year     =  2016
}

@ARTICLE{Liu2016-nv,
  title     = "Interbank Market Freezes and Creditor Runs",
  author    = "Liu, Xuewen",
  abstract  = "We model the interplay between trade in the interbank market and
               creditor runs on financial institutions. We show that the
               feedback between them can amplify a small shock into ``interbank
               market freezing'' with ``liquidity evaporating.'' Credit
               crunches of the interbank market drive up the interbank rate.
               For an individual institution, a higher interbank rate ---
               meaning a higher funding cost --- results in more severe
               coordination problems among creditors in debt rollover
               decisions. Creditors thus behave more conservatively and run
               more often. Facing an increased chance of creditor runs,
               institutions demand more and supply less liquidity, tightening
               the interbank market.Received September 29, 2014; accepted March
               7, 2016 by Editor Itay Goldstein.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  29,
  number    =  7,
  pages     = "1860--1910",
  month     =  jul,
  year      =  2016
}

@ARTICLE{Liu2015-zc,
  title     = "{Short-Selling} Attacks and Creditor Runs",
  author    = "Liu, Xuewen",
  abstract  = "This paper investigates the mechanism through which short
               selling of a bank's stocks can trigger the failure of the bank.
               In the model, creditors, who learn information from stock
               prices, will grow increasingly unsure about the bank's true
               fundamentals in facing noisier stock prices; thus a run on the
               bank is more likely because of creditors' concave payoff.
               Understanding this, speculators conduct short selling beforehand
               to amplify (il)liquidity and add noise to stock prices,
               triggering a bank run, and subsequently profit from the bank's
               failure. We show that short-selling attacks on a bank involve
               two runs: the aggressive run among speculators and the
               conservative run among creditors. These two runs interact and
               reinforce each other, with compound feedback loops that
               drastically increase the probability of the collapse of the
               bank. We discuss policy implications of the model. This paper
               was accepted by Wei Jiang, finance.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  61,
  number    =  4,
  pages     = "814--830",
  month     =  apr,
  year      =  2015
}

@ARTICLE{Di_Maggio2018-qu,
  title     = "Financial Disclosure and Market Transparency with Costly
               Information Processing",
  author    = "Di Maggio, Marco and Pagano, Marco",
  abstract  = "We study a model where some investors (``hedgers'') are bad at
               information processing, while others (``speculators'') have
               superior information-processing ability and trade purely to
               exploit it. The disclosure of financial information induces a
               trade externality: if speculators refrain from trading, hedgers
               do the same, depressing the asset price. Market transparency
               reinforces this mechanism, by making speculators' trades more
               visible to hedgers. Hence, issuers will oppose both the
               disclosure of fundamentals and trading transparency. Issuers may
               either under- or over-provide information compared to the
               socially efficient level if speculators have more bargaining
               power than hedgers, while they never under-provide it otherwise.
               When hedgers have low financial literacy, forbidding their
               access to the market may be socially efficient.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  22,
  number    =  1,
  pages     = "117--153",
  month     =  feb,
  year      =  2018
}

@ARTICLE{Kahn2010-lv,
  title    = "Liquidity, Payment and Endogenous Financial Fragility",
  author   = "Kahn, Charles and Santos, Jo{\~a}o",
  abstract = "We study the fragility of the banking system in relation to its
              role in liquidity creation. In our framework, fragility stems
              from the interconnections banks es",
  journal  = "EFA 2005 Moscow Meetings Paper",
  month    =  may,
  year     =  2010,
  keywords = "Systemic risk, liquidity, payment services, bank regulation"
}

@ARTICLE{Dueker2005-vr,
  title     = "Dynamic Forecasts of Qualitative Variables",
  author    = "Dueker, Michael",
  abstract  = "This article presents a new Qual VAR model for incorporating
               information from qualitative and/or discrete variables in vector
               autoregressions. With a Qual VAR, it is possible to create
               dynamic forecasts of the qualitative variable using standard VAR
               projections. Previous forecasting methods for qualitative
               variables, in contrast, produce only static forecasts. I apply
               the Qual VAR to forecasting the 2001 business recession out of
               sample and to analyzing the Romer and Romer narrative measure of
               monetary policy contractions as an endogenous variable in a VAR.
               Out of sample, the model predicts the timing of the 2001
               recession quite well relative to the recession probabilities put
               forth at the time by professional forecasters. Qual VARs?which
               include information about the qualitative variable?can also
               enhance the quality of density forecasts of the other variables
               in the system.",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  23,
  number    =  1,
  pages     = "96--104",
  month     =  jan,
  year      =  2005
}

@ARTICLE{Behn2016-wm,
  title    = "The Limits of Model-based Regulation",
  author   = "Behn, Markus and Haselmann, Rainer F H and Vig, Vikrant",
  abstract = "In this paper, we investigate how the introduction of
              sophisticated, model-based capital regulation affected the
              measurement of credit risk by financial institu",
  month    =  jul,
  year     =  2016,
  keywords = "capital regulation, internal ratings, complexity of regulation,
              Basel regulation"
}

@ARTICLE{Gray2014-us,
  title    = "Governing inside the organization: interpreting regulation and
              compliance",
  author   = "Gray, Garry C and Silbey, Susan S",
  abstract = "Looking inside organizations at the different positions,
              expertise, and autonomy of the actors, the authors use multisite
              ethnographic data on safety practices to develop a typology of
              how the regulator, as the focal actor in the regulatory process,
              is interpreted within organizations. The findings show that
              organizational actors express constructions of the regulator as
              an ally, threat, and obstacle that vary with organizational
              expertise, authority, and continuity of relationship between the
              organizational member and the regulator. The article makes three
              contributions to the current understandings of organizational
              governance and regulatory compliance, thereby extending both
              institutional and ecological accounts of organizations' behavior
              with respect to their environments. First, the authors document
              not only variation across organizations but variable compliance
              within an organization. Second, the variations described do not
              derive from alternative institutional logics, but from variations
              in positions, autonomy, and expertise within each organization.
              From their grounded theory, the authors hypothesize that these
              constructions carry differential normative interpretations of
              regulation and probabilities for compliance, and thus the third
              contribution, the typology, when correlated with organizational
              hierarchy provides the link between microlevel action and
              discourse and organizational performance.",
  journal  = "AJS",
  volume   =  120,
  number   =  1,
  pages    = "96--145",
  month    =  jul,
  year     =  2014,
  language = "en"
}

@ARTICLE{Nguyen2018-fv,
  title    = "Does Corporate Culture Affect Bank {Risk-Taking}? Evidence from
              {Loan-Level} Data",
  author   = "Nguyen, Duc Duy and Nguyen, Linh and Sila, Vathunyoo",
  abstract = "Using comprehensive corporate and retail loan data, we show that
              the corporate culture of banks explains their risk-taking
              behaviour. Banks whose corporate cult",
  journal  = "British Journal of Management",
  year     =  2018,
  keywords = "Corporate culture; Bank risk-taking; Bank loans; Financial
              crisis; Financial stability"
}

@ARTICLE{Budzinski2015-en,
  title     = "Are Restrictions of Competition by Sports Associations
               Horizontal or Vertical in Nature?",
  author    = "Budzinski, Oliver and Szymanski, Stefan",
  abstract  = "In this article, we discuss, from an economic perspective, two
               alternative views of restrictions of competition by sports
               associations. The horizontal approach views such restrictions as
               an agreement among the participants of a sports league with the
               sports association merely representing an organization executing
               the horizontal cooperation. In contrast, the vertical approach
               views the sports association as being a dominant upstream firm
               enjoying a monopoly position on the market stage for competition
               organizing services, an important input for the actual
               product---the sports game. Taking the recent Financial Fair Play
               (FFP) initiative by UEFA (the Union of European Football
               Associations) as an example, we demonstrate that the different
               views lead to different assessments of restrictive effects and,
               thus, matter for competition policy decisions. The economic
               story of the potential restrictive effect of FFP on players' and
               player agents' income may fit more plausibly to the horizontal
               approach, whereas the potentially anticompetitive foreclosure
               and deterrence effects of FFP may be economically more soundly
               reasoned by taking the vertical approach.",
  journal   = "Jnl of Competition Law \& Economics",
  publisher = "Oxford University Press",
  volume    =  11,
  number    =  2,
  pages     = "409--429",
  month     =  jun,
  year      =  2015
}

@ARTICLE{Mossman1998-xv,
  title    = "An empirical comparison of bankruptcy models",
  author   = "Mossman, Charles E and Bell, Geoffrey G and Swartz, L Mick and
              Turtle, Harry",
  abstract = "Abstract Four types of bankruptcy prediction models based on
              financial statement ratios, cash flows, stock returns, and return
              standard deviations are compared. Based on a sample of
              bankruptcies from 1980 to 1991, results indicate that no existing
              model of bankruptcy adequately captures the data. During the last
              fiscal year preceding bankruptcy, none of the individual models
              may be excluded without a loss in explanatory power. If
              considered in isolation, the cash flow model discriminates most
              consistently two to three years before bankruptcy. By comparison,
              the ratio model is the best single model during the year
              immediately preceding bankruptcy. Quasi-jack-knifing procedures
              suggest that none of the models can reliably predict bankruptcy
              more than two years in advance.",
  journal  = "Financial Review",
  volume   =  33,
  number   =  2,
  pages    = "35--54",
  month    =  may,
  year     =  1998
}

@ARTICLE{Diebold2015-pc,
  title     = "Comparing predictive accuracy, twenty years later: A personal
               perspective on the use and abuse of {Diebold--Mariano} tests",
  author    = "Diebold, Francis X",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  33,
  number    =  1,
  pages     = "1--1",
  year      =  2015
}

@ARTICLE{Brodersen2015-wv,
  title     = "Inferring causal impact using Bayesian structural time-series
               models",
  author    = "Brodersen, Kay H and Gallusser, Fabian and Koehler, Jim and
               Remy, Nicolas and Scott, Steven L and {Others}",
  journal   = "Ann. Appl. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  9,
  number    =  1,
  pages     = "247--274",
  year      =  2015
}

@ARTICLE{Cumming2005-oz,
  title    = "An Exploratory Framework for the Empirical Measurement of
              Resilience",
  author   = "Cumming, G S and Barnes, G and Perz, S and Schmink, M and
              Sieving, K E and Southworth, J and Binford, M and Holt, R D and
              Stickler, C and Van Holt, T",
  abstract = "Deliberate progress towards the goal of long-term sustainability
              depends on understanding the dynamics of linked social and
              ecological systems. The concept of social-ecological resilience
              holds promise for interdisciplinary syntheses. Resilience is a
              multifaceted concept that as yet has not been directly
              operationalized, particularly in systems for which our ignorance
              is such that detailed, parameter-rich simulation models are
              difficult to develop. We present an exploratory framework as a
              step towards the operationalization of resilience for empirical
              studies. We equate resilience with the ability of a system to
              maintain its identity, where system identity is defined as a
              property of key components and relationships (networks) and their
              continuity through space and time. Innovation and memory are also
              fundamental to understanding identity and resilience. By parsing
              our systems into the elements that we subjectively consider
              essential to identity, we obtain a small set of specific focal
              variables that reflect changes in identity. By assessing the
              potential for changes in identity under specified drivers and
              perturbations, in combination with a scenario-based approach to
              considering alternative futures, we obtain a surrogate measure of
              the current resilience of our study system as the likelihood of a
              change in system identity under clearly specified conditions,
              assumptions, drivers and perturbations. Although the details of
              individual case studies differ, the concept of identity provides
              a level of generality that can be used to compare measure of
              resilience across cases. Our approach will also yield insights
              into the mechanisms of change and the potential consequences of
              different policy and management decisions, providing a level of
              decision support for each case study area.",
  journal  = "Ecosystems",
  volume   =  8,
  number   =  8,
  pages    = "975--987",
  month    =  dec,
  year     =  2005
}

@ARTICLE{Su2017-id,
  title    = "The role of news-based implied volatility among {US} financial
              markets",
  author   = "Su, Zhi and Fang, Tong and Yin, Libo",
  abstract = "We investigate the role of uncertainty measured by news-based
              implied volatility in anticipating US long-term market
              volatilities from a GARCH-MIDAS model. We find that news-based
              implied volatility performs well in predicting long-term
              aggregate market volatilities. A subsample analysis provides that
              the predictive power of news-based implied volatility is
              decreasing.",
  journal  = "Econ. Lett.",
  volume   =  157,
  pages    = "24--27",
  month    =  aug,
  year     =  2017,
  keywords = "News-based implied volatility; Financial markets; Long-term
              volatility; Predictability"
}

@ARTICLE{Moore2012-fl,
  title     = "English professional football clubs: Can business parameters of
               small and medium-sized enterprises be applied?",
  author    = "Moore, Neil and Levermore, Roger",
  journal   = "Sport, Business and Management: An International Journal",
  publisher = "Emerald Group Publishing Limited",
  volume    =  2,
  number    =  3,
  pages     = "196--209",
  year      =  2012
}

@ARTICLE{Barney1996-bb,
  title     = "The {Resource-Based} Theory of the Firm",
  author    = "Barney, Jay B",
  abstract  = "Interest in the resource-based view of the firm continues to
               grow in the field of business policy and strategy. Recently,
               most of this interest seems to have been focused on
               understanding the empirical implications of this theory and
               especially on how a firm's resources and capabilities can affect
               its performance. Evaluating the empirical implications of the
               resource based view is, of course, a worthwhile endeavor. We
               present an overview of contributions in this issue.",
  journal   = "Organization Science",
  publisher = "INFORMS",
  volume    =  7,
  number    =  5,
  pages     = "469--469",
  month     =  oct,
  year      =  1996
}

@ARTICLE{Demirer2018-wy,
  title    = "Estimating global bank network connectedness",
  author   = "Demirer, Mert and Diebold, Francis X and Liu, Laura and Yilmaz,
              Kamil",
  abstract = "Summary We use LASSO methods to shrink, select, and estimate the
              high-dimensional network linking the publicly traded subset of
              the world's top 150 banks, 2003?2014. We characterize static
              network connectedness using full-sample estimation and dynamic
              network connectedness using rolling-window estimation.
              Statically, we find that global bank equity connectedness has a
              strong geographic component, whereas country sovereign bond
              connectedness does not. Dynamically, we find that equity
              connectedness increases during crises, with clear peaks during
              the Great Financial Crisis and each wave of the subsequent
              European Debt Crisis, and with movements coming mostly from
              changes in cross-country as opposed to within-country bank
              linkages.",
  journal  = "J. Appl. Econ.",
  volume   =  33,
  number   =  1,
  pages    = "1--15",
  series   = "(Barcelona GSE Working Paper 723)",
  month    =  jan,
  year     =  2018
}

@ARTICLE{Schubert2016-qf,
  title     = "The guardians of European football: {UEFA} Financial Fair Play
               and the career of social problems",
  author    = "Schubert, Mathias and K{\"o}necke, Thomas and Pitthan, Hermann",
  abstract  = "AbstractUEFA?s Financial Fair Play (FFP) policy represents a
               severe regulatory intervention in European club football
               competitions. While potential outcomes of the concept have been
               thoroughly assessed, there is little research on the genesis and
               background behind its implementation. The present paper fills
               this gap by analyzing the discourse in the run-up to the passage
               of FFP. We focus on interpreted practice and context and argue
               that the rising indebtedness of clubs and their increasing
               reliance on benefactors were a necessary but not sufficient
               requirement. Further ingredients imperative for the successful
               policy development were claims-making activities by influential
               actors to secure support for their problem perception. Strong
               discourse coalitions were formed around powerful storylines,
               such as the interpretation of making debts as ?cheating? as well
               as the notion of traditional sporting values being undermined by
               financial forces. By detecting such mechanisms of interaction,
               the study helps to better understand the beliefs and ideologies
               underpinning the policy. It furthermore identifies discrepancies
               between public discourse and scholarly debate concerning the
               problem assessment as well as the proposed solutions and their
               effectiveness. Additionally, it proposes an improved heuristic
               for understanding discursive practices which contributes to
               future investigations of social problems in sport.",
  journal   = "European Journal for Sport and Society",
  publisher = "Routledge",
  volume    =  13,
  number    =  4,
  pages     = "296--324",
  month     =  oct,
  year      =  2016
}

@INCOLLECTION{Schubert2018-fg,
  title     = "Financial Doping and Financial Fair Play in European Club
               Football Competitions",
  booktitle = "The Palgrave Handbook on the Economics of Manipulation in Sport",
  author    = "Schubert, Mathias and Hamil, Sean",
  editor    = "Breuer, Markus and Forrest, David",
  abstract  = "This contribution provides an explanation of `financial doping'
               as a management concept in the context of European club football
               as well as on UEFA's Financial Fair Play (FFP) regime, which can
               well be framed as the governing body of European football's
               attempt to address this issue. The chapter is structured as
               follows: First, the historic development of financial regulation
               in European football is briefly outlined. The notion of
               financial doping as well as in what way it undermines the
               integrity of the sporting competition is addressed next.
               Afterwards, the FFP policy is explained, followed by a critical
               review of its discussion in scholarly literature as well as the
               presentation of an outlook on potential future research.",
  publisher = "Springer International Publishing",
  pages     = "135--157",
  year      =  2018,
  address   = "Cham"
}

@ARTICLE{Budzinski2013-tk,
  title    = "Finanzregulierung Und Internationale Wettbewerbsf{\"a}higkeit:
              Der Fall Deutsche Bundesliga (Financial Fairplay Regulations: The
              Case of the German Bundesliga)",
  author   = "Budzinski, Oliver and M{\"u}ller-Kock, Anika",
  abstract = "The paper discusses the intensively discussed problem of
              financial crisis, overburdening debts and insolvencies in
              professional European football. The academic",
  month    =  jan,
  year     =  2013,
  keywords = "European football, sports economics, competition, market failure,
              over-investment, rat race, financial fair play"
}

@ARTICLE{Thanassoulis1996-hb,
  title    = "A comparison of data envelopment analysis and ratio analysis as
              tools for performance assessment",
  author   = "Thanassoulis, E and Boussofiane, A and Dyson, R G",
  abstract = "This paper compares data envelopment analysis (DEA) and ratio
              analysis as alternative tools for assessing the performance of
              organisational units such as bank branches and schools. Such
              units typically use one or more resources to secure one or more
              outputs, the inputs and/or outputs being possibly incommensurate.
              The assessment of District Health Authorities in England on the
              provision of perinatal care is used as a vehicle for comparing
              the two methods. The comparison focuses on how well the two
              methods agree on the performance of a unit relative to that of
              other units, and on the estimates of targets each method provides
              for improving the performance of units. It is found that provided
              the performance indicators capture all variables used in the DEA
              assessment the two methods agree reasonably closely on the
              performance of the units as a whole, though this depends on the
              way the performance indicators are combined into a summary figure
              of performance. The two methods can disagree substantially on the
              relative performance of individual units. Ratio analysis, unlike
              DEA, is not found to be suitable for setting targets so that
              units can become more efficient. This is mainly due to the fact
              that DEA takes simultaneous account of all resources and outputs
              in assessing performance while ratio analysis relates only one
              resource to one output at a time. However, the two methods can
              support each other if used jointly. Ratios do provide useful
              information on the performance of a unit on specific aspects and
              they can support the communication of DEA results to
              non-specialists when the two methods agree on performance.",
  journal  = "Omega",
  volume   =  24,
  number   =  3,
  pages    = "229--244",
  month    =  jun,
  year     =  1996,
  keywords = "DEA; performance measurement; efficiency; health service; ratio
              analysis"
}

@MISC{Budzinski2018-vl,
  title     = "Competition Policy in Sports Markets",
  author    = "Budzinski, O",
  publisher = "Sage Handbook of Sports Economics",
  year      =  2018
}

@BOOK{Bridgewater2016-xu,
  title     = "Football Management",
  author    = "Bridgewater, S",
  abstract  = "Football Managers have to deal with a number of unique
               pressures, yet the challenges of the football manager are
               similar to those of managers in other sectors. This book
               examines the management of football and looks at ways that
               managers and leaders in other industries can use tools and
               techniques from the sporting world within their own sector.",
  publisher = "Springer",
  month     =  apr,
  year      =  2016,
  address   = "New York",
  language  = "en"
}

@ARTICLE{Manela2014-yo,
  title    = "The value of diffusing information",
  author   = "Manela, Asaf",
  abstract = "How does the speed by which information diffuses affect its value
              to a stock market investor? In a structural model solved in
              closed-form, this speed has two opposing effects on the
              empirically dominant term of the value of information.
              Faster-diffusing information means quicker and less noisy
              profits, but, also increases competing informed trading,
              impounding more information into prices and eroding profits.
              Structural empirical analysis of stock market reaction to drug
              approvals using media coverage as a proxy for the transmission
              rate of information finds that the value of information is
              hump-shaped in its future transmission rate. Moreover, the
              estimated amount of noise trading is small.",
  journal  = "J. financ. econ.",
  volume   =  111,
  number   =  1,
  pages    = "181--199",
  month    =  jan,
  year     =  2014,
  keywords = "Value of information; Information diffusion; Percolation; Media
              coverage; Drug approvals"
}

@ARTICLE{Aquilina_undated-qa,
  title  = "Dark Trading: Transparency and Adverse Selection in Aggregate
            Markets",
  author = "Aquilina, Matteo and Diaz-Rainey, Ivan and Ibikunle, Gbenga and
            Sun, Yuxin"
}

@ARTICLE{Marszk2018-ys,
  title    = "Tracing financial innovation diffusion and substitution
              trajectories. Recent evidence on exchange-traded funds in Japan
              and South Korea",
  author   = "Marszk, Adam and Lechman, Ewa",
  abstract = "Since the rapid growth of the popularity of ETFs, the potential
              substitution between innovative financial products,
              exchange-traded funds (ETFs), and traditional investment funds
              (open-end and closed-end funds) is recognized as one of the
              most-discussed issues in the financial industry. This is the
              first study to empirically verify and compare the diffusion and
              substitution of ETFs using monthly data on their assets in two
              selected countries. The main aim of this paper is to provide
              in-depth insights into the development of innovative financial
              products available in two Asian economies: Japan and South Korea.
              The empirical study uses monthly total net assets data for
              2004--2017. Our methodological framework combines models of
              innovation diffusion and technological substitution. The results
              reported in the study show that in both countries the diffusion
              of ETFs has occurred. The rate of diffusion and the phase of
              growth reached differed -- in Japan the ETF market was in the
              early exponential growth stage, whereas in South Korea it was
              closer to achieving the expected maximum saturation. The results
              of the substitution analysis between the largest category of the
              innovative funds -- equity ETFs and equity open-end funds clearly
              demonstrate that the process of ``switching'' from equity
              open-end funds into ETFs may be easily traced in both countries.
              Substitution processes were, however, gradual and reversals of
              the trajectories were noticed.",
  journal  = "Technol. Forecast. Soc. Change",
  volume   =  133,
  pages    = "51--71",
  month    =  aug,
  year     =  2018,
  keywords = "Financial innovation; ETF; Japan; South Korea; Diffusion;
              Substitution"
}

@ARTICLE{Marshall2017-ll,
  title     = "Time series momentum and moving average trading rules",
  author    = "Marshall, Ben R and Nguyen, Nhut H and Visaltanachoti, Nuttawat",
  abstract  = "We compare and contrast time series momentum (TSMOM) and moving
               average (MA) trading rules so as to better understand the
               sources of their profitability. These rules are closely related;
               however, there are important differences. TSMOM signals occur at
               points that coincide with a MA direction change, whereas MA buy
               (sell) signals only require price to move above (below) a MA.
               Our empirical results show MA rules frequently give earlier
               signals leading to meaningful return gains. Both rules perform
               best outside of large stock series which may explain the puzzle
               of their popularity with investors, yet lack of supportive
               evidence in academic studies.",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  volume    =  17,
  number    =  3,
  pages     = "405--421",
  month     =  mar,
  year      =  2017
}

@ARTICLE{Billio2012-sd,
  title     = "Econometric measures of connectedness and systemic risk in the
               finance and insurance sectors",
  author    = "Billio, Monica and Getmansky, Mila and Lo, Andrew W and
               Pelizzon, Loriana",
  abstract  = "We propose several econometric measures of connectedness based
               on principal-components analysis and Granger-causality networks,
               and apply them to the monthly returns of hedge funds, banks,
               broker/dealers, and insurance companies. We find that all four
               sectors have become highly interrelated over the past decade,
               likely increasing the level of systemic risk in the finance and
               insurance industries through a complex and time-varying network
               of relationships. These measures can also identify and quantify
               financial crisis periods, and seem to contain predictive power
               in out-of-sample tests. Our results show an asymmetry in the
               degree of connectedness among the four sectors, with banks
               playing a much more important role in transmitting shocks than
               other financial institutions.",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  104,
  number    =  3,
  pages     = "535--559",
  month     =  jun,
  year      =  2012,
  keywords  = "Systemic risk; Financial institutions; Liquidity; Financial
               crises"
}

@ARTICLE{Harrison2018-uv,
  title     = "Saving Face: How Exit in Response to Negative Press and Star
               Analyst Downgrades Reflects Reputation Maintenance by Directors",
  author    = "Harrison, Joseph S and Boivie, Steven and Sharp, Nathan Y and
               Gentry, Richard J",
  abstract  = "This paper explores the extrinsic and intrinsic motivations
               driving individual-level responses to reputational threats in
               the context of the director labor market. Integrating work on
               reputation with self-determination and identity theories, we
               theorize that negative attention from the media and star equity
               analysts threatens directors? reputations, motivating proactive
               behavior to mitigate both the external and internal consequences
               of reputation damage. Using a sample of directors of S\&P 1500
               firms between 2003 and 2014, we argue and find that negative
               media coverage and downgrades by star equity analysts are
               positively related to director exit, even after controlling for
               firm performance, overall media visibility, and negative events
               such as lawsuits and financial restatements. We also find that
               director status intensifies the effect of negative media
               coverage on exit, serving as the board chair attenuates the
               effect of star analyst downgrades on exit, and director tenure
               intensifies the effects of both negative media coverage and star
               downgrades on exit. In post-hoc testing, we provide further
               evidence of director reputation maintenance by demonstrating the
               counterintuitive finding that negative attention from the media
               and star analysts also increases directors? likelihood of
               joining the boards of other S\&P 1500 firms.",
  journal   = "AMJ",
  publisher = "Academy of Management",
  volume    =  61,
  number    =  3,
  pages     = "1131--1157",
  month     =  jun,
  year      =  2018
}

@ARTICLE{Malkiel2003-lt,
  title     = "The Efficient Market Hypothesis and Its Critics",
  author    = "Malkiel, Burton G",
  abstract  = "The Efficient Market Hypothesis and Its Critics by Burton G.
               Malkiel. Published in volume 17, issue 1, pages 59-82 of Journal
               of Economic Perspectives, Winter 2003, Abstract: Revolutions
               often spawn counterrevolutions and the efficient market
               hypothesis in finance is no exception. The intellec...",
  journal   = "J. Econ. Perspect.",
  publisher = "aeaweb.org",
  volume    =  17,
  number    =  1,
  pages     = "59--82",
  month     =  mar,
  year      =  2003
}

@ARTICLE{noauthor_undated-qg,
  title = "[{PDF]Financial} Theory and Corporate Policy/"
}

@BOOK{Copeland2013-ql,
  title     = "Financial Theory and Corporate Policy: Pearson New International
               Edition",
  author    = "Copeland, Thomas E and Fred Weston, J and Shastri, Kuldeep",
  abstract  = "This classic textbook in the field, now completely revised and
               updated, provides a bridge between theory and practice.
               Appropriate for the second course in Finance for MBA students
               and the first course in Finance for doctoral students, the text
               prepares students for the complex world of modern financial
               scholarship and practice. It presents a unified treatment of
               finance combining theory, empirical evidence and applications.",
  publisher = "Pearson Higher Ed",
  month     =  aug,
  year      =  2013,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gai2010-vz,
  title     = "Contagion in financial networks",
  author    = "Gai, P and Kapadia, S",
  abstract  = "This paper develops an analytical model of contagion in
               financial networks with arbitrary structure. We explore how the
               probability and potential impact of contagion is influenced by
               aggregate and idiosyncratic shocks, changes in network structure
               and asset market liquidity ‚Ä¶",
  journal   = "of the Royal Society of London A ‚Ä¶",
  publisher = "rspa.royalsocietypublishing.org",
  year      =  2010
}

@ARTICLE{Glasserman2015-fi,
  title    = "How likely is contagion in financial networks?",
  author   = "Glasserman, Paul and Young, H Peyton",
  abstract = "Interconnections among financial institutions create potential
              channels for contagion and amplification of shocks to the
              financial system. We estimate the extent to which
              interconnections increase expected losses and defaults under a
              wide range of shock distributions. In contrast to most work on
              financial networks, we assume only minimal information about
              network structure and rely instead on information about the
              individual institutions that are the nodes of the network. The
              key node-level quantities are asset size, leverage, and a
              financial connectivity measure given by the fraction of a
              financial institution's liabilities held by other financial
              institutions. We combine these measures to derive explicit bounds
              on the potential magnitude of network effects on contagion and
              loss amplification. Spillover effects are most significant when
              node sizes are heterogeneous and the originating node is highly
              leveraged and has high financial connectivity. Our results also
              highlight the importance of mechanisms that go beyond simple
              spillover effects to magnify shocks; these include bankruptcy
              costs, and mark-to-market losses resulting from credit quality
              deterioration or a loss of confidence. We illustrate the results
              with data on the European banking system.",
  journal  = "Journal of Banking \& Finance",
  volume   =  50,
  pages    = "383--399",
  month    =  jan,
  year     =  2015,
  keywords = "Systemic risk; Contagion; Financial network"
}

@ARTICLE{Glasserman2016-th,
  title    = "Contagion in Financial Networks",
  author   = "Glasserman, Paul and Young, H Peyton",
  abstract = "Contagion in Financial Networks by Paul Glasserman and H. Peyton
              Young. Published in volume 54, issue 3, pages 779-831 of Journal
              of Economic Literature, September 2016, Abstract: The recent
              financial crisis has prompted much new research on the
              interconnectedness of the modern financial system and...",
  journal  = "J. Econ. Lit.",
  volume   =  54,
  number   =  3,
  pages    = "779--831",
  month    =  sep,
  year     =  2016
}

@ARTICLE{Chiang2007-ms,
  title     = "Dynamic correlation analysis of financial contagion: Evidence
               from Asian markets",
  author    = "Chiang, Thomas C and Jeon, Bang Nam and Li, Huimin",
  abstract  = "We apply a dynamic conditional-correlation model to nine Asian
               daily stock-return data series from 1990 to 2003. The empirical
               evidence confirms a contagion effect. By analyzing the
               correlation-coefficient series, we identify two phases of the
               Asian crisis. The first shows an increase in correlation
               (contagion); the second shows a continued high correlation
               (herding). Statistical analysis of the correlation coefficients
               also finds a shift in variance during the crisis period, casting
               doubt on the benefit of international portfolio diversification.
               Evidence shows that international sovereign credit-rating
               agencies play a significant role in shaping the structure of
               dynamic correlations in the Asian markets.",
  journal   = "J. Int. Money Finance",
  publisher = "Elsevier",
  volume    =  26,
  number    =  7,
  pages     = "1206--1228",
  month     =  nov,
  year      =  2007,
  keywords  = "Financial contagion; Asian crises; Herding; Dynamic conditional
               correlation; Sovereign credit rating"
}

@ARTICLE{Peckham2014-dq,
  title    = "Contagion: epidemiological models and financial crises",
  author   = "Peckham, Robert",
  abstract = "Since the 1990s, economists have drawn on the epidemiology of
              emerging infectious diseases to explain the diffusion of shock
              through an increasingly complex financial system. The successful
              coordination of public health responses to disease threats, and
              in particular the epidemiological modelling underpinning
              infection control, has influenced economists' understanding of
              the risks posed to the stability of the financial system by
              'contagion'. While the exportation of analytic models and frames
              of reference can be fruitful, reinvigorating the destination
              domain, such analogizing can have a distorting effect. There are
              differences between biological and financial systems. Moreover,
              the migration of highly context-specific epidemiological models
              may undermine the basis of the analogy. Finally, there may be
              repercussions for the efficacy of public health in the way that
              its aims are misconstrued in financial analyses.",
  journal  = "J. Public Health",
  volume   =  36,
  number   =  1,
  pages    = "13--17",
  month    =  mar,
  year     =  2014,
  keywords = "analogic models; networks; public health; risk",
  language = "en"
}

@ARTICLE{Kentikelenis2015-xv,
  title    = "How do economic crises affect migrants' risk of infectious
              disease? A systematic-narrative review",
  author   = "Kentikelenis, Alexander and Karanikolos, Marina and Williams,
              Gemma and Mladovsky, Philipa and King, Lawrence and Pharris,
              Anastasia and Suk, Jonathan E and Hatzakis, Angelos and McKee,
              Martin and Noori, Teymur and Stuckler, David",
  abstract = "BACKGROUND: It is not well understood how economic crises affect
              infectious disease incidence and prevalence, particularly among
              vulnerable groups. Using a susceptible-infected-recovered
              framework, we systematically reviewed literature on the impact of
              the economic crises on infectious disease risks in migrants in
              Europe, focusing principally on HIV, TB, hepatitis and other
              STIs. METHODS: We conducted two searches in PubMed/Medline, Web
              of Science, Cochrane Library, Google Scholar, websites of key
              organizations and grey literature to identify how economic
              changes affect migrant populations and infectious disease. We
              perform a narrative synthesis in order to map critical pathways
              and identify hypotheses for subsequent research. RESULTS: The
              systematic review on links between economic crises and migrant
              health identified 653 studies through database searching; only
              seven met the inclusion criteria. Fourteen items were identified
              through further searches. The systematic review on links between
              economic crises and infectious disease identified 480 studies
              through database searching; 19 met the inclusion criteria. Eight
              items were identified through further searches. The reviews show
              that migrant populations in Europe appear disproportionately at
              risk of specific infectious diseases, and that economic crises
              and subsequent responses have tended to exacerbate such risks.
              Recessions lead to unemployment, impoverishment and other risk
              factors that can be linked to the transmissibility of disease
              among migrants. Austerity measures that lead to cuts in
              prevention and treatment programmes further exacerbate infectious
              disease risks among migrants. Non-governmental health service
              providers occasionally stepped in to cater to specific
              populations that include migrants. CONCLUSIONS: There is evidence
              that migrants are especially vulnerable to infectious disease
              during economic crises. Ring-fenced funding of prevention
              programs, including screening and treatment, is important for
              addressing this vulnerability.",
  journal  = "Eur. J. Public Health",
  volume   =  25,
  number   =  6,
  pages    = "937--944",
  month    =  dec,
  year     =  2015,
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Battiston2016-lp,
  title     = "Complexity theory and financial regulation",
  author    = "Battiston, Stefano and Doyne Farmer, J and Flache, Andreas and
               Garlaschelli, Diego and Haldane, Andrew G and Heesterbeek, Hans
               and Hommes, Cars and Jaeger, Carlo and May, Robert and Scheffer,
               Marten",
  abstract  = "Traditional economic theory could not explain, much less
               predict, the near collapse of the financial system and its
               long-lasting effects on the global economy. Since the 2008
               crisis, there has been increasing interest in using ideas from
               complexity theory to make sense of economic and financial
               markets. Concepts such as tipping points, networks, contagion,
               feedback, and resilience have entered the financial and
               regulatory lexicon, but actual use of complexity models and
               results remains at an early stage. Recent insights and
               techniques offer potential for better monitoring and management
               of highly interconnected economic and financial systems and,
               thus, may help anticipate and manage future crises. TIPPING
               POINTS, WARNING SIGNALS. Financial markets have historically
               exhibited sudden and largely unforeseen collapses, at a systemic
               scale. Such ``phase transitions'' may in some cases have been
               triggered by unpredictable stochastic events. More often,
               however, there have been endogenous underlying processes at
               work. Analyses of complex systems ranging from the climate to
               ecosystems reveal that, before a major transition, there is
               often a gradual and unnoticed loss of resilience. This makes the
               system brittle: A small disruption can trigger a domino effect
               that propagates through the system and propels it into a crisis
               state. Recent research has revealed generic empirical
               quantitative indicators of resilience that may be used across
               complex systems to detect tipping points. Markers include rising
               correlation between nodes in a network and rising temporal
               correlation, variance, and skewedness of fluctuation patterns.
               These indicators were first predicted mathematically and
               subsequently demonstrated experimentally in real complex
               systems, including living systems ([ 1 ][1]). A recent study of
               the Dutch interbank network ([ 2 ][2]) showed that standard
               analysis using a homogeneous network model could only lead to
               late detection of the 2008 crisis, although a more realistic and
               heterogeneous network model could identify an early warning
               signal 3 years before the crisis (see the chart). Ecologists
               have developed tools to quantify the stability, robustness, and
               resilience of food webs and have shown how these depend on the
               topology of the network and the strengths of interactions ([ 3
               ][3]). Epidemiologists have tools to gauge the potential for
               events to propagate in systems of interacting entities, to
               identify superspreaders and core groups relevant to infection
               persistence, and to design strategies to prevent or limit the
               spread of contagion ([ 4 ][4]). Extrapolating results from the
               natural sciences to economics and finance presents challenges.
               For instance, publication of an early warning signal will change
               behavior and affect future dynamics [the Lucas critique ([ 5
               ][5])]. But this does not affect the case where indicators are
               known only to regulators or when the goal is to build better
               network barriers to slow contagion. TOO CENTRAL TO FAIL. Network
               effects matter to financial-economic stability because shock
               amplification may occur via strong cascading effects. For
               example, the Bank of International Settlements recently
               developed a framework drawing on data on the interconnectedness
               between banks to gauge the systemic risk posed to the financial
               network by Global Systemically Important Banks. Recent research
               on contagion in financial networks has shown that network
               topology and positions of banks matter; the global financial
               network may collapse even when individual banks appear safe ([ 6
               ][6]). Capturing these effects is essential for quantifying
               stress on individual banks and for looking at systemic risk for
               the network as a whole. Despite on-going efforts, these effects
               are unlikely to be routinely considered anytime soon.
               Information asymmetry within a network---e.g. where a bank does
               not know about troubled assets of other banks---can be
               problematic. The banking network typically displays a
               core-periphery structure, with a core consisting of a relatively
               small number of large, densely interconnected banks that are not
               very diverse in terms of business and risk models. This implies
               that core banks' defaults tend to be highly correlated. That, in
               turn, can generate a collective moral hazard problem (i.e.,
               players take on more risk, because others will bear the costs in
               case of default), as banks recognize that they are likely to be
               supported by the authorities in situations of distress, the
               likelihood amplifies their incentives to herd in the first
               place. Estimating systemic risk relies on granular data on the
               financial network. Unfortunately, business interactions between
               banks are often hidden because of confidentiality issues. Tools
               being developed to reconstruct networks from partial information
               and to estimate systemic risk ([ 7 ][7]) suggest that publicly
               available bank information does not allow reliable estimation of
               systemic risk. The estimate would improve greatly if banks
               publicly reported the number of connections with other banks,
               even without disclosing their identity. In addition to data,
               understanding the effects of interconnections also relies on
               integrative quantitative metrics and concepts that reveal
               important network aspects, such as systemic repercussions of the
               failure of individual nodes. For example, DebtRank, which
               measures the systemic importance of individual institutions in a
               financial network ([ 8 ][8]), shows that the issue of
               too-central-to-fail may be even more important than
               too-big-to-fail. AGENTS AND BEHAVIOR. Agent-based models (ABMs)
               are computer models in which the behavior of agents and their
               interactions are explicitly represented as decision rules
               mapping agents' observations onto actions. Although ABMs are
               less well established in analyzing financial-economic systems
               than in, e.g., traffic control, epidemiology, or battlefield
               conflict analyses, they have produced promising results. Axtell
               ([ 9 ][9]) developed a simple ABM that explains more than three
               dozen empirical properties of firm formation without recourse to
               external shocks. ABMs provide a good explanation for why the
               volatility of prices is clustered and time-varying ([ 10 ][10])
               and have been used to test systemic risk implications of reforms
               developed by the Basel Committee on Banking Supervision, which
               show how dynamically changing risk limits can lead to booms and
               busts in prices ([ 11 ][11], [ 12 ][12]). ABMs of market
               dynamics can be linked with ABM work on opinion dynamics in the
               social sciences ([ 13 ][13]) to understand how propagation of
               opinions through social networks affects emergent macro
               behavior, which is crucial to managing the stability and
               resilience of socioeconomic systems. ![Figure][14] Early-warning
               signals of the 2008 crisis in the Dutch interbank network. The
               figure portrays a temporal analysis of two loops, pairs of banks
               that are at the same time debtor and creditor to each other.
               Although the raw number of two loops is not very informative
               about possible ongoing structural changes, its comparison with a
               random network model benchmark is. A z-score represents the
               number of standard deviations by which the number of two loops
               in the real network deviates from its expected value in the
               model. Small magnitude z-scores ( 1. B. LeBaron , in Handbook of
               Computational Economics, vol.2, Agent-Based Computational
               Economics, L. Tesfatsion, K. L. Judd, Eds. (North-Holland,
               Amsterdam, 2006), pp. 1187--1233. 11. [‚Üµ][49]1. S. Thurner 2. et
               al ., Quant. Finan. 12, 695 (2012). [OpenUrl][50][CrossRef][51]
               12. [‚Üµ][52]1. C. Aymanns, 2. J. D. Farmer , J. Econ. Dyn.
               Control 50, 155 (2015). [OpenUrl][53][CrossRef][54] 13.
               [‚Üµ][55]1. A. Flache, 2. M. W. Macy , J. Conflict Resolut. 55,
               970 (2011). [OpenUrl][56][CrossRef][57] 14. [‚Üµ][58]1. T. Bao, 2.
               C. Hommes, 3. T. Makarewicz , Bubble formation and (in)efficient
               markets in learning-to-forecast and --optimize experiments (TI
               2015-107/II Working paper, Tinbergen Institute, Amsterdam,
               2015); 15. [‚Üµ][59]1. C. H. Hommes , Behavioral Rationality and
               Heterogeneous Expectations in Complex Economic Systems
               (Cambridge Univ. Press, Cambridge, 2013). 16. [‚Üµ][60]1. T. Bao,
               2. C. H. Hommes , When speculators meet constructors: Positive
               and negative feedback in experimental housing markets (CeNDEF
               Working paper 15-10, University of Amsterdam, Netherlands,
               2015); . 17. [‚Üµ][61]1. A. G. Haldane , On microscopes and
               telescopes, Workshop on Socio-Economic Complexity, Lorentz
               Center, Leiden, 23 to 27 March 2015 (Bank of England, London,
               2015); . 18. Acknowledgments: We acknowledge financial support
               from The Netherlands Institute of Advanced Studies in the
               Humanities and Social Sciences, The Netherlands Organisation for
               Scientific Research, the Lorentz Center, and the Tinbergen
               Institute. [1]: \#ref-1 [2]: \#ref-2 [3]: \#ref-3 [4]: \#ref-4
               [5]: \#ref-5 [6]: \#ref-6 [7]: \#ref-7 [8]: \#ref-8 [9]: \#ref-9
               [10]: \#ref-10 [11]: \#ref-11 [12]: \#ref-12 [13]: \#ref-13
               [14]: pending:yes [15]: \#ref-14 [16]: \#ref-15 [17]: \#ref-16
               [18]: \#ref-17 [19]: \#xref-ref-1-1 ``View reference 1 in text''
               [20]:
               \{openurl\}?query=rft.jtitle\%253DScience\%26rft.stitle\%253DScience\%26rft.aulast\%253DScheffer\%26rft.auinit1\%253DM.\%26rft.volume\%253D338\%26rft.issue\%253D6105\%26rft.spage\%253D344\%26rft.epage\%253D348\%26rft.atitle\%253DAnticipating\%2BCritical\%2BTransitions\%26rft\_id\%253Dinfo\%253Adoi\%252F10.1126\%252Fscience.1225244\%26rft\_id\%253Dinfo\%253Apmid\%252F23087241\%26rft.genre\%253Darticle\%26rft\_val\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Ajournal\%26ctx\_ver\%253DZ39.88-2004\%26url\_ver\%253DZ39.88-2004\%26url\_ctx\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Actx
               [21]:
               /lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Mzoic2NpIjtzOjU6InJlc2lkIjtzOjEyOiIzMzgvNjEwNS8zNDQiO3M6NDoiYXRvbSI7czoyMjoiL3NjaS8zNTEvNjI3NS84MTguYXRvbSI7fXM6ODoiZnJhZ21lbnQiO3M6MDoiIjt9
               [22]: \#xref-ref-2-1 ``View reference 2 in text'' [23]:
               \{openurl\}?query=rft.jtitle\%253DSci.\%2BRep.\%26rft.volume\%253D3\%26rft.spage\%253D3357\%26rft\_id\%253Dinfo\%253Apmid\%252F24285089\%26rft.genre\%253Darticle\%26rft\_val\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Ajournal\%26ctx\_ver\%253DZ39.88-2004\%26url\_ver\%253DZ39.88-2004\%26url\_ctx\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Actx
               [24]:
               /lookup/external-ref?access\_num=24285089\&link\_type=MED\&atom=\%2Fsci\%2F351\%2F6275\%2F818.atom
               [25]: \#xref-ref-3-1 ``View reference 3 in text'' [26]:
               \{openurl\}?query=rft.jtitle\%253DNature\%26rft.stitle\%253DNature\%26rft.aulast\%253DMay\%26rft.auinit1\%253DR.\%2BM.\%26rft.volume\%253D451\%26rft.issue\%253D7181\%26rft.spage\%253D893\%26rft.epage\%253D895\%26rft.atitle\%253DComplex\%2Bsystems\%253A\%2Becology\%2Bfor\%2Bbankers.\%26rft\_id\%253Dinfo\%253Adoi\%252F10.1038\%252F451893a\%26rft\_id\%253Dinfo\%253Apmid\%252F18288170\%26rft.genre\%253Darticle\%26rft\_val\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Ajournal\%26ctx\_ver\%253DZ39.88-2004\%26url\_ver\%253DZ39.88-2004\%26url\_ctx\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Actx
               [27]:
               /lookup/external-ref?access\_num=10.1038/451893a\&link\_type=DOI
               [28]:
               /lookup/external-ref?access\_num=18288170\&link\_type=MED\&atom=\%2Fsci\%2F351\%2F6275\%2F818.atom
               [29]:
               /lookup/external-ref?access\_num=000253313100022\&link\_type=ISI
               [30]: \#xref-ref-4-1 ``View reference 4 in text'' [31]:
               \{openurl\}?query=rft.jtitle\%253DScience\%26rft.stitle\%253DScience\%26rft.aulast\%253DHeesterbeek\%26rft.auinit1\%253DH.\%26rft.volume\%253D347\%26rft.issue\%253D6227\%26rft.spage\%253Daaa4339\%26rft.epage\%253Daaa4339\%26rft.atitle\%253DModeling\%2Binfectious\%2Bdisease\%2Bdynamics\%2Bin\%2Bthe\%2Bcomplex\%2Blandscape\%2Bof\%2Bglobal\%2Bhealth\%26rft\_id\%253Dinfo\%253Adoi\%252F10.1126\%252Fscience.aaa4339\%26rft\_id\%253Dinfo\%253Apmid\%252F25766240\%26rft.genre\%253Darticle\%26rft\_val\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Ajournal\%26ctx\_ver\%253DZ39.88-2004\%26url\_ver\%253DZ39.88-2004\%26url\_ctx\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Actx
               [32]:
               /lookup/ijlink/YTozOntzOjQ6InBhdGgiO3M6MTQ6Ii9sb29rdXAvaWpsaW5rIjtzOjU6InF1ZXJ5IjthOjQ6e3M6ODoibGlua1R5cGUiO3M6NDoiQUJTVCI7czoxMToiam91cm5hbENvZGUiO3M6Mzoic2NpIjtzOjU6InJlc2lkIjtzOjE2OiIzNDcvNjIyNy9hYWE0MzM5IjtzOjQ6ImF0b20iO3M6MjI6Ii9zY2kvMzUxLzYyNzUvODE4LmF0b20iO31zOjg6ImZyYWdtZW50IjtzOjA6IiI7fQ==
               [33]: \#xref-ref-5-1 ``View reference 5 in text'' [34]:
               \{openurl\}?query=rft.jtitle\%253DCarnegie-Rochester\%2BConf.\%2BSer.\%2BPublic\%2BPolicy\%26rft.volume\%253D1\%26rft.spage\%253D19\%26rft\_id\%253Dinfo\%253Adoi\%252F10.1016\%252FS0167-2231\%252876\%252980003-6\%26rft.genre\%253Darticle\%26rft\_val\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Ajournal\%26ctx\_ver\%253DZ39.88-2004\%26url\_ver\%253DZ39.88-2004\%26url\_ctx\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Actx
               [35]:
               /lookup/external-ref?access\_num=10.1016/S0167-2231(76)80003-6\&link\_type=DOI
               [36]: \#xref-ref-6-1 ``View reference 6 in text'' [37]:
               \{openurl\}?query=rft.jtitle\%253DJ.\%2BEcon.\%2BDynam.\%2BControl\%26rft.volume\%253D36\%26rft.spage\%253D1121\%26rft\_id\%253Dinfo\%253Adoi\%252F10.1016\%252Fj.jedc.2012.04.001\%26rft.genre\%253Darticle\%26rft\_val\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Ajournal\%26ctx\_ver\%253DZ39.88-2004\%26url\_ver\%253DZ39.88-2004\%26url\_ctx\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Actx
               [38]:
               /lookup/external-ref?access\_num=10.1016/j.jedc.2012.04.001\&link\_type=DOI
               [39]: \#xref-ref-7-1 ``View reference 7 in text'' [40]:
               \{openurl\}?query=rft.jtitle\%253DSci.\%2BRep.\%26rft.volume\%253D5\%26rft.spage\%253D15758\%26rft\_id\%253Dinfo\%253Adoi\%252F10.1038\%252Fsrep15758\%26rft\_id\%253Dinfo\%253Apmid\%252F26507849\%26rft.genre\%253Darticle\%26rft\_val\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Ajournal\%26ctx\_ver\%253DZ39.88-2004\%26url\_ver\%253DZ39.88-2004\%26url\_ctx\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Actx
               [41]:
               /lookup/external-ref?access\_num=10.1038/srep15758\&link\_type=DOI
               [42]:
               /lookup/external-ref?access\_num=26507849\&link\_type=MED\&atom=\%2Fsci\%2F351\%2F6275\%2F818.atom
               [43]: \#xref-ref-8-1 ``View reference 8 in text'' [44]:
               \{openurl\}?query=rft.stitle\%253DSci\%2BRep\%26rft.aulast\%253DBattiston\%26rft.auinit1\%253DS.\%26rft.volume\%253D2\%26rft.spage\%253D541\%26rft.epage\%253D541\%26rft.atitle\%253DDebtRank\%253A\%2Btoo\%2Bcentral\%2Bto\%2Bfail\%253F\%2BFinancial\%2Bnetworks\%252C\%2Bthe\%2BFED\%2Band\%2Bsystemic\%2Brisk.\%26rft\_id\%253Dinfo\%253Apmid\%252F22870377\%26rft.genre\%253Darticle\%26rft\_val\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Ajournal\%26ctx\_ver\%253DZ39.88-2004\%26url\_ver\%253DZ39.88-2004\%26url\_ctx\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Actx
               [45]:
               /lookup/external-ref?access\_num=22870377\&link\_type=MED\&atom=\%2Fsci\%2F351\%2F6275\%2F818.atom
               [46]: \#xref-ref-9-1 ``View reference 9 in text'' [47]:
               http://www.css.gmu.edu/~axtell/Rob/Research/Pages/Firms.html
               [48]: \#xref-ref-10-1 ``View reference 10 in text'' [49]:
               \#xref-ref-11-1 ``View reference 11 in text'' [50]:
               \{openurl\}?query=rft.jtitle\%253DQuant.\%2BFinan.\%26rft.volume\%253D12\%26rft.spage\%253D695\%26rft\_id\%253Dinfo\%253Adoi\%252F10.1080\%252F14697688.2012.674301\%26rft.genre\%253Darticle\%26rft\_val\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Ajournal\%26ctx\_ver\%253DZ39.88-2004\%26url\_ver\%253DZ39.88-2004\%26url\_ctx\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Actx
               [51]:
               /lookup/external-ref?access\_num=10.1080/14697688.2012.674301\&link\_type=DOI
               [52]: \#xref-ref-12-1 ``View reference 12 in text'' [53]:
               \{openurl\}?query=rft.jtitle\%253DJ.\%2BEcon.\%2BDyn.\%2BControl\%26rft.volume\%253D50\%26rft.spage\%253D155\%26rft\_id\%253Dinfo\%253Adoi\%252F10.1016\%252Fj.jedc.2014.09.015\%26rft.genre\%253Darticle\%26rft\_val\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Ajournal\%26ctx\_ver\%253DZ39.88-2004\%26url\_ver\%253DZ39.88-2004\%26url\_ctx\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Actx
               [54]:
               /lookup/external-ref?access\_num=10.1016/j.jedc.2014.09.015\&link\_type=DOI
               [55]: \#xref-ref-13-1 ``View reference 13 in text'' [56]:
               \{openurl\}?query=rft.jtitle\%253DJ.\%2BConflict\%2BResolut.\%26rft.volume\%253D55\%26rft.spage\%253D970\%26rft\_id\%253Dinfo\%253Adoi\%252F10.1177\%252F0022002711414371\%26rft.genre\%253Darticle\%26rft\_val\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Ajournal\%26ctx\_ver\%253DZ39.88-2004\%26url\_ver\%253DZ39.88-2004\%26url\_ctx\_fmt\%253Dinfo\%253Aofi\%252Ffmt\%253Akev\%253Amtx\%253Actx
               [57]:
               /lookup/external-ref?access\_num=10.1177/0022002711414371\&link\_type=DOI
               [58]: \#xref-ref-14-1 ``View reference 14 in text'' [59]:
               \#xref-ref-15-1 ``View reference 15 in text'' [60]:
               \#xref-ref-16-1 ``View reference 16 in text'' [61]:
               \#xref-ref-17-1 ``View reference 17 in text''",
  journal   = "Science",
  publisher = "American Association for the Advancement of Science",
  volume    =  351,
  number    =  6275,
  pages     = "818--819",
  month     =  feb,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Peckham2013-zo,
  title     = "Economies of contagion: financial crisis and pandemic",
  author    = "Peckham, Robert",
  abstract  = "Abstract The outbreak of an influenza A (H1N1) pandemic in 2009
               coincided with a severe global financial downturn (2007?8). This
               paper examines the use of ?contagion? as a model for assessing
               the dynamics of both episodes: the spread of infection and the
               diffusion of shock through an intrafinancial system. The
               argument is put that a discourse of globally ?emerging? and
               ?re-emerging? infection helped to shape a theory of financial
               contagion, which developed, particularly from 1997, in relation
               to financial crises in ?emerging markets? in Southeast Asia.
               Conversely, concerns about the economic impact of a global
               pandemic have been influential in shaping public health
               responses to communicable diseases from the early 1990s. The
               paper argues that tracing the conceptual entanglement of
               financial and biological ?contagions? is important for
               understanding the interconnected global environment within which
               risk is increasingly being evaluated. The paper also considers
               the consequences of this analogizing for how financial crises
               are understood and, ultimately, how responses to them are
               formulated.",
  journal   = "Econ. Soc.",
  publisher = "Routledge",
  volume    =  42,
  number    =  2,
  pages     = "226--248",
  month     =  may,
  year      =  2013
}

@ARTICLE{Patel2015-qr,
  title    = "Predicting stock and stock price index movement using Trend
              Deterministic Data Preparation and machine learning techniques",
  author   = "Patel, Jigar and Shah, Sahil and Thakkar, Priyank and Kotecha, K",
  abstract = "This paper addresses problem of predicting direction of movement
              of stock and stock price index for Indian stock markets. The
              study compares four prediction models, Artificial Neural Network
              (ANN), Support Vector Machine (SVM), random forest and
              naive-Bayes with two approaches for input to these models. The
              first approach for input data involves computation of ten
              technical parameters using stock trading data (open, high, low \&
              close prices) while the second approach focuses on representing
              these technical parameters as trend deterministic data. Accuracy
              of each of the prediction models for each of the two input
              approaches is evaluated. Evaluation is carried out on 10years of
              historical data from 2003 to 2012 of two stocks namely Reliance
              Industries and Infosys Ltd. and two stock price indices CNX Nifty
              and S\&P Bombay Stock Exchange (BSE) Sensex. The experimental
              results suggest that for the first approach of input data where
              ten technical parameters are represented as continuous values,
              random forest outperforms other three prediction models on
              overall performance. Experimental results also show that the
              performance of all the prediction models improve when these
              technical parameters are represented as trend deterministic data.",
  journal  = "Expert Syst. Appl.",
  volume   =  42,
  number   =  1,
  pages    = "259--268",
  month    =  jan,
  year     =  2015,
  keywords = "Naive-Bayes classification; Artificial neural networks; Support
              vector machine; Random forest; Stock market"
}

@ARTICLE{Huerta2013-kr,
  title    = "Nonlinear support vector machines can systematically identify
              stocks with high and low future returns",
  author   = "Huerta, Ramon and Corbacho, Fernando and Elkan, Charles",
  abstract = "This paper investigates the profitability of a trading strategy
              based on training a model to identify stocks with high or low
              predicted returns. A tail set is defined to be a group of stocks
              whose volatility-adjusted price change is in the highest or
              lowest quantile, for example the highest or lowest 5\%. Each
              stock is represented by a set of technical and fundamental
              features computed using CRSP and Compustat data. A classifier is
              trained on historical tail sets and tested on future data. The
              classifier is chosen to be a nonlinear support vector machine
              (SVM) due to its simplicity and effectiveness. The SVM is trained
              once per month, in order to adjust to changing market conditions.
              Portfolios are formed by ranking stocks using the classifier
              output. The highest ranked stocks are used for long positions and
              the lowest ranked ones for short sales. The Global Industry
              Classification Standard is used to build a model for each sector
              such that a total of 8 long-short portfolios for Energy,
              Materials, Industrials, Consumer Discretionary, Consumer Staples,
              Health Care, Financials, and Information Technology are formed.
              The data range from 1981 to 2010. Without measuring trading
              costs, but using 91 day holding periods to minimize these, the
              strategy leads to annual excess returns (Jensen alpha) of 15\%
              with volatilities under 8\% using the top 25\% of the stocks of
              the distribution for training long positions and the bottom 25\%
              for the short ones.",
  journal  = "Algorithmic Finance",
  volume   =  2,
  pages    = "45--58",
  year     =  2013
}

@ARTICLE{Cleveland1984-vd,
  title     = "Graphical Perception: Theory, Experimentation, and Application
               to the Development of Graphical Methods",
  author    = "Cleveland, William S and McGill, Robert",
  abstract  = "Abstract The subject of graphical methods for data analysis and
               for data presentation needs a scientific foundation. In this
               article we take a few steps in the direction of establishing
               such a foundation. Our approach is based on graphical
               perception?the visual decoding of information encoded on
               graphs?and it includes both theory and experimentation to test
               the theory. The theory deals with a small but important piece of
               the whole process of graphical perception. The first part is an
               identification of a set of elementary perceptual tasks that are
               carried out when people extract quantitative information from
               graphs. The second part is an ordering of the tasks on the basis
               of how accurately people perform them. Elements of the theory
               are tested by experimentation in which subjects record their
               judgments of the quantitative information on graphs. The
               experiments validate these elements but also suggest that the
               set of elementary tasks should be expanded. The theory provides
               a guideline for graph construction: Graphs should employ
               elementary tasks as high in the ordering as possible. This
               principle is applied to a variety of graphs, including bar
               charts, divided bar charts, pie charts, and statistical maps
               with shading. The conclusion is that radical surgery on these
               popular graphs is needed, and as replacements we offer
               alternative graphical forms?dot charts, dot charts with
               grouping, and framed-rectangle charts.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  79,
  number    =  387,
  pages     = "531--554",
  month     =  sep,
  year      =  1984
}

@INCOLLECTION{Cheuk2016-lj,
  title     = "Towards Predicting Financial Failure in {Non-Profit}
               Organisations",
  booktitle = "Handbook of Research on Financial and Banking Crisis Prediction
               through Early Warning Systems",
  author    = "Cheuk, Sharon",
  abstract  = "Towards Predicting Financial Failure in Non-Profit
               Organisations: 10.4018/978-1-4666-9484-2.ch019: Non-profit
               organisation financial failure has been a prevalent problem in
               developed and developing economies. Such incidences bring
               adverse effects to the",
  publisher = "IGI Global",
  pages     = "387--404",
  year      =  2016,
  language  = "en"
}

@ARTICLE{Chen2002-ps,
  title    = "Breadth of ownership and stock returns",
  author   = "Chen, Joseph and Hong, Harrison and Stein, Jeremy C",
  abstract = "We develop a stock market model with differences of opinion and
              short-sales constraints. When breadth is low---i.e., when few
              investors have long positions---this signals that the short-sales
              constraint is binding tightly, and that prices are high relative
              to fundamentals. Thus reductions in breadth should forecast lower
              returns. Using data on mutual fund holdings, we find that stocks
              whose change in breadth in the prior quarter is in the lowest
              decile of the sample underperform those in the top decile by
              6.38\% in the twelve months after formation. Adjusting for size,
              book-to-market, and momentum, the figure is 4.95\%.",
  journal  = "J. financ. econ.",
  volume   =  66,
  number   =  2,
  pages    = "171--205",
  month    =  nov,
  year     =  2002,
  keywords = "Differences of opinion; Short-sales constraints; Return
              predictability"
}

@ARTICLE{Duttagupta2011-uq,
  title    = "Anatomy of banking crises in developing and emerging market
              countries",
  author   = "Duttagupta, Rupa and Cashin, Paul",
  abstract = "This paper uses a Binary Classification Tree (BCT) model to
              analyze banking crises in 50 emerging market and developing
              countries during 1990--2005. The BCT model identifies three
              conditions (and the specific threshold of the key indictors) at
              which the vulnerability to banking crisis increases---(i) very
              high inflation, (ii) highly dollarized bank deposits combined
              with nominal depreciation or low bank liquidity, and (iii) low
              bank profitability---which highlight that foreign currency risk,
              poor financial soundness, and macroeconomic instability are
              important drivers of banking crises. The results also emphasize
              the importance of conditional thresholds in triggering crises, in
              that banking crises are underlined by a combination of
              vulnerabilities---or a sequence of (non-linear)
              conditions---rather than the deterioration of a unique factor.",
  journal  = "J. Int. Money Finance",
  volume   =  30,
  number   =  2,
  pages    = "354--376",
  month    =  mar,
  year     =  2011,
  keywords = "Banking crises; Binary classification tree; Exchange rate;
              Dollarization"
}

@ARTICLE{Alessi2018-ao,
  title    = "Identifying excessive credit growth and leverage",
  author   = "Alessi, Lucia and Detken, Carsten",
  abstract = "Unsustainable credit developments lead to the build-up of
              systemic risks to financial stability. While this is an accepted
              truth, how to assess whether risks are getting out of hand
              remains a challenge. To identify excessive credit growth and
              aggregate leverage we propose an early warning system, which aims
              at predicting banking crises. In particular, we use a modern
              classification tree ensemble technique, the ``Random Forest'',
              and include (global) credit as well as real estate variables as
              predictors.",
  journal  = "Journal of Financial Stability",
  volume   =  35,
  pages    = "215--225",
  month    =  apr,
  year     =  2018,
  keywords = "Early warning systems; Banking crises; Credit; Macroprudential
              policy; Decision trees"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Fatemi2006-uk,
  title    = "Credit risk management: a survey of practices",
  author   = "Fatemi, Ali and Fooladi, Iraj",
  abstract = "Purpose -- Proposes to investigate the current practices of
              credit risk management by the largest US‚Äêbased financial
              institutions. Owing to the increasing variety in the types of
              counterparties and the ever‚Äêexpanding variety in the forms of
              obligations, credit risk management has jumped to the forefront
              of risk management activities carried out by firms in the
              financial services industry. This study is designed to shed light
              on the current practices of these
              firms.Design/methodology/approach -- A short questionnaire,
              containing seven questions, was mailed to each of the top 100
              banking firms headquartered in the USA.Findings -- It was found
              that identifying counterparty default risk is the single
              most‚Äêimportant purpose served by the credit risk models utilized.
              Close to half of the responding institutions utilize models that
              are also capable of dealing with counterparty migration risk.
              Surprisingly, only a minority of banks currently utilize either a
              proprietary or a vendor‚Äêmarketed model for the management of
              their credit risk. Interestingly, those that utilize their own
              in‚Äêhouse model also utilize a vendor‚Äêmarketed model. Not
              surprisingly, such models are more widely used for the management
              of non‚Äêtraded credit loan portfolios than they are for the
              management of traded bonds.Originality/value -- The results help
              one to understand the current practices of these firms. As such,
              they enable us to make inferences about the perceived importance
              of the risks. The paper is of particular value to the treasurers
              intending to better understand the current trends in credit risk
              management, and to academics intending to carry out research in
              the field.",
  journal  = "Managerial Finance",
  volume   =  32,
  number   =  3,
  pages    = "227--233",
  year     =  2006
}

@ARTICLE{noauthor_undated-ci,
  title    = "The Current State of Quantitative Equity Investing",
  author   = "Becker, Ying and Reinganum, Marc R",
  abstract = "Quantitative equity management is concerned with rigorous,
              disciplined approaches to help investors structure optimal
              portfolios to achieve the outcomes they seek. At the root of
              disciplined, modern investment processes are two things: risk and
              return. The notion of total return is obvious---price
              appreciation plus any dividend payments. Risk may not be so
              straightforward. In most quantitative approaches, risk is viewed
              as more akin to a roulette wheel; that is, the possible outcomes
              are well specified and the likelihood of each outcome is known,
              but in advance, an investor does not know which outcome will be
              realized. In this piece, we curate the history of quantitative
              equity investing, which traces its origins to the development of
              portfolio theory and the capital asset pricing model (CAPM). In
              equities, some of the first quantitative approaches were aimed at
              confirming the theoretical predictions of the CAPM. In
              particular, the expected return of a risky asset depends only on
              the risk of that asset as measured by its beta, a covariance
              measure of risk. In this paradigm, all investors hold the same
              risky portfolio, the market portfolio of risky assets that
              maximizes the Sharpe ratio. At the same time, stock prices are
              viewed to be informationally efficient and reflecting all
              available information. By the early 1980s, this simple view of
              the world was punctured by the discovery of stock market
              anomalies. Researchers discovered that variables other than beta
              could explain the cross section of expected returns. In
              particular, size and value were found to contain useful
              explanatory power. By the 1990s, the anomalies morphed into the
              mainstream as the anomalies were re-labeled as factors, and the
              benchmark model, at least in academic research, was a
              three-factor model with beta, size, and value. Concurrent with
              the three-factor model, other credible factors muscled their way
              into the credible empirical asset pricing world, including
              momentum, liquidity, quality, and volatility. Indeed, in 2011,
              the president of the American Finance Association described the
              proliferation of factors as a ``zoo of new factors.'' Recent work
              suggests using a much higher standard to accept new factors. With
              diminishing acceptance of the view that capitalization-weighted
              indexes are optimal for all investors, factor investing has taken
              off in practice. Sometime these ``smart factors'' are called
              smart beta. Morningstar reported that factor investing is the
              fastest-growing segment of the investment management marketplace.
              Investors have recognized that low-cost exposure to other factors
              might give them superior risk/return trade-offs. Of course,
              active investors are still looking for ways to improve
              performance over more-passive smart beta indexes. In this race,
              big data approaches offer the potential to grab an insight before
              it becomes widely known. Another promising avenue is the ability
              to dynamically adjust allocations to different factors based on
              the macroeconomic environment and investment conditions. Active
              managers are also exploring better ways to construct portfolios.
              In short, quantitative equity management is alive and well and
              intellectually active as investors seek to better manage risk and
              return. Commercially, factor investing has taken off in the form
              of smart beta. Products and strategies, vetted by decades of
              prior and current research, are continually being developed. One
              might reasonably forecast that dynamic factor-timing strategies
              will be a growth area for the quantitative equity field. A new
              generation of big data approaches is developing in the field and
              is likely to grow as technology becomes more capable and more
              data are digitally available. Despite the advances in theory,
              modeling, and technology, the goal of quantitative equity
              management techniques is an old one: aiding investors to achieve
              more efficient and appropriate investment outcomes.",
  month    =  jun,
  year     =  2018
}

@UNPUBLISHED{Fabozzi2018-lt,
  title    = "Equity Valuation Science, Art, or Craft?",
  author   = "Fabozzi, Frank J and Focardi, Sergio M and Jonas, Caroline",
  abstract = "The price at which a stock is traded in the market reflects the
              ability of the firm to generate cash flow and the risks
              associated with generating the expected future cash flows. The
              authors point to the limits of widely used valuation techniques.
              The most important of these limits is the inability to forecast
              cash flows and to determine the appropriate discount rate.
              Another important limit is the inability to determine absolute
              value. Widely used valuation techniques such as market multiples
              - the price-to-earnings ratio, firm value multiples or a use of
              multiple ratios, for example - capture only relative value, that
              is, the value of a firm's stocks related to the value of
              comparable firms (assuming that comparable firms can be
              identified). The study underlines additional problems when it
              comes to valuing IPOs and private equity: Both are sensitive to
              the timing of the offer, suffer from information asymmetry, and
              are more subject to behavioral elements than is the case for
              shares of listed firms. In the case of IPOs in particular, the
              authors discuss how communication strategies and media hype play
              an important role in the IPO valuation/pricing process.",
  month    =  sep,
  year     =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Arner2016-vm,
  title     = "{FinTech}, {RegTech}, and the reconceptualization of financial
               regulation",
  author    = "Arner, Douglas W and Barberis, J{\`a}nos and Buckey, Ross P",
  abstract  = "Regulatory change and technological developments following the
               2008 Global Financial Crisis are changing the nature offinancial
               markets, services, and institutions. At the juncture of these
               phenomena lies regulatory technology or`` RegTech''-the use of
               technology ‚Ä¶",
  journal   = "Nw. J. Int'l L. \& Bus.",
  publisher = "HeinOnline",
  volume    =  37,
  pages     = "371",
  year      =  2016
}

@ARTICLE{Treleaven2015-tb,
  title     = "Financial Regulation of Fintech",
  author    = "Treleaven, Philip",
  abstract  = "Effective financial regulation is clearly crucial to innovation
               and the future success of the financial services industry and,
               in specific, FinTech. There are also unprecedented opportunities
               for reforming regulation and also creating new businesses in the
               process. Examples include: using ``big data'' regulatory online
               reporting and analytics to streamline reporting; and stimulating
               a new generation of ``RegTech'' companies to provide the
               regulatory/compliance software. This paper reviews the current
               regulatory pressures faced by the financial services industry,
               and discusses new ``big data'' approaches to regulating
               financial companies. Three actions are highlighted: a) an
               open-source platform for FinTech regulation, b) a regulatory XML
               to help standardize reporting and c) an overarching
               international standards body. Lastly, we examine responses by
               the U.K. Financial Conduct Authority (FCA), such as Project
               Innovate.",
  journal   = "The Journal of Financial Perspectives",
  publisher = "papers.ssrn.com",
  month     =  nov,
  year      =  2015
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Frame2018-qh,
  title    = "Technological Change and Financial Innovation in Banking: Some
              Implications for Fintech",
  author   = "Frame, W Scott and Wall, Larry D and White, Lawrence J",
  abstract = "Financial intermediation has changed dramatically over the past
              30 years, due in large part to technological change. The paper
              first describes the role of the financial system in a modern
              economy and how technological change and financial innovation can
              affect social ‚Ä¶",
  journal  = "Federal Reserve Bank of Atlanta Working Paper Series",
  volume   =  11,
  number   =  11,
  year     =  2018
}

@UNPUBLISHED{Athey2016-uu,
  title    = "Bitcoin Pricing, Adoption, and Usage: Theory and Evidence",
  author   = "Athey, Susan and Parashkevov, Ivo and Sarukkai, Vishnu and Xia,
              Jing",
  abstract = "This paper develops a model of user adoption and use of virtual
              currency (such as Bitcoin), and focusing on the dynamics of
              adoption in the presence of frictions arising from exchange rate
              uncertainty. The theoretical model can be used to analyze how
              market fundamentals determine the exchange rate of fiat currency
              to Bitcoin. Empirical evidence from Bitcoin prices and
              utilization provides mixed evidence about the ability of the
              model to explain prices. Further analysis of the history of all
              individual transactions on Bitcoin's public ledger establishes
              patterns of adoption and utilization across user types,
              transaction type, and geography. We show that as of mid-2015,
              active usage was not growing quickly, and that investors and
              infrequent users held the majority of Bitcoins. We document the
              extent to which the attributes of the anonymous users of Bitcoin
              can be inferred through their behavior, and we find that users
              who engage in illegal activity are more likely to try to protect
              their financial privacy.",
  journal  = "Stanford University Working Paper",
  month    =  aug,
  year     =  2016
}

@ARTICLE{Wall2018-pd,
  title    = "Some financial regulatory implications of artificial intelligence",
  author   = "Wall, Larry D",
  abstract = "Artificial intelligence has been playing an increasingly large
              role in the economy and this trend seems likely to continue. This
              paper begins with a high-level overview of artificial
              intelligence, including some of its important strengths and
              weaknesses. It then discusses some of the ways that AI affect the
              evolution of the financial system and financial regulation.",
  journal  = "J. Econ. Bus.",
  month    =  jun,
  year     =  2018,
  keywords = "Financial institutions; Artificial intelligence; Machine
              learning; Financial supervision; Financial regulation"
}

@ARTICLE{Anagnostopoulos2018-le,
  title    = "Fintech and regtech: Impact on regulators and banks",
  author   = "Anagnostopoulos, Ioannis",
  abstract = "The purpose of this paper is to develop an insight and review the
              effect of FinTech development against the broader environment in
              financial technology. We further aim to offer various
              perspectives in order to aid the understanding of the disruptive
              potential of FinTech, and its implications for the wider
              financial ecosystem. By drawing upon very recent and highly
              topical research on this area this study examines the
              implications for financial institutions, and regulation
              especially when technology poses a challenge to the global
              banking and regulatory system. It is driven by a wide-ranging
              overview of the development, the current state, and possible
              future of fintech. This paper attempts to connect
              practitioner-led and academic research. While it draws on
              academic research, the perspective it takes is also
              practice-oriented. It relies on the current academic literature
              as well as insights from industry sources, action research and
              other publicly available commentaries. It also draws on
              professional practitioners' roundtable discussions, and
              think-tanks in which the author has been an active participant.
              We attempt to interpret banking, and regulatory issues from a
              behavioural perspective. The last crisis exposed significant
              failures in regulation and supervision. It has made the Financial
              Market Law and Compliance a key topic on the current agenda.
              Disruptive technological change also seems to be important in
              investigating regulatory compliance followed by change. We
              contribute to the current literature review on financial and
              digital innovation by new entrants where this has also practical
              implications. We also provide for an updated review of the
              current regulatory issues addressing the contextual root causes
              of disruption within the financial services domain. The aim here
              is to assist market participants to improve effectiveness and
              collaboration. The difficulties arising from extensive regulation
              may suggest a more liberal and principled approach to financial
              regulation. Disruptive innovation has the potential for welfare
              outcomes for consumers, regulatory, and supervisory gains as well
              as reputational gains for the financial services industry. It
              becomes even more important as the financial services industry
              evolves. For example, the preparedness of the regulators to
              instil culture change and harmonise technological advancements
              with regulation could likely achieve many desired outcomes. Such
              results range from achieving an orderly market growth, further
              aiding systemic stability and restoring trust and confidence in
              the financial system. Our action-led research results have
              implications for both research and practice. These should be of
              interest to regulatory standard setters, investors, international
              organisations and other academics who are researching regulatory
              and competition issues, and their manifestation within the
              financial and social contexts. As a perspective on a social
              construct, this study appeals to regulators and law makers,
              entrepreneurs, and investors who participate in technology
              applied within the innovative financial services domain. It is
              also of interest to bankers who might consider FinTech and
              strategic partnerships as a prospective, future strategic
              direction.11We thank two anonymous referees for their very
              thoughtful and constructing comments who took a keen interest in
              reviewing our research and in this way contributed materially to
              the development of the paper. All other omissions and/or errors
              remain our own.",
  journal  = "J. Econ. Bus.",
  month    =  jul,
  year     =  2018,
  keywords = "FinTech; RegTech; Business models; Regulation; Financial
              services; Future research direction"
}

@ARTICLE{Henderson2011-aj,
  title    = "The dark side of financial innovation: A case study of the
              pricing of a retail financial product",
  author   = "Henderson, Brian J and Pearson, Neil D",
  abstract = "The offering prices of 64 issues of a popular retail structured
              equity product were, on average, almost 8\% greater than
              estimates of the products' fair market values obtained using
              option pricing methods. Under reasonable assumptions about the
              underlying stocks' expected returns, the mean expected return
              estimate on the structured products is slightly below zero. The
              products do not provide tax, liquidity, or other benefits, and it
              is difficult to rationalize their purchase by informed rational
              investors. Our findings are, however, consistent with the recent
              hypothesis that issuing firms might shroud some aspects of
              innovative securities or introduce complexity to exploit
              uninformed investors.",
  journal  = "J. financ. econ.",
  volume   =  100,
  number   =  2,
  pages    = "227--247",
  month    =  may,
  year     =  2011,
  keywords = "Structured products; Financial innovation; Derivatives; Pricing"
}

@ARTICLE{Cather2018-zf,
  title    = "Cream Skimming: Innovations in Insurance Risk Classification and
              Adverse Selection",
  author   = "Cather, David A",
  abstract = "Abstract We demonstrate how innovations in insurance risk
              classification can lead to adverse selection, or cream skimming,
              against insurers that are slow to adopt such pricing innovations.
              Using a model in which insurers with insufficient pricing data
              cannot differentiate between low- and high-risk policyholders and
              therefore charge both the same premium, we show how innovative
              insurers develop new risk classification data to identify
              overcharged low-risk policyholders and attract them from rival
              insurers with reduced prices. Less innovative insurers thus
              insure a growing percentage of high-risk customers, resulting in
              adverse selection attributable to their informational
              disadvantage. Next, we examine two cases in which ?Big Data?
              innovations in risk classification led to concerns about cream
              skimming among U.S. auto insurers. First, we track the rapid
              adoption of credit-based insurance scores as pricing variables in
              personal auto insurance markets. Second, we examine the growing
              popularity of usage-based insurance programs like telematics,
              plans in which insurers use data on policyholders? actual driving
              behavior to set prices that attract low-risk customers. Issues
              associated with the execution of such pricing strategies are
              discussed. In both cases, we document how rival insurers quickly
              adopt successful innovations to reduce their exposure to adverse
              selection.",
  journal  = "Risk Management and Insurance Review",
  volume   =  21,
  number   =  2,
  pages    = "335--366",
  month    =  sep,
  year     =  2018
}

@ARTICLE{Frame2004-ys,
  title     = "Empirical Studies of Financial Innovation: Lots of Talk, Little
               Action?",
  author    = "Frame, W Scott and White, Lawrence J",
  abstract  = "Empirical Studies of Financial Innovation: Lots of Talk, Little
               Action? by W. Scott Frame and Lawrence J. White. Published in
               volume 42, issue 1, pages 116-144 of Journal of Economic
               Literature, March 2004, Abstract: This paper reviews the extant
               empirical studies of financial innovation. Adopting b...",
  journal   = "J. Econ. Lit.",
  publisher = "aeaweb.org",
  volume    =  42,
  number    =  1,
  pages     = "116--144",
  month     =  mar,
  year      =  2004
}

@ARTICLE{Heaton2016-jm,
  title         = "Deep Learning in Finance",
  author        = "Heaton, J B and Polson, N G and Witte, J H",
  abstract      = "We explore the use of deep learning hierarchical models for
                   problems in financial prediction and classification.
                   Financial prediction problems -- such as those presented in
                   designing and pricing securities, constructing portfolios,
                   and risk management -- often involve large data sets with
                   complex data interactions that currently are difficult or
                   impossible to specify in a full economic model. Applying
                   deep learning methods to these problems can produce more
                   useful results than standard methods in finance. In
                   particular, deep learning can detect and exploit
                   interactions in the data that are, at least currently,
                   invisible to any existing financial economic theory.",
  month         =  feb,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1602.06561"
}

@ARTICLE{Finkelstein2004-gz,
  title     = "Adverse Selection in Insurance Markets: Policyholder Evidence
               from the {U.K}. Annuity Market",
  author    = "Finkelstein, Amy and Poterba, James",
  abstract  = "We use a unique data set of annuities in the United Kingdom to
               test for adverse selection. We find systematic relationships
               between ex post mortality and annuity characteristics, such as
               the timing of payments and the possibility of payments to the
               annuitant's estate. These patterns are consistent with the
               presence of asymmetric information. However, we find no evidence
               of substantive mortality differences by annuity size. These
               results suggest that the absence of selection on one contract
               dimension does not preclude its presence on others. This
               highlights the importance of considering detailed features of
               insurance contracts when testing theoretical models of
               asymmetric information.",
  journal   = "J. Polit. Econ.",
  publisher = "journals.uchicago.edu",
  volume    =  112,
  number    =  1,
  pages     = "183--208",
  year      =  2004
}

@ARTICLE{Cutler1998-sg,
  title     = "Paying for Health Insurance: The {Trade-Off} between Competition
               and Adverse Selection",
  author    = "Cutler, David M and Reber, Sarah J",
  abstract  = "We use data on health plan choices by employees of Harvard
               University to compare the benefits of insurance competition with
               the costs of adverse selection. Moving to a voucher-type system
               induced significant adverse selection, with a welfare loss of 2
               to 4 percent of baseline spending. But increased competition
               reduced Harvard's premiums by 5 to 8 percent. The premium
               reductions came from insurer profits, so while Harvard was
               better off, the net effect for society was only the adverse
               selection loss. Adverse selection can be minimized by adjusting
               voucher amounts for individual risk. We discuss how such a
               system would work.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  113,
  number    =  2,
  pages     = "433--466",
  month     =  may,
  year      =  1998
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Pauly1978-me,
  title     = "Overinsurance and public provision of insurance: The roles of
               moral hazard and adverse selection",
  booktitle = "Uncertainty in Economics",
  author    = "Pauly, Mark V",
  abstract  = "This chapter discusses overinsurance and public provision of
               insurance . In the absence of perfect information, the
               competitive outcome in markets for insurance may be nonoptimal
               not only compared to the infeasible optimum that would have
               occurred if information were ‚Ä¶",
  publisher = "Elsevier",
  pages     = "307--331",
  year      =  1978
}

@ARTICLE{Fama2008-hz,
  title    = "Dissecting Anomalies",
  author   = "Fama, Eugene F and French, Kenneth R",
  abstract = "ABSTRACT The anomalous returns associated with net stock issues,
              accruals, and momentum are pervasive; they show up in all size
              groups (micro, small, and big) in cross-section regressions, and
              they are also strong in sorts, at least in the extremes. The
              asset growth and profitability anomalies are less robust. There
              is an asset growth anomaly in average returns on microcaps and
              small stocks, but it is absent for big stocks. Among profitable
              firms, higher profitability tends to be associated with
              abnormally high returns, but there is little evidence that
              unprofitable firms have unusually low returns.",
  journal  = "J. Finance",
  volume   =  63,
  number   =  4,
  pages    = "1653--1678",
  month    =  aug,
  year     =  2008
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Fama2016-oz,
  title     = "Dissecting Anomalies with a {Five-Factor} Model",
  author    = "Fama, Eugene F and French, Kenneth R",
  abstract  = "A five-factor model that adds profitability (RMW) and investment
               (CMA) factors to the three-factor model of Fama and French
               (1993) suggests a shared story for several average-return
               anomalies. Specifically, positive exposures to RMW and CMA
               (stock returns that behave like those of profitable firms that
               invest conservatively) capture the high average returns
               associated with low market $\beta$‚Å†, share repurchases, and low
               stock return volatility. Conversely, negative RMW and CMA slopes
               (like those of relatively unprofitable firms that invest
               aggressively) help explain the low average stock returns
               associated with high $\beta$‚Å†, large share issues, and highly
               volatile returns.Received November 11, 2014; accepted April 27,
               2015 by Editor Andrew Karolyi.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  29,
  number    =  1,
  pages     = "69--103",
  month     =  jan,
  year      =  2016
}

@INCOLLECTION{Hastie2009-kr,
  title     = "Unsupervised Learning",
  booktitle = "The Elements of Statistical Learning: Data Mining, Inference,
               and Prediction",
  author    = "Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome",
  editor    = "Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome",
  publisher = "Springer New York",
  pages     = "485--585",
  year      =  2009,
  address   = "New York, NY"
}

@ARTICLE{John_Lu2010-or,
  title     = "The elements of statistical learning: data mining, inference,
               and prediction",
  author    = "John Lu, Z Q",
  journal   = "J. R. Stat. Soc. Ser. A Stat. Soc.",
  publisher = "Wiley Online Library",
  volume    =  173,
  number    =  3,
  pages     = "693--694",
  year      =  2010
}

@ARTICLE{Hutchinson1994-ax,
  title    = "A Nonparametric Approach to Pricing and Hedging Derivative
              Securities Via Learning Networks",
  author   = "Hutchinson, James M and Lo, Andrew W and Poggio, Tomaso",
  abstract = "Abstract We propose a nonparametric method for estimating the
              pricing formula of a derivative asset using learning networks.
              Although not a substitute for the more traditional
              arbitrage-based pricing formulas, network-pricing formulas may be
              more accurate and computationally more efficient alternatives
              when the underlying asset's price dynamics are unknown, or when
              the pricing equation associated with the no-arbitrage condition
              cannot be solved analytically. To assess the potential value of
              network pricing formulas, we simulate Black-Scholes option prices
              and show that learning networks can recover the Black-Scholes
              formula from a two-year training set of daily options prices, and
              that the resulting network formula can be used successfully to
              both price and delta-hedge options out-of-sample. For comparison,
              we estimate models using four popular methods: ordinary least
              squares, radial basis function networks, multilayer perceptron
              networks, and projection pursuit. To illustrate the practical
              relevance of our network pricing approach, we apply it to the
              pricing and delta-hedging of S\&P 500 futures options from 1987
              to 1991.",
  journal  = "J. Finance",
  volume   =  49,
  number   =  3,
  pages    = "851--889",
  month    =  jul,
  year     =  1994
}

@ARTICLE{Garcia2000-uf,
  title    = "Pricing and hedging derivative securities with neural networks
              and a homogeneity hint",
  author   = "Garcia, Ren{\'e} and Gen{\c c}ay, Ramazan",
  abstract = "We estimate a generalized option pricing formula that has a
              functional shape similar to the usual Black--Scholes formula by a
              feedforward neural network model. This functional shape is
              obtained when the option pricing function is homogeneous of
              degree one with respect to the underlying asset price (St) and
              the strike price (K). We show that pricing accuracy gains can be
              made by exploiting this generalized Black--Scholes shape. Instead
              of setting up a learning network mapping the ratio St/K and the
              time to maturity ($\tau$) directly into the derivative price, we
              break down the pricing function into two parts, one controlled by
              the ratio St/K, the other one by a function of time to maturity.
              The results indicate that the homogeneity hint always reduces the
              out-of-sample mean squared prediction error compared with a
              feedforward neural network with no hint. Both feedforward network
              models, with and without the hint, provide similar delta-hedging
              errors that are small relative to the hedging performance of the
              Black--Scholes model. However, the model with hint produces a
              more stable hedging performance.",
  journal  = "J. Econom.",
  volume   =  94,
  number   =  1,
  pages    = "93--115",
  month    =  jan,
  year     =  2000,
  keywords = "Option pricing; Nonparametric methods; Feedforward networks;
              Homogeneity hint"
}

@ARTICLE{Stutzer1996-sj,
  title    = "A Simple Nonparametric Approach to Derivative Security Valuation",
  author   = "Stutzer, Michael",
  abstract = "ABSTRACT Canonical valuation uses historical time series to
              predict the probability distribution of the discounted value of
              primary assets' discounted prices plus accumulated dividends at
              any future date. Then the axiomatically-rationalized maximum
              entropy principle is used to estimate risk-neutral (equivalent
              martingale) probabilities that correctly price the primary
              assets, as well as any predesignated subset of derivative
              securities whose payoffs occur at this date. Valuation of other
              derivative securities proceeds by calculation of its discounted,
              risk-neutral expected value. Both simulation and empirical
              evidence suggest that canonical valuation has merit.",
  journal  = "J. Finance",
  volume   =  51,
  number   =  5,
  pages    = "1633--1652",
  month    =  dec,
  year     =  1996
}

@UNPUBLISHED{Feng2018-an,
  title    = "Deep Learning Factor Alpha",
  author   = "Feng, Guanhao and Polson, Nick and Xu, Jianeng",
  abstract = "Does a factor model exist to absorb all existing anomalies? We
              provide a deep learning automated solution to generate long-short
              factors using a high-dimensional firm characteristic. Sorting
              securities on firm characteristics is a common practice in
              finance and a nonlinear activation function built into deep
              learning. Our algorithm performs a nonlinear search and finds the
              optimal transformation of characteristics used for security
              sorting, with one asset pricing objective: minimizing alphas. Our
              deep factors, hidden neurons in the neural network, are trained
              greedily with the backward propagation feedback from the loss
              function that considers both time series and cross-sectional
              variations. Our conditional forecast generalizes a benchmark,
              such as CAPM, and includes Fama-French type models as special
              cases. We have designed a train-validation-test study for monthly
              U.S. equity returns from 1975 to 2017 and 57 published firm
              characteristics. In an out-of-sample evaluation, the conditional
              deep factor model shows a forecasting improvement over the
              benchmark with factors that offer significant alphas. The
              conclusion is the improvement of insignificant alphas for some
              anomalies as well as sorted portfolios.",
  month    =  sep,
  year     =  2018,
  keywords = "Characteristic-based Anomalies, Cross-Sectional Returns, Deep
              Learning, Long- Short Factors, Security Sorting, Mispricing
              Alpha, Neural Network"
}

@ARTICLE{Hamermesh2005-ls,
  title    = "Beauty in the classroom: instructors' pulchritude and putative
              pedagogical productivity",
  author   = "Hamermesh, Daniel S and Parker, Amy",
  abstract = "Adjusted for many other determinants, beauty affects earnings;
              but does it lead directly to the differences in productivity that
              we believe generate earnings differences? We take a large sample
              of student instructional ratings for a group of university
              teachers and acquire six independent measures of their beauty,
              and a number of other descriptors of them and their classes.
              Instructors who are viewed as better looking receive higher
              instructional ratings, with the impact of a move from the 10th to
              the 90th percentile of beauty being substantial. This impact
              exists within university departments and even within particular
              courses, and is larger for male than for female instructors.
              Disentangling whether this outcome represents productivity or
              discrimination is, as with the issue generally, probably
              impossible.",
  journal  = "Econ. Educ. Rev.",
  volume   =  24,
  number   =  4,
  pages    = "369--376",
  month    =  aug,
  year     =  2005,
  keywords = "Beauty; Discrimination; Class evaluations; College teaching"
}

@ARTICLE{Buchak2018-jd,
  title    = "Fintech, regulatory arbitrage, and the rise of shadow banks",
  author   = "Buchak, Greg and Matvos, Gregor and Piskorski, Tomasz and Seru,
              Amit",
  abstract = "Shadow bank market share in residential mortgage origination
              nearly doubled from 2007 to 2015, with particularly dramatic
              growth among online ``fintech'' lenders. We study how two forces,
              regulatory differences and technological advantages, contributed
              to this growth. Difference in difference tests exploiting
              geographical heterogeneity induced by four specific increases in
              regulatory burden--capital requirements, mortgage servicing
              rights, mortgage-related lawsuits, and the movement of
              supervision to Office of Comptroller and Currency following
              closure of the Office of Thrift Supervision--all reveal that
              traditional banks contracted in markets where they faced more
              regulatory constraints; shadow banks partially filled these gaps.
              Relative to other shadow banks, fintech lenders serve more
              creditworthy borrowers and are more active in the refinancing
              market. Fintech lenders charge a premium of 14--16 basis points
              and appear to provide convenience rather than cost savings to
              borrowers. They seem to use different information to set interest
              rates relative to other lenders. A quantitative model of mortgage
              lending suggests that regulation accounts for roughly 60\% of
              shadow bank growth, while technology accounts for roughly 30\%.",
  journal  = "J. financ. econ.",
  volume   =  130,
  number   =  3,
  pages    = "453--483",
  month    =  dec,
  year     =  2018,
  keywords = "Fintech; Shadow banks; Regulatory arbitrage; Lending; Mortgages;
              FHA"
}

@ARTICLE{Philippon2015-op,
  title    = "Has the {US} Finance Industry Become Less Efficient? On the
              Theory and Measurement of Financial Intermediation",
  author   = "Philippon, Thomas",
  journal  = "Am. Econ. Rev.",
  volume   =  105,
  number   =  4,
  pages    = "1408--1438",
  month    =  apr,
  year     =  2015
}

@ARTICLE{Greenwood2018-fw,
  title    = "Bubbles for Fama",
  author   = "Greenwood, Robin and Shleifer, Andrei and You, Yang",
  abstract = "We evaluate Eugene F. Fama's claim that stock prices do not
              exhibit price bubbles. Based on US industry returns (1926-2014)
              and international sector returns (1985-2014), we present four
              findings (1) Fama is correct in that a sharp price increase of an
              industry portfolio does not, on average, predict unusually low
              returns going forward; (2) such sharp price increases predict a
              substantially heightened probability of a crash but not of a
              further price boom; (3) attributes of the price run-up, including
              volatility, turnover, issuance, and the price path of the run-up,
              help forecast an eventual crash; and (4) these attributes also
              help forecast future returns. Results hold similarly in US and
              international samples.",
  journal  = "J. financ. econ.",
  month    =  sep,
  year     =  2018,
  keywords = "Bubble; Market efficiency; Predictability"
}

@ARTICLE{Du2016-pn,
  title    = "Mergers, acquisitions, and bank efficiency: Cross-country
              evidence from emerging markets",
  author   = "Du, Kai and Sim, Nicholas",
  abstract = "In emerging countries, bank mergers and acquisitions (M\&A) are
              frequently motivated by the objective of promoting stability in
              the banking industry. However, the evidence that M\&A can lead to
              better performing banks is tenuous at best. In this article, we
              investigate if this tenuous relationship could be due to the
              treatment of target and acquiring banks as the same type in
              empirical analysis, which overlooks the possibility that M\&A may
              affect these banks differently. Using panel data on six emerging
              countries, our results confirm that the effect of M\&A is
              generally weak except when our regressions are implemented
              separately for target and acquiring banks. For the latter, we
              find that target banks tend to be more efficient after an M\&A
              but no efficiency improvements are found for acquiring banks.
              These results suggest that in emerging countries, bank M\&A can
              lead to efficiency improvements for the combined entity, although
              target banks are mainly the ones to benefit from it. They also
              highlight the importance of distinguishing between target and
              acquiring banks so as to obtain sharper estimates of how M\&A
              might affect bank performance.",
  journal  = "Research in International Business and Finance",
  volume   =  36,
  pages    = "499--510",
  month    =  jan,
  year     =  2016,
  keywords = "Emerging countries; Mergers and acquisitions; Bank efficiency"
}

@ARTICLE{Rahman2016-em,
  title    = "Value creation and appropriation following {M\&A}: A data
              envelopment analysis",
  author   = "Rahman, Mahabubur and Lambkin, Mary and Hussain, Dildar",
  abstract = "Mergers and acquisitions (M\&As) are typically inspired by a
              desire for revenue growth and/or cost efficiency leading to an
              improvement in financial performance. Post-merger performance has
              received considerable research attention from scholars in finance
              and accounting, but the marketing dimension has remained largely
              unexplored. This research focuses on marketing efficiency as a
              measure of post-merger performance, and this is investigated via
              an empirical study of 20 M\&A deals within the US commercial
              banking industry. Data Envelopment Analysis (DEA) is used to
              measure efficiency, employing two input and two output variables.
              The results demonstrate that M\&A transactions do have a positive
              effect on the marketing efficiency of the combined firms,
              although the effect size is small.",
  journal  = "J. Bus. Res.",
  volume   =  69,
  number   =  12,
  pages    = "5628--5635",
  month    =  dec,
  year     =  2016,
  keywords = "Mergers and acquisitions (M\&As); Post-merger marketing
              performance; Data envelopment analysis (DEA); Marketing
              efficiency"
}

@ARTICLE{Jagtiani2016-bj,
  title    = "Community bank mergers and their impact on small business lending",
  author   = "Jagtiani, Julapa and Kotliar, Ian and Maingi, Raman Quinn",
  abstract = "There have been increasing concerns about the potential of larger
              banks acquiring community banks and the declining number of
              community banks, which would significantly reduce small business
              lending (SBL) and disrupt relationship lending. This paper
              examines the roles and characteristics of U.S. community banks in
              the past decade, covering the recent economic boom and downturn.
              We analyze risk characteristics of acquired community banks,
              compare pre- and post-acquisition performance, and investigate
              how the acquisitions have affected SBL. Contrary to the concerns,
              our analysis shows that the overall amount of SBL increases more
              after a merger when a community bank is acquired by a large bank.
              Data also suggest an overall (regardless of mergers) declining
              SBL trend for all bank size groups. In fact, the decline in the
              SBL ratio, on average, has been more severe among community
              banks, relative to large banks. Our results indicate that mergers
              involving community bank targets over the past decade have
              enhanced the overall safety and soundness of the banking system
              without adversely impacting SBL.",
  journal  = "Journal of Financial Stability",
  volume   =  27,
  pages    = "106--121",
  month    =  dec,
  year     =  2016,
  keywords = "Community banks; Small business lending; Bank mergers"
}

@ARTICLE{Baglioni2013-wz,
  title     = "Is the leverage of European banks procyclical?",
  author    = "Baglioni, A and Beccalli, E and Boitani, A and Monticini, A",
  abstract  = "Detecting whether banks' leverage is indeed procyclical is
               relevant to support the view that booms and crises may be
               reinforced by some sort of supply side financial accelerator,
               whilst finding a plausible explanation of banks' behaviour is
               crucial to trace the road for a sensible reform of financial
               regulation and managers' incentives. By analyzing a large sample
               of European banks, we show that procyclical leverage appears to
               be well entrenched in the behaviour of those banks for which
               investment banking prevails over the traditional commercial
               banking activity.",
  journal   = "Empir. Econ.",
  publisher = "Springer Berlin Heidelberg",
  volume    =  45,
  number    =  3,
  pages     = "1251--1266",
  month     =  dec,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Beccalli2016-rw,
  title    = "Why are some banks recapitalized and others taken over?",
  author   = "Beccalli, Elena and Frantz, Pascal",
  abstract = "This study investigates the likelihood of takeovers or
              recapitalizations for EU listed banks before and during the
              financial crisis, using both static and sequential multinomial
              logistic models. Takeovers and recapitalizations are potential
              alternatives used to shore up financial institutions. We find
              that takeovers occur when the bank has low net interest margins.
              Instead, private recapitalizations occur for banks with lower
              equity, higher net interest margins, and positive growth at the
              bank level. Public recapitalizations occur for larger, less
              liquid banks with positive prospects that operate in bigger
              banking systems. Both types of recapitalizations occur in
              countries with lower growth. The determinants for takeovers and
              recapitalization differ between the pre-crisis and crisis
              periods. Overall, a need for corporate control exists when
              traditional banking suffers lower performance, whereas the search
              for stability explains recapitalizations.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  45,
  pages    = "79--95",
  month    =  nov,
  year     =  2016,
  keywords = "Banking; Recapitalizations; Takeovers; EU"
}

@ARTICLE{Shenoy2012-ap,
  title     = "An Examination of the Efficiency, Foreclosure, and Collusion
               Rationales for Vertical Takeovers",
  author    = "Shenoy, Jaideep",
  abstract  = "We investigate the efficiency, foreclosure, and collusion
               rationales for vertical integration in a large sample of
               vertically related takeovers. The efficiency rationale, as
               discussed under the transaction cost economics and property
               rights theories, posits that vertical integration mitigates
               contractual inefficiencies between suppliers and customers
               (termed as holdup) and provides incentives to undertake
               relationship-specific investments. In contrast, the foreclosure
               and collusion rationales suggest that vertical integration is
               anticompetitive in nature. Specifically, the foreclosure
               argument suggests that vertical integration is used to raise
               costs of rival firms, and the collusion argument suggests that
               vertical integration facilitates coordination between the
               integrated firm and its rivals. To distinguish between the three
               hypotheses, we examine (1) the announcement period wealth
               effects to the merging firms, rival firms, and customer firms;
               and (2) the operating performance changes to the merging firms
               in vertical takeovers. We find that firms expand their vertical
               boundaries consistent with an efficiency enhancing rationale.
               This paper was accepted by Brad Barber, finance.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  58,
  number    =  8,
  pages     = "1482--1501",
  month     =  aug,
  year      =  2012
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{De_Haan2016-dz,
  title     = "Corporate governance of banks: A survey",
  author    = "De Haan, J and Vlahu, R",
  abstract  = "This paper reviews the empirical literature on the corporate
               governance of banks. We start by highlighting the main
               differences between banks and nonfinancial firms and focus on
               three characteristics that make banks special:(i)
               regulation,(ii) the capital structure of banks ‚Ä¶",
  journal   = "J. Econ. Surv.",
  publisher = "Wiley Online Library",
  year      =  2016
}

@ARTICLE{Kowalik2015-ls,
  title   = "Banking Consolidation and Merger Activity following the Crisis",
  author  = "Kowalik, Michal and Davig, Troy and Morris, Charles S and Regehr,
             Kristen",
  journal = "Economic Review of Kansas City Federal Reserve",
  year    =  2015
}

@ARTICLE{King2005-ru,
  title    = "Are the Causes of Bank Distress Changing? Can Researchers Keep
              Up?",
  author   = "King, Thomas B and Nuxoll, Daniel and Yeager, Timothy J",
  abstract = "Since 1990, the banking sector has experienced enormous
              legislative, technological and financial changes, yet research
              into the causes of bank distress has slowed. One consequence is
              that current supervisory surveillance models may no longer
              accurately represent the banking environment. After reviewing the
              history of these models, we provide empirical evidence that the
              characteristics of failing banks has changed in the last ten
              years and argue that the time is right for new research employing
              new empirical techniques. In particular, dynamic models that
              utilize forward-looking variables and address various types of
              bank risk individually are promising lines of inquiry.
              Supervisory agencies have begun to move in these directions, and
              we describe several examples of this new generation of
              early-warning models that are not yet widely known among academic
              banking economists.",
  journal  = "Federal Reserve Bank of St. Louis Review",
  volume   =  88,
  number   =  1,
  month    =  jan,
  year     =  2005,
  keywords = "Bank failure, bank distress, bank surveillance, early warning"
}

@ARTICLE{Gelman2014-qf,
  title     = "Ethics and Statistics: The {AAA} Tranche of Subprime Science",
  author    = "Gelman, Andrew and Loken, Eric",
  journal   = "Chance",
  publisher = "Taylor \& Francis",
  volume    =  27,
  number    =  1,
  pages     = "51--56",
  month     =  jan,
  year      =  2014
}

@ARTICLE{Brewer2013-bt,
  title    = "How Much Did Banks Pay to Become {Too-Big-To-Fail} and to Become
              Systemically Important?",
  author   = "Brewer, Elijah and Jagtiani, Julapa",
  abstract = "This paper estimates the value of the too-big-to-fail (TBTF)
              subsidy. Using data from the merger boom of 1991--2004, we find
              that banking organizations were willing to pay an added premium
              for mergers that would put them over the asset sizes that are
              commonly viewed as the thresholds for being TBTF. We estimate at
              least $15 billion in added premiums for the eight merger deals
              that brought the organizations to over $100 billion in assets. In
              addition, we find that both the stock and bond markets reacted
              positively to these TBTF merger deals. Our estimated TBTF subsidy
              is large enough to create serious concern, particularly since the
              recently assisted mergers have effectively allowed for TBTF
              banking organizations to become even bigger and for nonbanks to
              become part of TBTF banking organizations, thus extending the
              TBTF subsidy beyond banking.",
  journal  = "Journal of Financial Services Research",
  volume   =  43,
  number   =  1,
  pages    = "1--35",
  month    =  feb,
  year     =  2013
}

@ARTICLE{Voronkova2004-om,
  title    = "Equity market integration in Central European emerging markets: A
              cointegration analysis with shifting regimes",
  author   = "Voronkova, Svitlana",
  abstract = "We investigate the existence of long-run relations between
              emerging Central European stock markets and the mature stock
              markets of Europe and the United States. Allowing for instability
              in these long-run relations, we obtain evidence of links between
              the Central European markets that is stronger than has previously
              been reported. We also show that the Central European markets
              display equilibrium relations with their mature counterparts,
              which persist after controlling for structural changes. It
              follows that Central European markets have become more integrated
              with global markets.",
  journal  = "International Review of Financial Analysis",
  volume   =  13,
  number   =  5,
  pages    = "633--647",
  month    =  jan,
  year     =  2004,
  keywords = "Cointegration; International stock markets; Central Europe"
}

@UNPUBLISHED{Lee2018-pi,
  title    = "States of psychological anchors and price behavior of Japanese
              yen futures",
  author   = "Lee, H-C and Lee, Y-H and Lu, Y-C and Wang, Y-C",
  abstract = "This study explores whether the relationship between Japanese yen
              futures returns and the corresponding equity returns is affected
              by the states of psychological anchors of the currency and stock
              markets. This study employs the linear-regression-based tree
              model (a machine learning method) to account for the framing
              effect of the anchors. The empirical results of the
              linear-regression-based tree model show that the currency price
              behaviors of momentum and reversal, and prediction by equity
              markets, vary with the anchors. Empirical evidence also indicates
              that the linear-regression-based tree model outperforms the OLS
              model based on the estimation results and out-of-sample
              forecasting. The forecasting performance of the
              linear-regression-based tree model can be improved along with an
              increase in the forecasting period. \copyright{} 2018 Elsevier
              Inc.",
  journal  = "North American Journal of Economics and Finance",
  year     =  2018,
  keywords = "Forecasting performance; Framing; Japanese yen futures;
              Linear-regression-based tree model; Machine learning;
              Psychological anchor"
}

@UNPUBLISHED{Kuousmanen2018-py,
  title  = "Shadow Prices and Marginal Abatement Costs: Convex Quantile
            Regression Approach",
  author = "Kuousmanen, Timo",
  year   =  2018
}

@ARTICLE{Baker2002-sa,
  title    = "Limited arbitrage in mergers and acquisitions",
  author   = "Baker, Malcolm and Sava{\c s}oglu, Serkan",
  abstract = "A diversified portfolio of risk arbitrage positions produces an
              abnormal return of 0.6--0.9\% per month over the period from 1981
              to 1996. We trace these profits to practical limits on risk
              arbitrage. In our model of risk arbitrage, arbitrageurs'
              risk-bearing capacity is constrained by deal completion risk and
              the size of the position they hold. Consistent with this model,
              we document that the returns to risk arbitrage increase in an ex
              ante measure of completion risk and target size. We also examine
              the influence of the general supply of arbitrage capital,
              measured by the total equity holdings of arbitrageurs, on
              arbitrage profits.",
  journal  = "J. financ. econ.",
  volume   =  64,
  number   =  1,
  pages    = "91--115",
  month    =  apr,
  year     =  2002,
  keywords = "Arbitrage; Mergers and acquisitions; Market efficiency"
}

@TECHREPORT{Bruno2018-ek,
  title       = "How do Banks Respond to {NPLs}? Evidence from the Euro Area",
  author      = "Bruno, Brunella and Marino, Immacolata and {Others}",
  institution = "Centre for Studies in Economics and Finance (CSEF), University
                 of Naples, Italy",
  year        =  2018
}

@MISC{RFID_Journal_undated-zq,
  title        = "Genesis of the Versatile {RFID} Tag - 2003-04-21 - Page 1 -
                  {RFID} Journal",
  author       = "{RFID Journal}",
  abstract     = "Mario Cardullo received the first patent for a passive,
                  read-write RFID tag. He tells how he came up with the idea in
                  1969. - Page 1",
  howpublished = "\url{https://www.rfidjournal.com/articles/view?392}",
  note         = "Accessed: 2018-11-22"
}

@INCOLLECTION{Heckman2010-ke,
  title     = "Selection Bias and {Self-Selection}",
  booktitle = "Microeconometrics",
  author    = "Heckman, James J",
  editor    = "Durlauf, Steven N and Blume, Lawrence E",
  abstract  = "The problem of selection bias in economic and social statistics
               arises when a rule other than simple random sampling is used to
               sample the underlying population that is the object of interest.
               The distorted representation of a true population as a
               consequence of a sampling rule is the essence of the selection
               problem. Distorting selection rules may be the outcome of
               decisions of sample survey statisticians, self-selection
               decisions by the agents being studied, or both.",
  publisher = "Palgrave Macmillan UK",
  pages     = "242--266",
  year      =  2010,
  address   = "London"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Heckman1990-xx,
  title     = "Varieties of Selection Bias",
  author    = "Heckman, James",
  abstract  = "‚Ä¶ that causes Lewis to dismiss econometric meth- ods for solving
               self - selection problems arises ‚Ä¶ This is a fundamental advance
               in our knowl- edge about selection bias models ‚Ä¶ Selection on
               the basis of nonwage components (nepotism, discrimination, etc.)
               is another source (see ‚Ä¶",
  journal   = "Am. Econ. Rev.",
  publisher = "American Economic Association",
  volume    =  80,
  number    =  2,
  pages     = "313--318",
  year      =  1990
}

@ARTICLE{Wainer1998-sy,
  title     = "A Selection of Selection Anomalies",
  author    = "Wainer, Howard and Palmer, Samuel and Bradlow, Eric T",
  journal   = "Chance",
  publisher = "Taylor \& Francis",
  volume    =  11,
  number    =  2,
  pages     = "3--7",
  month     =  mar,
  year      =  1998
}

@ARTICLE{Cuddeback2004-wi,
  title     = "Detecting and Statistically Correcting Sample Selection Bias",
  author    = "Cuddeback, Gary and Wilson, Elizabeth and Orme, John G and
               Combs-Orme, Terri",
  abstract  = "ABSTRACT Researchers seldom realize 100\% participation for any
               research study. If participants and non-participants are
               systematically different, substantive results may be biased in
               unknown ways, and external or internal validity may be
               compromised. Typically social work researchers use bivariate
               tests to detect selection bias (e.g., ?2 to compare the race of
               participants and non-participants). Occasionally multiple
               regression methods are used (e.g., logistic regression with
               participation/non-participation as the dependent variable).
               Neither of these methods can be used to correct substantive
               results for selection bias. Sample selection models are a
               well-developed class of econometric models that can be used to
               detect and correct for selection bias, but these are rarely used
               in social work research. Sample selection models can help
               further social work research by providing researchers with
               methods of detecting and correcting sample selection bias.",
  journal   = "J. Soc. Serv. Res.",
  publisher = "Routledge",
  volume    =  30,
  number    =  3,
  pages     = "19--33",
  month     =  may,
  year      =  2004
}

@ARTICLE{Li_undated-np,
  title  = "A Method for Avoiding Bias from Feature Selection with Application
            to Naive Bayes Classi cation Models",
  author = "Li, Longhai and Zhang, Jianguo and Neal, Radford M"
}

@ARTICLE{Goodfellow1988-om,
  title     = "Response Bias Using {Two-Stage} Data Collection: A Study of
               Elderly Participants in a Program",
  author    = "Goodfellow, Marianne and Kiernan, Nancy-Ellen and Ahern, Frank
               and Smyer, Michael A",
  abstract  = "Researchers examining the effects of programs, in this case a
               state-level pharmaceutical assistance program for the elderly,
               sometimes must rely on multiple methods of data collection.
               Two-stage data collection (e.g., a telephone interview followed
               by a mail questionnaire) was used to obtain a full range of
               information. Older age groups were found to participate less
               frequently in the telephone interview, while certain demographic
               factors characterized mail questionnaire nonparticipants, all of
               which supports past research. Results also show that those in
               the poorest health are less likely to participate in the mail
               survey. Combining the two methods did not result in high
               attrition, suggesting that innovation can be successfully
               employed. Knowledge of the bias associated with each method will
               aid in targeting special groups.",
  journal   = "Eval. Rev.",
  publisher = "SAGE Publications Inc",
  volume    =  12,
  number    =  6,
  pages     = "638--654",
  month     =  dec,
  year      =  1988
}

@BOOK{Baumer2017-ue,
  title     = "Modern Data Science with {R}",
  author    = "Baumer, Benjamin S and Kaplan, Daniel T and Horton, Nicholas J",
  abstract  = "Modern Data Science with R is a comprehensive data science
               textbook for undergraduates that incorporates statistical and
               computational thinking to solve real-world problems with data.
               Rather than focus exclusively on case studies or programming
               syntax, this book illustrates how statistical programming in the
               state-of-the-art R/RStudio computing environment can be
               leveraged to extract meaningful information from a variety of
               data in the service of addressing compelling statistical
               questions. Contemporary data science requires a tight
               integration of knowledge from statistics, computer science,
               mathematics, and a domain of application. This book will help
               readers with some background in statistics and modest prior
               experience with coding develop and practice the appropriate
               skills to tackle complex data science projects. The book
               features a number of exercises and has a flexible organization
               conducive to teaching a variety of semester courses.",
  publisher = "CRC Press",
  month     =  mar,
  year      =  2017,
  language  = "en"
}

@BOOK{Wickham2016-gv,
  title     = "{R} for Data Science: Import, Tidy, Transform, Visualize, and
               Model Data",
  author    = "Wickham, Hadley and Grolemund, Garrett",
  abstract  = "Learn how to use R to turn raw data into insight, knowledge, and
               understanding. This book introduces you to R, RStudio, and the
               tidyverse, a collection of R packages designed to work together
               to make data science fast, fluent, and fun. Suitable for readers
               with no previous programming experience, R for Data Science is
               designed to get you doing data science as quickly as
               possible.Authors Hadley Wickham and Garrett Grolemund guide you
               through the steps of importing, wrangling, exploring, and
               modeling your data and communicating the results. You'll get a
               complete, big-picture understanding of the data science cycle,
               along with basic tools you need to manage the details. Each
               section of the book is paired with exercises to help you
               practice what you've learned along the way.You'll learn how
               to:Wrangle---transform your datasets into a form convenient for
               analysisProgram---learn powerful R tools for solving data
               problems with greater clarity and easeExplore---examine your
               data, generate hypotheses, and quickly test themModel---provide
               a low-dimensional summary that captures true ``signals'' in your
               datasetCommunicate---learn R Markdown for integrating prose,
               code, and results",
  publisher = "``O'Reilly Media, Inc.''",
  month     =  dec,
  year      =  2016,
  language  = "en"
}

@BOOK{Kuhn2013-ri,
  title     = "Applied Predictive Modeling",
  author    = "Kuhn, Max and Johnson, Kjell",
  abstract  = "Applied Predictive Modeling covers the overall predictive
               modeling process, beginning with the crucial steps of data
               preprocessing, data splitting and foundations of model tuning.
               The text then provides intuitive explanations of numerous common
               and modern regression and classification techniques, always with
               an emphasis on illustrating and solving real data problems. The
               text illustrates all parts of the modeling process through many
               hands-on, real-life examples, and every chapter contains
               extensive R code for each step of the process. This
               multi-purpose text can be used as an introduction to predictive
               models and the overall modeling process, a practitioner's
               reference handbook, or as a text for advanced undergraduate or
               graduate level predictive modeling courses. To that end, each
               chapter contains problem sets to help solidify the covered
               concepts and uses data available in the book's R package.This
               text is intended for a broad audience as both an introduction to
               predictive models as well as a guide to applying them.
               Non-mathematical readers will appreciate the intuitive
               explanations of the techniques while an emphasis on
               problem-solving with real data across a wide variety of
               applications will aid practitioners who wish to extend their
               expertise. Readers should have knowledge of basic statistical
               ideas, such as correlation and linear regression analysis. While
               the text is biased against complex equations, a mathematical
               background is needed for advanced topics.",
  publisher = "Springer Science \& Business Media",
  month     =  may,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Moews2019-ln,
  title    = "Lagged correlation-based deep learning for directional trend
              change prediction in financial time series",
  author   = "Moews, Ben and Herrmann, J Michael and Ibikunle, Gbenga",
  abstract = "Trend change prediction in complex systems with a large number of
              noisy time series is a problem with many applications for
              real-world phenomena, with stock markets as a notoriously
              difficult to predict example of such systems. We approach
              predictions of directional trend changes via complex lagged
              correlations between them, excluding any information about the
              target series from the respective inputs to achieve predictions
              purely based on such correlations with other series. We propose
              the use of deep neural networks that employ step-wise linear
              regressions with exponential smoothing in the preparatory feature
              engineering for this task, with regression slopes as trend
              strength indicators for a given time interval. We apply this
              method to historical stock market data from 2011 to 2016 as a use
              case example of lagged correlations between large numbers of time
              series that are heavily influenced by externally arising new
              information as a random factor. The results demonstrate the
              viability of the proposed approach, with state-of-the-art
              accuracies and accounting for the statistical significance of the
              results for additional validation, as well as important
              implications for modern financial economics.",
  journal  = "Expert Syst. Appl.",
  volume   =  120,
  pages    = "197--206",
  month    =  apr,
  year     =  2019,
  keywords = "Lagged correlation; Deep learning; Trend analysis; Stock market"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gao2017-uz,
  title    = "The 2 ¬∞C Global Temperature Target and the Evolution of the
              {Long-Term} Goal of Addressing Climate {Change---From} the United
              Nations Framework Convention on Climate Change to the Paris
              Agreement",
  author   = "Gao, Yun and Gao, Xiang and Zhang, Xiaohua",
  abstract = "The Paris Agreement proposed to keep the increase in global
              average temperature to well below 2 ¬∞C above pre-industrial
              levels and to pursue efforts to limit the temperature increase to
              1.5 ¬∞C above pre-industrial levels. It was thus the first
              international treaty to endow the 2 ¬∞C global temperature target
              with legal effect. The qualitative expression of the ultimate
              objective in Article 2 of the United Nations Framework Convention
              on Climate Change (UNFCCC) has now evolved into the numerical
              temperature rise target in Article 2 of the Paris Agreement.
              Starting with the Second Assessment Report (SAR) of the
              Intergovernmental Panel on Climate Change (IPCC), an important
              task for subsequent assessments has been to provide scientific
              information to help determine the quantified long-term goal for
              UNFCCC negotiation. However, due to involvement in the value
              judgment within the scope of non-scientific assessment, the IPCC
              has never scientifically affirmed the unacceptable extent of
              global temperature rise. The setting of the long-term goal for
              addressing climate change has been a long process, and the 2 ¬∞C
              global temperature target is the political consensus on the basis
              of scientific assessment. This article analyzes the evolution of
              the long-term global goal for addressing climate change and its
              impact on scientific assessment, negotiation processes, and
              global low-carbon development, from aspects of the origin of the
              target, the series of assessments carried out by the IPCC
              focusing on Article 2 of the UNFCCC, and the promotion of the
              global temperature goal at the political level.",
  journal  = "Proc. Est. Acad. Sci. Eng.",
  volume   =  3,
  number   =  2,
  pages    = "272--278",
  month    =  apr,
  year     =  2017,
  keywords = "Climate change; International negotiation; Intergovernmental
              Panel on Climate Change; United Nations Framework Convention on
              Climate Change; Long-term goal; Critical vulnerability; Intuitive
              building"
}

@ARTICLE{Rosen2015-na,
  title    = "The economics of mitigating climate change: What can we know?",
  author   = "Rosen, Richard A and Guenther, Edeltraud",
  abstract = "The long-term economics of mitigating climate change over the
              long run has played a high profile role in the most important
              analyses of climate change in the last decade, namely the Stern
              Report and the IPCC's Fourth Assessment. However, the various
              kinds of uncertainties that affect these economic results raise
              serious questions about whether or not the net costs and benefits
              of mitigating climate change over periods as long as 50 to
              100years can be known to such a level of accuracy that they
              should be reported to policymakers and the public. This paper
              provides a detailed analysis of the derivation of these estimates
              of the long-term economic costs and benefits of mitigation. It
              particularly focuses on the role of technological change,
              especially for energy efficiency technologies, in making the net
              economic results of mitigating climate change unknowable over the
              long run. Because of these serious technical problems,
              policymakers should not base climate change mitigation policy on
              the estimated net economic impacts computed by integrated
              assessment models. Rather, mitigation policies must be forcefully
              implemented anyway given the actual physical climate change
              crisis, in spite of the many uncertainties involved in trying to
              predict the net economics of doing so.",
  journal  = "Technol. Forecast. Soc. Change",
  volume   =  91,
  pages    = "93--106",
  month    =  feb,
  year     =  2015,
  keywords = "Climate change; Economics; Mitigation; Uncertainty; Energy
              efficiency; Technological change"
}

@ARTICLE{Ang2002-cn,
  title     = "International asset allocation with regime shifts",
  author    = "Ang, Andrew and Bekaert, Geert",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  15,
  number    =  4,
  pages     = "1137--1187",
  year      =  2002
}

@ARTICLE{Dai2003-mq,
  title     = "Term structure dynamics in theory and reality",
  author    = "Dai, Qiang and Singleton, Kenneth",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  16,
  number    =  3,
  pages     = "631--678",
  year      =  2003
}

@ARTICLE{Bollerslev2001-up,
  title    = "Financial econometrics: Past developments and future challenges",
  author   = "Bollerslev, Tim",
  abstract = "The field of financial econometrics has had a glamorous run
              during the life span of the Journal of Econometrics. This note
              provides a selective summary of the most important developments
              in the field over the past two decades, notably ARCH and GMM,
              along with a discussion of promising avenues for future research.",
  journal  = "J. Econom.",
  volume   =  100,
  number   =  1,
  pages    = "41--51",
  month    =  jan,
  year     =  2001,
  keywords = "ARCH; GMM; High-frequency data; Long memory; Continuous time
              modeling; Risk-neutral distributions"
}

@ARTICLE{Walke2018-yc,
  title    = "Risk-based loan pricing consequences for credit unions",
  author   = "Walke, Adam G and Fullerton, Thomas M and Tokle, Robert J",
  abstract = "Risk-based pricing, in which interest rate offers vary according
              to individual borrower risk levels, has been increasingly used to
              price credit union loans in the United States. The key question
              examined in this research, given credit union not-for-profit
              objectives, is whether this pricing strategy increases the
              availability of loans, particularly for high-risk borrowers. Data
              on the number of loans per credit union member and loan
              delinquency rates are used to assess loan access and average
              risk-levels, respectively. The results indicate that risk-based
              pricing adopters increase the availability of loans relative to
              otherwise similar non-adopters. However, average risk levels, as
              measured by delinquency rates, appear to be somewhat lower for
              adopters of risk-based pricing compared to matched non-adopters.
              This suggests that lending increases primarily for low-risk
              borrowers, which contrasts with claims that risk-based pricing
              chiefly benefits high-risk borrowers who might otherwise be
              denied credit.",
  journal  = "Journal of Empirical Finance",
  volume   =  47,
  pages    = "105--119",
  month    =  jun,
  year     =  2018,
  keywords = "Risk-based pricing; Credit union loan rates; Delinquency rates;
              Propensity score matching; Kernel density plots; Balanced
              covariates"
}

@UNPUBLISHED{Keating2005-rf,
  title    = "Assessing Financial Vulnerability in the Nonprofit Sector",
  author   = "Keating, Elizabeth K and Fischer, Mary and Gordon, Teresa P and
              Greenlee, Janet S",
  abstract = "Effective nonprofit governance relies upon understanding an
              organization's financial condition and vulnerabilities. However,
              financial vulnerability of nonprofit organizations is a
              relatively new area of study. In this paper, we compare two
              models used to forecast bankruptcy in the corporate sector
              (Altman 1968 and Ohlson 1980) with the model used by nonprofit
              researchers (Tuckman and Chang 1991). We find that the Ohlson
              model has higher explanatory power than either Tuckman and
              Chang's or Altman's in predicting four different measures of
              financial vulnerability. However, we show that none of the
              models, individually or combined, are effective in predicting
              financial distress. We then propose a more comprehensive model of
              financial vulnerability by adding two new variables to represent
              reliance on commercial-type activities to generate revenues and
              endowment sufficiency. We find that this model outperforms
              Ohlson's model and performs substantially better in explaining
              and predicting financial vulnerability. Hence, the expanded model
              can be used as a guide for understanding the drivers of financial
              vulnerability and for identifying more effective proxies for
              nonprofit sector financial distress for use in future research.",
  month    =  jan,
  year     =  2005,
  keywords = "Financial vulnerability, financial distress, bankruptcy
              prediction, nonprofit"
}

@ARTICLE{Hager2001-tu,
  title     = "Financial Vulnerability among Arts Organizations: A Test of the
               {Tuckman-Chang} Measures",
  author    = "Hager, Mark A",
  abstract  = "In the past decade, nonprofits scholars have given increased
               attention to the topic of vulnerability and organizational
               demise. An early contribution to this literature was Tuckman and
               Chang?s elaboration of four financial ratios that they believe
               could be used to predict financial vulnerability. These measures
               have largely escaped empirical tests. This research note
               describes a test of the Tuckman-Chang financial vulnerability
               measures among a population of nonprofit arts organizations. The
               article concludes that although the Tuckman-Chang measures do
               not have utility for all types of arts nonprofits, the measures
               nonetheless show substantial promise for predicting the closure
               of some nonprofit organizations.",
  journal   = "Nonprofit and Voluntary Sector Quarterly",
  publisher = "SAGE Publications Inc",
  volume    =  30,
  number    =  2,
  pages     = "376--392",
  month     =  jun,
  year      =  2001
}

@ARTICLE{Dambolena1980-ik,
  title   = "Ratio Stability and Corporate Failure",
  author  = "Dambolena, Ismael G and Khoury, Sarkis J",
  journal = "J. Finance",
  volume  =  35,
  number  =  4,
  pages   = "1017--1026",
  month   =  sep,
  year    =  1980
}

@ARTICLE{Edmister1972-fp,
  title     = "An Empirical Test of Financial Ratio Analysis for Small Business
               Failure Prediction",
  author    = "Edmister, Robert O",
  abstract  = "This study develops and empirically tests a number of methods of
               analyzing financial ratios to predict small business failure.
               Although not all of the methods and ratios are predictors of
               failure, many ratio variables are found which do predict failure
               of Small Business Administration borrowers and guarantee
               recipients. Using step-wise multiple discriminant analysis with
               a restriction on the simple correlation of the entering variable
               with the included variables, a function of independent ratio
               variables, which is highly accurate in classifying borrowers in
               the test sample, is developed. Methods of analysis found useful
               are (1) classification of a borrower's ratio into quartiles
               relative to other borrowers in the sample, (2) observation of an
               up- or down-trend for a three-year period, (3) combinatorial
               analysis of a ratio's trend and recent level, (A) calculation of
               the three-year average, and (5) division of a ratio by its
               respective RMA industry average ratio. The discriminant function
               demonstrates an ability as great as those functions recently
               estimated for much larger firms. However, the small business
               function fails to discriminate when only one statement is
               available, whereas Altman [1] and Beaver [4, 5] show that one
               financial statement is sufficient for a highly discriminant
               function for large businesses. This leads the author to qualify
               his conclusion above with the provision that at least three
               consecutive financial statements be available for analysis of a
               small business.",
  journal   = "Journal of Financial and Quantitative Analysis",
  publisher = "Cambridge University Press",
  volume    =  7,
  number    =  2,
  pages     = "1477--1493",
  month     =  mar,
  year      =  1972
}

@BOOK{Block2004-hn,
  title     = "Why Nonprofits Fail: Overcoming Founder's Syndrome, Fundphobia
               and Other Obstacles to Success",
  author    = "Block, Stephen R",
  abstract  = "In Why Nonprofits Fail, author and nonprofit expert StephenBlock
               explains that many well-intentioned leaders hold on to viewsof
               their nonprofit organizations that perpetuate problems
               ratherthan help fix them. According to Block, the first step to
               successis to challenge one's own personal paradigms and ideas
               and be opento unique and alternative approaches to solving
               problems. Thismuch-needed book helps nonprofits get back on
               track and offersadvice about the seven most common stumbling
               blocks, including: Founder's syndromeFundphobiaFinancial
               misfortuneRecruitment disorientationCultural depression in
               nonprofit organizationsSelf-serving political performanceRole
               confusion between the board and executive director",
  publisher = "John Wiley \& Sons",
  month     =  jul,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Carroll2009-zp,
  title     = "Revenue Diversification in Nonprofit Organizations: Does it Lead
               to Financial Stability?",
  author    = "Carroll, Deborah A and Stater, Keely Jones",
  abstract  = "This article investigates whether revenue diversification leads
               to greater stability in the revenue structures of nonprofit
               organizations. Our findings suggest that nonprofits can indeed
               reduce their revenue volatility through diversification,
               particularly by equalizing their reliance on earned income,
               investments, and contributions. This positive effect of
               diversification on revenue stability implies that a diversified
               portfolio encourages more stable revenues and consequently could
               promote greater organizational longevity. Despite any additional
               complexity or crowding out, nonprofit managers may increase the
               financial stability of their organizations by adding additional
               revenue streams. However, our analysis also reveals several
               other important factors that contribute to nonprofit revenue
               stability. In particular, increasing a nonprofit organization's
               total expenses and fund balance reduces volatility, suggesting
               larger nonprofits and organizations with greater growth
               potential experience greater revenue stability. Finally, the
               results suggest nonprofits relying primarily on contributions
               will experience more volatility, whereas nonprofits located
               within urban areas will have more stable revenue structures over
               time.",
  journal   = "J Public Adm Res Theory",
  publisher = "Oxford University Press",
  volume    =  19,
  number    =  4,
  pages     = "947--966",
  month     =  oct,
  year      =  2009
}

@ARTICLE{Chikoto2014-af,
  title     = "Building Nonprofit Financial Capacity: The Impact of Revenue
               Concentration and Overhead Costs",
  author    = "Chikoto, Grace L and Neely, Daniel Gordon",
  abstract  = "Building on the impressive body of research on issues of
               nonprofit revenue choice and mix, this research empirically
               tests Foster and Fine?s claim that revenue concentration
               contributes to the growth of nonprofit organizations. Using
               National Center for Charitable Statistics (NCCS) digitized data
               (1998-2003), the authors test whether revenue concentration is a
               viable revenue-generating strategy that can help bolster a
               nonprofit?s financial capacity. Overall, study findings refute
               the mythology of revenue diversification; the authors find that
               implementing a revenue concentration strategy generates a
               positive growth in one?s financial capacity?in particular, a
               growth in one?s total revenue, over time. Contrary to the
               prevalent charges laid at the door of high administrative and
               fundraising efforts by some, the authors find that in order to
               support financial capacity growth, nonprofits must make positive
               investments in favor of administrative and fundraising support
               but not in the form of high executive salaries.",
  journal   = "Nonprofit and Voluntary Sector Quarterly",
  publisher = "SAGE Publications Inc",
  volume    =  43,
  number    =  3,
  pages     = "570--588",
  month     =  jun,
  year      =  2014
}

@ARTICLE{Helmig2014-sa,
  title     = "Success and Failure of Nonprofit Organizations: Theoretical
               Foundations, Empirical Evidence, and Future Research",
  author    = "Helmig, Bernd and Ingerfurth, Stefan and Pinz, Alexander",
  abstract  = "Success and failure of nonprofit organizations (NPOs) have been
               prominent themes in the nonprofit community for more than 30
               years. However, since there is no common understanding on
               success and failure of NPOs, the research field is still
               fragmented. Drawing from research on organizational success and
               failure in the for-profit context as a theoretical background,
               this paper systemizes the academic knowledge on NPO success and
               failure. By shedding light on theoretical approaches used,
               empirical evidence on the determinants of these constructs, and
               the sectors analyzed most frequently in this regard, the paper
               develops an instructive research agenda concerning studies on
               success and failure of NPOs.",
  journal   = "Voluntas",
  publisher = "Springer US",
  volume    =  25,
  number    =  6,
  pages     = "1509--1538",
  month     =  dec,
  year      =  2014,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Reisz2007-hx,
  title    = "A market-based framework for bankruptcy prediction",
  author   = "Reisz, Alexander S and Perlich, Claudia",
  abstract = "We estimate probabilities of bankruptcy for 5784 industrial firms
              in the period 1988--2002 in a model where common equity is viewed
              as a down-and-out barrier option on the firm's assets. Asset
              values and volatilities as well as firm-specific bankruptcy
              barriers are simultaneously backed out from the prices of traded
              equity. Implied barriers are significantly positive and monotonic
              in the firm's leverage and asset volatility. Our default
              probabilities display better calibration and discriminatory power
              than the ones inferred in a standard Black and Scholes [Black,
              F., Scholes, M., 1973. The pricing of options and corporate
              liabilities. J. Pol. Econ. 81, 637--659]/Merton [Merton, R.C.,
              1974. On the pricing of corporate debt: the risk structure of
              interest rates. J. Finance 29, 449--470] and KMV frameworks.
              However, accounting-based measures such as Altman Z- and
              Z‚Ä≥-scores outperform structural models in 1-year-ahead bankruptcy
              predictions, but lose relevance as the forecast horizon is
              extended.",
  journal  = "Journal of Financial Stability",
  volume   =  3,
  number   =  2,
  pages    = "85--131",
  month    =  jul,
  year     =  2007,
  keywords = "Probability of default; Structural models; Barrier option;
              Discriminatory power; Recalibration"
}

@ARTICLE{Tsai2013-vj,
  title   = "A Meta-learning Framework for Bankruptcy Prediction",
  author  = "Tsai, Chih-Fong and Hsu, Yu-Feng",
  journal = "J. Forecast.",
  volume  =  32,
  number  =  2,
  pages   = "167--179",
  month   =  mar,
  year    =  2013
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Agarwal2007-bo,
  title     = "Twenty‚Äêfive years of the Taffler z‚Äêscore model: Does it really
               have predictive ability?",
  author    = "Agarwal, Vineet and Taffler, Richard J",
  abstract  = "Abstract Although copious statistical failure prediction models
               are described in the literature, appropriate tests of whether
               such methodologies really work in practice are lacking.
               Validation exercises typically use small samples of non?failed
               firms and are not true tests of ex ante predictive ability, the
               key issue of relevance to model users. This paper provides the
               operating characteristics of the well?known Taffler (1983)
               UK?based z?score model for the first time and evaluates its
               performance over the 25?year period since it was originally
               developed. The model is shown to have clear predictive ability
               over this extended time period and dominates more na{\"\i}ve
               prediction approaches. This study also illustrates the economic
               value to a bank of using such methodologies for default risk
               assessment purposes. Prima facie, such results also demonstrate
               the predictive ability of the published accounting numbers and
               associated financial ratios used in the z?score model
               calculation.",
  journal   = "Accounting and Business Research",
  publisher = "Routledge",
  volume    =  37,
  number    =  4,
  pages     = "285--300",
  month     =  dec,
  year      =  2007
}

@ARTICLE{Clement2010-oo,
  title   = "The term'macroprudential': origins and evolution",
  author  = "Clement, Piet",
  journal = "BIS Quarterly Review, March",
  year    =  2010
}

@ARTICLE{Sirignano2018-dt,
  title    = "Deep learning for limit order books",
  author   = "Sirignano, J A",
  abstract = "This paper develops a new neural network architecture for
              modeling spatial distributions (i.e. distributions on (Formula
              presented.)) which is more computationally efficient than a
              traditional fully-connected feedforward architecture. The design
              of the architecture takes advantage of the specific structure of
              limit order books. The new architecture, which we refer to as a
              `spatial neural network', yields a low-dimensional model of price
              movements deep into the limit order book, allowing more effective
              use of information from deep in the limit order book (i.e. many
              levels beyond the best bid and best ask). The spatial neural
              network models the joint distribution of the state of the limit
              order book at a future time conditional on the current state of
              the limit order book. The spatial neural network outperforms
              status quo models such as the naive empirical model, logistic
              regression (with nonlinear features), and a standard neural
              network architecture. Both neural networks strongly outperform
              the logistic regression model. Due to its more effective use of
              information deep in the limit order book, the spatial neural
              network especially outperforms the standard neural network in the
              tail of the distribution, which is important for risk management
              applications. The models are trained and tested on nearly 500
              U.S. stocks. Techniques from deep learning such as dropout are
              employed to improve performance. Due to the significant
              computational challenges associated with the large amount of
              data, models are trained with a cluster of 50 GPUs. \copyright{}
              2018, \copyright{} 2018 Informa UK Limited, trading as Taylor \&
              Francis Group.",
  journal  = "Quant. Finance",
  year     =  2018,
  keywords = "Big data; Data science; Deep learning; High-frequency; Limit
              order market; Machine learning; Order book"
}

@UNPUBLISHED{Casgrain2018-le,
  title    = "Trading algorithms with learning in latent alpha models",
  author   = "Casgrain, P and Jaimungal, S",
  abstract = "Alpha signals for statistical arbitrage strategies are often
              driven by latent factors. This paper analyzes how to optimally
              trade with latent factors that cause prices to jump and diffuse.
              Moreover, we account for the effect of the trader's actions on
              quoted prices and the prices they receive from trading. Under
              fairly general assumptions, we demonstrate how the trader can
              learn the posterior distribution over the latent states, and
              explicitly solve the latent optimal trading problem. We provide a
              verification theorem, and a methodology for calibrating the model
              by deriving a variation of the expectation--maximization
              algorithm. To illustrate the efficacy of the optimal strategy, we
              demonstrate its performance through simulations and compare it to
              strategies that ignore learning in the latent factors. We also
              provide calibration results for a particular model using Intel
              Corporation stock as an example. \copyright{} 2018 Wiley
              Periodicals, Inc.",
  journal  = "Mathematical Finance",
  year     =  2018,
  keywords = "algorithmic trading; C40; C61; G11; latent alpha; machine
              learning; partial information; statistical arbitrage; stochastic
              control"
}

@ARTICLE{Turner2018-gk,
  title     = "Bitcoin Transactions: A digital discovery of illicit activity on
               the blockchain",
  author    = "Turner, A and Irwin, A S M",
  abstract  = "Purpose-The purpose of this paper is to determine if Bitcoin
               transactions could be de-anonymised by analysing the Bitcoin
               blockchain and transactions conducted through the blockchain. In
               addition, graph analysis and the use of modern social media
               technology were examined to determine how they may help reveal
               the identity of Bitcoin users. A review of machine learning
               techniques and heuristics was carried out to learn how certain
               behaviours from the Bitcoin network could be augmented with
               social media technology and other data to identify illicit
               transactions. Design/methodology/approach-A number of
               experiments were conducted and time was spend observing the
               network to ascertain how Bitcoin transactions work, how the
               Bitcoin protocol operates over the network and what Bitcoin
               artefacts can be examined from a digital forensics perspective.
               Packet sniffing software, Wireshark, was used to see whether the
               identity of a user is revealed when they set up a wallet via an
               online wallet service. In addition, a block parser was used to
               analyse the Bitcoin client synchronisation and reveal
               information on the behaviour of a Bitcoin node when it joins the
               network and synchronises to the latest blockchain. The final
               experiment involved setting up and witnessing a transaction
               using the Bitcoin Client API. These experiments and observations
               were then used to design a proof of concept and functional
               software architecture for searching, indexing and analyzing
               publicly available data flowing from the blockchain and other
               big data sources. Findings-Using heuristics and graph analysis
               techniques show us that it is possible to build up a picture of
               behaviour of Bitcoin addresses and transactions, then utilise
               existing typologies of illicit behaviour to collect, process and
               exploit potential red flag indicators. Augmenting Bitcoin data,
               big data and social media may be used to reveal potentially
               illicit financial transaction going through the Bitcoin
               blockchain and machine learning applied to the data sets to rank
               and cluster suspicious transactions. Originality/value-The
               development of a functional software architecture that, in
               theory, could be used to detect suspicious illicit transactions
               on the Bitcoin network. \copyright{} 2017 Emerald Publishing
               Limited.",
  journal   = "Journal of Financial Crime",
  publisher = "Emerald Group Publishing Ltd.",
  volume    =  25,
  number    =  1,
  pages     = "109--130",
  year      =  2018,
  keywords  = "Bitcoin Blockchain; Functional Architecture; Heuristics; Illicit
               Transactions; Machine Learning; Social Media"
}

@UNPUBLISHED{Carmona2018-gg,
  title    = "Predicting failure in the {U.S}. banking sector: An extreme
              gradient boosting approach",
  author   = "Carmona, P and Climent, F and Momparler, A",
  abstract = "Banks play a central role in developed economies. Consequently,
              systemic banking crises destabilize financial markets and hamper
              global economic growth. In this study, extreme gradient boosting
              was used to predict bank failure in the U.S. banking sector. Key
              variables were identified to anticipate and prevent bank
              defaults. The data, which spanned the period 2001 to 2015,
              consisted of annual series of 30 financial ratios for 156 U.S.
              national commercial banks. Identifying leading indicators of bank
              failure is vital to help regulators and bank managers act swiftly
              before distressed financial institutions reach the point of no
              return. The findings indicate that lower values for retained
              earnings to average equity, pretax return on assets, and total
              risk-based capital ratio are associated with a higher risk of
              bank failure. In addition, an exceedingly high yield on earning
              assets increases the chance of bank financial distress.
              \copyright{} 2018 Elsevier Inc.",
  journal  = "International Review of Economics and Finance",
  year     =  2018,
  keywords = "Bank failure prediction; Bank failure prevention; Bank financial
              distress; Extreme gradient boosting; Machine learning; XGBoost"
}

@ARTICLE{Hesterberg2015-el,
  title    = "What Teachers Should Know About the Bootstrap: Resampling in the
              Undergraduate Statistics Curriculum",
  author   = "Hesterberg, Tim C",
  abstract = "Bootstrapping has enormous potential in statistics education and
              practice, but there are subtle issues and ways to go wrong. For
              example, the common combination of nonparametric bootstrapping
              and bootstrap percentile confidence intervals is less accurate
              than using t-intervals for small samples, though more accurate
              for larger samples. My goals in this article are to provide a
              deeper understanding of bootstrap methods-how they work, when
              they work or not, and which methods work better-and to highlight
              pedagogical issues. Supplementary materials for this article are
              available online. [Received December 2014. Revised August 2015].",
  journal  = "Am. Stat.",
  volume   =  69,
  number   =  4,
  pages    = "371--386",
  month    =  oct,
  year     =  2015,
  keywords = "Bias; Confidence intervals; Sampling distribution; Standard
              error; Statistical concepts; Teaching.",
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bendavid2018-hh,
  title    = "Do {ETFs} Increase Volatility?",
  author   = "Ben‚Äêdavid, Itzhak and Franzoni, Francesco and Moussawi, Rabih",
  abstract = "Due to their low trading costs, exchange‚Äêtraded funds (ETFs) are
              a potential catalyst for short‚Äêhorizon liquidity traders. The
              liquidity shocks can propagate to the underlying securities
              through the arbitrage channel, and ETFs may increase the
              nonfundamental volatility of the securities in their baskets. We
              exploit exogenous changes in index membership and find that
              stocks with higher ETF ownership display significantly higher
              volatility. ETF ownership increases the negative autocorrelation
              in stock prices. The increase in volatility appears to introduce
              undiversifiable risk in prices because stocks with high ETF
              ownership earn a significant risk premium of up to 56 basis
              points monthly.",
  journal  = "J. Finance",
  volume   =  73,
  number   =  6,
  pages    = "2471--2535",
  month    =  dec,
  year     =  2018
}

@ARTICLE{Du_Jardin2016-hy,
  title    = "A two-stage classification technique for bankruptcy prediction",
  author   = "du Jardin, Philippe",
  abstract = "Ensemble techniques such as bagging or boosting, which are based
              on combinations of classifiers, make it possible to design models
              that are often more accurate than those that are made up of a
              unique prediction rule. However, the performance of an ensemble
              solely relies on the diversity of its different components and,
              ultimately, on the algorithm that is used to create this
              diversity. It means that such models, when they are designed to
              forecast corporate bankruptcy, do not incorporate or use any
              explicit knowledge about this phenomenon that might supplement or
              enrich the information they are likely to capture. This is the
              reason why we propose a method that is precisely based on some
              knowledge that governs bankruptcy, using the concept of
              ``financial profiles'', and we show how the complementarity
              between this technique and ensemble techniques can improve
              forecasts.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  254,
  number   =  1,
  pages    = "236--252",
  month    =  oct,
  year     =  2016,
  keywords = "Decision support systems; Finance; Bankruptcy; Forecasting;
              Financial profile"
}

@BOOK{Taleb2007-fz,
  title     = "The black swan: The impact of the highly improbable",
  author    = "Taleb, Nassim Nicholas",
  publisher = "Random house",
  volume    =  2,
  year      =  2007
}

@UNPUBLISHED{Jensen1978-sb,
  title    = "Some Anomalous Evidence Regarding Market Efficiency",
  author   = "Jensen, Michael C",
  abstract = "The efficient market hypothesis has been widely tested and, with
              few exceptions, found consistent with the data in a wide variety
              of markets: the New York and American Stock Exchanges, the
              Australian, English, and German stock markets, various commodity
              futures markets, the Over-the-Counter markets, the corporate and
              government bond markets, the option market, and the market for
              seats on the New York Stock Exchange. Yet, in a manner remarkably
              similar to that described by Thomas Kuhn in his book, The
              Structure of Scientific Revolutions, we seem to be entering a
              stage where widely scattered and as yet incohesive evidence is
              arising which seems to be inconsistent with the theory. As better
              data become available (e.g., daily stock price data) and as our
              econometric sophistication increases, we are beginning to find
              inconsistencies that our cruder data and techniques missed in the
              past. It is evidence which we will not be able to ignore. The
              purpose of this special issue of the Journal of Financial
              Economics is to bring together a number of these scattered pieces
              of anomalous evidence regarding Market Efficiency. As Ball (1978)
              points out in his survey article: taken individually many
              scattered pieces of evidence on the reaction of stock prices to
              earnings announcements which are inconsistent with the theory
              don't amount to much. Yet viewed as a whole, these pieces of
              evidence begin to stack up in a manner which make a much stronger
              case for the necessity to carefully review both our acceptance of
              the efficient market theory and our methodological procedures.",
  month    =  may,
  year     =  1978,
  keywords = "market efficiency, Efficient Market Theory, theory of 'random
              walks', rational expectations theory, abnormal returns, asset
              pricing model"
}

@ARTICLE{Lehmann1990-za,
  title     = "Fads, Martingales, and Market Efficiency",
  author    = "Lehmann, Bruce N",
  abstract  = "Predictable variation in equity returns might reflect either (1)
               predictable changes in expected returns or (2) market
               inefficiency and stock price ``overreaction.'' These
               explanations can be distinguished by examining returns over
               short time intervals since systematic changes in fundamental
               valuation over intervals like a week should not occur in
               efficient markets. The evidence suggests that the ``winners''
               and ``losers'' one week experience sizeable return reversals the
               next week in a way that reflects apparent arbitrage profits
               which persist after corrections for bid-ask spreads and
               plausible transactions costs. This probably reflects
               inefficiency in the market for liquidity around large price
               changes.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  105,
  number    =  1,
  pages     = "1--28",
  month     =  feb,
  year      =  1990
}

@ARTICLE{Seyhun1986-ep,
  title    = "Insiders' profits, costs of trading, and market efficiency",
  author   = "Seyhun, H Nejat",
  abstract = "This study investigates the anomalous findings of the previous
              insider trading studies that any investor can earn abnormal
              profits by reading the Official Summary. Availability of abnormal
              profits to insiders, availability of abnormal profits to
              outsiders who imitate insiders, determinants of insiders'
              predictive ability, and effect of insider trading on costs of
              trading for other investors are examined by using approximately
              60,000 insider sale and purchase transactions from 1975 to 1981.
              Implications for market efficiency and evaluation of abnormal
              profits to active trading strategies are discussed.",
  journal  = "J. financ. econ.",
  volume   =  16,
  number   =  2,
  pages    = "189--212",
  month    =  jun,
  year     =  1986
}

@INCOLLECTION{Schwert2003-ys,
  title     = "Chapter 15 Anomalies and market efficiency",
  booktitle = "Handbook of the Economics of Finance",
  author    = "Schwert, G William",
  abstract  = "Anomalies are empirical results that seem to be inconsistent
               with maintained theories of asset-pricing behavior. They
               indicate either market inefficiency (profit opportunities) or
               inadequacies in the underlying asset-pricing model. After they
               are documented and analyzed in the academic literature,
               anomalies often seem to disappear, reverse, or attenuate. This
               raises the question of whether profit opportunities existed in
               the past, but have since been arbitraged away, or whether the
               anomalies were simply statistical aberrations that attracted the
               attention of academics and practitioners. One of the interesting
               findings from the empirical work in this chapter is that many of
               the well-known anomalies in the finance literature do not hold
               up in different sample periods. In particular, the size effect
               and the value effect seem to have disappeared after the papers
               that highlighted them were published. At about the same time,
               practitioners began investment vehicles that implemented the
               strategies implied by the academic papers. The weekend effect
               and the dividend yield effect also seem to have lost their
               predictive power after the papers that made them famous were
               published. In these cases, however, I am not aware of any
               practitioners who have tried to use these anomalies as a major
               basis of their investment strategy. The small-firm
               turn-of-the-year effect became weaker in the years after it was
               first documented in the academic literature, although there is
               some evidence that it still exists. Interestingly, however, it
               does not seem to exist in the portfolio returns of practitioners
               who focus on small-capitalization firms. Likewise, the evidence
               that stock market returns are predictable using variables such
               as dividend yields or inflation is much weaker in the periods
               after the papers that documented these findings were published.
               All of these findings raise the possibility that anomalies are
               more apparent than real. The notoriety associated with the
               findings of unusual evidence tempts authors to further
               investigate puzzling anomalies and later to try to explain them.
               But even if the anomalies existed in the sample period in which
               they were first identified, the activities of practitioners who
               implement strategies to take advantage of anomalous behavior can
               cause the anomalies to disappear (as research findings cause the
               market to become more efficient).",
  publisher = "Elsevier",
  volume    =  1,
  pages     = "939--974",
  month     =  jan,
  year      =  2003,
  keywords  = "market efficiency; anomaly; size effect; value effect; selection
               bias; momentum"
}

@ARTICLE{Finnerty1976-zz,
  title   = "{INSIDERS} {AND} {MARKET} {EFFICIENCY}",
  author  = "Finnerty, Joseph E",
  journal = "J. Finance",
  volume  =  31,
  number  =  4,
  pages   = "1141--1148",
  month   =  sep,
  year    =  1976
}

@ARTICLE{Chiras1978-nj,
  title    = "The information content of option prices and a test of market
              efficiency",
  author   = "Chiras, Donald P and Manaster, Steven",
  abstract = "The Black-Scholes option pricing model, as generalized for
              dividend payments by Merton, is used to calculate implied
              variances of future stock returns. These variances are found to
              be better predictors of future stock return variances than those
              obtained from historic stock price data. A trading strategy is
              developed that exploits the informational content of the implied
              variances. The trading strategy, contrary to the efficient market
              hypothesis, produces abnormally high returns.",
  journal  = "J. financ. econ.",
  volume   =  6,
  number   =  2,
  pages    = "213--234",
  month    =  jun,
  year     =  1978
}

@ARTICLE{Cohen1972-ov,
  title   = "{THE} {VALUATION} {OF} {OPTION} {CONTRACTS} {AND} A {TEST} {OF}
             {MARKET} {EFFICIENCY}",
  author  = "Cohen, Jerome B and Black, Fischer and Scholes, Myron",
  journal = "J. Finance",
  volume  =  27,
  number  =  2,
  pages   = "399--417",
  month   =  may,
  year    =  1972
}

@UNPUBLISHED{Glosten2016-ja,
  title       = "{ETF} Activity and Informational Efficiency of Underlying
                 Securities",
  author      = "Glosten, Lawrence R and Nallareddy, Suresh and Zou, Yuan",
  abstract    = "This paper investigates the effect of exchange-traded funds'
                 (ETF) activity on the informational efficiency of their
                 underlying securities. We find that ETF activity increases
                 informational efficiency for stocks with weak information
                 environments and for stocks with imperfectly competitive
                 equity markets. The increase in informational efficiency
                 results from the timely incorporation of systematic earnings
                 information. In contrast, we find no such effect for stocks
                 with stronger information environments and stocks with
                 perfectly competitive equity markets. ETF activity increases
                 return co-movement, and this increase is partly attributable
                 to the timely incorporation of systematic earnings
                 information. Using S\&P 500 index additions and deletions as
                 an additional setting and Russell 1000/2000 index
                 reconstitution as an identification, we corroborate our main
                 findings.",
  journal     = "Columbia Business School Research Paper No. 16-71",
  series      = "Columbia Business School Research Paper No. 16-71",
  institution = "Columbia Business School Research Paper No. 16-71",
  month       =  dec,
  year        =  2016,
  keywords    = "ETF, Information Efficiency"
}

@ARTICLE{Menkveld2018-tz,
  title     = "The Flash Crash: A Cautionary Tale About Highly Fragmented
               Markets",
  author    = "Menkveld, Albert J and Yueshen, Bart Zhou",
  abstract  = "A breakdown of cross-market arbitrage activity could make
               markets more fragile and result in price crashes. We provide
               suggestive evidence for this novel channel based on a
               high-frequency analysis of the most salient crash in recent
               history: The Flash Crash. We further show that such an event can
               be extremely costly for a large seller trading in a particular
               venue as the seller effectively relies on local liquidity supply
               only. These findings highlight the vulnerability of today?s
               highly fragmented markets. The online appendix is available at
               https://doi.org/10.1287/mnsc.2018.3040. This paper was accepted
               by Gustavo Manso, finance.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  month     =  nov,
  year      =  2018
}

@ARTICLE{Akansu2017-en,
  title    = "The Flash Crash: A Review",
  author   = "Akansu, Ali",
  abstract = "Currently, there is a consensus among practitioners, industry
              veterans and academic researchers that the use of the
              state-of-the-art technology in financial sector including trading
              is inevitable, and HFT is generally beneficial. On the other
              hand, our collective memory reminds us that any intentional or
              unintentional misuse of HFT has the risk of market collapse as
              experienced during the Flash Crash of May 6, 2010. This paper
              provides an overview of what happened in the financial markets
              within a few minutes on that day and the collapse happened with
              its historically unmatched impact. Although the underlying
              reasons that triggered the Flash Crash are well understood by
              traders, regulators and researchers and tangible progress has
              been achieved since then, but still there are crucially
              significant open issues requiring more sophisticated policies,
              procedures and regulations to build more robust, fair and
              transparent financial markets.",
  journal  = "Journal of Capital Market Studies",
  month    =  nov,
  year     =  2017,
  keywords = "Algorithmic trading, Electronic trading, High-frequency trading
              (HFT), Limit order book (LOB), The flash crash, LOB imbalance,
              Security Information Processor (SIP), National best bid and offer
              (NBBO), Regulation national market system (Reg NMS), United
              States Securities and Exchange Commission (SEC)"
}

@ARTICLE{Giles2012-iq,
  title    = "Flash crash forecast",
  author   = "Giles, Jim",
  abstract = "A strange feature of high-speed trading could help warn of
              ``black swans'' in the stock market",
  journal  = "New Sci.",
  volume   =  213,
  number   =  2852,
  pages    = "21",
  month    =  feb,
  year     =  2012
}

@UNPUBLISHED{Kirilenko2018-cx,
  title    = "Automation, Intermediation and the Flash Crash",
  author   = "Kirilenko, Andrei A and Kyle, Albert S and Samadi, Mehrdad and
              Tuzun, Tugkan",
  abstract = "The Flash Crash of May 6, 2010, shook the confidence of market
              participants and raised questions about the market structure of
              electronic markets. In these markets, intraday intermediation has
              been increasingly provided by market participants without formal
              obligations to do so. We examine intraday intermediation in the
              E-mini S\&P 500 stock index futures market before and during the
              Flash Crash. We discuss the evolution of trading from human to
              electronic environments and the implications of our results for
              market design.",
  month    =  feb,
  year     =  2018,
  keywords = "High-Frequency, Automation, Volatility, Flash Crash,
              Intermediation, Market Making"
}

@ARTICLE{Braun2018-yk,
  title    = "Impact and recovery process of mini flash crashes: An empirical
              study",
  author   = "Braun, Tobias and Fiegen, Jonas A and Wagner, Daniel C and
              Krause, Sebastian M and Guhr, Thomas",
  abstract = "In an Ultrafast Extreme Event (or Mini Flash Crash), the price of
              a traded stock increases or decreases strongly within
              milliseconds. We present a detailed study of Ultrafast Extreme
              Events in stock market data. In contrast to popular belief, our
              analysis suggests that most of the Ultrafast Extreme Events are
              not necessarily due to feedbacks in High Frequency Trading: In at
              least 60 percent of the observed Ultrafast Extreme Events, the
              largest fraction of the price change is due to a single market
              order. In times of financial crisis, large market orders are more
              likely which leads to a significant increase of Ultrafast Extreme
              Events occurrences. Furthermore, we analyze the 100 trades
              following each Ultrafast Extreme Events. While we observe a
              tendency of the prices to partially recover, less than 40 percent
              recover completely. On the other hand we find 25 percent of the
              Ultrafast Extreme Events to be almost recovered after only one
              trade which differs from the usually found price impact of market
              orders.",
  journal  = "PLoS One",
  volume   =  13,
  number   =  5,
  pages    = "e0196920",
  month    =  may,
  year     =  2018,
  language = "en"
}

@ARTICLE{Easley2011-mz,
  title     = "The microstructure of the flash crash: Flow toxicity, liquidity
               crashes and the probability of informed trading",
  author    = "Easley, David and De Prado, M M Lopez and O'Hara, Maureen",
  journal   = "Journal of Portfolio Management",
  publisher = "Euromoney Institutional Investor PLC",
  volume    =  37,
  number    =  2,
  pages     = "118--128",
  year      =  2011
}

@ARTICLE{Miller1986-an,
  title     = "Financial Innovation: The Last Twenty Years and the Next",
  author    = "Miller, Merton H",
  abstract  = "The word revolution is entirely appropriate for describing the
               changes in financial institutions and instruments that have
               occurred in the past twenty years. The major impulses to
               successful financial innovations have come from regulations and
               taxes. The outlook for the future is for a slowing down of the
               rate of financial innovation, but much growth and improvement
               are still in prospect.",
  journal   = "Journal of Financial and Quantitative Analysis",
  publisher = "Cambridge University Press",
  volume    =  21,
  number    =  4,
  pages     = "459--471",
  month     =  dec,
  year      =  1986
}

@BOOK{Allen2012-fg,
  title     = "Financial Innovation (Collection)",
  author    = "Allen, Franklin and Yago, Glenn and Barth, James",
  abstract  = "Sustainable, responsible financial innovation: lessons from the
               crisis, and new paths to global prosperity After the global
               financial crisis, responsible financial innovation is more
               crucial than ever. However, financial innovation will only
               succeed if it reflects the true lessons of the past decade. In
               this collection, three leading global finance researchers share
               those lessons, offering crucial insights for market
               participants, policymakers, and other stakeholders. Drawing on
               their pioneering work, they illuminate new opportunities for
               sustainable innovation in finance that can help restore housing
               markets and the overall global economy, while avoiding the
               failures of predecessors. In Financing the Future, Franklin
               Allen and Glenn Yago carefully discuss the current role of
               financial innovation in capitalizing businesses, industries,
               breakthrough technologies, housing solutions, medical
               treatments, and environmental projects. Allen and Yago explain
               how sophisticated capital structures can enable companies and
               individuals to raise funding in larger amounts for longer terms
               at lower cost, accomplishing tasks that would otherwise be
               impossible -- and offer a full chapter of essential lessons for
               using financial innovation to add value, manage risk, and
               improve the stability of the global economy. Next, in Fixing the
               Housing Market, Allen, Yago, and James R. Barth explain how
               responsible financial innovation can ``reboot'' damaged housing
               markets, improve their efficiency, and make housing more
               accessible to millions. The authors walk through the history of
               housing finance, evaluate housing finance systems in mature
               economies during and after the crisis, highlight benefits and
               risks associated with each leading mortgage funding structure
               and product, and assess current housing finance structures in
               BRIC economies. Building on these comparisons, they show how to
               create a more stable and sustainable financing system for
               housing: one that provides better shelter for more people, helps
               the industry recover, and creates thousands of new jobs. From
               world-renowned leaders and experts Franklin Allen, Glenn Yago,
               and James R. Barth",
  publisher = "FT Press",
  month     =  jun,
  year      =  2012,
  language  = "en"
}

@BOOK{Wendt2018-nv,
  title     = "Sustainable Financial Innovation",
  author    = "Wendt, Karen",
  abstract  = "The purpose of doing business is not business : it is
               flourishing / Diego Hangartner -- Environmental impact finance
               -- Investment turn around : a new way of investing based on SDGs
               / Karen Wendt -- Innovations in the financing of energy
               efficiency / Steven Fawkes -- Green bonds : for you to advertise
               only? : a compact guide through the current state of the green
               bond -- Market and the conditions for its further development /
               Stefan Klotz and Sabine Pex -- Pensions at a time of climate
               change : the inconvenient truth about fiduciary duty / Bethan
               Livesey -- Innovative 21st century water utilities / Robert C.
               Brears -- Social impact finance -- Social impact incentives
               (SIINC) : enabling high-impact social enterprises to improve
               profitability and reach scale / Bjoern Struewer, Rory Tews and
               Christina Moehrle -- University Magna Graecia of Catanzaro
               (Italy) : social impact bond: beyond financial innovation /
               Rosella Car{\`e} -- Swiss investments for development :
               characteristics of a market with strong growth dynamics / Julia
               Meyer and Kelly Hess -- Social entrepreneurship and technology
               -- Sitting at the edge and looking for a way to create scaled
               and meaningful impact / Tanya Woods -- Stride, unschool for
               entrepreneurs : from doing well to doing good / Anais Saegesser
               -- Triple purpose: social capitalism: how capitalism can be
               transformed to make the world a better place / Jim Bignal --
               Regional focus ESG in China -- Regulating CRS disclosure :
               quantity or quality in practice? / Shidi Dong, Lei Xu and Ron
               McIver -- Political connections, ownership structure and
               performance in China's mining sector / Lei Xu, Ron P. McIver,
               Shiao-Lan Chou and Harjap Bassan -- Addressing corporate income
               tax avoidance in China : using regulatory change to encourage
               corporate tax sustainability / Guodong Yuan, Ron P. McIver, Lei
               Xu and Sang Hong Kang -- Index",
  publisher = "Taylor \& Francis",
  month     =  oct,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Duffie1995-dl,
  title    = "Financial Market Innovation and Security Design: An Introduction",
  author   = "Duffie, Darrell and Rahi, Rohit",
  abstract = "The article describes, in the context of theory and practice,
              this special issue of the Journal of Economic Theory on financial
              market innovation and security design. The main focus is on
              security design in incomplete financial markets, possibly with
              asymmetrically informed traders. We first survey the general
              equilibrium literature, which emphasizes the spanning role of
              securities. We then provide a unified framework encompassing the
              literature that employs a CARA-Guassian setting to study the
              impact of financial innovation on risk-sharing and information
              aggregation. Journal of Economic Literature Classification
              Numbers G10, D52.",
  journal  = "J. Econ. Theory",
  volume   =  65,
  number   =  1,
  pages    = "1--42",
  month    =  feb,
  year     =  1995
}

@ARTICLE{Grinblatt2000-vl,
  title    = "Financial Innovation and the Role of Derivative Securities: An
              Empirical Analysis of the Treasury {STRIPS} Program",
  author   = "Grinblatt, Mark and Longstaff, Francis A",
  abstract = "The role that financial innovation plays in financial markets is
              very controversial. To provide insight into this role, we examine
              how market participants use the highly successful Treasury STRIPS
              program. We find that investors use the option to create
              Treasury-derivative STRIPS primarily to make markets more
              complete and take advantage of tax and accounting asymmetries.
              Although liquidity-related factors help explain differences in
              the prices of Treasury bonds and STRIPS, we find little evidence
              that the option to strip and reconstitute securities is used for
              speculative or arbitrage-related purposes.",
  journal  = "J. Finance",
  volume   =  55,
  number   =  3,
  pages    = "1415--1436",
  month    =  jun,
  year     =  2000
}

@ARTICLE{Gennaioli2012-pt,
  title    = "Neglected risks, financial innovation, and financial fragility",
  author   = "Gennaioli, Nicola and Shleifer, Andrei and Vishny, Robert",
  abstract = "We present a standard model of financial innovation, in which
              intermediaries engineer securities with cash flows that investors
              seek, but modify two assumptions. First, investors (and possibly
              intermediaries) neglect certain unlikely risks. Second, investors
              demand securities with safe cash flows. Financial intermediaries
              cater to these preferences and beliefs by engineering securities
              perceived to be safe but exposed to neglected risks. Because the
              risks are neglected, security issuance is excessive. As investors
              eventually recognize these risks, they fly back to the safety of
              traditional securities and markets become fragile, even without
              leverage, precisely because the volume of new claims is
              excessive.",
  journal  = "J. financ. econ.",
  volume   =  104,
  number   =  3,
  pages    = "452--468",
  month    =  jun,
  year     =  2012,
  keywords = "Banks; Local thinking; Crisis"
}

@BOOK{Laeven2014-ak,
  title     = "Bank Size and Systemic Risk",
  author    = "Laeven, Mr Luc and Ratnovski, Lev and Tong, Hui",
  abstract  = "The proposed SDN documents the evolution of bank size and
               activities over the past 20 years. It discusses whether this
               evolution can be explained by economies of scale or ``too big to
               fail'' subsidies. The paper then presents evidence on the extent
               to which bank size and market-based activities contribute to
               systemic risk. The paper concludes with policy messages in the
               area of capital regulation and activity restrictions to reduce
               the systemic risk posed by large banks. The analysis of the
               paper complements earlier Fund work, including SDN 13/04 and the
               recent GFSR chapter on ``too big to fail'' subsidies, and its
               policy message is in line with this earlier work.",
  publisher = "International Monetary Fund",
  month     =  may,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Zhou2018-ik,
  title     = "{P2P} Network Lending, Loss Given Default and Credit Risks",
  author    = "Zhou, Guangyou and Zhang, Yijia and Luo, Sumei",
  abstract  = "Peer-to-peer (P2P) network lending is a new mode of internet
               finance that still holds credit risk as its main risk. According
               to the internal rating method of the New Basel Accord, in
               addition to the probability of default, loss given default is
               also one of the important indicators of evaluation credit risks.
               Proceeding from the perspective of loss given default (LGD),
               this paper conducts an empirical study on the probability
               distribution of LGDs of P2P as well as its influencing factors
               with the transaction data of Lending Club. The results show
               that: (1) the LGDs of P2P loans presents an obvious unimodal
               distribution, the peak value is relatively high and tends to
               concentrate with the decrease of the borrower's credit rating,
               indicating that the distribution of LGDs of P2P lending is
               similar to that of unsecured bonds; (2) The total asset of the
               borrower has no significant impact on LGD, the credit rating and
               the debt-to-income ratio exert a significant negative impact,
               while the term and amount of the loan produce a relatively
               strong positive impact. Therefore, when evaluating the
               borrower's repayment ability, it is required to pay more
               attention to its assets structure rather than the size of its
               total assets. When carrying out risk control for the P2P
               platform, it is necessary to give priority to the control of
               default rate.",
  journal   = "Sustain. Sci. Pract. Policy",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  10,
  number    =  4,
  pages     = "1010",
  month     =  mar,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Law2017-mt,
  title     = "Practical Bayesian support vector regression for financial time
               series prediction and market condition change detection",
  author    = "Law, T and Shawe-Taylor, J",
  abstract  = "Support vector regression (SVR) has long been proven to be a
               successful tool to predict financial time series. The core idea
               of this study is to outline an automated framework for achieving
               a faster and easier parameter selection process, and at the same
               time, generating useful prediction uncertainty estimates in
               order to effectively tackle flexible real-world financial time
               series prediction problems. A Bayesian approach to SVR is
               discussed, and implemented. It is found that the direct
               implementation of the probabilistic framework of Gao et al.
               returns unsatisfactory results in our experiments. A novel
               enhancement is proposed by adding a new kernel scaling parameter
               $\mu$ to overcome the difficulties encountered. In addition, the
               multi-armed bandit Bayesian optimization technique is applied to
               automate the parameter selection process. Our framework is then
               tested on financial time series of various asset classes (i.e.
               equity index, credit default swaps spread, bond yields, and
               commodity futures) to ensure its flexibility. It is shown that
               the generalization performance of this parameter selection
               process can reach or sometimes surpass the computationally
               expensive cross-validation procedure. An adaptive calibration
               process is also described to allow practical use of the
               prediction uncertainty estimates to assess the quality of
               predictions. It is shown that the machine-learning approach
               discussed in this study can be developed as a very useful
               pricing tool, and potentially a market condition change
               detector. A further extension is possible by taking the
               prediction uncertainties into consideration when building a
               financial portfolio. \copyright{} 2017 The Author(s). Published
               by Informa UK Limited, trading as Taylor \& Francis Group.",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  volume    =  17,
  number    =  9,
  pages     = "1403--1416",
  year      =  2017,
  keywords  = "Bayesian inference; Gaussian process; Kernel scaling; Machine
               learning; Multi-armed bandit Bayesian optimization; Support
               vector machines regression"
}

@ARTICLE{Renault2017-gt,
  title     = "Intraday online investor sentiment and return patterns in the
               {U.S}. stock market",
  author    = "Renault, T",
  abstract  = "We implement a novel approach to derive investor sentiment from
               messages posted on social media before we explore the relation
               between online investor sentiment and intraday stock returns.
               Using an extensive dataset of messages posted on the
               microblogging platform StockTwits, we construct a lexicon of
               words used by online investors when they share opinions and
               ideas about the bullishness or the bearishness of the stock
               market. We demonstrate that a transparent and replicable
               approach significantly outperforms standard dictionary-based
               methods used in the literature while remaining competitive with
               more complex machine learning algorithms. Aggregating individual
               message sentiment at half-hour intervals, we provide empirical
               evidence that online investor sentiment helps forecast intraday
               stock index returns. After controlling for past market returns,
               we find that the first half-hour change in investor sentiment
               predicts the last half-hour S\&P 500 index ETF return. Examining
               users' self-reported investment approach, holding period and
               experience level, we find that the intraday sentiment effect is
               driven by the shift in the sentiment of novice traders. Overall,
               our results provide direct empirical evidence of
               sentiment-driven noise trading at the intraday level.
               \copyright{} 2017 Elsevier B.V.",
  journal   = "Journal of Banking and Finance",
  publisher = "Elsevier B.V.",
  volume    =  84,
  pages     = "25--40",
  year      =  2017,
  keywords  = "Asset pricing; Intraday return predictability; Investor
               sentiment; Machine learning; Social media; Textual analysis"
}

@UNPUBLISHED{Knoll2018-qg,
  title    = "Exploiting social media with higher-order Factorization Machines:
              statistical arbitrage on high-frequency data of the {S\&P} 500",
  author   = "Knoll, J and St{\"u}binger, J and Grottke, M",
  abstract = "Over the past 15 years, there have been a number of studies using
              text mining for predicting stock market data. Two recent
              publications employed support vector machines and second-order
              Factorization Machines, respectively, to this end. However, these
              approaches either completely neglect interactions between the
              features extracted from the text, or they only account for
              second-order interactions. In this paper, we apply higher-order
              Factorization Machines, for which efficient training algorithms
              have only been available since 2016. As Factorization Machines
              require hyperparameters to be specified, we also introduce a
              novel adaptive-order algorithm for automatically determining
              them. Our study is the first one to make use of social media data
              for predicting minute-by-minute stock returns, namely the ones of
              the S\&P 500 stock constituents. We show that, unlike a trading
              strategy employing support vector machines,
              Factorization-Machine-based strategies attain positive returns
              after transactions costs for the years 2014 and 2015. Especially
              the approach applying the adaptive-order algorithm outperforms
              classical approaches with respect to a multitude of criteria, and
              it features very favorable characteristics. \copyright{} 2018,
              \copyright{} 2018 Informa UK Limited, trading as Taylor \&
              Francis Group.",
  journal  = "Quantitative Finance",
  year     =  2018,
  keywords = "Factorization Machine; Finance; High-frequency trading; Machine
              learning; Social media; Statistical arbitrage"
}

@ARTICLE{Amat2018-tl,
  title     = "Fundamentals and exchange rate forecastability with simple
               machine learning methods",
  author    = "Amat, C and Michalski, T and Stoltz, G",
  abstract  = "Using methods from machine learning we show that fundamentals
               from simple exchange rate models (PPP or UIRP) or Taylor-rule
               based models lead to improved exchange rate forecasts for major
               currencies over the floating period era 1973--2014 at a 1-month
               forecast horizon which beat the no-change forecast. Fundamentals
               thus contain useful information and exchange rates are
               forecastable even for short horizons. Such conclusions cannot be
               obtained when using rolling or recursive OLS regressions as used
               in the literature. The methods we use -- sequential ridge
               regression and the exponentially weighted average strategy, both
               with discount factors -- do not estimate an underlying model but
               combine the fundamentals to directly output forecasts.
               \copyright{} 2018 Elsevier Ltd",
  journal   = "J. Int. Money Finance",
  publisher = "Elsevier Ltd",
  volume    =  88,
  pages     = "1--24",
  year      =  2018,
  keywords  = "Exchange rates; Forecasting; Machine learning; Purchasing power
               parity; Taylor-rule exchange rate models; Uncovered interest
               rate parity"
}

@UNPUBLISHED{De_Spiegeleer2018-oo,
  title    = "Machine learning for quantitative finance: fast derivative
              pricing, hedging and fitting",
  author   = "De Spiegeleer, J and Madan, D B and Reyners, S and Schoutens, W",
  abstract = "In this paper, we show how we can deploy machine learning
              techniques in the context of traditional quant problems. We
              illustrate that for many classical problems, we can arrive at
              speed-ups of several orders of magnitude by deploying machine
              learning techniques based on Gaussian process regression. The
              price we have to pay for this extra speed is some loss of
              accuracy. However, we show that this reduced accuracy is often
              well within reasonable limits and hence very acceptable from a
              practical point of view. The concrete examples concern fitting
              and estimation. In the fitting context, we fit sophisticated
              Greek profiles and summarize implied volatility surfaces. In the
              estimation context, we reduce computation times for the
              calculation of vanilla option values under advanced models, the
              pricing of American options and the pricing of exotic options
              under models beyond the Black--Scholes setting. \copyright{}
              2018, \copyright{} 2018 Informa UK Limited, trading as Taylor \&
              Francis Group.",
  journal  = "Quantitative Finance",
  number   =  10,
  pages    = "1635--1643",
  year     =  2018,
  keywords = "Derivative pricing; Gaussian processes; Hedging; Machine
              learning; Volatility surface"
}

@BOOK{Brunnermeier2009-fl,
  title     = "The fundamental principles of financial regulation",
  author    = "Brunnermeier, Markus Konrad and Crockett, Andrew and Goodhart,
               Charles A E and Persaud, Avinash and Shin, Hyun Song",
  abstract  = "Today's financial regulatory systems assume that regulations
               which make individual banks safe also make the financial system
               safe. The eleventh Geneva Report on the World Economy shows that
               this thinking is flawed. Actions that banks take to make
               themselves safer can - in times of crisis - undermine the
               system's stability. The Report argues for a different approach.
               What is needed is micro-prudential (i.e. bank-level) regulation,
               macro-prudential (i.e. system-wide) regulation, and careful
               coordination of the two. Macro-prudential regulation in
               particular needs reform to ensure it countervails the natural
               decline in measured risk during booms and its rise in subsequent
               collapses. ``Counter-cyclical capital charges'' are the way
               forward; regulators should adjust capital adequacy requirements
               over the cycle by two multiples - the first related to
               above-average growth of credit expansion and leverage, the
               second related to the mismatch in the maturity of assets and
               liabilities. Changes to mark-to-market procedures are also
               needed. Macro- and micro-prudential regulation should be carried
               out by separate institutions since they differ in focus and
               expertise required. Central Banks should be tasked with
               macro-prudential regulation, Financial Services Authorities with
               micro-prudential regulation. Improved international coordination
               is also important. Since financial and asset-price cycles differ
               from country to country, counter-cyclical regulatory policy
               needs to be implemented mainly by the ``host'' rather than the
               ``home'' country.",
  publisher = "ICMB, Internat. Center for Monetary and Banking Studies",
  volume    =  11,
  pages     = "134",
  series    = "Geneva Reports on the World Economy",
  month     =  jul,
  year      =  2009,
  address   = "London, UK"
}

@ARTICLE{Wernerfelt1984-km,
  title     = "A resource-based view of the firm",
  author    = "Wernerfelt, Birger",
  abstract  = "The paper explores the usefulness of analysing firms from the
               resource side rather than from the product side. In analogy to
               entry barriers and growth-share matrices, the concepts of
               resource position barrier and resource-product matrices are
               suggested. These tools are then used to highlight the new
               strategic options which naturally emerge from the resource
               perspective.",
  journal   = "Strategic Manage. J.",
  publisher = "John Wiley \& Sons, Ltd.",
  volume    =  5,
  number    =  2,
  pages     = "171--180",
  month     =  apr,
  year      =  1984
}

@ARTICLE{Mohsni2018-ms,
  title     = "Does regulatory regime matter for bank risk taking? A
               comparative analysis of {US} and Canada",
  author    = "Mohsni, Sana and Otchere, Isaac",
  abstract  = "In the wake of the worst financial crisis in 2008, most US banks
               were bailed out while Canadian banks sailed through the crisis
               relatively unscathed. This has compelled some analysts to
               question why banking crises happen in America but not in Canada.
               We examine the risk-taking behavior of banks in the US and
               Canada prior to the recent financial crisis and find that
               Canadian banks had lower risk than their US counterparts over
               the study period. Further analysis shows that entry
               restrictions, which create concentrated banking structure,
               strong supervisory power, and discipline constrain excessive
               risk taking by Canadian banks. Entry restrictions enable
               Canadian banks to generate higher profits and lower variability
               of asset returns, while restrictions on activities reduce
               profitability and increase variability in asset return. However,
               the former seems to overwhelm the effect of asset restrictions,
               given the lower risk that we observe for the Canadian banks. The
               less concentrated but competitive banking structure in the US is
               associated with higher bank risk taking. \copyright{} 2017
               Elsevier B.V.",
  journal   = "Journal of International Financial Markets, Institutions and
               Money",
  publisher = "Elsevier Ltd",
  volume    =  53,
  pages     = "1--16",
  month     =  mar,
  year      =  2018,
  keywords  = "Banks; Regulations; Supervisory power; Concentrated banking
               structure; Risk-taking"
}

@ARTICLE{Carretta2015-mc,
  title     = "Don't Stand So Close to Me: The role of supervisory style in
               banking stability",
  author    = "Carretta, Alessandro and Farina, Vincenzo and Fiordelisi, Franco
               and Schwizer, Paola and Lopes, Francesco Saverio Stentella",
  abstract  = "With the establishment of an integrated Banking Union, the
               harmonization of supervisory styles (regulation being equal)
               plays a central role. Our paper addresses a central question:
               what supervisory culture has been demonstrated to be most
               effective at ensuring the stability of European banks? We
               identify six different supervisory cultures and observe to what
               extent the words used in public speeches by the deans of the
               national supervision authority reflect the national cultural
               values of the Hofstede framework (Hofstede et al., 2010). By
               analyzing a panel of banks operating in the EU-15 from 1999 and
               2011, our paper provides empirical evidence that supervisory
               culture influences the stability of banks. Our results have
               important policy implications: our paper is the first to provide
               empirical evidence of heterogeneity in the supervision styles in
               Europe and its effect on banking stability.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  52,
  pages     = "180--188",
  month     =  mar,
  year      =  2015,
  keywords  = "Supervisory styles; Banking stability; Banking Union; Text
               analysis"
}

@ARTICLE{Embrechts2001-vs,
  title     = "An academic response to Basel {II}",
  author    = "Embrechts, Paul and Dan{\'\i}elsson, J{\'o}n and Goodhart,
               Charles A E and Keating, Con and Muennich, Felix and Renault,
               Olivier and Shin, Hyun Song",
  journal   = "FMG Special paper 130",
  publisher = "FMG",
  volume    =  130,
  year      =  2001
}

@BOOK{Berger2014-ph,
  title     = "The Oxford Handbook of Banking, Second Edition",
  author    = "Berger, Allen N and Molyneux, Philip and Wilson, John O S",
  abstract  = "The Oxford Handbook of Banking, Second Edition provides an
               overview and analysis of developments and research in banking
               written by leading researchers in the field. This handbook will
               appeal to graduate students of economics, banking and finance,
               academics, practitioners, regulators, and policy makers.
               Consequently, the book strikes a balance between abstract
               theory, empirical analysis, and practitioner, and policy-related
               material. The Handbook is split into five parts. Part I, The
               Theory of Banking, examines the role of banks in the wider
               financial system, why banks exist, how they function, and their
               corporate governance and risk management practices. Part II
               deals with Bank Operations and Performance. A range of issues
               are covered including bank performance, financial innovation,
               and technological change. Aspects relating to small business,
               consumer, and mortgage lending are analysed together with
               securitization, shadow banking, and payment systems. Part III
               entitled Regulatory and Policy Perspectives discusses central
               banking, monetary policy transmission, market discipline, and
               prudential regulation and supervision. Part IV of the book
               covers various Macroeconomic Perspectives in Banking. This part
               includes a discussion of systemic risk and banking and sovereign
               crises, the role of the state in finance and development as well
               as how banks influence real economic activity. The final Part V
               examines International Differences in Banking Structures and
               Environments. This part of the Handbook examines banking systems
               in the United States, European Union, Japan, Africa, Transition
               countries, and the developing nations of Asia and Latin America.",
  publisher = "OUP Oxford",
  volume    =  11,
  pages     = "1104",
  month     =  nov,
  year      =  2014,
  language  = "en"
}

@UNPUBLISHED{Bollerslev2016-sz,
  title    = "Daily House Price Indices: Construction, Modeling, and Longer-run
              Predictions",
  author   = "Bollerslev, Tim and Patton, Andrew J and Wang, Wenjing",
  abstract = "Summary We construct daily house price indices for 10 major US
              metropolitan areas. Our calculations are based on a comprehensive
              database of several million residential property transactions and
              a standard repeat?sales method that closely mimics the
              methodology of the popular monthly Case?Shiller house price
              indices. Our new daily house price indices exhibit dynamic
              features similar to those of other daily asset prices, with mild
              autocorrelation and strong conditional heteroskedasticity of the
              corresponding daily returns. A relatively simple multivariate
              time series model for the daily house price index returns,
              explicitly allowing for commonalities across cities and GARCH
              effects, produces forecasts of longer?run monthly house price
              changes that are superior to various alternative forecast
              procedures based on lower?frequency data. Copyright ? 2015?John
              Wiley \& Sons, Ltd.",
  month    =  sep,
  year     =  2016
}

@ARTICLE{Pille2002-ec,
  title    = "Financial performance analysis of Ontario (Canada) Credit Unions:
              An application of {DEA} in the regulatory environment",
  author   = "Pille, Peter and Paradi, Joseph C",
  abstract = "Models are developed to detect weaknesses in Credit Unions in
              Ontario, Canada, so that potential financial failures can be
              predicted. Four data envelopment analysis (DEA) models are
              presented and compared with the equity to asset ratio, and with
              the government regulator's extensively modified ``Z-score''
              model. The equity/asset ratio is shown to provide as good a
              prediction of failure as any of the other models, and is not
              improved upon by the much more complex Z-score model. The best
              DEA model provides results comparable to the equity/asset ratio
              when a slack adjusted efficiency score is used to measure
              efficiency, particularly for Credit Unions with larger asset
              sizes. However, DEA also provides indications of where
              opportunities lie for improvements by weak units by providing
              specific information, relevant to managers. Hence, for each
              Credit Union, comparison is made with a peer group of efficient
              entities that the inefficient institution's management can
              emulate to improve their performance. \copyright{} 2002 Elsevier
              Science B.V. All rights reserved.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  139,
  number   =  2,
  pages    = "339--350",
  month    =  jun,
  year     =  2002,
  keywords = "Banking; Data envelopment analysis; Efficiency; Failure;
              Forecasting; Failure analysis; Finance; Forecasting; Mathematical
              models; Operations research; Regulatory compliance; Financial
              performance analysis; Financial data processing"
}

@ARTICLE{Brean2011-zg,
  title     = "Canada and the United States: Different roots, different routes
               to financial sector regulation",
  author    = "Brean, Donald J S and Kryzanowski, Lawrence and Roberts, Gordon
               S",
  abstract  = "This paper explores the lessons to be learned from why the
               neighbouring banking systems of Canada and the United States,
               that share numerous commonalities, fared so differently during
               two major financial crises. The explanations are deeply rooted
               in different tolerances for industry concentration and state
               involvement, and the divergent routes of the development of
               their financial systems, founding institutions, on-going
               governance and regulation, and competitive structures. Canada's
               success during the more recent 2007-09 financial crisis is
               attributed to more effective regulation and conservative banking
               practices, including (self-) imposed stricter limits on bank
               leverage, much stricter limits on unconventional mortgages, and
               less reliance on the use of more 'creative' investment types
               (e.g. subprime lending) and structured products. \copyright{}
               2011 Taylor \& Francis.",
  journal   = "Bus. Hist.",
  publisher = "Taylor and Francis Ltd.",
  volume    =  53,
  number    =  2,
  pages     = "249--269",
  month     =  apr,
  year      =  2011,
  keywords  = "Bank concentration; Bank supervision; Banking systems; Canada;
               Financial regulation; History of credit crises; Legislative
               history; United states; banking; capital flow; economic history;
               financial crisis; financial system; governance approach;
               historical geography; historical perspective; investment;
               regulatory approach; Canada; United States"
}

@ARTICLE{Radic2012-hc,
  title     = "Efficiency and {Risk-Taking} in {Pre-Crisis} Investment Banks",
  author    = "Radic, Nemanja and Fiordelisi, Franco and Girardone, Claudia",
  abstract  = "Investment banks' core functions expose them to a wide array of
               risks. This paper analyses cost and profit efficiency for a
               sample of investment banks for the G7 countries (Canada, France,
               Germany, Italy, Japan, UK and US) and Switzerland prior to the
               recent financial crisis. We follow Coelli et al. (J Prod Anal
               11:251-273, 1999)'s methodology to adjust the estimated cost and
               profit efficiency scores for environmental influences including
               key banks' risks, bank- and industry- specific factors and
               macroeconomic conditions. Our evidence suggests that failing to
               account for environmental factors can considerably bias the
               efficiency scores for investment banks. Specifically, bank
               risk-taking factors (including liquidity and capital risk
               exposures) are found particularly important to accurately assess
               profit efficiency: i. e. profit efficiency estimates are
               consistently underestimated without accounting for bank
               risk-taking. Interestingly, our evidence suggests that size
               matters for both cost and profit efficiency, however this does
               not imply that more concentrated markets are more efficient.
               \copyright{} 2011 Springer Science+Business Media, LLC.",
  journal   = "Journal of Financial Services Research",
  publisher = "Springer Science \& Business Media",
  volume    =  41,
  number    = "1-2",
  pages     = "81--101",
  month     =  apr,
  year      =  2012,
  address   = "Dordrecht, Netherlands",
  keywords  = "Banking risks; Efficiency; Environmental conditions; Investment
               banking; Stochastic frontier analysis"
}

@ARTICLE{Athey2017-rv,
  title    = "Beyond prediction: Using big data for policy problems",
  author   = "Athey, Susan",
  abstract = "Machine-learning prediction methods have been extremely
              productive in applications ranging from medicine to allocating
              fire and health inspectors in cities. However, there are a number
              of gaps between making a prediction and making a decision, and
              underlying assumptions need to be understood in order to optimize
              data-driven decision-making.",
  journal  = "Science",
  volume   =  355,
  number   =  6324,
  pages    = "483--485",
  month    =  feb,
  year     =  2017,
  language = "en"
}

@ARTICLE{Marshall2012-uo,
  title     = "Commodity Liquidity Measurement and Transaction Costs",
  author    = "Marshall, Ben R and Nguyen, Nhut H and Visaltanachoti, Nuttawat",
  abstract  = "We examine the performance of liquidity proxies in commodities.
               The Amihud measure has the largest correlation with liquidity
               benchmarks. Amivest and Effective Tick measures also perform
               well. These proxies are useful for studies of commodity
               liquidity over a long time period and those that lack access to
               high-frequency data. We use various aspects of transaction
               costs, such as spread, depth, immediacy, and resiliency, to give
               insight into the costs of different execution approaches.
               Transaction costs increase with volatility and exhibit mean
               reversion. Splitting trades over one hour can reduce trading
               costs by two-thirds compared to an immediate execution.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  25,
  number    =  2,
  pages     = "599--638",
  month     =  feb,
  year      =  2012
}

@ARTICLE{Rubio2016-cl,
  title     = "The new financial regulation in Basel {III} and monetary policy:
               A macroprudential approach",
  author    = "Rubio, Margarita and Carrasco-Gallego, Jos{\'e} A",
  abstract  = "The aim of this paper is to study the interaction between Basel
               I, II and III regulations with monetary policy. In order to do
               that, we use a dynamic stochastic general equilibrium (DSGE)
               model with a housing market, banks, borrowers, and savers.
               Results show that monetary policy needs to be more aggressive
               when the capital requirement ratio (CRR) increases because it is
               less effective in this case. However, this policy combination
               brings a more stable economic and financial system. We also
               analyze the optimal way to implement the countercyclical capital
               buffer stated by Basel III. We propose that the CRR follows a
               rule that responds to deviations of credit from its steady
               state. We find that the optimal implementation of this
               macroprudential rule together with monetary policy brings extra
               financial stability with respect to Basel I and II. \copyright{}
               2016 Elsevier B.V.",
  journal   = "Journal of Financial Stability",
  publisher = "Elsevier B.V.",
  volume    =  26,
  pages     = "294--305",
  month     =  oct,
  year      =  2016,
  keywords  = "Banks; Basel I; Basel II; Basel III; Borrowers; Capital
               requirement ratio; Cedit; Countercyclical capital buffer;
               Macroprudential; Savers"
}

@ARTICLE{Rouwenhorst1998-uj,
  title     = "International momentum strategies",
  author    = "Rouwenhorst, K Geert",
  abstract  = "International equity markets exhibit medium-term return
               continuation. Between 1980 and 1995 an internationally
               diversified portfolio of past medium-term Winners outperforms a
               portfolio of medium-term Losers after correcting for risk by
               more than 1 percent per month. Return continuation is present in
               all twelve sample countries and lasts on average for about one
               year. Return continuation is negatively related to firm size,
               but is not limited to small firms. The international momentum
               returns are correlated with those of the United States which
               suggests that exposure to a common factor may drive the
               profitability of momentum strategies.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishers, Inc.",
  volume    =  53,
  number    =  1,
  pages     = "267--284",
  month     =  feb,
  year      =  1998
}

@ARTICLE{Moskowitz2012-yd,
  title     = "Time series momentum",
  author    = "Moskowitz, Tobias J and Ooi, Yao Hua and Pedersen, Lasse Heje",
  abstract  = "We document significant ``time series momentum'' in equity
               index, currency, commodity, and bond futures for each of the 58
               liquid instruments we consider. We find persistence in returns
               for one to 12 months that partially reverses over longer
               horizons, consistent with sentiment theories of initial
               under-reaction and delayed over-reaction. A diversified
               portfolio of time series momentum strategies across all asset
               classes delivers substantial abnormal returns with little
               exposure to standard asset pricing factors and performs best
               during extreme markets. Examining the trading activities of
               speculators and hedgers, we find that speculators profit from
               time series momentum at the expense of hedgers. \copyright{}
               2011 Elsevier B.V..",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  104,
  number    =  2,
  pages     = "228--250",
  month     =  may,
  year      =  2012,
  keywords  = "Asset pricing; Trading volume; Futures pricing; International
               financial markets; Market efficiency"
}

@ARTICLE{Menkhoff2012-lr,
  title     = "Currency momentum strategies",
  author    = "Menkhoff, Lukas and Sarno, Lucio and Schmeling, Maik and
               Schrimpf, Andreas",
  abstract  = "Abstract We provide a broad empirical investigation of momentum
               strategies in the foreign exchange market. We find a significant
               cross-sectional spread in excess returns of up to 10\% per annum
               (p.a.) between past winner and loser currencies. This spread in
               excess returns is not explained by traditional risk factors, it
               is partially explained by transaction costs and shows behavior
               consistent with investor under- and overreaction. Moreover,
               cross-sectional currency momentum has very different properties
               from the widely studied carry trade and is not highly correlated
               with returns of benchmark technical trading rules. However,
               there seem to be very effective limits to arbitrage that prevent
               momentum returns from being easily exploitable in currency
               markets.",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  106,
  number    =  3,
  pages     = "660--684",
  month     =  dec,
  year      =  2012,
  keywords  = "Momentum returns; Limits to arbitrage; Idiosyncratic volatility;
               Carry trades"
}

@ARTICLE{Machler2004-og,
  title     = "Variable Length Markov Chains: Methodology, Computing, and
               Software",
  author    = "M{\"a}chler, Martin and B{\"u}hlmann, Peter",
  abstract  = "This article presents a tutorial and new, publicly available
               computational tools for variable length Markov chains (VLMC).
               VLMCs are Markov chains with the additional attractive structure
               that their memories depend on a variable number of lagged
               values, depending on what the actual past (the lagged values)
               looks like. They build a very flexible class of tree-structured
               models for categorical time series. Fitting VLMCs from data is a
               nontrivial computational task. We provide an efficient
               implementation of the so-called context algorithm which requires
               only O(n log(n)) operations. The implementation, which is
               publicly available, includes additional important new features
               and options: diagnostics, goodness of fit, simulation and
               bootstrap, residuals, and tuning the context algorithm. Our
               tutorial is presented with a version in R which is available
               from the Comprehensive R Archive Network (CRAN). The exposition
               is self-contained, gives rigorous and partly new mathematical
               descriptions, and is illustrated by analyzing a DNA sequence
               from the Epstein-Barr virus.",
  journal   = "J. Comput. Graph. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  13,
  number    =  2,
  pages     = "435--455",
  month     =  jun,
  year      =  2004
}

@ARTICLE{Liu2008-mx,
  title     = "Momentum Profits, Factor Pricing, and Macroeconomic Risk",
  author    = "Liu, Laura Xiaolei and Zhang, Lu",
  abstract  = "Recent winners have temporarily higher loadings than recent
               losers on the growth rate of industrial production. The loading
               spread derives mostly from the positive loadings of winners. The
               growth rate of industrial production is a priced risk factor in
               standard asset pricing tests. In many specifications, this
               macroeconomic risk factor explains more than half of momentum
               profits. We conclude that risk plays an important role in
               driving momentum profits.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  6,
  pages     = "2417--2448",
  month     =  nov,
  year      =  2008
}

@ARTICLE{Leung2000-pi,
  title     = "Forecasting stock indices: a comparison of classification and
               level estimation models",
  author    = "Leung, Mark T and Daouk, Hazem and Chen, An-Sing",
  abstract  = "Abstract Despite abundant research which focuses on estimating
               the level of return on stock market index, there is a lack of
               studies examining the predictability of the direction/sign of
               stock index movement. Given the notion that a prediction with
               little forecast error does not necessarily translate into
               capital gain, we evaluate the efficacy of several multivariate
               classification techniques relative to a group of level
               estimation approaches. Specifically, we conduct time series
               comparisons between the two types of models on the basis of
               forecast performance and investment return. The tested
               classification models, which predict direction based on
               probability, include linear discriminant analysis, logit,
               probit, and probabilistic neural network. On the other hand, the
               level estimation counterparts, which forecast the level, are
               exponential smoothing, multivariate transfer function, vector
               autoregression with Kalman filter, and multilayered feedforward
               neural network. Our comparative study also measures the relative
               strength of these models with respect to the trading profit
               generated by their forecasts. To facilitate more effective
               trading, we develop a set of threshold trading rules driven by
               the probabilities estimated by the classification models.
               Empirical experimentation suggests that the classification
               models outperform the level estimation models in terms of
               predicting the direction of the stock market movement and
               maximizing returns from investment trading. Further, investment
               returns are enhanced by the adoption of the threshold trading
               rules.",
  journal   = "Int. J. Forecast.",
  publisher = "Elsevier",
  volume    =  16,
  number    =  2,
  pages     = "173--190",
  month     =  apr,
  year      =  2000,
  keywords  = "Forecasting; Multivariate classification; Stock index; Trading
               strategy"
}

@ARTICLE{Leitch1991-iw,
  title     = "Economic Forecast Evaluation: Profits Versus the Conventional
               Error Measures",
  author    = "Leitch, Gordon and Tanner, J Ernest",
  abstract  = "Economists are often puzzled as to why profit-maximizing firms
               buy professional forecasts when statistics such as the
               root-mean-squared error or the mean absolute error often
               indicate that a naive model will forecast about as well. This
               paper argues that the reason is that these traditional summary
               statistics may not be closely related to a forecast's profits.
               Using profit measures, the authors find only very weak
               relationships between such summary error statistics and forecast
               value. If these results are robust, then least-squares
               regression analysis may not be appropriate for many studies of
               economic behavior. Copyright 1991 by American Economic
               Association.",
  journal   = "Am. Econ. Rev.",
  publisher = "American Economic Association",
  volume    =  81,
  number    =  3,
  pages     = "580--590",
  year      =  1991
}

@ARTICLE{Jegadeesh2001-mp,
  title     = "Profitability of Momentum Strategies: An Evaluation of
               Alternative Explanations",
  author    = "Jegadeesh, Narasimhan and Titman, Sheridan",
  abstract  = "This paper evaluates various explanations for the profitability
               of momentum strategies documented in Jegadeesh and Titman
               (1993). The evidence indicates that momentum profits have
               continued in the 1990s, suggesting that the original results
               were not a product of data snooping bias. The paper also
               examines the predictions of recent behavioral models that
               propose that momentum profits are due to delayed overreactions
               that are eventually reversed. Our evidence provides support for
               the behavioral models, but this support should be tempered with
               caution.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishers, Inc.",
  volume    =  56,
  number    =  2,
  pages     = "699--720",
  month     =  apr,
  year      =  2001
}

@ARTICLE{Jegadeesh1993-xw,
  title     = "Returns to Buying Winners and Selling Losers: Implications for
               Stock Market Efficiency",
  author    = "Jegadeesh, Narasimhan and Titman, Sheridan",
  abstract  = "This paper documents that strategies that buy stocks that have
               performed well in the past and sell stocks that hav e performed
               poorly in the past generate significant positive returns o ver
               three- to twelve-month holding periods. The authors find that
               the profitability of these strategies are not due to their
               systematic risk or to delay ed stock price reactions to common
               factors. However, part of the abnorm al returns generated in the
               first year after portfolio formation dissipates in the following
               two years. A similar pattern of returns around the earnings
               announcements of past winners and losers is also documented.
               Copyright 1993 by American Finance Association.",
  journal   = "J. Finance",
  publisher = "American Finance Association",
  volume    =  48,
  number    =  1,
  pages     = "65--91",
  month     =  mar,
  year      =  1993
}

@ARTICLE{Hong1999-gd,
  title     = "A unified theory of underreaction, momentum trading, and
               overreaction in asset markets",
  author    = "Hong, Harrison and Stein, Jeremy C",
  abstract  = "We model a market populated by two groups of boundedly rational
               agents: ``newswatchers'' and ``momentum traders.'' Each
               newswatcher observes some private information, but fails to
               extract other newswatchers' information from prices. If
               information diffuses gradually across the population, prices
               underreact in the short run. The underreaction means that the
               momentum traders can profit by trend-chasing. However, if they
               can only implement simple (i.e., univariate) strategies, their
               attempts at arbitrage must inevitably lead to overreaction at
               long horizons. In addition to providing a unified account of
               under- and overreactions, the model generates several other
               distinctive implications.",
  journal   = "J. Finance",
  publisher = "Blackwell Publishers, Inc.",
  volume    =  54,
  number    =  6,
  pages     = "2143--2184",
  month     =  dec,
  year      =  1999
}

@ARTICLE{Hansen2012-ai,
  title     = "A test for superior predictive ability",
  author    = "Hansen, Peter Reinhard",
  abstract  = "We propose a new test for superior predictive ability. The new
               test compares favorably to the reality check (RC) for data
               snooping, because it is more powerful and less sensitive to poor
               and irrelevant alternatives. The improvements are achieved by
               two modifications of the Rc. We use a studentized test statistic
               that reduces the influence of erratic forecasts and invoke a
               sample-dependent null distribution. The advantages of the new
               test are confirmed by Monte Carlo experiments and an empirical
               exercise in which we compare a large number of regression-based
               forecasts of annual U.S. inflation to a simple random-walk
               forecast. The random-walk forecast is found to be inferior to
               regression-based forecasts and, interestingly, the best sample
               performance is achieved by models that have a Phillips curve
               structure.",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  23,
  number    =  4,
  pages     = "365--380",
  year      =  2012
}

@ARTICLE{Hansen2011-hm,
  title     = "The Model Confidence Set",
  author    = "Hansen, Peter R and Lunde, Asger and Nason, James M",
  abstract  = "This paper introduces the model confidence set (MCS) and applies
               it to the selection of models. A MCS is a set of models that is
               constructed such that it will contain the best model with a
               given level of confidence. The MCS is in this sense analogous to
               a confidence interval for a parameter. The MCS acknowledges the
               limitations of the data, such that uninformative data yield a
               MCS with many models, whereas informative data yield a MCS with
               only a few models. The MCS procedure does not assume that a
               particular model is the true model; in fact, the MCS procedure
               can be used to compare more general objects, beyond the
               comparison of models. We apply the MCS procedure to two
               empirical problems. First, we revisit the inflation forecasting
               problem posed by Stock and Watson (1999), and compute the MCS
               for their set of inflation forecasts. Second, we compare a
               number of Taylor rule regressions and determine the MCS of the
               best regression in terms of in-sample likelihood criteria.",
  journal   = "Econometrica",
  publisher = "Blackwell Publishing Ltd",
  volume    =  79,
  number    =  2,
  pages     = "453--497",
  month     =  mar,
  year      =  2011,
  keywords  = "Model confidence set; model selection; forecasting; multiple
               comparisons"
}

@ARTICLE{Georgopoulou2017-xm,
  title     = "The Trend Is Your Friend: {Time-Series} Momentum Strategies
               across Equity and Commodity Markets",
  author    = "Georgopoulou, Athina and Wang, Jiaguo (george)",
  abstract  = "This article documents a significant time-series momentum effect
               that is consistent and robust across all examined conventional
               asset classes from 1969 to 2015. We find that the duration and
               magnitude of time-series momentum is different in developed and
               emerging markets, but this is no longer the case when
               controlling for the currency component. We further demonstrate
               that time-series momentum captures a significant proportion of
               international mutual fund performance, but this is predominantly
               with respect to its long aspect. Finally, the market
               interventions by central banks in recent years have distorted
               correlations across assets; this challenges the performance of
               such portfolios.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  21,
  number    =  4,
  pages     = "1557--1592",
  month     =  jul,
  year      =  2017
}

@ARTICLE{Fama1988-wg,
  title     = "Permanent and Temporary Components of Stock Prices",
  author    = "Fama, Eugene F and French, Kenneth R",
  abstract  = "A slowly mean-reverting component of stock prices tends to
               induce negative autocorrelation in returns. The autocorrelation
               is weak for the daily and weekly holding periods common in
               market efficiency tests but stronger for long-horizon returns.
               In tests for the 1926-85 period, large negative autocorrelations
               for return horizons beyond a year suggest that predictable price
               variation due to mean reversion accounts for large fractions of
               3-5-year return variances. Predict- able variation is estimated
               to be about 40 percent of 3-5-year return variances for
               portfolios of small firms. The percentage falls to around 25
               percent for portfolios of large firms.",
  journal   = "J. Polit. Econ.",
  publisher = "The University of Chicago Press",
  volume    =  96,
  number    =  2,
  pages     = "246--273",
  year      =  1988
}

@ARTICLE{Fama1965-uu,
  title     = "Random walks in stock market prices",
  author    = "Fama, Eugene F",
  abstract  = "FOR MANY YEARS cconomists, Statisticians, and teachers of
               finance have been interested in developing and testing models of
               stock price behavior. One important model that has evolved from
               this research is the theory of random walks. This theory casts
               serious doubt on many other methods for describing and
               predicting stock price behavior methods that have considerable
               popularity outside the academic world. For example, we shall see
               later that if the random walk theory is an accurate description
               of reality, then the various ``technical'' or ``chartist''
               procedures for predicting stock prices are completely without
               value.",
  journal   = "Financial Analysts Journal",
  publisher = "CFA Institute",
  volume    =  21,
  number    =  5,
  pages     = "55--59",
  month     =  sep,
  year      =  1965
}

@ARTICLE{Diebold2002-ug,
  title     = "Comparing Predictive Accuracy",
  author    = "Diebold, Francis X and Mariano, Roberto S",
  abstract  = "We propose and evaluate explicit tests of the null hypothesis of
               no difference in the accuracy of two competing forecasts. In
               contrast to previously developed tests, a wide variety of
               accuracy measures can be used (in particular, the loss of
               function need not be quadratic and need not even be symmetric),
               and forecast errors can be non-Gaussian, nonzero mean, serially
               correlated, and contemporaneously correlated. Asymptotic and
               exact finite-sample tests are proposed, evaluated, and
               illustrated.",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  20,
  number    =  1,
  pages     = "134--144",
  month     =  jan,
  year      =  2002
}

@ARTICLE{Buhlmann1999-sw,
  title     = "Variable length Markov chains",
  author    = "B{\"u}hlmann, Peter and Wyner, Abraham J and {Others}",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  27,
  number    =  2,
  pages     = "480--513",
  month     =  apr,
  year      =  1999,
  keywords  = "Bootstrap; categorical time series; central limit theorem;
               context algorithm; data compression; finite-memory sources; FSMX
               model; Kullback-Leibler distance; model selection; tree model."
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bhojraj2006-cn,
  title     = "Macromomentum: Returns Predictability in International Equity
               Indices",
  author    = "Bhojraj, Sanjeev and Swaminathan, Bhaskaran",
  abstract  = "This study examines momentum and reversals in international
               stock market indices. We find that country stock indices exhibit
               momentum during the first year after the portfolio formation
               date and reversals during the subsequent 2 years. Positive
               currency momentum predicts low stock index returns in the
               future, thereby weakening momentum and strengthening reversals
               in U.S. dollar‚Äêdenominated stock index returns. Cross‚Äêsectional
               regression tests involving individual stock indices confirm the
               portfolio findings. Our results are consistent with a key
               prediction of recent behavioral theories, that initial momentum
               should be accompanied by subsequent reversals.",
  journal   = "The Journal of Business",
  publisher = "The University of Chicago Press",
  volume    =  79,
  number    =  1,
  pages     = "429--451",
  year      =  2006
}

@ARTICLE{Barroso2015-sp,
  title     = "Momentum has its moments",
  author    = "Barroso, Pedro and Santa-Clara, Pedro",
  abstract  = "Abstract Compared with the market, value, or size factors,
               momentum has offered investors the highest Sharpe ratio.
               However, momentum has also had the worst crashes, making the
               strategy unappealing to investors who dislike negative skewness
               and kurtosis. We find that the risk of momentum is highly
               variable over time and predictable. Managing this risk virtually
               eliminates crashes and nearly doubles the Sharpe ratio of the
               momentum strategy. Risk-managed momentum is a much greater
               puzzle than the original version.",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  116,
  number    =  1,
  pages     = "111--120",
  month     =  apr,
  year      =  2015,
  keywords  = "Anomalies; Momentum; Time-varying risk; Transaction costs of
               momentum"
}

@ARTICLE{Asness2013-nd,
  title     = "Value and Momentum Everywhere",
  author    = "Asness, Clifford S and Moskowitz, Tobias J and Pedersen, Lasse
               Heje",
  abstract  = "We find consistent value and momentum return premia across eight
               diverse markets and asset classes, and a strong common factor
               structure among their returns. Value and momentum returns
               correlate more strongly across asset classes than passive
               exposures to the asset classes, but value and momentum are
               negatively correlated with each other, both within and across
               asset classes. Our results indicate the presence of common
               global risks that we characterize with a three-factor model.
               Global funding liquidity risk is a partial source of these
               patterns, which are identifiable only when examining value and
               momentum jointly across markets. Our findings present a
               challenge to existing behavioral, institutional, and rational
               asset pricing theories that largely focus on U.S. equities.",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  68,
  number    =  3,
  pages     = "929--985",
  month     =  jun,
  year      =  2013
}

@ARTICLE{Van_Oordt2018-zj,
  title    = "Systemic risk and bank business models",
  author   = "van Oordt, Maarten and Zhou, Chen",
  abstract = "Summary In this paper, we decompose banks' systemic risk into two
              dimensions: the risk of a bank (?bank tail risk?) and the link of
              the bank to the system in financial distress (?systemic
              linkage?). Based on extreme value theory, we estimate a systemic
              risk measure that can be decomposed into two subcomponents
              reflecting these dimensions. Empirically, we assess the
              relationships of bank business models to the two dimensions of
              systemic risk. The observed differences in these relationships
              partly explain why micro- and macroprudential perspectives
              sometimes have different implications for banking regulation.",
  journal  = "J. Appl. Econ.",
  volume   =  69,
  pages    = "2689",
  month    =  nov,
  year     =  2018
}

@UNPUBLISHED{Schilling2018-xu,
  title       = "Some Simple Bitcoin Economics",
  author      = "Schilling, Linda and Uhlig, Harald",
  abstract    = "How do Bitcoin prices evolve? What are the consequences for
                 monetary policy? We answer these questions in a novel, yet
                 simple endowment economy. There are two types of money, both
                 useful for transactions: Bitcoins and Dollars. A central bank
                 keeps the real value of Dollars constant, while Bitcoin
                 production is decentralized via proof-of-work. We obtain a
                 ``fundamental condition,'' which is a version of the
                 exchange-rate indeterminacy result in Kareken-Wallace (1981),
                 and a ``speculative'' condition. Under some conditions, we
                 show that Bitcoin prices form convergent supermartingales or
                 submartingales and derive implications for monetary policy.",
  journal     = "NBER Working Paper Series",
  number      =  24483,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  apr,
  year        =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bouvatier2018-af,
  title     = "Bank insolvency risk and Z-score measures: caveats and best
               practice",
  author    = "Bouvatier, V and Lepetit, L and Rehault, P N and Strobel, F",
  abstract  = "We highlight caveats arising in the application of traditional
               ROA-based Z-scores for the measurement of bank insolvency risk,
               develop alternative Z-score measures to resolve these issues,
               and make recommendations for best practice for the US/Europe
               based on the experience of the financial crisis of 2007-2008.
               Using a probabilistic approach (i) our novel regulatory capital
               Z-score dominates traditional Z-score measures for both
               US/Europe;(ii) Z- scores computed with exponentially weighted
               moments dominate those with moving ‚Ä¶",
  publisher = "papers.ssrn.com",
  year      =  2018
}

@ARTICLE{Frazzini2006-pi,
  title    = "The Disposition Effect and Underreaction to News",
  author   = "Frazzini, Andrea",
  abstract = "ABSTRACT This paper tests whether the ?disposition effect,? that
              is the tendency of investors to ride losses and realize gains,
              induces ?underreaction? to news, leading to return
              predictability. I use data on mutual fund holdings to construct a
              new measure of reference purchasing prices for individual stocks,
              and I show that post-announcement price drift is most severe
              whenever capital gains and the news event have the same sign. The
              magnitude of the drift depends on the capital gains (losses)
              experienced by the stock holders on the event date. An
              event-driven strategy based on this effect yields monthly alphas
              of over 200 basis points.",
  journal  = "J. Finance",
  volume   =  61,
  number   =  4,
  pages    = "2017--2046",
  month    =  aug,
  year     =  2006
}

@ARTICLE{Kaustia2010-lo,
  title     = "Prospect Theory and the Disposition Effect",
  author    = "Kaustia, Markku",
  abstract  = "This paper shows that prospect theory is unlikely to explain the
               disposition effect. Prospect theory predicts that the propensity
               to sell a stock declines as its price moves away from the
               purchase price in either direction. Trading data, on the other
               hand, show that the propensity to sell jumps at zero return, but
               it is approximately constant over a wide range of losses and
               increasing or constant over a wide range of gains. Further, the
               pattern of realized returns does not seem to stem from optimal
               after-tax portfolio rebalancing, a belief in mean-reverting
               returns, or investors acting on target prices.",
  journal   = "Journal of Financial and Quantitative Analysis",
  publisher = "Cambridge University Press",
  volume    =  45,
  number    =  3,
  pages     = "791--812",
  month     =  jun,
  year      =  2010
}

@UNPUBLISHED{Hargrave2018-mw,
  title    = "How Value is Created in Tokenized Assets",
  author   = "Hargrave, John and Sahdev, Navroop K and Feldmeier, Olga",
  abstract = "A tidal wave of change is coming to the world of Economic
              Science. Digital tokens---including bitcoin, altcoins, and
              cryptocurrencies---will require a fundamental rethinking of
              valuation, in the same way that the introduction of the stock
              market required a new understanding of value. As of this writing,
              the total value of all tokens stands at \$500 billion. How do
              investors place value on computer code, with no central bank or
              physical asset to support it? Drawing from the literature on
              behavioral economics and tools from cognitive psychology, we aim
              to provide the first anchor to understand the criteria that
              investors are deploying to value new digital assets, making this
              the first study of applied behavioral economics on token
              valuation. Using a new instrument called the Framework for Token
              Confidence, we show how value can be created out of ``thin air,''
              and how tokens, and indeed the entire economic system, operate as
              something like a ``vote of confidence.''",
  month    =  feb,
  year     =  2018,
  keywords = "Blockchain, Token Economics, Tokenomics, ICOs, Initial Coin
              Offerings, Token Sales, Distributed Ledger Technology,
              Cryptocurrencies, Economic Value, Behavioral Economics, Cognitive
              Psychology, Applied Behavioral Economics, Framework for Token
              Confidence, Altcoins, Bitcoin, Ethereum"
}

@ARTICLE{Berry2013-yq,
  title     = "Socially Responsible Investing: An Investor Perspective",
  author    = "Berry, Thomas C and Junkus, Joan C",
  abstract  = "Given the growing importance of Socially Responsible Investing
               (SRI), it is surprising that there is no consensus of what the
               term SRI means to an investor. Further, most studies of this
               question rely solely on the views of investors who already
               invest in SRI funds. Our study surveys a unique pool of
               approximately 5,000 investors that contains both investors who
               have used SRI criteria in investment decisions and those who
               have not, and involves a broad array of criteria associated with
               SR investing. Our findings offer new insight into the SRI
               debate. For both sets of investors, environmental and
               sustainability issues dominate as the major category associated
               with SR investing. We find strong agreement in the ranking of
               the relative importance of various SRI factors despite
               differences between these two groups in their opinion of their
               overall importance. We also find that investors prefer to
               consider the SRI question in more holistic terms rather than
               using the exclusionary format favored by most SRI funds.
               Investors seem to prefer to reward firms who display overall
               positive social behavior rather than to exclude firms on the
               basis of certain products or practices. These findings can help
               providers of SR investment vehicles to improve the SRI products
               that they offer to the general investor, thus both encouraging
               the initial adoption of SR criteria by investors and increasing
               overall investment in SR choices.",
  journal   = "J. Bus. Ethics",
  publisher = "Springer Netherlands",
  volume    =  112,
  number    =  4,
  pages     = "707--720",
  month     =  feb,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Hill2007-bw,
  title     = "Corporate Social Responsibility and Socially Responsible
               Investing: A Global Perspective",
  author    = "Hill, Ronald Paul and Ainscough, Thomas and Shank, Todd and
               Manullang, Daryl",
  abstract  = "This research examines the relationship between corporate social
               responsibility (CSR) and company stock valuation across three
               regions of the world. After a brief introduction, the article
               gives an overview of the evolving definition of CSR as well as a
               discussion of the ways in which this construct has been
               operationalized. Presentation of the potential impact of
               corporate social performance on firm financial performance
               follows, including investor characteristics, the rationale
               behind their choices, and their influence on the marketplace for
               securities worldwide. The unique method used to select socially
               responsible investments is then provided that also includes a
               description of the quantitative techniques employed in the
               analyses. Results are offered subsequently, and the close
               describes implications for global enterprises as socially
               responsible investments.",
  journal   = "J. Bus. Ethics",
  publisher = "Springer Netherlands",
  volume    =  70,
  number    =  2,
  pages     = "165--174",
  month     =  jan,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Bello2005-uq,
  title    = "{SOCIALLY} {RESPONSIBLE} {INVESTING} {AND} {PORTFOLIO}
              {DIVERSIFICATION}",
  author   = "Bello, Zakri Y",
  abstract = "Abstract I use a sample of socially responsible stock mutual
              funds matched to randomly selected conventional funds of similar
              net assets to investigate differences in characteristics of
              assets held, portfolio diversification, and variable effects of
              diversification on investment performance. I find that socially
              responsible funds do not differ significantly from conventional
              funds in terms of any of these attributes. Moreover, the effect
              of diversification on investment performance is not different
              between the two groups. Both groups underperform the Domini 400
              Social Index and S\&P 500 during the study period.",
  journal  = "J. Financ. Res.",
  volume   =  28,
  number   =  1,
  pages    = "41--57",
  month    =  mar,
  year     =  2005
}

@ARTICLE{Kempf2007-me,
  title    = "The Effect of Socially Responsible Investing on Portfolio
              Performance",
  author   = "Kempf, Alexander and Osthoff, Peer",
  abstract = "Abstract More and more investors apply socially responsible
              screens when building their stock portfolios. This raises the
              question whether these investors can increase their performance
              by incorporating such screens into their investment process. To
              answer this question we implement a simple trading strategy based
              on socially responsible ratings from the KLD Research \&
              Analytics: Buy stocks with high socially responsible ratings and
              sell stocks with low socially responsible ratings. We find that
              this strategy leads to high abnormal returns of up to 8.7\% per
              year. The maximum abnormal returns are reached when investors
              employ the best-in-class screening approach, use a combination of
              several socially responsible screens at the same time, and
              restrict themselves to stocks with extreme socially responsible
              ratings. The abnormal returns remain significant even after
              taking into account reasonable transaction costs.",
  journal  = "Eur Financial Management",
  volume   =  13,
  number   =  5,
  pages    = "908--922",
  month    =  nov,
  year     =  2007
}

@ARTICLE{Schueth2003-tk,
  title     = "Socially Responsible Investing in the United States",
  author    = "Schueth, Steve",
  abstract  = "Socially responsible investing (SRI) has emerged in recent years
               as a dynamic and quickly growing segment of the U.S. financial
               services industry involving over \$2 trillion in professionally
               managed assets. Its conceptual origins can be found in the early
               history of civilization, with it's modern roots in the 1960s.
               This paper provides an overview of the breadth and depth of the
               concept and practice of socially and environmentally responsible
               investing, describes the investment strategies that together
               define SRI as currently practiced in the U.S., offers several
               observations about some of the factors fueling its dramatic
               growth, and presents data showing that investors who choose to
               invest in a socially and environmentally responsible manner can
               do so without giving up investment returns. SRI has matured to a
               point where virtually any investment need can be met through
               portfolio design that integrates an investor's personal values,
               institutional mission, and/or social priorities.The socially
               responsible investment industry in the UnitedStates is a young
               phenomenon. Even referring to it as an ``industry'' ten years
               ago may have been a bit of a stretch. While it has grown
               dramatically in recent years, it is an area of work, of study
               and of practical application that continues to evolve in many
               significant ways.One intriguing example of the ongoing
               development of the field can be found in the analysis of the
               language used to describe it. The terms social investing,
               socially responsible investing, ethical investing, socially
               aware investing, socially conscious investing, green investing,
               values-based investing, and mission-based or mission-related
               investing all refer to the same general process and are often
               used interchangeably.",
  journal   = "J. Bus. Ethics",
  publisher = "Kluwer Academic Publishers",
  volume    =  43,
  number    =  3,
  pages     = "189--194",
  month     =  mar,
  year      =  2003,
  language  = "en"
}

@TECHREPORT{noauthor_2018-zn,
  title       = "{UK} Cryptoasset Taskforce Final Report",
  institution = "FCA, Bank of England, HM Treasury",
  year        =  2018
}

@ARTICLE{Nakamoto2008-rj,
  title     = "Bitcoin: A peer-to-peer electronic cash system",
  author    = "Nakamoto, Satoshi",
  publisher = "Working Paper",
  year      =  2008
}

@ARTICLE{Maxmen2018-ok,
  title    = "{AI} researchers embrace Bitcoin technology to share medical data",
  author   = "Maxmen, Amy",
  journal  = "Nature",
  volume   =  555,
  number   =  7696,
  pages    = "293--294",
  month    =  mar,
  year     =  2018,
  language = "en"
}

@ARTICLE{Juhasz2018-ip,
  title    = "A Bayesian approach to identify Bitcoin users",
  author   = "Juh{\'a}sz, P{\'e}ter L and St{\'e}ger, J{\'o}zsef and Kondor,
              D{\'a}niel and Vattay, G{\'a}bor",
  abstract = "Bitcoin is a digital currency and electronic payment system
              operating over a peer-to-peer network on the Internet. One of its
              most important properties is the high level of anonymity it
              provides for its users. The users are identified by their Bitcoin
              addresses, which are random strings in the public records of
              transactions, the blockchain. When a user initiates a Bitcoin
              transaction, his Bitcoin client program relays messages to other
              clients through the Bitcoin network. Monitoring the propagation
              of these messages and analyzing them carefully reveal hidden
              relations. In this paper, we develop a mathematical model using a
              probabilistic approach to link Bitcoin addresses and transactions
              to the originator IP address. To utilize our model, we carried
              out experiments by installing more than a hundred modified
              Bitcoin clients distributed in the network to observe as many
              messages as possible. During a two month observation period we
              were able to identify several thousand Bitcoin clients and bind
              their transactions to geographical locations.",
  journal  = "PLoS One",
  volume   =  13,
  number   =  12,
  pages    = "e0207000",
  month    =  dec,
  year     =  2018,
  language = "en"
}

@ARTICLE{noauthor_2018-ya,
  title    = "'Mining' Bitcoin takes more energy than mining gold",
  journal  = "Nature",
  volume   =  563,
  number   =  7731,
  pages    = "296",
  month    =  nov,
  year     =  2018,
  keywords = "Energy",
  language = "en"
}

@ARTICLE{Bohme2015-vz,
  title    = "Bitcoin: Economics, Technology, and Governance",
  author   = "B{\"o}hme, Rainer and Christin, Nicolas and Edelman, Benjamin and
              Moore, Tyler",
  abstract = "Bitcoin: Economics, Technology, and Governance by Rainer
              B{\"o}hme, Nicolas Christin, Benjamin Edelman and Tyler Moore.
              Published in volume 29, issue 2, pages 213-38 of Journal of
              Economic Perspectives, Spring 2015, Abstract: Bitcoin is an
              online communication protocol that facilitates the use of a
              vir...",
  journal  = "J. Econ. Perspect.",
  volume   =  29,
  number   =  2,
  pages    = "213--238",
  month    =  may,
  year     =  2015
}

@TECHREPORT{Calomiris2019-co,
  title       = "Interbank Connections, Contagion and Bank Distress in the
                 Great Depression",
  author      = "Calomiris, Charles W and Jaremski, Matthew and Wheelock, David
                 C",
  institution = "Federal Reserve Bank of St. Louis",
  year        =  2019
}

@ARTICLE{Sun_undated-vu,
  title  = "Financial Networks and Systemic Risk in China's Banking System",
  author = "Sun, Lixin"
}

@ARTICLE{Hotz-Behofsits2018-ix,
  title    = "Predicting crypto-currencies using sparse non-Gaussian state
              space models",
  author   = "Hotz-Behofsits, Christian and Huber, Florian and Z{\"o}rner,
              Thomas Otto",
  abstract = "Abstract In this paper we forecast daily returns of
              crypto-currencies using a wide variety of different econometric
              models. To capture salient features commonly observed in
              financial time series like rapid changes in the conditional
              variance, non-normality of the measurement errors and sharply
              increasing trends, we develop a time-varying parameter VAR with
              t-distributed measurement errors and stochastic volatility. To
              control for overparametrization, we rely on the Bayesian
              literature on shrinkage priors, which enables us to shrink
              coefficients associated with irrelevant predictors and/or perform
              model specification in a flexible manner. Using around one year
              of daily data, we perform a real-time forecasting exercise and
              investigate whether any of the proposed models is able to
              outperform the naive random walk benchmark. To assess the
              economic relevance of the forecasting gains produced by the
              proposed models we, moreover, run a simple trading exercise.",
  journal  = "J. Forecast.",
  volume   =  37,
  number   =  6,
  pages    = "627--640",
  month    =  sep,
  year     =  2018
}

@ARTICLE{Hou2019-uz,
  title     = "Which Factors?",
  author    = "Hou, Kewei and Mo, Haitao and Xue, Chen and Zhang, Lu",
  abstract  = "Many recently proposed, seemingly different factor models are
               closely related. In spanning tests, the q-factor model largely
               subsumes the Fama--French five- and six-factor models, and the
               q5 model subsumes the Stambaugh--Yuan four-factor model. Their
               ``mispricing'' factors are sensitive to the construction
               procedure, and once replicated via the traditional approach, are
               close to the q-factors, with correlations of 0.8 and 0.84.
               Finally, consistent with the investment CAPM, valuation theory
               predicts a positive relation between the expected investment and
               the expected return.",
  journal   = "Rev Financ",
  publisher = "Oxford University Press",
  volume    =  23,
  number    =  1,
  pages     = "1--35",
  month     =  feb,
  year      =  2019
}

@ARTICLE{Anundsen2015-xp,
  title    = "Econometric Regime Shifts and the {US} Subprime Bubble",
  author   = "Anundsen, Andr{\'e} K",
  abstract = "Abstract Using aggregate quarterly data for the period
              1975:Q1?2010:Q4, I find that the US housing market changed from a
              stable regime with prices determined by fundamentals, to a highly
              unstable regime at the beginning of the previous decade. My
              results indicate that these imbalances could have been detected
              with the aid of real-time econometric modeling. With reference to
              Stiglitz's general conception of a bubble, I use the econometric
              results to construct two bubble indicators, which clearly
              demonstrate the transition to an unstable regime in the early
              2000s. The indicators are shown to Granger cause a set of
              coincident indicators and financial (in)stability measures.
              Finally, it is shown that the increased subprime exposure during
              the 2000s can explain the econometric breakdown, i.e.?the housing
              bubble may be attributed to the increased borrowing to a more
              risky segment of the market. Copyright ? 2013 John Wiley \& Sons,
              Ltd.",
  journal  = "J. Appl. Econ.",
  volume   =  30,
  number   =  1,
  pages    = "145--169",
  month    =  jan,
  year     =  2015
}

@ARTICLE{Ang2012-gf,
  title     = "Regime Changes and Financial Markets",
  author    = "Ang, Andrew and Timmermann, Allan",
  abstract  = "Regime-switching models can match the tendency of financial
               markets to often change their behavior abruptly and the
               phenomenon that the new behavior of financial variables often
               persists for several periods after such a change. Although the
               regimes captured by regime-switching models are identified by an
               econometric procedure, they often correspond to different
               periods in regulation, policy, and other secular changes. In
               empirical estimates, the means, volatilities, autocorrelations,
               and cross-covariances of asset returns often differ across
               regimes in a manner that allows regime-switching models to
               capture the stylized behavior of many financial series including
               fat tails, heteroskedasticity, skewness, and time-varying
               correlations. In equilibrium models, regimes in fundamental
               processes, such as consumption or dividend growth, strongly
               affect the dynamic properties of equilibrium asset prices and
               can induce nonlinear risk-return trade-offs. Regime switches
               also lead to potentially large consequences for investors'
               optimal portfolio choice.",
  journal   = "Annu. Rev. Financ. Econ.",
  publisher = "Annual Reviews",
  volume    =  4,
  number    =  1,
  pages     = "313--337",
  month     =  oct,
  year      =  2012
}

@ARTICLE{Adrian2015-as,
  title     = "Financial Stability Monitoring",
  author    = "Adrian, Tobias and Covitz, Daniel and Liang, Nellie",
  abstract  = "We present a forward-looking monitoring program to identify and
               track the sources of systemic risk over time and to facilitate
               the development of preemptive policies to promote financial
               stability. We offer a framework that distinguishes between
               shocks, which are difficult to prevent, and vulnerabilities,
               which amplify shocks. Building on substantial research, we focus
               on leverage, maturity transformation, interconnectedness,
               complexity, and the pricing of risk as the primary
               vulnerabilities in the financial system. The monitoring program
               tracks these vulnerabilities in four areas: the banking sector,
               shadow banking, asset markets, and the nonfinancial sector. The
               framework also highlights the policy trade-off between reducing
               systemic risk and raising the cost of financial intermediation
               by taking preemptive actions to reduce vulnerabilities.",
  journal   = "Annu. Rev. Financ. Econ.",
  publisher = "Annual Reviews",
  volume    =  7,
  number    =  1,
  pages     = "357--395",
  month     =  dec,
  year      =  2015
}

@ARTICLE{Claessens2015-vd,
  title     = "An Overview of Macroprudential Policy Tools",
  author    = "Claessens, Stijn",
  abstract  = "Macroprudential policies?caps on loan to value ratios, limits on
               credit growth and other balance-sheet restrictions,
               (countercyclical) capital and reserve requirements and
               surcharges, and Pigouvian levies?have become part of the policy
               paradigm in emerging markets and developed countries alike. But
               knowledge of these tools is still limited. Macroprudential
               policies ought to be motivated by market failures and
               externalities, but these can be hard to identify. They may also
               interact with various other policies, such as monetary and
               microprudential, raising coordination issues. Countries,
               especially emerging markets, have used these tools, and analyses
               suggest that some of those tools reduce procyclicality and
               crisis risks. Yet, much remains to be studied, including the
               costs of such tools, as they may adversely affect resource
               allocations; how best to adapt these tools to a country's
               circumstances; and preferred institutional designs, including
               how to address political economy risks. As such, policy makers
               should move carefully in adopting these tools.",
  journal   = "Annu. Rev. Financ. Econ.",
  publisher = "Annual Reviews",
  volume    =  7,
  number    =  1,
  pages     = "397--422",
  month     =  dec,
  year      =  2015
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gelman_undated-im,
  title  = "The failure of null hypothesis significance testing when studying
            incremental changes, and what to do about it‚àó",
  author = "Gelman, Andrew"
}

@ARTICLE{Trinks2017-iz,
  title    = "The Opportunity Cost of Negative Screening in Socially
              Responsible Investing",
  author   = "Trinks, Pieter Jan and Scholtens, Bert",
  abstract = "This paper investigates the impact of negative screening on the
              investment universe as well as on financial performance. We come
              up with a novel identification process and as such depart from
              mainstream socially responsible investing literature by
              concentrating on individual firms' conduct and by studying a much
              wider range of issues. Firstly, we study the size and financial
              performance of fourteen potentially controversial issues:
              abortion, adult entertainment, alcohol, animal testing,
              contraceptives, controversial weapons, fur, gambling, genetic
              engineering, meat, nuclear power, pork, (embryonic) stem cells,
              and tobacco. We investigate an international sample of more than
              1,600 stocks for more than twenty years. We then analyze the
              impact of applying negative screens to a market portfolio. Our
              findings suggest that the choice for negative screening
              strategies does matter for the size of the investment universe as
              well as for risk-adjusted return performance. Investing in
              controversial stocks in many cases results in additional
              risk-adjusted returns, whereas excluding them may reduce
              financial performance. These findings suggest that there are
              opportunity costs to negative screening.",
  journal  = "J. Bus. Ethics",
  volume   =  140,
  number   =  2,
  pages    = "193--208",
  month    =  jan,
  year     =  2017
}

@ARTICLE{Kelly2008-ii,
  title     = "Understanding the Role of the Football Manager in Britain and
               Ireland: A Weberian Approach",
  author    = "Kelly, Seamus",
  abstract  = "Abstract Using semi-structured tape-recorded interviews with
               twenty-two players and eighteen managers, this paper utilizes
               the work of Max Weber in exploring the role of the manager in
               professional football (soccer). More specifically, Weber's
               writings on legitimate authority are used to explore traditional
               aspects of the role of the contemporary football manager. The
               findings highlight the lack of formal management training and
               the widespread assumption within football that previous playing
               experience is sufficient preparation for entry into management.
               While many aspects of the management of football clubs have
               involved increasing professionalization and bureaucratization,
               the role of the manager has proved remarkably resistant to these
               processes. The authority of the football manager continues to be
               based on traditional forms of authoritarianism and this allows
               managers an unusually high degree of autonomy in defining their
               own role and places few constraints on the appointment of their
               support staff.",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  8,
  number    =  4,
  pages     = "399--419",
  month     =  dec,
  year      =  2008
}

@ARTICLE{Jiang2019-en,
  title    = "Manager sentiment and stock returns",
  author   = "Jiang, Fuwei and Lee, Joshua and Martin, Xiumin and Zhou, Guofu",
  abstract = "This paper constructs a manager sentiment index based on the
              aggregated textual tone of corporate financial disclosures. We
              find that manager sentiment is a strong negative predictor of
              future aggregate stock market returns, with monthly in-sample and
              out-of-sample R2s of 9.75\% and 8.38\%, respectively, which is
              far greater than the predictive power of other previously studied
              macroeconomic variables. Its predictive power is economically
              comparable and is informationally complementary to existing
              measures of investor sentiment. Higher manager sentiment precedes
              lower aggregate earnings surprises and greater aggregate
              investment growth. Moreover, manager sentiment negatively
              predicts cross-sectional stock returns, particularly for firms
              that are difficult to value and costly to arbitrage.",
  journal  = "J. financ. econ.",
  volume   =  132,
  number   =  1,
  pages    = "126--149",
  month    =  apr,
  year     =  2019,
  keywords = "Manager sentiment; Textual tone; Investor sentiment; Asset
              pricing; Return predictability"
}

@ARTICLE{Birru2018-yp,
  title    = "Day of the week and the cross-section of returns",
  author   = "Birru, Justin",
  abstract = "Long-short anomaly returns are strongly related to the day of the
              week. Anomalies for which the speculative leg is the short (long)
              leg experience the highest (lowest) returns on Monday. The
              opposite pattern is observed on Friday. The effects are large.
              Monday (Friday) alone accounts for over 100\% of returns for all
              anomalies examined for which the short (long) leg is the
              speculative leg. Consistent with a mispricing explanation, the
              pattern is driven by the speculative leg. The observed patterns
              are consistent with the abundance of evidence in the psychology
              literature that mood increases on Friday and decreases on Monday.",
  journal  = "J. financ. econ.",
  volume   =  130,
  number   =  1,
  pages    = "182--214",
  month    =  oct,
  year     =  2018,
  keywords = "Investor sentiment; Return predictability; Anomalies"
}

@ARTICLE{Gelman_undated-tj,
  title  = "Too little attention has been paid to the statistical challenges in
            estimating small effects",
  author = "Gelman, Andrew and Weakliem, David"
}

@ARTICLE{Engle2016-ez,
  title     = "Dynamic Conditional Beta",
  author    = "Engle, Robert F",
  abstract  = "Dynamic conditional beta is an approach to estimating
               regressions with time varying parameters. The conditional
               covariance matrices of the exogenous and dependent variable for
               each time period are used to formulate the dynamic beta. Joint
               estimation of the covariance matrices and other regression
               parameters is developed. Tests of the hypothesis that betas are
               constant are non-nested tests and several approaches are
               developed including a novel nested model. The methodology is
               applied to industry multifactor asset pricing and to global
               systemic risk estimation with non-synchronous prices.",
  journal   = "Journal of Financial Econometrics",
  publisher = "Narnia",
  volume    =  14,
  number    =  4,
  pages     = "643--667",
  month     =  sep,
  year      =  2016
}

@TECHREPORT{Engle2012-fp,
  title       = "Dynamic Conditional Beta and Systemic Risk in Europe",
  author      = "Engle, R and Jondeau, E and Rockinger, M",
  institution = "Working Paper",
  year        =  2012
}

@MISC{Alessandrini_undated-ur,
  title   = "{ESG} Investing: From Sin Stocks to Smart Beta",
  author  = "Alessandrini, Fabio and Jondeau, Eric",
  journal = "SSRN Electronic Journal"
}

@ARTICLE{Fuertes2006-br,
  title    = "Early warning systems for sovereign debt crises: The role of
              heterogeneity",
  author   = "Fuertes, Ana-Maria and Kalotychou, Elena",
  abstract = "Sovereign default models that differ in their treatment of
              unobservable country, regional and time heterogeneities are
              systematically compared. The analysis is based on annual data
              over the 1983--2002 period for 96 developing economies.
              Inference-based criteria and parameter plausibility
              overwhelmingly favour more complex models that allow the link
              between the probability response and the fundamentals to vary
              over time and across countries. However, out-of-sample forecast
              evaluation using several loss functions and
              equal-predictive-ability tests suggests that simplicity beats
              complexity. Parsimonious pooled logit models produce the most
              accurate sovereign default forecasts and outperform the naive
              benchmarks.",
  journal  = "Comput. Stat. Data Anal.",
  volume   =  51,
  number   =  2,
  pages    = "1420--1441",
  month    =  nov,
  year     =  2006,
  keywords = "Credit risk; Default probability; Emerging markets; Loss
              function; Panel logit; Predictive performance; Unobserved
              heterogeneity"
}

@ARTICLE{noauthor_undated-vr,
  title = "Risk Topography: Systemic Risk and Macro Modeling (National Bureau
           of Economic Research Conference Report)"
}

@ARTICLE{Brunnermeier2014-qf,
  title    = "Risk Topography: Systemic Risk and Macro Modeling",
  author   = "Brunnermeier, Markus and Krishnamurthy, Arvind",
  month    =  aug,
  year     =  2014
}

@ARTICLE{Cai2018-su,
  title    = "Syndication, interconnectedness, and systemic risk",
  author   = "Cai, Jian and Eidam, Frederik and Saunders, Anthony and Steffen,
              Sascha",
  abstract = "Syndication increases the overlap of bank loan portfolios and
              makes them more vulnerable to contagious effects. We develop a
              novel measure of bank interconnectedness using syndicated
              corporate loan portfolios, overlap based on industry and region,
              and different weights such as equal weights, size and
              relationships. We find that interconnectedness is driven mainly
              by bank diversification, less by bank size or overall loan market
              size. Interconnectedness is positively correlated with different
              bank-level systemic risk measures including SRISK, DIP and CoVaR,
              and such a positive correlation mainly arises from an elevated
              effect of interconnectedness on systemic risk during recessions.
              Overall, our results highlight that institution-level risk
              reduction through diversification ignores the negative
              externalities of an interconnected financial system.",
  journal  = "Journal of Financial Stability",
  volume   =  34,
  pages    = "105--120",
  month    =  feb,
  year     =  2018,
  keywords = "Interconnectedness; Networks; Syndicated loans; Systemic risk"
}

@ARTICLE{Fare2003-vg,
  title     = "Nonparametric Productivity Analysis with Undesirable Outputs:
               Comment",
  author    = "F{\"a}re, Rolf and Grosskopf, Shawna",
  abstract  = "In a recent article in this journal Hailu and Veeman develop a
               production model that includes undesirable outputs, which they
               use as a basis for measuring produ",
  journal   = "Am. J. Agric. Econ.",
  publisher = "Oxford University Press",
  volume    =  85,
  number    =  4,
  pages     = "1070--1074",
  month     =  nov,
  year      =  2003
}

@ARTICLE{Cole2015-bt,
  title    = "Too many digits: the presentation of numerical data",
  author   = "Cole, T J",
  journal  = "Arch. Dis. Child.",
  volume   =  100,
  number   =  7,
  pages    = "608--609",
  month    =  jul,
  year     =  2015,
  keywords = "Measurement; Statistics",
  language = "en"
}

@ARTICLE{Bai2019-um,
  title    = "The {CAPM} strikes back? An equilibrium model with disasters",
  author   = "Bai, Hang and Hou, Kewei and Kung, Howard and Li, Erica X N and
              Zhang, Lu",
  abstract = "Embedding disasters into a general equilibrium model with
              heterogeneous firms induces strong nonlinearity in the pricing
              kernel, helping explain the empirical failure of the
              (consumption) CAPM. Our single-factor model reproduces the
              failure of the CAPM in explaining the value premium in finite
              samples without disasters and its relative success in samples
              with disasters. Due to beta measurement errors, the estimated
              beta-return relation is flat, consistent with the beta
              ``anomaly,'' even though the true beta-return relation is
              strongly positive. Finally, the consumption CAPM fails in
              simulations, even though a nonlinear model with the true pricing
              kernel holds exactly by construction.",
  journal  = "J. financ. econ.",
  volume   =  131,
  number   =  2,
  pages    = "269--298",
  month    =  feb,
  year     =  2019,
  keywords = "CAPM; Rare disasters; Measurement errors; Consumption CAPM;
              General equilibrium"
}

@MISC{noauthor_undated-dy,
  title        = "Climate change and health",
  abstract     = "WHO fact sheet on climate change and health: provides key
                  facts, patterns of infection, measuring health effects and
                  WHO response.",
  howpublished = "\url{https://www.who.int/news-room/fact-sheets/detail/climate-change-and-health}",
  note         = "Accessed: 2019-6-5"
}

@MISC{Cihak2013-dy,
  title   = "Bank regulation and supervision in the context of the global
             crisis",
  author  = "Cihak, Martin and Demirg{\"u}{\c c}-Kunt, Asli and Peria, Maria
             Soledad Martinez and Mohseni-Cheraghlou, Amin",
  journal = "Journal of Financial Stability",
  volume  =  9,
  number  =  4,
  pages   = "733--746",
  year    =  2013
}

@ARTICLE{Alexander2019-sc,
  title    = "A parsimonious parametric model for generating margin
              requirements for futures",
  author   = "Alexander, Carol and Kaeck, Andreas and Sumawong, Anannit",
  abstract = "Major exchanges employ the Standard Portfolio Analysis of Risk
              (SPAN) software to measure maintenance margins. However, its
              methodology has become cumbersome and opaque, having evolved over
              several decades and by now it requires that several hundred
              parameter values are re-set every day. We present a new,
              parsimonious parametric model for calculating margin requirements
              for futures which has a rigorous econometric foundation, being
              derived entirely from the median tail loss (MTL) of the returns
              distribution. This facilitates maximum likelihood volatility
              model calibration and state-of-the-art backtests. Then the
              parameters of the margin scheme which overlays the MTL may be
              calibrated using a variety of objectives. We examine three such
              objectives, including two which are designed to generate margins
              which mimic SPAN.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  273,
  number   =  1,
  pages    = "31--43",
  month    =  feb,
  year     =  2019,
  keywords = "Finance; Backtesting; Margin rules; Median tail loss; WTI crude
              oil"
}

@ARTICLE{Hao2019-ip,
  title     = "We analyzed 16,625 papers to figure out where {AI} is headed
               next",
  author    = "Hao, Karen",
  abstract  = "Our study of 25 years of artificial-intelligence research
               suggests the era of deep learning may come to an end.",
  journal   = "MIT Technology Review",
  publisher = "MIT Technology Review",
  month     =  jan,
  year      =  2019
}

@ARTICLE{Brooks2018-vq,
  title    = "The effects of environmental, social and governance disclosures
              and performance on firm value: A review of the literature in
              accounting and finance",
  author   = "Brooks, Chris and Oikonomou, Ioannis",
  abstract = "This paper not only attempts to survey the burgeoning literature
              on environmental, social and governance disclosures and
              performance and their effects on firm value, but its focus also
              lies on highlighting stylised observations coming from the most
              recent work that has not yet become part of the `conventional
              wisdom' in the field. In addition, it outlines some of the
              crucial knowledge gaps and interesting questions that have not,
              as of yet, been addressed and thus outlines a potential agenda
              for future research on socially responsible investing. Lastly, it
              introduces the papers published in this special issue of the
              British Accounting Review.",
  journal  = "The British Accounting Review",
  volume   =  50,
  number   =  1,
  pages    = "1--15",
  month    =  jan,
  year     =  2018,
  keywords = "Corporate social responsibility (CSR); Environmental; Social and
              governance (ESG) disclosures; Firm value; Stock market
              performance; Literature review; Socially responsible investing
              (SRI); Ethical investing"
}

@MISC{noauthor_undated-dw,
  title        = "Damodaran Online: Home Page for Aswath Damodaran",
  howpublished = "\url{http://pages.stern.nyu.edu/~adamodar/}",
  note         = "Accessed: 2019-3-26"
}

@ARTICLE{Kogut2001-tq,
  title     = "Capabilities as Real Options",
  author    = "Kogut, Bruce and Kulatilaka, Nalin",
  abstract  = "Strategy research consists of a balance between positive and
               normative theory. Normative theories suggest particular
               heuristics, or cognitive representations, to find appropriate
               solutions. Heuristics permit faster solutions to real-time
               problems; they also suffer from the potential of negative
               transfer to inappropriate applications. The theory of real
               options provides the appropriate heuristic framing of
               competencies and exploratory search. A real options approach
               marries the theory of financial options to foundational ideas in
               strategy, organizational theory, and complex systems. We join
               these approaches to identify three pairs of concepts: scarce
               factor and the underlying asset in option theory, inertia and
               irreversibility, and the ruggedness of landscape and option
               values. Strategic theories of resources largely define a core
               competence as unique and nonimmutable. In doing so, this
               definition has wrongly forgotten Barney's initial insight into
               scarce factor markets as determining the valuation of a
               competitive asset. Financial theory of real options derives its
               heuristics of investing in exploratory search by inferring
               future value of today's investments from market prices. We apply
               the three conceptual pairs to the evaluation of capabilities as
               real options through a formal descriptive model. The valuation
               of core capabilities is derived from observing the price
               dynamics of correlated strategic factors in the market. Because
               of inertia, managers cannot easily adjust the wrong set of
               organizational capabilities to the emergence of market
               opportunities. However, firms that have made investments in
               capabilities appropriate to these opportunities are able to
               respond. From this description, we define core competence as the
               choice of capabilities that permits the firm to make the best
               response to market opportunities. The heuristic framing of
               capabilities as real options guides the normative evaluation of
               the balance between exploitation and exploration.",
  journal   = "Organization Science",
  publisher = "INFORMS",
  volume    =  12,
  number    =  6,
  pages     = "744--758",
  month     =  dec,
  year      =  2001
}

@ARTICLE{Trigeorgis2017-nj,
  title     = "Real options theory in strategic management",
  author    = "Trigeorgis, Lenos and Reuer, Jeffrey J",
  abstract  = "Research summary: This article provides a review of real options
               theory (ROT) in strategic management research. We review the
               fundamentals of ROT and provide a taxonomy of this research. By
               synthesizing and critiquing research on real options, we
               identify a number of important challenges as well as
               opportunities for ROT if it is to enhance its impact on
               strategic management and potentially develop into a theoretical
               pillar in the field. We examine how ROT can inform the key
               tensions that managers face between commitment versus
               flexibility as well as between competition versus cooperation,
               and we show how it can uniquely address the fundamental issues
               in strategy. We conclude with suggestions on future research
               directions that could enhance and unify the thus-far distinct
               main approaches to real options research. Managerial summary:
               Real options theory (ROT) applies the heuristics and valuation
               models originally designed for financial securities to the
               domain of corporate investment decisions (e.g., joint ventures
               [JVs], foreign direct investment, research and development
               [R\&D], etc.) and strategic decision making under uncertainty.
               This article provides a synthesis of this body of research in
               strategic management and related disciplines. We suggest how ROT
               can address fundamental issues of strategy, including the
               dilemmas managers face between commitment versus flexibility as
               well as between competition versus cooperation. We discuss how
               three distinct approaches to real options analysis can
               complement each other, and we identify some of the main
               challenges and opportunities for ROT to become a theoretical
               pillar in strategy. Copyright ? 2016 John Wiley \& Sons, Ltd.",
  journal   = "Strat. Mgmt. J.",
  publisher = "Wiley Online Library",
  volume    =  38,
  number    =  1,
  pages     = "42--63",
  month     =  jan,
  year      =  2017
}

@ARTICLE{Husted2005-gr,
  title     = "Risk Management, Real Options, Corporate Social Responsibility",
  author    = "Husted, Bryan W",
  abstract  = "The relationship of corporate social responsibility to risk
               management has been treated sporadically in the business society
               literature. Using real options theory, I develop the notion of
               corporate social responsibility as a real option its
               implications for risk management. Real options theory allows for
               a strategic view of corporate social responsibility.
               Specifically, real options theory suggests that corporate social
               responsibility should be negatively related to the firm's ex
               ante downside business risk.",
  journal   = "J. Bus. Ethics",
  publisher = "Kluwer Academic Publishers",
  volume    =  60,
  number    =  2,
  pages     = "175--183",
  month     =  aug,
  year      =  2005,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kothari2007-vz,
  title     = "Econometrics of event studies",
  author    = "Kothari, S P and Warner, J B",
  abstract  = "The number of published event studies exceeds 500, and the
               literature continues to grow. We provide an overview of event
               study methods. Short-horizon methods are quite reliable. While
               long-horizon methods have improved, serious limitations remain.
               A challenge is to ‚Ä¶",
  journal   = "Handbook of empirical corporate finance",
  publisher = "Elsevier",
  year      =  2007
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Kai2007-oh,
  title     = "Self-selection models in corporate finance",
  booktitle = "Handbook of empirical corporate finance",
  author    = "Kai, Li and Prabhala, Nagpurnanand R",
  abstract  = "Corporate finance decisions are not made at random, but are
               usually deliberate decisions by firms or their managers to
               self-select into their preferred choices. This chapter reviews
               econometric models of self-selection. The review is organized
               into two parts. The first part ‚Ä¶",
  publisher = "Elsevier",
  pages     = "37--86",
  year      =  2007
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Harvey2016-vd,
  title     = "‚Ä¶ and the {Cross-Section} of Expected Returns",
  author    = "Harvey, Campbell R and Liu, Yan and Zhu, Heqing",
  abstract  = "Hundreds of papers and factors attempt to explain the
               cross-section of expected returns. Given this extensive data
               mining, it does not make sense to use the usual criteria for
               establishing significance. Which hurdle should be used for
               current research? Our paper introduces a new multiple testing
               framework and provides historical cutoffs from the first
               empirical tests in 1967 to today. A new factor needs to clear a
               much higher hurdle, with a t-statistic greater than 3.0. We
               argue that most claimed research findings in financial economics
               are likely false.Received October 22, 2014; accepted June 15,
               2015 by Editor Andrew Karolyi.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Narnia",
  volume    =  29,
  number    =  1,
  pages     = "5--68",
  month     =  jan,
  year      =  2016
}

@ARTICLE{Flammer2013-qn,
  title     = "Corporate Social Responsibility and Shareholder Reaction: The
               Environmental Awareness of Investors",
  author    = "Flammer, Caroline",
  abstract  = "This study examines whether shareholders are sensitive to
               corporations' environmental footprint. Specifically, I conduct
               an event study around the announcement of corporate news related
               to environment for all US publicly traded companies from 1980 to
               2009. In keeping with the view that environmental corporate
               social responsibility (CSR) generates new and competitive
               resources for firms, I find that companies reported to behave
               responsibly toward the environment experience a significant
               stock price increase, whereas firms that behave irresponsibly
               face a significant decrease. Extending this view of
               ?environment-as-a-resource,? I posit that the value of
               environmental CSR depends on external and internal moderators.
               First, I argue that external pressure to behave responsibly
               towards the environment?which has increased dramatically over
               recent decades?exacerbates the punishment for eco-harmful
               behavior and reduces the reward for eco-friendly initiatives.
               This argument is supported by the data: over time, the negative
               stock market reaction to eco-harmful behavior has increased,
               while the positive reaction to eco-friendly initiatives has
               decreased. Second, I argue that environmental CSR is a resource
               with decreasing marginal returns and insurance-like features. In
               keeping with this view, I find that the positive (negative)
               stock market reaction to eco-friendly (-harmful) events is
               smaller for companies with higher levels of environmental CSR.",
  journal   = "AMJ",
  publisher = "Academy of Management",
  volume    =  56,
  number    =  3,
  pages     = "758--781",
  month     =  jun,
  year      =  2013
}

@ARTICLE{Maiorano2017-cm,
  title     = "Is There a Credit Union Difference? Comparing Canadian Credit
               Union and Bank Branch Locations",
  author    = "Maiorano, John and Mook, Laurie and Quarter, Jack",
  abstract  = "This study of credit union and bank branch locations and
               neighbourhoods in Canada seeks to discover if there is a
               distinct credit union niche. The study builds on an earlier
               paper of credit unions and banks in the US which found that
               credit unions in Wisconsin, Arizona and New Hampshire were more
               likely to be located in lower-income areas than bank branches
               (Mook, Maiorano \& Quarter, 2015). In Canada, we find that
               credit union branches are over-represented in rural areas, and
               under-represented in large population centres relative to bank
               branches. Additionally, credit unions are overrepresented in
               middle income areas and underrepresented in high income areas
               compared to bank branches both at the national level and in all
               provinces where differences are statistically significant.
               Another significant finding is that while both credit unions and
               banks cater to marginalized communities, the type of
               marginalized communities they cater to distinguishes them.
               Making use of the Canadian Marginalization Index, we find credit
               union branches in Canada to be overrepresented in communities
               marginalized along the dimensions of Material Deprivation and
               Dependency, while bank branches are overrepresented in
               communities marginalized along the dimension of Residential
               Instability and Ethnic Concentration.",
  journal   = "Canadian journal of nonprofit and social economy research",
  publisher = "anserj.ca",
  volume    =  7,
  number    =  2,
  month     =  jan,
  year      =  2017,
  keywords  = "Credit Unions; Canada; niche; location; branch",
  language  = "en"
}

@ARTICLE{Serenko2016-ns,
  title     = "An application of the knowledge management maturity model: the
               case of credit unions",
  author    = "Serenko, Alexander and Bontis, Nick and Hull, Emily",
  abstract  = "AbstractThe purpose of this study is to investigate the level of
               knowledge management (KM) maturity of credit unions. The
               application of a maturity model to 15 credit unions in North
               America revealed that an overall level of KM maturity is at an
               early stage of development, but there are signs of future
               improvement. Credit unions operate in a highly competitive,
               knowledge-intensive financial industry and experience various
               pressures to increase their efficiency, which they can achieve
               through the implementation of KM solutions. Despite the absence
               of official KM strategies, KM projects were introduced locally
               in order to fill particular knowledge gaps. The availability of
               IT infrastructure and the implementation of KM-related
               technologies alone are insufficient to ensure universal success
               of organizational KM activities. Credit union managers
               periodically access and use academic research in their decision
               making. At the same time, they prefer accessing scholarly
               knowledge in translated form from books, practitioner magazines,
               and consultants. It was concluded that organizations competing
               in the knowledge-intensive sector have an inner need for KM
               solutions.",
  journal   = "Knowledge Management Research \& Practice",
  publisher = "Taylor \& Francis",
  volume    =  14,
  number    =  3,
  pages     = "338--352",
  month     =  aug,
  year      =  2016
}

@UNPUBLISHED{Hughes2013-rs,
  title    = "Measuring the Performance of Banks: Theory, Practice, Evidence,
              and Some Policy Implications",
  author   = "Hughes, Joseph P and Mester, Loretta J",
  abstract = "The unique capital structure of commercial banking -- funding
              production with demandable debt that participates in the
              economy's payments system -- affects various aspects of banking.
              It shapes banks' comparative advantage in providing financial
              products and services to informationally opaque customers, their
              ability to diversify credit and liquidity risk, and how they are
              regulated, including the need to obtain a charter to operate and
              explicit and implicit federal guarantees of bank liabilities to
              reduce the probability of bank runs. These aspects of banking
              affect a bank's choice of risk vs. expected return, which, in
              turn, affects bank performance. Banks have an incentive to reduce
              risk to protect the valuable charter from episodes of financial
              distress and they also have an incentive to increase risk to
              exploit the cost-of-funds subsidy of mispriced deposit insurance.
              These are contrasting incentives tied to bank size. Measuring the
              performance of banks and its relationship to size requires
              untangling cost and profit from decisions about risk versus
              expected-return because both cost and profit are functions of
              endogenous risk-taking. This chapter gives an overview of two
              general empirical approaches to measuring bank performance and
              discusses some of the applications of these approaches found in
              the literature. One application explains how better
              diversification available at a larger scale of operations
              generates scale economies that are obscured by higher levels of
              risk-taking. Studies of banking cost that ignore endogenous
              risk-taking find little evidence of scale economies at the
              largest banks while those that control for this risk-taking find
              large scale economies at the largest banks -- evidence with
              important implications for regulation.",
  month    =  aug,
  year     =  2013,
  keywords = "Bank, Efficiency, Risk, Cost, Profit, Scale Economies,
              X-Inefficiency"
}

@INCOLLECTION{Hughes2014-gx,
  title     = "Measuring the Performance of Banks",
  booktitle = "The Oxford Handbook of Banking, Second Edition (2 ed.)",
  author    = "Hughes, Joe and Mester, Loretta J",
  editor    = "Berger, Allen N and Molyneux, Philip and Wilson, John O S",
  abstract  = "The unique capital structure of commercial banking---funding
               production with demandable debt that participates in the
               economy's payments system---affects various aspects of banking.
               It shapes banks' comparative advantage in providing financial
               products and services to informationally opaque customers, their
               ability to diversify credit and liquidity risk, and how they are
               regulated, including the need to obtain a charter to operate and
               explicit and implicit federal guarantees of bank liabilities to
               reduce the probability of bank runs. These aspects of banking
               affect a bank's choice of risk vs. expected return, which, in
               turn, affects bank performance. This chapter gives an overview
               of two general empirical approaches to measuring bank
               performance and discusses some of the applications of these
               approaches found in the literature. One application explains how
               better diversification available at a larger scale of operations
               generates scale economies that are obscured by higher levels of
               risk-taking. Studies of banking cost that ignore endogenous
               risk-taking find little evidence of scale economies at the
               largest banks while those that control for this risk-taking find
               large scale economies at the largest banks---evidence with
               important implications for regulation.",
  publisher = "Oxford University Press",
  month     =  nov,
  year      =  2014,
  keywords  = "bank; efficiency; risk; cost; profit; scale economies;
               X-inefficiency"
}

@UNPUBLISHED{Bremus2018-no,
  title    = "Interactions between Regulatory and Corporate Taxes: How Is Bank
              Leverage Affected?",
  author   = "Bremus, Franziska and Schmidt, Kirsten and Tonzer, Lena",
  abstract = "Regulatory bank levies set incentives for banks to reduce
              leverage. At the same time, corporate income taxation makes
              funding through debt more attractive. In this paper, we explore
              how regulatory levies affect bank capital structure, depending on
              corporate income taxation. Based on bank balance sheet data from
              2006 to 2014 for a panel of EU-banks, our analysis yields three
              main results: The introduction of bank levies leads to lower
              leverage as liabilities become more expensive. This effect is
              weaker the more elevated corporate income taxes are. In countries
              charging very high corporate income taxes, the incentives of bank
              levies to reduce leverage turn ineffective. Thus, bank levies can
              counteract the debt bias of taxation only.",
  month    =  sep,
  year     =  2018,
  keywords = "Bank levies, debt bias of taxation, bank capital structure"
}

@ARTICLE{Apergis2019-rx,
  title    = "Do gold prices respond to real interest rates? Evidence from the
              Bayesian Markov Switching {VECM} model",
  author   = "Apergis, Nicholas and Cooray, Arusha and Khraief, Naceur and
              Apergis, Iraklis",
  abstract = "The goal of this paper is to examine the transmission dynamics
              between the real interest rate and gold prices in the G7. The
              methodology follows the Bayesian Markov-Switching Vector
              Error-Correction (MS-VECM) model, along with regime-dependent
              impulse response functions, spanning the period 1975--2016. The
              findings suggest a positive association between gold prices and
              real interest rates, with the estimates remaining consistently
              positive and statistically significant across all G7 countries.
              The results indicate that gold prices can provide hedging
              services against real interest rate movements mainly during
              recessionary times. Our results continue to be robust when we
              extend the bivariate version of our modeling approach to include
              more drivers for gold prices.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  60,
  pages    = "134--148",
  month    =  may,
  year     =  2019,
  keywords = "Gold; Interest rates; MS-VECM model"
}

@MISC{DeYoung_undated-nq,
  title   = "The Information Revolution and Small Business Lending: The Missing
             Evidence",
  author  = "DeYoung, Robert and Scott Frame, W and Glennon, Dennis and Nigro,
             Peter J",
  journal = "SSRN Electronic Journal"
}

@ARTICLE{Gospodinov2019-xl,
  title    = "Too good to be true? Fallacies in evaluating risk factor models",
  author   = "Gospodinov, Nikolay and Kan, Raymond and Robotti, Cesare",
  abstract = "This paper is concerned with statistical inference and model
              evaluation in possibly misspecified and unidentified linear asset
              pricing models estimated by maximum likelihood. Strikingly, when
              spurious factors (that is, factors that are uncorrelated with the
              returns on the test assets) are present, the model exhibits
              perfect fit, as measured by the squared correlation between the
              model's fitted expected returns and the average realized returns.
              Furthermore, factors that are spurious are selected with high
              probability, and factors that are useful are driven out of the
              model. While ignoring potential misspecification and lack of
              identification can be very problematic for models with
              macroeconomic factors, empirical specifications with traded
              factors (e.g., Fama and French, 1993; Hou et al., 2015) do not
              suffer from the identification problems shown in this study.",
  journal  = "J. financ. econ.",
  volume   =  132,
  number   =  2,
  pages    = "451--471",
  month    =  may,
  year     =  2019,
  keywords = "Asset pricing; Spurious risk factors; Unidentified models; Model
              misspecification; Maximum likelihood; Goodness of fit; Rank test"
}

@ARTICLE{Urquhart2016-jm,
  title    = "The inefficiency of Bitcoin",
  author   = "Urquhart, Andrew",
  abstract = "Bitcoin has received much attention in the media and by investors
              in recent years, although there remains scepticism and a lack of
              understanding of this cryptocurrency. We add to the literature on
              Bitcoin by studying the market efficiency of Bitcoin. Through a
              battery of robust tests, evidence reveals that returns are
              significantly inefficient over our full sample, but when we split
              our sample into two subsample periods, we find that some tests
              indicate that Bitcoin is efficient in the latter period.
              Therefore we conclude that Bitcoin in an inefficient market but
              may be in the process of moving towards an efficient market.",
  journal  = "Econ. Lett.",
  volume   =  148,
  pages    = "80--82",
  month    =  nov,
  year     =  2016,
  keywords = "Bitcoin; Market efficiency; Cryptocurrency; Random walk"
}

@ARTICLE{Urquhart2017-gn,
  title    = "Price clustering in Bitcoin",
  author   = "Urquhart, Andrew",
  abstract = "Investor and media attention in Bitcoin has increased
              substantially in recently years, reflected by the incredible
              surge in news articles and considerable rise in the price of
              Bitcoin. Given the increased attention, there little is known
              about the behaviour of Bitcoin prices and therefore we add to the
              literature by studying price clustering. We find significant
              evidence of clustering at round numbers, with over 10\% of prices
              ending with 00 decimals compared to other variations but there is
              no significant pattern of returns after the round number. We also
              support the negotiation hypothesis of Harris (1991) by showing
              that price and volume have a significant positive relationship
              with price clustering at whole numbers.",
  journal  = "Econ. Lett.",
  volume   =  159,
  pages    = "145--148",
  month    =  oct,
  year     =  2017,
  keywords = "Bitcoin; Price clustering; Cryptocurrency"
}

@MISC{Shen2019-yd,
  title   = "Does twitter predict Bitcoin?",
  author  = "Shen, Dehua and Urquhart, Andrew and Wang, Pengfei",
  journal = "Economics Letters",
  volume  =  174,
  pages   = "118--122",
  year    =  2019
}

@ARTICLE{Katsiampa2017-fv,
  title    = "Volatility estimation for Bitcoin: A comparison of {GARCH} models",
  author   = "Katsiampa, Paraskevi",
  abstract = "We explore the optimal conditional heteroskedasticity model with
              regards to goodness-of-fit to Bitcoin price data. It is found
              that the best model is the AR-CGARCH model, highlighting the
              significance of including both a short-run and a long-run
              component of the conditional variance.",
  journal  = "Econ. Lett.",
  volume   =  158,
  pages    = "3--6",
  month    =  sep,
  year     =  2017,
  keywords = "Bitcoin; Cryptocurrency; GARCH; Volatility"
}

@ARTICLE{Nadarajah2017-av,
  title     = "On the inefficiency of Bitcoin",
  author    = "Nadarajah, Saralees and Chu, Jeffrey",
  journal   = "Econ. Lett.",
  publisher = "Elsevier",
  volume    =  150,
  pages     = "6--9",
  year      =  2017
}

@ARTICLE{Bariviera2017-jh,
  title    = "The inefficiency of Bitcoin revisited: A dynamic approach",
  author   = "Bariviera, Aurelio F",
  abstract = "This letter revisits the informational efficiency of the Bitcoin
              market. In particular we analyze the time-varying behavior of
              long memory of returns on Bitcoin and volatility 2011 until 2017,
              using the Hurst exponent. Our results are twofold. First, R/S
              method is prone to detect long memory, whereas DFA method can
              discriminate more precisely variations in informational
              efficiency across time. Second, daily returns exhibit persistent
              behavior in the first half of the period under study, whereas its
              behavior is more informational efficient since 2014. Finally,
              price volatility, measured as the logarithmic difference between
              intraday high and low prices exhibits long memory during all the
              period. This reflects a different underlying dynamic process
              generating the prices and volatility.",
  journal  = "Econ. Lett.",
  volume   =  161,
  pages    = "1--4",
  month    =  dec,
  year     =  2017,
  keywords = "Bitcoin; Long range dependence; Volatility; Hurst exponent"
}

@ARTICLE{Brauneis2018-qx,
  title    = "Price discovery of cryptocurrencies: Bitcoin and beyond",
  author   = "Brauneis, Alexander and Mestel, Roland",
  abstract = "Academic research on cryptocurrencies is almost exclusively
              directed towards Bitcoin. We extend existing literature by
              performing various tests on efficiency of several
              cryptocurrencies and additionally link efficiency to measures of
              liquidity. Cryptocurrencies become less predictable / inefficient
              as liquidity increases.",
  journal  = "Econ. Lett.",
  volume   =  165,
  pages    = "58--61",
  month    =  apr,
  year     =  2018,
  keywords = "Cryptocurrencies; (in-)efficiency; Price discovery; Liquidity"
}

@ARTICLE{Lucey2004-aa,
  title     = "Robust estimates of daily seasonality in the Irish equity market",
  author    = "Lucey, Brian M",
  abstract  = "This article examines, in a robust manner, the question of
               whether or not an unusual form of daily seasonality existed in
               the Irish market. Previous studies have indicated that the
               pattern of such seasonality in Ireland differs from that found
               elsewhere. Other research indicates that daily seasonality may
               not exist at all. The findings are that after adjusting for
               sample size and taking into account the non-normality of the
               data, the evidence for daily seasonality in the Irish market is
               very weak. This is confirmed by resampling methods.",
  journal   = "Applied Financial Economics",
  publisher = "Routledge",
  volume    =  14,
  number    =  7,
  pages     = "517--523",
  month     =  apr,
  year      =  2004
}

@ARTICLE{Urquhart2018-of,
  title     = "What causes the attention of Bitcoin?",
  author    = "Urquhart, Andrew",
  journal   = "Econ. Lett.",
  publisher = "Elsevier",
  volume    =  166,
  pages     = "40--44",
  year      =  2018
}

@ARTICLE{Jegadeesh2019-aa,
  title    = "Empirical tests of asset pricing models with individual assets:
              Resolving the errors-in-variables bias in risk premium estimation",
  author   = "Jegadeesh, Narasimhan and Noh, Joonki and Pukthuanthong, Kuntara
              and Roll, Richard and Wang, Junbo",
  abstract = "To attenuate an inherent errors-in-variables bias, portfolios are
              widely employed to test asset pricing models; but portfolios
              might mask relevant risk- or return-related features of
              individual stocks. We propose an instrumental variables approach
              that allows the use of individual stocks as test assets, yet
              delivers consistent estimates of ex post risk premiums. This
              estimator also yields well-specified tests in small samples. The
              market risk premium under the capital asset pricing model (CAPM)
              and the liquidity-adjusted CAPM, premiums on risk factors under
              the Fama--French three- and five-factor models, and the Hou et
              al. (2015) four-factor model are all insignificant after
              controlling for asset characteristics.",
  journal  = "J. financ. econ.",
  month    =  feb,
  year     =  2019,
  keywords = "Risk premium estimation; Errors-in-variables bias; Instrumental
              variables; Individual stocks; Asset pricing models"
}

@MISC{Hughes2019-sv,
  title   = "{CONSUMER} {LENDING} {EFFICIENCY}: {COMMERCIAL} {BANKS} {VERSUS} A
             {FINTECH} {LENDER}",
  author  = "Hughes, Joseph P and Jagtiani, Julapa and Moon, Choon-Geol",
  journal = "Working paper (Federal Reserve Bank of Philadelphia)",
  year    =  2019
}

@ARTICLE{Vives2017-el,
  title     = "The impact of {FinTech} on banking",
  author    = "Vives, Xavier",
  journal   = "European Economy",
  publisher = "Europeye srl",
  number    =  2,
  pages     = "97--105",
  year      =  2017
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@UNPUBLISHED{Hughes2018-sl,
  title    = "Does Scale Matter in Community Bank Performance? Evidence
              Obtained by Applying Several New Measures of Performance",
  author   = "Hughes, Joseph P and Jagtiani, Julapa and Mester, Loretta J and
              Moon, Choon-Geol",
  abstract = "SUPERSEDES WP16-15 We consider how size matters for banks in
              three size groups: banks with assets of less than \$1 billion
              (small community banks), banks with assets between \$1 billion
              and \$10 billion (large community banks), and banks with assets
              between \$10 billion and \$50 billion (midsize banks). Community
              banks have potential advantages in relationship lending compared
              with large banks. However, increases in regulatory compliance and
              technological burdens may have disproportionately increased
              community banks{\^a}‚Ç¨‚Ñ¢ costs, raising concerns about small
              businesses{\^a}‚Ç¨‚Ñ¢ access to credit. Our evidence suggests that
              (1) the average costs related to regulatory compliance and
              technology decrease with size; (2) while small community banks
              exhibit relatively more valuable investment opportunities, larger
              community banks and midsize banks exploit theirs more efficiently
              and achieve better financial performance; (3) unlike small
              community banks, large community banks have financial incentives
              to increase lending to small businesses; and (4) for business
              lending and commercial real estate lending, large community banks
              and midsize banks assume higher inherent credit risk and exhibit
              more efficient lending. Thus, concern that small business lending
              would be adversely affected if small community banks find it
              beneficial to increase their scale is not supported by our
              results.",
  month    =  mar,
  year     =  2018,
  keywords = "community banking, scale, financial performance, small business
              lending"
}

@MISC{Hughes_undated-el,
  title   = "How Bad is a Bad Loan? Distinguishing Inherent Credit Risk from
             Inefficient Lending (Does the Capital Market Price this
             Difference?)",
  author  = "Hughes, Joseph P and Moon, Choon-Geol",
  journal = "SSRN Electronic Journal"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Schnatterly2008-jr,
  title     = "Information advantages of large institutional owners",
  author    = "Schnatterly, K and Shaw, K W and {others}",
  abstract  = "We study the relation between the percentage of outstanding
               shares held by a firm's largest institutional owner and the
               bid--ask spread on that firm's shares, a measure of information
               risk. We find that the greater the percentage of shares held by
               the largest institutional ‚Ä¶",
  journal   = "Strategic Manage. J.",
  publisher = "Wiley Online Library",
  year      =  2008
}

@ARTICLE{Gallagher2019-pi,
  title     = "Regulatory own goals: the unintended consequences of economic
               regulation in professional football",
  author    = "Gallagher, Ronan and Quinn, Barry",
  abstract  = "ABSTRACTResearch question: In 2010, the governing body of
               European football, UEFA, approved ?Financial Fair Play?
               regulations. Designed to encourage financial discipline, promote
               stability and foster competitive balance, they focus on a
               financial breakeven constraint. We analyse the impact of such
               constraints on the joint sporting and financial efficiency of
               English football clubs.Research methods: The simultaneous
               production of both sporting and financial outputs are modelled
               using stochastic, non-parametric efficiency analysis. The sample
               is an unbalanced panel representing 60 clubs spanning the
               2003/2004 to 2016/2017 seasons.Results and findings: The
               Financial Fair Play breakeven regulation reduces average club
               efficiency, raises the relative importance of financial goals
               (capturing revenue share) whilst lowering the relative
               importance of sporting goals (capturing point share). The
               efficiency costs of regulation are not borne equally by
               clubs.Implications: Breakeven regulations reduce the joint
               sporting and financial efficiency of regulated clubs, with the
               efficiency loss positively related to the severity of the
               breakeven constraint. The Financial Fair Play regulations
               further entrench the financial and sporting power of elite clubs
               and potentially undermine league competitive intensity by
               shifting the relative focus of clubs away from sporting
               productivity toward financial productivity.",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  pages     = "1--20",
  month     =  apr,
  year      =  2019
}

@ARTICLE{Kandel1996-aw,
  title     = "On the predictability of stock returns: an asset-allocation
               perspective",
  author    = "Kandel, Shmuel and Stambaugh, Robert F",
  journal   = "J. Finance",
  publisher = "Wiley Online Library",
  volume    =  51,
  number    =  2,
  pages     = "385--424",
  year      =  1996
}

@ARTICLE{Pettenuzzo2011-us,
  title    = "Predictability of stock returns and asset allocation under
              structural breaks",
  author   = "Pettenuzzo, Davide and Timmermann, Allan",
  abstract = "This paper adopts a new approach that accounts for breaks to the
              parameters of return prediction models both in the historical
              estimation period and at future points. Empirically, we find
              evidence of multiple breaks in return prediction models based on
              the dividend yield or a short interest rate. Our analysis
              suggests that model instability is a very important source of
              investment risk for buy-and-hold investors with long horizons and
              that breaks can lead to a negative slope in the relationship
              between the investment horizon and the proportion of wealth that
              investors allocate to stocks. Once past and future breaks are
              considered, an investor with medium risk aversion reduces the
              allocation to stocks from close to 100\% at short horizons to
              10\% at the five-year horizon. Welfare losses from ignoring
              breaks can amount to several hundred basis points per year for
              investors with long horizons.",
  journal  = "J. Econom.",
  volume   =  164,
  number   =  1,
  pages    = "60--78",
  month    =  sep,
  year     =  2011
}

@ARTICLE{Wang2012-fm,
  title    = "Stock index forecasting based on a hybrid model",
  author   = "Wang, Ju-Jie and Wang, Jian-Zhou and Zhang, Zhe-George and Guo,
              Shu-Po",
  abstract = "Forecasting the stock market price index is a challenging task.
              The exponential smoothing model (ESM), autoregressive integrated
              moving average model (ARIMA), and the back propagation neural
              network (BPNN) can be used to make forecasts based on time
              series. In this paper, a hybrid approach combining ESM, ARIMA,
              and BPNN is proposed to be the most advantageous of all three
              models. The weight of the proposed hybrid model (PHM) is
              determined by genetic algorithm (GA). The closing of the Shenzhen
              Integrated Index (SZII) and opening of the Dow Jones Industrial
              Average Index (DJIAI) are used as illustrative examples to
              evaluate the performances of the PHM. Numerical results show that
              the proposed model outperforms all traditional models, including
              ESM, ARIMA, BPNN, the equal weight hybrid model (EWH), and the
              random walk model (RWM).",
  journal  = "Omega",
  volume   =  40,
  number   =  6,
  pages    = "758--766",
  month    =  dec,
  year     =  2012,
  keywords = "Stock price; Forecasting; Exponential smoothing; ARIMA; BPNN;
              Genetic algorithm; Hybrid model"
}

@ARTICLE{Catania2019-ds,
  title    = "Forecasting cryptocurrencies under model and parameter
              instability",
  author   = "Catania, Leopoldo and Grassi, Stefano and Ravazzolo, Francesco",
  abstract = "This paper studies the predictability of cryptocurrency time
              series. We compare several alternative univariate and
              multivariate models for point and density forecasting of four of
              the most capitalized series: Bitcoin, Litecoin, Ripple and
              Ethereum. We apply a set of crypto-predictors and rely on dynamic
              model averaging to combine a large set of univariate dynamic
              linear models and several multivariate vector autoregressive
              models with different forms of time variation. We find
              statistically significant improvements in point forecasting when
              using combinations of univariate models, and in density
              forecasting when relying on the selection of multivariate models.
              Both schemes deliver sizable directional predictability.",
  journal  = "Int. J. Forecast.",
  volume   =  35,
  number   =  2,
  pages    = "485--501",
  month    =  apr,
  year     =  2019,
  keywords = "Cryptocurrency; Bitcoin; Forecasting; Density forecasting; VAR;
              Dynamic model averaging"
}

@ARTICLE{Zhang2005-vb,
  title    = "Neural network forecasting for seasonal and trend time series",
  author   = "Zhang, G Peter and Qi, Min",
  abstract = "Neural networks have been widely used as a promising method for
              time series forecasting. However, limited empirical studies on
              seasonal time series forecasting with neural networks yield mixed
              results. While some find that neural networks are able to model
              seasonality directly and prior deseasonalization is not
              necessary, others conclude just the opposite. In this paper, we
              investigate the issue of how to effectively model time series
              with both seasonal and trend patterns. In particular, we study
              the effectiveness of data preprocessing, including
              deseasonalization and detrending, on neural network modeling and
              forecasting performance. Both simulation and real data are
              examined and results are compared to those obtained from the
              Box--Jenkins seasonal autoregressive integrated moving average
              models. We find that neural networks are not able to capture
              seasonal or trend variations effectively with the unpreprocessed
              raw data and either detrending or deseasonalization can
              dramatically reduce forecasting errors. Moreover, a combined
              detrending and deseasonalization is found to be the most
              effective data preprocessing approach.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  160,
  number   =  2,
  pages    = "501--514",
  month    =  jan,
  year     =  2005,
  keywords = "Neural networks; Box--Jenkins method; Seasonality; Time series;
              Forecasting"
}

@ARTICLE{Zhang1998-dl,
  title    = "Neural network forecasting of the British {Pound/US} Dollar
              exchange rate",
  author   = "Zhang, Gioqinang and Hu, Michael Y",
  abstract = "Neural networks have successfully been used for exchange rate
              forecasting. However, due to a large number of parameters to be
              estimated empirically, it is not a simple task to select the
              appropriate neural network architecture for an exchange rate
              forecasting problem. Researchers often overlook the effect of
              neural network parameters on the performance of neural network
              forecasting. This paper examines the effects of the number of
              input and hidden nodes as well as the size of the training sample
              on the in-sample and out-of-sample performance. The British
              pound/US dollar is used for detailed examinations. It is found
              that neural networks outperform linear models, particularly when
              the forecast horizon is short. In addition, the number of input
              nodes has a greater impact on performance than the number of
              hidden nodes, while a larger number of observations do reduce
              forecast errors.",
  journal  = "Omega",
  volume   =  26,
  number   =  4,
  pages    = "495--506",
  month    =  aug,
  year     =  1998,
  keywords = "foreign exchange rate; time series forecasting; neural networks"
}

@ARTICLE{Zhang2003-ji,
  title    = "Time series forecasting using a hybrid {ARIMA} and neural network
              model",
  author   = "Zhang, G Peter",
  abstract = "Autoregressive integrated moving average (ARIMA) is one of the
              popular linear models in time series forecasting during the past
              three decades. Recent research activities in forecasting with
              artificial neural networks (ANNs) suggest that ANNs can be a
              promising alternative to the traditional linear methods. ARIMA
              models and ANNs are often compared with mixed conclusions in
              terms of the superiority in forecasting performance. In this
              paper, a hybrid methodology that combines both ARIMA and ANN
              models is proposed to take advantage of the unique strength of
              ARIMA and ANN models in linear and nonlinear modeling.
              Experimental results with real data sets indicate that the
              combined model can be an effective way to improve forecasting
              accuracy achieved by either of the models used separately.",
  journal  = "Neurocomputing",
  volume   =  50,
  pages    = "159--175",
  month    =  jan,
  year     =  2003,
  keywords = "ARIMA; Box--Jenkins methodology; Artificial neural networks; Time
              series forecasting"
}

@ARTICLE{Kaastra1996-zt,
  title    = "Designing a neural network for forecasting financial and economic
              time series",
  author   = "Kaastra, Iebeling and Boyd, Milton",
  abstract = "Artificial neural networks are universal and highly flexible
              function approximators first used in the fields of cognitive
              science and engineering. In recent years, neural network
              applications in finance for such tasks as pattern recognition,
              classification, and time series forecasting have dramatically
              increased. However, the large number of parameters that must be
              selected to develop a neural network forecasting model have meant
              that the design process still involves much trial and error. The
              objective of this paper is to provide a practical introductory
              guide in the design of a neural network for forecasting economic
              time series data. An eight-step procedure to design a neural
              network forecasting model is explained including a discussion of
              tradeoffs in parameter selection, some common pitfalls, and
              points of disagreement among practitioners.",
  journal  = "Neurocomputing",
  volume   =  10,
  number   =  3,
  pages    = "215--236",
  month    =  apr,
  year     =  1996,
  keywords = "Neural networks; Financial forecasting; Backpropagation; Data
              preprocessing; Training; Testing; Validation; Network paradigms;
              Learning rate; Momentum and forecast evaluation criteria"
}

@BOOK{Covell2012-bq,
  title     = "Managing Sports Organizations",
  author    = "Covell, Daniel and Walker, Sharianne and Hess, Peter and
               Siciliano, Julie",
  abstract  = "Managing Sport Organizations, second edition, is a newly updated
               and comprehensive introduction to the themes and elements
               surrounding sport management. The book teaches management theory
               and principles in a coherent manner, helping to reinforce these
               concepts for students in schools of business, and serving to
               introduce them to students in other school settings
               (kinesiology, exercise science, sport science). The features of
               this book include: Important industry segment information is
               introduced chapter by chapter, allowing students to wed theory
               and application throughout Effectively weaves sport industry
               issues with fundamental management theories and practices
               Provides informative introductions to all fundamental aspects of
               sport management- Leadership, Information Technology, Media,
               Facility management, HR and much more With an online
               Instructor's Manual and a Test Bank available as well, this book
               is an essential tool for students and teachers of sport
               management.",
  publisher = "Routledge",
  month     =  jun,
  year      =  2012,
  language  = "en"
}

@BOOK{Beccalli2015-qj,
  title     = "Bank Risk, Governance and Regulation",
  author    = "Beccalli, Elena and Poli, Federica",
  abstract  = "This book presents research from leading researchers in the
               European banking field to explore three key areas of banking. In
               Bank Risk, Governance and Regulation, the authors conduct micro-
               and macro- level analysis of banking risks and their
               determinants. They explore areas such as credit quality, bank
               provisioning, deposit guarantee schemes, corporate governance
               and cost of capital. The book then goes on to analyse different
               aspects of the relationship between bank risk management,
               governance and performance. Lastly the book explores the
               regulation of systemic risks posed by banks, and examines the
               effects of novel regulatory sets on bank conduct and
               profitability. The research in this book focuses on aspects of
               the European banking system; however it also offers wider
               insight into the global banking space and offers comparisons to
               international banking systems. The study provides in-depth
               insight into many areas of bank risk, governance and regulation,
               before finally addressing the question: which banking strategies
               are actually feasible?",
  publisher = "Springer",
  month     =  aug,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Bakke2012-nf,
  title    = "The real effects of delisting: Evidence from a regression
              discontinuity design",
  author   = "Bakke, Tor-Erik and Jens, Candace E and Whited, Toni M",
  abstract = "We study how the delisting of a firm's stock, and the
              accompanying drop in liquidity, causally affects a firm's real
              economic decisions. Although delisting is endogenous, we identify
              a causal effect by using regression discontinuity design (RDD).
              This technique suits the delisting problem because the
              probability of delisting rises discontinuously when observable
              variables pass known thresholds. We find that delisting results
              in a modest decline in investment and cash saving and an
              important and robust decline in employment.",
  journal  = "Finance Research Letters",
  volume   =  9,
  number   =  4,
  pages    = "183--193",
  month    =  dec,
  year     =  2012,
  keywords = "Regression discontinuitiy design; Delisting; Investment;
              Employment"
}

@ARTICLE{Rao2015-sv,
  title     = "The Basics of Financial Econometrics: Tools, Concepts, and Asset
               Management Applications",
  author    = "Rao, K Surekha",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  volume    =  15,
  number    =  11,
  pages     = "1773--1775",
  month     =  nov,
  year      =  2015
}

@MISC{Tsay2005-xd,
  title   = "Analysis of Financial Time Series",
  author  = "Tsay, Ruey S",
  journal = "Wiley Series in Probability and Statistics",
  year    =  2005
}

@UNPUBLISHED{Mamatzakis2019-js,
  title       = "The Interplay Between Problem Loans and Japanese Bank
                 Productivity",
  author      = "Mamatzakis, Emmanuel and Matousek, Roman and Vu, Anh N",
  abstract    = "This paper examines for the first time the impact of problem
                 loans on Japanese productivity growth. We exploit a new data
                 set of Japanese problem loans classified into two categories:
                 bankrupt and restructured loans. We opt for a novel and
                 flexible productivity growth decomposition that allows to
                 measure the direct impact of these problem loans on
                 productivity growth. The results reveal that Japanese bank
                 productivity growth was severely constrained by bankrupt and
                 restructured loans early in 2000s, whilst some persistence of
                 the negative impact of problem loans on productivity growth is
                 observed in the late 2000s. Thereafter, there is only some
                 partial recovery in the productivity growth from 2012 to 2015.
                 Further, we also perform cluster analysis to examine
                 convergence or divergence across regions and over time. We
                 observe limited convergence, though Regional Banks seem to
                 form clusters in some regions.",
  institution = "University Library of Munich, Germany",
  month       =  mar,
  year        =  2019,
  keywords    = "Bank productivity; Bankrupt loans; Restructured loans; Cluster
                 analysis; Japan"
}

@MISC{Green2017-fu,
  title   = "The Characteristics that Provide Independent Information about
             Average {U.S}. Monthly Stock Returns",
  author  = "Green, Jeremiah and Hand, John R M and Frank Zhang, X",
  journal = "The Review of Financial Studies",
  volume  =  30,
  number  =  12,
  pages   = "4389--4436",
  year    =  2017
}

@MISC{Kelly2018-hd,
  title        = "[No title]",
  author       = "Kelly, Bryan and Pruitt, Seth and Su, Yinan",
  abstract     = "We propose a new modeling approach for the cross section of
                  returns. Our method, Instrumented Principal Components
                  Analysis (IPCA), allows for latent factors and time-varying
                  loadings by introducing observable characteristics that
                  instrument for the unobservable dynamic loadings. If the
                  characteristics/expected return relationship is driven by
                  compensation for exposure to latent risk factors, IPCA will
                  identify the corresponding latent factors. If no such factors
                  exist, IPCA infers that the characteristic effect is
                  compensation without risk and allocates it to an ``anomaly''
                  intercept. Studying returns and characteristics at the
                  stock-level, we find that four IPCA factors explain the cross
                  section of average returns significantly more accurately than
                  existing factor models and produce characteristic-associated
                  anomaly intercepts that are small and statistically
                  insignificant. Furthermore, among a large collection of
                  characteristics explored in the literature, only eight are
                  statistically significant in the IPCA specification and are
                  responsible for nearly 100\% of the model's accuracy.",
  month        =  apr,
  year         =  2018,
  howpublished = "\url{https://www.nber.org/2018LTAM/kelly.pdf}"
}

@UNPUBLISHED{Kelly2017-xt,
  title    = "Instrumented Principal Component Analysis",
  author   = "Kelly, Bryan T and Pruitt, Seth and Su, Yinan",
  abstract = "We propose a method of factor estimation for a data panel Y by
              using the data tensor Z to parameterize loadings --- Instrumented
              Principal Component Analysis. IPCA allows us to identify a model
              wherein factor loadings vary over both panel dimensions, which is
              an implication of various economic theories. Our benchmark
              estimator is computed virtually instantaneously using the
              singular value decomposition --- we show the consistency and
              asymptotic distribution for resulting estimates. An application
              to international macroeconomics suggests that a nation's import
              share, gross capital formation share, and overall level of GDP
              drive its relationship to a global growth factor, whereas
              population density does not.",
  month    =  jun,
  year     =  2017,
  keywords = "factor model, principal components, tensor, asymptotic theory,
              international macroeconomics, dynamic loading"
}

@ARTICLE{Diebold2012-pp,
  title     = "Better to give than to receive: Predictive directional
               measurement of volatility spillovers",
  author    = "Diebold, Francis X and Yilmaz, Kamil",
  abstract  = "Using a generalized vector autoregressive framework in which
               forecast-error variance decompositions are invariant to the
               variable ordering, we propose measures of both the total and
               directional volatility spillovers. We use our methods to
               characterize daily volatility spillovers across US stock, bond,
               foreign exchange and commodities markets, from January 1999 to
               January 2010. We show that despite significant volatility
               fluctuations in all four markets during the sample, cross-market
               volatility spillovers were quite limited until the global
               financial crisis, which began in 2007. As the crisis
               intensified, so too did the volatility spillovers, with
               particularly important spillovers from the stock market to other
               markets taking place after the collapse of the Lehman Brothers
               in September 2008.",
  journal   = "Int. J. Forecast.",
  publisher = "Elsevier",
  volume    =  28,
  number    =  1,
  pages     = "57--66",
  month     =  jan,
  year      =  2012,
  keywords  = "Asset market; Asset return; Stock market; Market linkage;
               Financial crisis; Contagion; Vector autoregression; Variance
               decomposition"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Diebold2009-vt,
  title     = "Measuring financial asset return and volatility spillovers, with
               application to global equity markets",
  author    = "Diebold, F X and Yilmaz, K",
  abstract  = "We provide a simple and intuitive measure of interdependence of
               asset returns and/or volatilities. In particular, we formulate
               and examine precise and separate measures of return spillovers
               and volatility spillovers. Our framework facilitates study of
               both non‚Äêcrisis and ‚Ä¶",
  journal   = "Econ. J. Nepal",
  publisher = "Wiley Online Library",
  year      =  2009
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Klosner2014-ab,
  title     = "Exploring all {VAR} orderings for calculating spillovers? Yes,
               we can!---a note on Diebold and Yilmaz (2009)",
  author    = "Kl{\"o}{\ss}ner, Stefan and Wagner, Sven",
  abstract  = "‚Ä¶ Applying the new algorithm to the data analyzed by Diebold and
               Yilmaz (2009) reveals that ‚Ä¶ Crossref. David E. Allen, Michael
               McAleer, Robert Powell and Abhay K. Singh, Volatility spillover
               and multivariate volatility impulse response analysis of GFC
               news events, Applied ‚Ä¶",
  journal   = "J. Appl. Econometrics",
  publisher = "Wiley Online Library",
  volume    =  29,
  number    =  1,
  pages     = "172--179",
  year      =  2014
}

@ARTICLE{DOrazio2019-xl,
  title    = "Fostering green investments and tackling climate-related
              financial risks: Which role for macroprudential policies?",
  author   = "D'Orazio, Paola and Popoyan, Lilit",
  abstract = "While there is a growing debate among researchers and
              practitioners on the possible role of central banks and financial
              regulators in supporting a smooth transition to a low-carbon
              economy, the information on which macroprudential instruments
              could be used for reaching the ``green structural change'' is
              still quite limited. Moreover, the achievement of climate goals
              is still affected by the so-called ``green finance gap''. This
              paper addresses these issues by proposing a critical review of
              existing and novel prudential approaches to incentivizing the
              decarbonization of banks' balance sheets and aligning finance
              with sustainable growth and development objectives. The analysis
              carried out in the paper allows understanding of under which
              conditions macroprudential policy could tackle climate change and
              promote green lending, while also containing climate-related
              financial risks.",
  journal  = "Ecol. Econ.",
  volume   =  160,
  pages    = "25--37",
  month    =  jun,
  year     =  2019,
  keywords = "Climate change; Climate finance gap; Banking regulation;
              Macroprudential policy; Central banking; Climate-finance risk"
}

@UNPUBLISHED{Richter2018-xd,
  title    = "The Macroeconomic Effects of Macroprudential Policy",
  author   = "Richter, Bj{\"o}rn and Schularick, Moritz and Shim, Ilhyock",
  abstract = "Central banks increasingly rely on macroprudential measures to
              manage the financial cycle. However, the effects of such policies
              on the core objectives of monetary policy to stabilise output and
              inflation are largely unknown. In this paper we quantify the
              effects of changes in maximum loan-to-value (LTV) ratios on
              output and inflation. We rely on a narrative identification
              approach based on detailed reading of policy-makers' objectives
              when implementing the measures. We find that over a four year
              horizon, a 10 percentage point decrease in the maximum LTV ratio
              leads to a 1.1\% reduction in output. As a rule of thumb, the
              impact of a 10 percentage point LTV tightening can be viewed as
              roughly comparable to that of a 25 basis point increase in the
              policy rate. However, the effects are imprecisely estimated and
              the effect is only present in emerging market economies. We also
              find that tightening LTV limits has larger economic effects than
              loosening them. At the same time, we show that changes in maximum
              LTV ratios have substantial effects on credit and house price
              growth. Using inverse propensity weights to rerandomise LTV
              actions, we show that these effects are likely causal.",
  month    =  aug,
  year     =  2018,
  keywords = "macroprudential policy, loan-to-value ratios, local projections,
              narrative approach"
}

@ARTICLE{Chen2013-oa,
  title   = "Regression and Causation: A Critical Examination of Six
             Econometrics Textbooks",
  author  = "Chen, Bryant and Pearl, Judea",
  journal = "Real-World Economics Review, Issue No.",
  volume  =  65,
  pages   = "2--20",
  year    =  2013
}

@ARTICLE{Brana2018-cc,
  title    = "({Un)Conventional} monetary policy and bank risk-taking: A
              nonlinear relationship",
  author   = "Brana, Sophie and Campmas, Alexandra and Lapteacru, Ion",
  abstract = "This paper investigates the effect of monetary policy -
              especially unconventional monetary policy - on bank risk-taking
              behavior in Europe over the period 2000--2015. Using a dynamic
              panel model with a threshold effect, we estimate this effect on
              two measures of bank risk: the Distance to Default, which
              reflects the market perception of risk, and the asymmetric
              Z-score, which corresponds to an accounting-based measure of the
              risk. We find that loosening monetary policy (via low interest
              rates and increasing central banks' liquidity) has a harmful
              effect on banks' risk, confirming the existence of the
              risk-taking channel. Moreover, we show that this relationship is
              nonlinear, i.e., with the sustainable implementation of
              unconventional monetary policies, the effects are stronger below
              a certain threshold.",
  journal  = "Econ. Model.",
  month    =  jul,
  year     =  2018,
  keywords = "Risk-taking channel; Monetary policy; Asymmetric Z-score;
              Distance to Default; Shadow interest rate; Panel threshold model"
}

@MISC{Enoch_undated-iv,
  title   = "Foreword: The 2010 Banking Law Symposium on Managing Systemic Risk",
  author  = "Enoch, Charles",
  journal = "Managing Risk in the Financial System"
}

@MISC{Moloney2015-jy,
  title   = "Systemic risk and Macro-prudential supervision",
  author  = "Moloney, Niamh and Ferran, Eil{\'\i}s and Payne, Jennifer and
             Lastra, Rosa",
  journal = "The Oxford Handbook of Financial Regulation",
  year    =  2015
}

@INCOLLECTION{Monnin2018-mc,
  title     = "Central banks and the transition to a low-carbon economy",
  booktitle = "Technical Report",
  author    = "Monnin, Pierre",
  publisher = "Discussion note 2018/1. Zurich: Council on Economic Policies",
  year      =  2018
}

@ARTICLE{Richter2019-qf,
  title    = "The costs of macroprudential policy",
  author   = "Richter, Bj{\"o}rn and Schularick, Moritz and Shim, Ilhyock",
  abstract = "Central banks increasingly rely on macroprudential measures to
              manage the financial cycle. However, the effects of such measures
              on the core objectives of monetary policy to stabilise output and
              inflation are largely unknown. In this paper we quantify the
              effects of changes in maximum loan-to-value (LTV) ratios on
              output and inflation. We rely on a narrative identification
              approach based on detailed reading of policy-makers' objectives
              when implementing the measures. We find that over a four year
              horizon, a 10 percentage point decrease in the maximum LTV ratio
              leads to a 1.1\% reduction in output. As a rule of thumb, the
              impact of a 10 percentage point LTV tightening can be viewed as
              roughly comparable to that of a 25 basis point increase in the
              policy rate. However, the effects are imprecisely estimated and
              the effect is only present in emerging market economies. We also
              find that tightening LTV limits has larger economic effects than
              loosening them. At the same time, we show that changes in maximum
              LTV ratios have substantial effects on credit and house price
              growth. Using inverse propensity weights to rerandomise LTV
              actions, we show that these effects are likely causal.",
  journal  = "J. Int. Econ.",
  volume   =  118,
  pages    = "263--282",
  month    =  may,
  year     =  2019,
  keywords = "Macroprudential policy; Loan-to-value ratios; Local projections;
              Narrative approach"
}

@TECHREPORT{Poghosyan2019-lh,
  title       = "How Effective is Macroprudential Policy? Evidence from Lending
                 Restriction Measures in {EU} Countries",
  author      = "Poghosyan, Tigra and {Others}",
  institution = "International Monetary Fund",
  year        =  2019
}

@TECHREPORT{Buch2018-uy,
  title       = "Evaluating macroprudential policies",
  author      = "Buch, Claudia M and Vogel, Edgar and Weigert, Benjamin",
  institution = "ESRB Working Paper Series",
  year        =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Freyberger_undated-us,
  title  = "Dissecting Characteristics Nonparametrically‚àó",
  author = "Freyberger, Joachim and Neuhierl, Andreas and Weber, Michael"
}

@BOOK{Zellner1971-ug,
  title     = "An introduction to Bayesian inference in econometrics",
  author    = "Zellner, Arnold",
  publisher = "New York: John Wiley and Sons",
  year      =  1971
}

@BOOK{Greene2014-vo,
  title     = "Econometric Analysis: International Edition: Global Edition",
  author    = "Greene, William H",
  abstract  = "For first-year graduate courses in Econometrics for Social
               Scientists.This title is a Pearson Global Edition. The Editorial
               team at Pearson has worked closely with educators around the
               world to include content which is especially relevant to
               students outside the United States.This text serves as a bridge
               between an introduction to the field of econometrics and the
               professional literature for graduate students in the social
               sciences, focusing on applied econometrics and theoretical
               concepts.",
  publisher = "Pearson Higher Ed",
  month     =  sep,
  year      =  2014,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kunst2020-he,
  title     = "Economic forecasting: editors' introduction",
  author    = "Kunst, R M and Wagner, M",
  abstract  = "This special issue of Empirical Economics contains, with some
               exceptions, papers presented at the First Vienna Workshop on
               Economic Forecasting that took place at the Institute for
               Advanced Studies in Vienna in February 2018. The 2-day workshop,
               organized ‚Ä¶",
  journal   = "Empir. Econ.",
  publisher = "Springer",
  year      =  2020
}

@BOOK{Timmermann2020-ds,
  title     = "Economic Forecasting",
  author    = "Timmermann, Alan and Elliott, George",
  publisher = "Princeton University Press",
  year      =  2020
}

@ARTICLE{Black2018-ke,
  title     = "Model risk: illuminating the black box",
  author    = "Black, R and Tsanakas, A and Smith, A D and Beck, M B and
               Maclugash, I D and Grewal, J and Witts, L and Morjaria, N and
               Green, R J and Lim, Z",
  abstract  = "This paper presents latest thinking from the Institute and
               Faculty of Actuaries' Model Risk Working Party and follows on
               from their Phase I work, Model Risk: Daring to Open the Black
               Box. This is a more practical paper and presents the
               contributors' experiences of model risk gained from a wide range
               of financial and non-financial organisations with suggestions
               for good practice and proven methods to reduce model risk. After
               a recap of the Phase I work, examples of model risk
               communication are given covering communication: to the Board; to
               the regulator; and to external stakeholders. We present a
               practical framework for model risk management and quantification
               with examples of the key actors, processes and cultural
               challenge. Lessons learned are then presented from other
               industries that make extensive use of models and include the
               weather forecasting, software and aerospace industries. Finally,
               a series of case studies in practical model risk management and
               mitigation are presented from the contributors' own experiences
               covering primarily financial services.",
  journal   = "British Actuarial Journal",
  publisher = "Cambridge University Press",
  volume    =  23,
  year      =  2018,
  keywords  = "Model risk; Model; Model error; Model uncertainty; Risk culture"
}

@ARTICLE{Danielsson2016-mh,
  title    = "Model risk of risk models",
  author   = "Danielsson, Jon and James, Kevin R and Valenzuela, Marcela and
              Zer, Ilknur",
  abstract = "This paper evaluates the model risk of models used for
              forecasting systemic and market risk. Model risk, which is the
              potential for different models to provide inconsistent outcomes,
              is shown to be increasing with market uncertainty. During calm
              periods, the underlying risk forecast models produce similar risk
              readings; hence, model risk is typically negligible. However, the
              disagreement between the various candidate models increases
              significantly during market distress, further frustrating the
              reliability of risk readings. Finally, particular conclusions on
              the underlying reasons for the high model risk and the
              implications for practitioners and policy makers are discussed.",
  journal  = "Journal of Financial Stability",
  volume   =  23,
  pages    = "79--91",
  month    =  apr,
  year     =  2016,
  keywords = "Model risk; Systemic risk; Value-at-Risk; Expected shortfall;
              Basel III"
}

@ARTICLE{Horel2019-ye,
  title         = "Towards Explainable {AI}: Significance Tests for Neural
                   Networks",
  author        = "Horel, Enguerrand and Giesecke, Kay",
  abstract      = "Neural networks underpin many of the best-performing AI
                   systems. Their success is largely due to their strong
                   approximation properties, superior predictive performance,
                   and scalability. However, a major caveat is explainability:
                   neural networks are often perceived as black boxes that
                   permit little insight into how predictions are being made.
                   We tackle this issue by developing a pivotal test to assess
                   the statistical significance of the feature variables of a
                   neural network. We propose a gradient-based test statistic
                   and study its asymptotics using nonparametric techniques.
                   The limiting distribution is given by a mixture of
                   chi-square distributions. The tests enable one to discern
                   the impact of individual variables on the prediction of a
                   neural network. The test statistic can be used to rank
                   variables according to their influence. Simulation results
                   illustrate the computational efficiency and the performance
                   of the test. An empirical application to house price
                   valuation highlights the behavior of the test using actual
                   data.",
  month         =  feb,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "1902.06021"
}

@UNPUBLISHED{Hwang2019-el,
  title    = "Searching the Factor Zoo",
  author   = "Hwang, Soosung and Rubesam, Alexandre",
  abstract = "We propose a Bayesian variable selection method to explore the
              space of possible factor models for alarge set of candidate
              factors identified in the asset pricing literature. Using
              thousands of individualstocks, we identify several parsimonious
              models which perform at least as well, and in some cases
              betterthan, widely used factor models such as those proposed by
              Fama and French (2015) and Hou, Xue andZhang (2015). We find
              that, in addition to the market return, factors that reflect
              investors' behavioralbiases, market microstructure, and
              production-side risk, also matter to explain stock returns. The
              searchfor a single reduced-form model has limits in practice, as
              many combinations of characteristics-basedfactors achieve similar
              empirical performance.",
  month    =  mar,
  year     =  2019,
  keywords = "Multi-factor model, Factor zoo, Factor selection, Bayesian
              variable selection, SeeminglyUnrelated Regressions"
}

@UNPUBLISHED{Giglio2019-od,
  title    = "Thousands of Alpha Tests",
  author   = "Giglio, Stefano and Liao, Yuan and Xiu, Dacheng",
  abstract = "Data snooping is a major concern in empirical asset pricing. By
              exploiting the ``blessings of dimensionality`` we develop a new
              framework to rigorously perform multiple hypothesis testing in
              linear asset pricing models, while limiting the occurrence of
              false positive results typically associated with data-snooping.
              We first develop alpha test statistics that are asymptotically
              valid, weakly dependent in the cross-section, and robust to the
              possibility of omitted factors. We then combine them in a
              multiple-testing procedure that ensures that the rate of false
              discoveries is ex-ante bounded below a pre-specified 5\% level.
              We also show that this method can detect all positive alphas with
              reasonable strengths. Our procedure is designed for
              high-dimensional settings and works even when the number of tests
              is large relative to the sample size, as in many finance
              applications. We illustrate the empirical relevance of our
              methodology in the context of hedge fund performance (alpha)
              evaluation. We find that our procedure is able to select - among
              more than 3,000 available funds - a subset of funds that displays
              superior in-sample and out-of-sample performance compared to the
              funds selected by standard methods.",
  journal  = "Chicago Booth Research Paper",
  month    =  mar,
  year     =  2019,
  keywords = "Data Snooping, Multiple Testing, Alpha Testing, Factor Models,
              Hedge Fund Performance, False Discovery Rate, Machine Learning"
}

@UNPUBLISHED{Huang2019-lk,
  title    = "{Time-Series} Momentum: Is It There?",
  author   = "Huang, Dashan and Li, Jiangyuan and Wang, Liyao and Zhou, Guofu",
  abstract = "Time series momentum (TSM) refers to the predictability of the
              past 12-month return on the next one-month return and is the
              focus of several recent influential studies. This paper shows
              that asset-by-asset time series regressions reveal little
              evidence of TSM, both in- and out-of-sample. While the
              t-statistic in a pooled regression appears large, it is not
              statistically reliable as it is less than the critical values of
              parametric and nonparametric bootstraps. From an investment
              perspective, the TSM strategy is profitable, but its performance
              is virtually the same as that of a similar strategy that is based
              on historical sample mean and does not require predictability.
              Overall, the evidence on TSM is weak, particularly for the large
              cross section of assets.",
  journal  = "Journal of Financial Economics",
  month    =  jan,
  year     =  2019,
  keywords = "Time-series momentum; Risk premium; Return predictability; Pooled
              regression"
}

@ARTICLE{Kelly2015-lc,
  title    = "The three-pass regression filter: A new approach to forecasting
              using many predictors",
  author   = "Kelly, Bryan and Pruitt, Seth",
  abstract = "We forecast a single time series using many predictor variables
              with a new estimator called the three-pass regression filter
              (3PRF). It is calculated in closed form and conveniently
              represented as a set of ordinary least squares regressions. 3PRF
              forecasts are consistent for the infeasible best forecast when
              both the time dimension and cross section dimension become large.
              This requires specifying only the number of relevant factors
              driving the forecast target, regardless of the total number of
              common factors driving the cross section of predictors. The 3PRF
              is a constrained least squares estimator and reduces to partial
              least squares as a special case. Simulation evidence confirms the
              3PRF's forecasting performance relative to alternatives. We
              explore two empirical applications: Forecasting macroeconomic
              aggregates with a large panel of economic indices, and
              forecasting stock market returns with price--dividend ratios of
              stock portfolios.",
  journal  = "J. Econom.",
  volume   =  186,
  number   =  2,
  pages    = "294--316",
  month    =  jun,
  year     =  2015,
  keywords = "Forecast; Factor model; Principal components; Constrained least
              squares; Partial least squares"
}

@UNPUBLISHED{Kelly2018-pl,
  title    = "Understanding Momentum and Reversal",
  author   = "Kelly, Bryan T and Moskowitz, Tobias J and Pruitt, Seth",
  abstract = "We show that the momentum and long-term reversal effects are
              explained by a conditional factor pricing model. We use
              instrumented principal components analysis to construct an
              improved conditional model in which dynamic loadings are
              functions of observable firm characteristics. Model-based
              expected returns, which are solely determined by exposures to
              systematic risk factors, provide much stronger predictive power
              for future realized returns than momentum or long-term reversal.
              We show that these return trend variables ``work'' because they
              are imperfect proxies for time-varying beta compensation, and
              that properly measured conditional betas render the effects of
              momentum and long-term reversal small and insignificant. In
              contrast, the short-term reversal phenomenon is distinct from the
              conditional pricing model, consistent with a pure liquidity
              effect.",
  month    =  oct,
  year     =  2018,
  keywords = "momentum, factor model, conditional betas, conditional expected
              returns, IPCA"
}

@UNPUBLISHED{Chen2019-jv,
  title    = "Deep Learning in Asset Pricing",
  author   = "Chen, Luyang and Pelger, Markus and Zhu, Jason",
  abstract = "We estimate a general non-linear asset pricing model with deep
              neural networks applied to all U.S. equity data combined with a
              substantial set of macroeconomic and firm-specific information.
              Our crucial innovation is the use of the no-arbitrage condition
              as part of the neural network algorithm. We estimate the
              stochastic discount factor (SDF or pricing kernel) that explains
              all asset prices from the conditional moment constraints implied
              by no-arbitrage. For this purpose, we combine three different
              deep neural network structures in a novel way: A feedforward
              network to capture non-linearities, a recurrent
              Long-Short-Term-Memory network to find a small set of economic
              state processes, and a generative adversarial network to identify
              the portfolio strategies with the most unexplained pricing
              information. Our model allows us to understand what are the key
              factors that drive asset prices, identify mis-pricing of stocks
              and generate the mean-variance efficient portfolio. Empirically,
              our approach outperforms out-of- sample all other benchmark
              approaches: Our optimal portfolio has an annual Sharpe Ratio of
              2.1, we explain 8\% of the variation in individual stock returns
              and explain over 90\% of average returns for all anomaly sorted
              portfolios.",
  month    =  mar,
  year     =  2019,
  keywords = "No-arbitrage, stock returns, conditional asset pricing model,
              non-linear factor model, machine learning, deep learning, neural
              networks, big data, hidden states, GMM"
}

@ARTICLE{Smolyansky2019-go,
  title    = "Policy externalities and banking integration",
  author   = "Smolyansky, Michael",
  abstract = "Can policies directed at the banking sector in one jurisdiction
              spill over and affect real economic activity elsewhere? To
              investigate this question, I exploit changes in tax rates on bank
              profits across US states. Banks respond by reallocating small
              business lending to otherwise unaffected states. Moreover,
              counties in non-tax-changing states that have more exposure to
              treated banks experience greater changes in lending, which in
              turn impacts local employment. The findings demonstrate that
              policies aimed at the banking sector in one jurisdiction can
              impose externalities on other regions. Critically, financial
              linkages between regions serve as the transmission channel for
              these policy externalities.",
  journal  = "J. financ. econ.",
  volume   =  132,
  number   =  3,
  pages    = "118--139",
  month    =  jun,
  year     =  2019,
  keywords = "Banks; Credit supply; Policy arbitrage; Small business lending;
              Taxation"
}

@ARTICLE{Lee2019-na,
  title    = "Technological links and predictable returns",
  author   = "Lee, Charles M C and Sun, Stephen Teng and Wang, Rongfei and
              Zhang, Ran",
  abstract = "Employing a classic measure of technological closeness between
              firms, we show that the returns of technology-linked firms have
              strong predictive power for focal firm returns. A long-short
              strategy based on this effect yields monthly alpha of 117 basis
              points. This effect is distinct from industry momentum and is not
              easily attributable to risk-based explanations. It is more
              pronounced for focal firms that: (a) have a more intense and
              specific technology focus, (b) receive lower investor attention,
              and (c) are more difficult to arbitrage. Our results are broadly
              consistent with sluggish price adjustment to more nuanced
              technological news.",
  journal  = "J. financ. econ.",
  volume   =  132,
  number   =  3,
  pages    = "76--96",
  month    =  jun,
  year     =  2019,
  keywords = "Technology momentum; Return predictability; Technological
              closeness; Limited attention; Market efficiency"
}

@BOOK{Murphy2012-ox,
  title     = "Machine learning a probabilistic approach",
  author    = "Murphy, Kevin",
  publisher = "MIT press",
  year      =  2012
}

@ARTICLE{Malekipirbazari2015-ni,
  title    = "Risk assessment in social lending via random forests",
  author   = "Malekipirbazari, Milad and Aksakalli, Vural",
  abstract = "With the advance of electronic commerce and social platforms,
              social lending (also known as peer-to-peer lending) has emerged
              as a viable platform where lenders and borrowers can do business
              without the help of institutional intermediaries such as banks.
              Social lending has gained significant momentum recently, with
              some platforms reaching multi-billion dollar loan circulation in
              a short amount of time. On the other hand, sustainability and
              possible widespread adoption of such platforms depend heavily on
              reliable risk attribution to individual borrowers. For this
              purpose, we propose a random forest (RF) based classification
              method for predicting borrower status. Our results on data from
              the popular social lending platform Lending Club (LC) indicate
              the RF-based method outperforms the FICO credit scores as well as
              LC grades in identification of good borrowers.",
  journal  = "Expert Syst. Appl.",
  volume   =  42,
  number   =  10,
  pages    = "4621--4631",
  month    =  jun,
  year     =  2015,
  keywords = "Peer-to-peer lending; Social lending; Risk assessment; Machine
              learning; Random forest"
}

@ARTICLE{Jiang2018-gd,
  title    = "Loan default prediction by combining soft information extracted
              from descriptive text in online peer-to-peer lending",
  author   = "Jiang, Cuiqing and Wang, Zhao and Wang, Ruiya and Ding, Yong",
  abstract = "Predicting whether a borrower will default on a loan is of
              significant concern to platforms and investors in online
              peer-to-peer (P2P) lending. Because the data types online
              platforms use are complex and involve unstructured information
              such as text, which is difficult to quantify and analyze, loan
              default prediction faces new challenges in P2P. To this end, we
              propose a default prediction method for P2P lending combined with
              soft information related to textual description. We introduce a
              topic model to extract valuable features from the descriptive
              text concerning loans and construct four default prediction
              models to demonstrate the performance of these features for
              default prediction. Moreover, a two-stage method is designed to
              select an effective feature set containing both soft and hard
              information. An empirical analysis using real-word data from a
              major P2P lending platform in China shows that the proposed
              method can improve loan default prediction performance compared
              with existing methods based only on hard information.",
  journal  = "Ann. Oper. Res.",
  volume   =  266,
  number   =  1,
  pages    = "511--529",
  month    =  jul,
  year     =  2018
}

@ARTICLE{Zhao2017-on,
  title     = "{P2P} Lending Survey: Platforms, Recent Advances and Prospects",
  author    = "Zhao, Hongke and Ge, Yong and Liu, Qi and Wang, Guifeng and
               Chen, Enhong and Zhang, Hefu",
  journal   = "ACM Trans. Intell. Syst. Technol.",
  publisher = "ACM",
  volume    =  8,
  number    =  6,
  pages     = "72:1--72:28",
  month     =  jul,
  year      =  2017,
  address   = "New York, NY, USA",
  keywords  = "P2P lending, micro-finance, online loans, platforms, prospects"
}

@ARTICLE{Delis2014-fc,
  title     = "Bank Regulations and Income Inequality: Empirical Evidence",
  author    = "Delis, Manthos D and Hasan, Iftekhar and Kazakis, Pantelis",
  abstract  = "This article provides cross-country evidence that variations in
               bank regulatory policies result in differences in income
               distribution. In particular, the overall liberalization of
               banking systems decreases income inequality significantly.
               However, this effect becomes insignificant for countries with
               low levels of economic and institutional development and for
               market-based economies. Among liberalization policies, credit
               and interest rate controls have the most significant negative
               effect on inequality. Privatizations and liberalization of
               international capital flows also decrease income inequality; the
               latter also increases the income share of the relatively poor.
               In contrast, liberalization of securities markets increases
               income inequality substantially.",
  journal   = "Rev Financ",
  publisher = "Narnia",
  volume    =  18,
  number    =  5,
  pages     = "1811--1846",
  month     =  aug,
  year      =  2014
}

@ARTICLE{Cihak2012-pm,
  title   = "Bank regulation and supervision around the world : a crisis update",
  author  = "Cihak, Martin and Demirguc-Kunt, Asli and Peria, Maria Soledad
             Martinez and Mohseni-Cheraghlou, Amin",
  journal = "Policy Research working paper ; no. WPS 6286. Washington, DC:
             World Bank",
  number  = "Policy Research working paper ;WPS 6286",
  year    =  2012
}

@ARTICLE{Kuosmanen2018-hi,
  title    = "Shadow Prices and Marginal Abatement Costs: Convex Quantile
              Regression Approach",
  author   = "Kuosmanen, Timo and Zhou, Xun",
  abstract = "Shadow pricing environmental bads is critically important for
              efficient environmental policy and management. However, most
              empirical studies grossly overestimate the marginal abatement
              costs for three reasons. First, assuming downscaling of
              production as the only abatement option ignores abatement through
              increasing the input use. Second, estimating shadow prices on the
              frontier ignores the impact of inefficiency. Third, estimating
              the frontier by deterministic methods ignores the upward bias due
              to noise in empirical data. To address these problems, a novel
              data-driven approach is developed. Instead of projecting
              inefficient units to the frontier, we estimate the shadow prices
              locally based on the actual level of performance using convex
              quantile regression. Compared to the traditional approaches,
              convex quantile regression is more robust to noise, the choice of
              the direction vector, and heteroscedasticity. Application to the
              U.S. electric power plants provides empirical evidence and
              demonstrates the advantages of the proposed approach.",
  month    =  nov,
  year     =  2018
}

@ARTICLE{Becker2018-yf,
  title         = "Deep optimal stopping",
  author        = "Becker, Sebastian and Cheridito, Patrick and Jentzen, Arnulf",
  abstract      = "In this paper we develop a deep learning method for optimal
                   stopping problems which directly learns the optimal stopping
                   rule from Monte Carlo samples. As such, it is broadly
                   applicable in situations where the underlying randomness can
                   efficiently be simulated. We test the approach on three
                   problems: the pricing of a Bermudan max-call option, the
                   pricing of a callable multi barrier reverse convertible and
                   the problem of optimally stopping a fractional Brownian
                   motion. In all three cases it produces very accurate results
                   in high-dimensional situations with short computing times.",
  month         =  apr,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "math.NA",
  eprint        = "1804.05394"
}

@ARTICLE{Muth2018-xy,
  title   = "User-friendly Bayesian regression modeling: A tutorial with
             rstanarm and shinystan",
  author  = "Muth, Chelsea and Oravecz, Zita and Gabry, Jonah",
  journal = "TQMP",
  volume  =  14,
  number  =  2,
  pages   = "99--119",
  month   =  apr,
  year    =  2018
}

@MISC{Rezai2011-tl,
  title   = "The Opportunity Cost of Climate Policy: A Question of Reference*",
  author  = "Rezai, Armon",
  journal = "The Scandinavian Journal of Economics",
  volume  =  113,
  number  =  4,
  pages   = "885--903",
  year    =  2011
}

@BOOK{Tol2019-sn,
  title     = "Climate Economics: Economic Analysis of Climate, Climate Change
               and Climate Policy, Second Edition",
  author    = "Tol, Richard S J",
  abstract  = "This unique and erudite second edition can be used at three
               different levels -- advanced undergraduate, post-graduate and
               doctoral. It comprehensively covers the critical issues on the
               economics of climate change and climate policy features and
               clearly identifies the specific sections each level of reader
               should explore. Topics include the costs and benefits of
               adaptation and mitigation, discounting, uncertainty, policy
               instruments, and international agreements. Lectures can be
               combined with exercises, guided reading, or the building and
               application of an integrated assessment model. The book is
               accompanied by a website with background material, data, opinion
               pieces and videos. Although primarily intended for use in the
               classroom, anyone with an interest in climate policy can use
               this text as a reference.",
  publisher = "Edward Elgar Publishing",
  year      =  2019,
  language  = "en"
}

@ARTICLE{Bohringer2009-fe,
  title    = "{EU} climate policy up to 2020: An economic impact assessment",
  author   = "B{\"o}hringer, Christoph and L{\"o}schel, Andreas and Moslener,
              Ulf and Rutherford, Thomas F",
  abstract = "In its fight against climate change the EU is committed to
              reducing its overall greenhouse gas emissions to at least 20\%
              below 1990 levels by 2020. To meet this commitment, the EU builds
              on segmented market regulation with an EU-wide cap-and-trade
              system for emissions from energy-intensive installations (ETS
              sectors) and additional measures by each EU Member State covering
              emission sources outside the cap-and-trade system (the non-ETS
              sector). Furthermore, the EU has launched additional policy
              measures such as renewable energy subsidies in order to promote
              compliance with the climate policy target. Basic economic
              reasoning suggests that emission market segmentation and
              overlapping regulation can create substantial excess costs if we
              focus only on the climate policy target. In this paper, we
              evaluate the economic impacts of EU climate policy based on
              numerical simulations with a computable general equilibrium model
              of international trade and energy use. Our results highlight the
              importance of initial market distortions and imperfections as
              well as alternative baseline projections for the appropriate
              assessment of EU compliance cost.",
  journal  = "Energy Econ.",
  volume   =  31,
  pages    = "S295--S305",
  month    =  dec,
  year     =  2009,
  keywords = "Climate policy; Market distortions; Baseline projections;
              Computable general equilibrium"
}

@ARTICLE{Weitz2018-lz,
  title     = "Towards systemic and contextual priority setting for
               implementing the 2030 Agenda",
  author    = "Weitz, Nina and Carlsen, Henrik and Nilsson, M{\aa}ns and
               Sk{\aa}nberg, Kristian",
  abstract  = "How the sustainable development goals (SDGs) interact with each
               other has emerged as a key question in the implementation of the
               2030 Agenda, as it has potentially strong implications for
               prioritization of actions and their effectiveness. So far,
               analysis of interactions has been very basic, typically starting
               from one SDG, counting the number of interactions, and
               discussing synergies and trade-offs from the perspective of that
               issue area. This paper pushes the frontier of how interactions
               amongst SDG targets can be understood and taken into account in
               policy and planning. It presents an approach to assessing
               systemic and contextual interactions of SDG targets, using a
               typology for scoring interactions in a cross-impact matrix and
               using network analysis techniques to explore the data. By
               considering how a target interacts with another target and how
               that target in turn interacts with other targets, results
               provide a more robust basis for priority setting of SDG efforts.
               The analysis identifies which targets have the most and least
               positive influence on the network and thus guides, where efforts
               may be directed (and not); where strong positive and negative
               links sit, raising warning flags to areas requiring extra
               attention; and how targets that reinforce each others' progress
               cluster, suggesting where important cross-sectoral collaboration
               between actors is merited. How interactions play out is context
               specific and the approach is tested on the case of Sweden to
               illustrate how priority setting, with the objective to enhance
               progress across all 17 SDGs, might change if systemic impacts
               are taken into consideration.",
  journal   = "Sustainability Sci.",
  publisher = "Springer",
  volume    =  13,
  number    =  2,
  pages     = "531--548",
  year      =  2018,
  keywords  = "2030 Agenda; Network analysis; Policy coherence; Sustainable
               development goals (SDG); Systems analysis",
  language  = "en"
}

@ARTICLE{noauthor_undated-xa,
  title = "ipcc\_wg3\_ar5\_summary-for-policymakers.pdf"
}

@ARTICLE{Maddaloni2019-va,
  title   = "Rules and discretion(s) in prudential regulation and supervision:
             evidence from {EU} banks in the run-up to the crisis",
  author  = "Maddaloni, Angela and Scopelliti, Alessandro",
  journal = "European Central Bank Working Paper Series",
  number  =  2284,
  month   =  may,
  year    =  2019
}

@ARTICLE{Kuiper2015-mf,
  title     = "Using Online {Game-Based} Simulations to Strengthen Students'
               Understanding of Practical Statistical Issues in {Real-World}
               Data Analysis",
  author    = "Kuiper, Shonda and Sturdivant, Rodney X",
  abstract  = "Datasets provided to students are typically carefully chosen and
               vetted to illustrate a key statistical topic or method. Rarely
               are real studies and data so straightforward. In addition,
               carefully curated datasets that are brought into the statistics
               classroom may not feel realistic to students. We provide several
               examples of online activities where students can quickly collect
               their own local data, have input on the goals of the study and
               draw their own conclusions. These activities focus on core
               statistical issues that are often challenging to teach with
               traditional textbooks, such as working with messy data, bias,
               data relevance, and reliability. This approach to teaching
               integrates the challenges of data in a way that encourages
               students to see how easy it can be to inadvertently draw
               misleading conclusions. These activities are designed to be
               highly adaptable and have proven effective in a wide variety of
               introductory and advanced undergraduate courses.[Received
               December 2014. Revised July 2015.]",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  69,
  number    =  4,
  pages     = "354--361",
  month     =  oct,
  year      =  2015
}

@ARTICLE{Kuan1994-ec,
  title     = "Artificial neural networks: an econometric perspective",
  author    = "Kuan, Chung-Ming and White, Halbert",
  journal   = "Economet. Rev.",
  publisher = "Taylor \& Francis",
  volume    =  13,
  number    =  1,
  pages     = "1--91",
  month     =  jan,
  year      =  1994
}

@ARTICLE{Lee1993-wm,
  title    = "Testing for neglected nonlinearity in time series models: A
              comparison of neural network methods and alternative tests",
  author   = "Lee, Tae-Hwy and White, Halbert and Granger, Clive W J",
  abstract = "In this paper a new test, the neural network test for neglected
              nonlinearity, is compared with the Keenan test, the Tsay test,
              the White dynamic information matrix test, the McLeod-Li test,
              the Ramsey RESET test, the Brock-Dechert-Scheinkman test, and the
              Bispectrum test. The neural network test is based on the
              approximating ability of neural network modeling techniques
              recently developed by cognitive scientists. This test is a
              Lagrange multiplier test that statistically determines whether
              adding `hidden units' to the linear network would be
              advantageous. The performance of the tests is compared using a
              variety of nonlinear artificial series including bilinear,
              threshold autoregressive, and nonlinear moving average models,
              and the tests are applied to actual economic time series. The
              relative performance of the neural network test is encouraging.
              Our results suggest that it can play a valuable role in
              evaluating model adequacy. The neural network test has proper
              size and good power, and many of the economic series tested
              exhibit potential nonlinearities.",
  journal  = "J. Econom.",
  volume   =  56,
  number   =  3,
  pages    = "269--290",
  month    =  apr,
  year     =  1993
}

@UNPUBLISHED{Beck2013-eq,
  title    = "{Non-Performing} Loans: What Matters in Addition to the Economic
              Cycle?",
  author   = "Beck, Roland and Jakubik, Petr and Piloiu, Anamaria",
  abstract = "Using a novel panel data set we study the macroeconomic
              determinants of nonperforming loans (NPLs) across 75 countries
              during the past decade. According to our dynamic panel estimates,
              the following variables are found to significantly affect NPL
              ratios: real GDP growth, share prices, the exchange rate, and the
              lending interest rate. In the case of exchange rates, the
              direction of the effect depends on the extent of foreign exchange
              lending to unhedged borrowers which is particularly high in
              countries with pegged or managed exchange rates. In the case of
              share prices, the impact is found to be larger in countries which
              have a large stock market relative to GDP. These results are
              robust to alternative econometric specifications.",
  month    =  feb,
  year     =  2013,
  keywords = "Non-performing loans, credit risk, currency mismatches"
}

@UNPUBLISHED{Hubel2019-jq,
  title    = "Integrating Sustainability Risks in Asset Management: The Role of
              {ESG} Exposures and {ESG} Ratings",
  author   = "H{\"u}bel, Benjamin and Scholz, Hendrik",
  abstract = "The rising sustainability awareness among regulators, consumers
              and investors results in major sustainability risks for firms. We
              construct three ESG risk factors (Environmental, Social, and
              Governance) to quantify the ESG risk exposures of firms. Taking
              these factors into account significantly enhances the explanatory
              power of standard asset pricing models. We find that portfolios
              with pronounced ESG risk exposures exhibit substantially higher
              risks, but investors can compose portfolios with lower ESG risks
              while keeping risk-adjusted performance virtually unchanged.
              Moreover, investors can measure the ESG risk exposures of all
              firms in their portfolios using only stock returns, so that even
              stocks without qualitative ESG information can be easily
              considered in the management of ESG risks. Indeed, strategically
              managing ESG risks may result in potential benefits for
              investors.",
  month    =  mar,
  year     =  2019,
  keywords = "Corporate social responsibility; ESG ratings; ESG exposures;
              stock returns"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gelman_undated-td,
  title  = "The garden of forking paths: Why multiple comparisons can be a
            problem, even when there is no ``fishing expedition'' or
            ``p-hacking'' and the research hypothesis was posited ahead of
            time‚àó",
  author = "Gelman, Andrew and Loken, Eric"
}

@ARTICLE{Byrd2019-ds,
  title     = "Fund Asset Inference Using Machine Learning Methods: What's in
               That Portfolio?",
  author    = "Byrd, David and Bajaj, Sourabh and Balch, Tucker Hybinette",
  abstract  = "1. David Byrd 1. is a research scientist at the Georgia
               Institute of Technology in Atlanta, GA. (db\{at\}gatech.edu) 2.
               Sourabh Bajaj 1. is a former student of the Georgia Institute of
               Technology in Atlanta, GA. (sbajaj9\{at\}gatech.edu) 3. Tucker
               Hybinette Balch 1. is a professor of computer science at the
               Georgia Institute of Technology and a co-founder of Lucena
               Research, Inc in Atlanta, GA. (tucker\{at\}cc.gatech.edu) 1. To
               order reprints of this article, please contact David Rowe at
               d.rowe\{at\}pageantmedia.com or 646-891-2157. Given only the
               historic net asset value of a large-cap mutual fund, which
               members of some universe of stocks are held by the fund?
               Discovering an exact solution is combinatorially intractable
               because there are, for example, C (500, 30) or 1.4 $\times$ 1048
               possible portfolios of 30 stocks drawn from the S\&P 500. The
               authors extend an existing linear clones approach and introduce
               a new sequential oscillating selection method to produce a
               computationally efficient inference. Such techniques could
               inform efforts to detect fund window dressing of disclosure
               statements or to adjust market positions in advance of major
               fund disclosure dates. The authors test the approach by tasking
               the algorithm with inferring the constituents of exchange-traded
               funds for which the components can be later examined. Depending
               on the details of the specific problem, the algorithm runs on
               consumer hardware in 8 to 15 seconds and identifies target
               portfolio constituents with an accuracy of 88.2\% to 98.6\%.
               TOPICS: [Big data/machine learning][1], [statistical
               methods][2], [portfolio management/multi-asset allocation][3]
               [1]: https://www.iijournals.com/topic/big-datamachine-learning
               [2]: https://www.iijournals.com/topic/statistical-methods [3]:
               https://www.iijournals.com/topic/other-2",
  journal   = "The Journal of Financial Data Science",
  publisher = "Institutional Investor Journals Umbrella",
  pages     = "jfds.2019.1.005",
  month     =  jun,
  year      =  2019,
  language  = "en"
}

@UNPUBLISHED{Engle2019-cv,
  title    = "Hedging Climate Change News",
  author   = "Engle, Robert F and Giglio, Stefano and Lee, Heebum and Kelly,
              Bryan T and Stroebel, Johannes",
  abstract = "We propose and implement a procedure to dynamically hedge climate
              change risk. To create our hedge target, we extract innovations
              in climate news series that we construct through textual analysis
              of high-dimensional data on newspaper coverage of climate change.
              We then use a mimicking-portfolio approach to build climate
              change hedge portfolios using a large panel of equity returns. We
              discipline the exercise by using third-party ESG scores of firms
              to model their climate risk exposures. We show that this approach
              yields parsimonious and industry-balanced portfolios that perform
              well in hedging innovations in climate news both in sample and
              out of sample. The resulting hedge portfolios outperform
              alternative hedging strategies based primarily on industry tilts.
              We discuss multiple directions for future research on financial
              approaches to managing climate risk.",
  month    =  jan,
  year     =  2019,
  keywords = "Climate Risk, Hedge Portfolio"
}

@ARTICLE{Hunter2012-bb,
  title    = "A practical guide to volatility forecasting through calm and
              storm",
  author   = "Hunter, Dawn",
  journal  = "Journal of Risk",
  month    =  jan,
  year     =  2012
}

@UNPUBLISHED{Rasekhschaffe2019-mt,
  title    = "Machine Learning for Stock Selection",
  author   = "Rasekhschaffe, Keywan and Jones, Robert",
  abstract = "Machine learning is an increasingly important and controversial
              topic in quantitative finance. A lively debate persists as to
              whether machine learning techniques can be practical investment
              tools. Although machine learning algorithms can uncover subtle,
              contextual and non-linear relationships, overfitting poses a
              major challenge when trying to extract signals from noisy
              historical data. In this article, we describe some of the basic
              concepts surrounding machine leaning and provide a simple example
              of how investors can use machine learning techniques to forecast
              the cross-section of stock returns while limiting the risk of
              overfitting.",
  month    =  feb,
  year     =  2019,
  keywords = "Machine Learning, Return Prediction, Cross-Section of Returns,
              Gradient Boosting, SVM, AdaBoost, (Deep) Neural Networks, Feature
              Engineering, Fintech"
}

@ARTICLE{Ince2006-pr,
  title    = "{INDIVIDUAL} {EQUITY} {RETURN} {DATA} {FROM} {THOMSON}
              {DATASTREAM}: {HANDLE} {WITH} {CARE}!",
  author   = "Ince, Ozgur S and Porter, R Burt",
  abstract = "Abstract We compare individual U.S. equity return data from
              Thomson Datastream (TDS) with similar data from the Center for
              Research in Security Prices (CRSP) to evaluate TDS for use in
              studies involving large numbers of individual equities in markets
              outside the United States. We document important issues of
              coverage, classification, and data integrity and find that naive
              use of TDS data can have a large impact on economic inferences.
              We show that after careful screening of the TDS data, inferences
              drawn from TDS data are similar to those drawn from CRSP. We
              illustrate the importance of the screens we develop using U.S.
              TDS data by applying the screens to TDS data from four European
              equity markets.",
  journal  = "J. Financ. Res.",
  volume   =  29,
  number   =  4,
  pages    = "463--479",
  month    =  dec,
  year     =  2006
}

@ARTICLE{Budnik2018-kh,
  title     = "Macroprudential Regulation in the European Union in 1995-2014:
               Introducing a New Data Set on Policy Actions of a
               Macroprudential Nature",
  author    = "Budnik, Katarzyna Barbara and Kleibl, Johannes",
  abstract  = "This paper introduces a new comprehensive data set on policies
               of a macroprudential nature in the banking sectors of the 28
               member states of the European Union (EU) between 1995 and 2014.
               The Macroprudential Policies Evaluation Database (MaPPED) offers
               a detailed overview of the ``life-cycle'' of policy instruments
               which are either genuinely macroprudential or are essentially
               microprudential but likely to have a significant impact on the
               whole banking system. It tracks events of the introduction,
               recalibration and termination of eleven categories and 53
               subcategories of instruments. MaPPED has been based on a
               carefully designed questionnaire, which has been completed in
               cooperation with experts from national central banks and
               supervisory authorities of all EU member states. This paper
               describes the design and structure of the new data set and
               presents the first descriptive analysis of the use of policy
               measures with a macroprudential nature in the EU over the last
               two decades. The results indicate that there has been a
               remarkable variation in the use of policies of a macroprudential
               nature both across EU countries and over time. Moreover, the
               analysis provides some tentative evidence of an impact of
               capital buffers, lending restrictions and caps on maturity
               mismatches on credit to the non-financial private sector in the
               EU as well as of the relative ineffectiveness of sectoral risk
               weights in controlling credit growth.",
  journal   = "European Central Bank Working Paper Series",
  publisher = "papers.ssrn.com",
  number    =  2123,
  month     =  jan,
  year      =  2018,
  keywords  = "macroprudential policy, macroprudential instruments, financial
               stability, policy assessment"
}

@ARTICLE{Fliers2019-ab,
  title    = "What is the relation between financial flexibility and dividend
              smoothing?",
  author   = "Fliers, Philip T",
  abstract = "This paper investigates the relation between financial
              flexibility and dividend smoothing policies. We use two proxies
              for financial flexibility; we measure levels of unused debt
              capacity and capital structure adjustment speeds. We find a
              nonlinear relation between unused debt capacity and dividend
              smoothing. For firms with high levels of unused debt capacity,
              this relation is positive. However, we find a negative effect for
              firms with low levels of unused debt capacity. Additionally, we
              show a positive relation between capital structure adjustment
              speeds and dividend smoothing. We find that firms absorb shocks
              to net income by changing their capital structure, and this
              change enables dividend smoothing. The effects we document are
              stronger for positive changes to a firm's net income.",
  journal  = "J. Int. Money Finance",
  volume   =  92,
  pages    = "98--111",
  month    =  apr,
  year     =  2019,
  keywords = "Dividend smoothing; Capital structure; Financial flexibility;
              Debt capacity; Lintner; Partial adjustment models"
}

@ARTICLE{Pastor2003-iu,
  title     = "Liquidity Risk and Expected Stock Returns",
  author    = "P{\'a}stor, {\v L}ubo{\v s} and Stambaugh, Robert F",
  abstract  = "This study investigates whether marketwide liquidity is a state
               variable important for asset pricing. We find that expected
               stock returns are related cross?sectionally to the sensitivities
               of returns to fluctuations in aggregate liquidity. Our monthly
               liquidity measure, an average of individual?stock measures
               estimated with daily data, relies on the principle that order
               flow induces greater return reversals when liquidity is lower.
               From 1966 through 1999, the average return on stocks with high
               sensitivities to liquidity exceeds that for stocks with low
               sensitivities by 7.5 percent annually, adjusted for exposures to
               the market return as well as size, value, and momentum factors.
               Furthermore, a liquidity risk factor accounts for half of the
               profits to a momentum strategy over the same 34?year period.",
  journal   = "J. Polit. Econ.",
  publisher = "The University of Chicago Press",
  volume    =  111,
  number    =  3,
  pages     = "642--685",
  month     =  jun,
  year      =  2003
}

@ARTICLE{Acharya2005-no,
  title    = "Asset pricing with liquidity risk",
  author   = "Acharya, Viral V and Pedersen, Lasse Heje",
  abstract = "This paper solves explicitly a simple equilibrium model with
              liquidity risk. In our liquidity-adjusted capital asset pricing
              model, a security's required return depends on its expected
              liquidity as well as on the covariances of its own return and
              liquidity with the market return and liquidity. In addition, a
              persistent negative shock to a security's liquidity results in
              low contemporaneous returns and high predicted future returns.
              The model provides a unified framework for understanding the
              various channels through which liquidity risk may affect asset
              prices. Our empirical results shed light on the total and
              relative economic significance of these channels and provide
              evidence of flight to liquidity.",
  journal  = "J. financ. econ.",
  volume   =  77,
  number   =  2,
  pages    = "375--410",
  month    =  aug,
  year     =  2005,
  keywords = "Liquidity risk; Liquidity-adjusted CAPM; Flight to liquidity;
              Frictions; Transaction costs"
}

@ARTICLE{Anon_undated-kz,
  title   = "What is happening in the index linked gilt market?",
  author  = "{Anon}",
  journal = "Reveiw of Finance Letters"
}

@ARTICLE{Laurent1976-eo,
  title   = "Constructing optimal binary decision trees is {NP-complete}",
  author  = "Laurent, Hyafil and Rivest, Ronald L",
  journal = "Inf. Process. Lett.",
  volume  =  5,
  number  =  1,
  pages   = "15--17",
  year    =  1976
}

@MISC{Gibbons1989-eo,
  title   = "A Test of the Efficiency of a Given Portfolio",
  author  = "Gibbons, Michael R and Ross, Stephen A and Shanken, Jay",
  journal = "Econometrica",
  volume  =  57,
  number  =  5,
  pages   = "1121",
  year    =  1989
}

@TECHREPORT{Hall2018-eu,
  title       = "{GROWING} {THE} {ARTIFICIAL} {INTELLIGENCE} {INDUSTRY} {IN}
                 {THE} {UK}",
  author      = "Hall, Dame Wendy and Pesenti, J{\'e}r{\^o}me",
  institution = "UK Government",
  year        =  2018
}

@ARTICLE{Sirignano2016-wq,
  title         = "Deep Learning for Mortgage Risk",
  author        = "Sirignano, Justin and Sadhwani, Apaar and Giesecke, Kay",
  abstract      = "We develop a deep learning model of multi-period mortgage
                   risk and use it to analyze an unprecedented dataset of
                   origination and monthly performance records for over 120
                   million mortgages originated across the US between 1995 and
                   2014. Our estimators of term structures of conditional
                   probabilities of prepayment, foreclosure and various states
                   of delinquency incorporate the dynamics of a large number of
                   loan-specific as well as macroeconomic variables down to the
                   zip-code level. The estimators uncover the highly nonlinear
                   nature of the relationship between the variables and
                   borrower behavior, especially prepayment. They also
                   highlight the effects of local economic conditions on
                   borrower behavior. State unemployment has the greatest
                   explanatory power among all variables, offering strong
                   evidence of the tight connection between housing finance
                   markets and the macroeconomy. The sensitivity of a borrower
                   to changes in unemployment strongly depends upon current
                   unemployment. It also significantly varies across the entire
                   borrower population, which highlights the interaction of
                   unemployment and many other variables. These findings have
                   important implications for mortgage-backed security
                   investors, rating agencies, and housing finance
                   policymakers.",
  month         =  jul,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "q-fin.ST",
  eprint        = "1607.02470"
}

@ARTICLE{Belloni2014-wj,
  title     = "Inference on Treatment Effects after Selection among
               {High-Dimensional} Controls",
  author    = "Belloni, Alexandre and Chernozhukov, Victor and Hansen,
               Christian",
  abstract  = "Abstract. We propose robust methods for inference about the
               effect of a treatment variable on a scalar outcome in the
               presence of very many regressors in a mod",
  journal   = "Rev. Econ. Stud.",
  publisher = "Narnia",
  volume    =  81,
  number    =  2,
  pages     = "608--650",
  month     =  apr,
  year      =  2014
}

@ARTICLE{OHara2020-ak,
  title     = "{ETFs} and systemic risks",
  author    = "O'Hara, Maureen",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Bacon_undated-kr,
  title  = "Abnormal Returns Part 2: Returns for Short Positions and Portfolios",
  author = "Bacon, Carl and Advisor, Chief and Thompson, Stat Pro Ian and van
            der Westhuizen, Stat Pro Pierre and Pro, Stat"
}

@ARTICLE{Horton2015-oy,
  title     = "Teaching the Next Generation of Statistics Students to ``Think
               With Data'': Special Issue on Statistics and the Undergraduate
               Curriculum",
  author    = "Horton, Nicholas J and Hardin, Johanna S",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  69,
  number    =  4,
  pages     = "259--265",
  month     =  oct,
  year      =  2015
}

@ARTICLE{Hardin2015-gd,
  title     = "Data Science in Statistics Curricula: Preparing Students to
               ``Think with Data''",
  author    = "Hardin, J and Hoerl, R and Horton, Nicholas J and Nolan, D and
               Baumer, B and Hall-Holt, O and Murrell, P and Peng, R and
               Roback, P and Temple Lang, D and Ward, M D",
  abstract  = "A growing number of students are completing undergraduate
               degrees in statistics and entering the workforce as data
               analysts. In these positions, they are expected to understand
               how to use databases and other data warehouses, scrape data from
               Internet sources, program solutions to complex problems in
               multiple languages, and think algorithmically as well as
               statistically. These data science topics have not traditionally
               been a major component of undergraduate programs in statistics.
               Consequently, a curricular shift is needed to address additional
               learning outcomes. The goal of this article is to motivate the
               importance of data science proficiency and to provide examples
               and resources for instructors to implement data science in their
               own statistics curricula. We provide case studies from seven
               institutions. These varied approaches to teaching data science
               demonstrate curricular innovations to address new needs. Also
               included here are examples of assignments designed for courses
               that foster engagement of undergraduates with data and data
               science.[Received November 2014. Revised July 2015.]",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  69,
  number    =  4,
  pages     = "343--353",
  month     =  oct,
  year      =  2015
}

@ARTICLE{Tintle2015-mx,
  title     = "Combating {Anti-Statistical} Thinking Using {Simulation-Based}
               Methods Throughout the Undergraduate Curriculum",
  author    = "Tintle, Nathan and Chance, Beth and Cobb, George and Roy, Soma
               and Swanson, Todd and VanderStoep, Jill",
  abstract  = "The use of simulation-based methods for introducing inferen-ce
               is growing in popularity for the Stat 101 course, due in part to
               increasing evidence of the methods ability to improve studen-ts?
               statistical thinking. This impact comes from simulation-based
               methods (a) clearly presenting the overarching logic of
               inference, (b) strengthening ties between statistics and
               probability/mathematical concepts, (c) encouraging a focus on
               the entire research process, (d) facilitating student thinking
               about advanced statistical concepts, (e) allowing more time to
               explore, do, and talk about real research and messy data, and
               (f) acting as a firm-er foundation on which to build statistical
               intuition. Thus, we argue that simulation-based inference should
               be an entry point to an undergraduate statistics program for all
               students, and that simulation-based inference should be used
               throughout all under-graduate statistics courses. To achieve
               this goal and fully recognize the benefits of simulation-based
               inference on the undergraduate statistics program, we will need
               to break free of historical forces tying undergraduate
               statistics curricula to mathematics, consider radical and
               innovative new pedagogical approaches in our courses, fully
               implement assessment-driven content innovations, and embrace
               computation throughout the curriculum.[Received December 2014.
               Revised July 2015]",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  69,
  number    =  4,
  pages     = "362--370",
  month     =  oct,
  year      =  2015
}

@ARTICLE{Swanson1997-go,
  title     = "A Model Selection Approach to {Real-Time} Macroeconomic
               Forecasting Using Linear Models and Artificial Neural Networks",
  author    = "Swanson, Norman R and White, Halbert",
  abstract  = "We take a model selection approach to the question of whether a
               class of adaptive prediction models (artificial neural networks)
               is useful for predicting future values of nine macroeconomic
               variables. We use a variety of out-of-sample forecast-based
               model selection criteria, including forecast error measures and
               forecast direction accuracy. Ex ante or real-time forecasting
               results based on rolling window prediction methods indicate that
               multivariate adaptive linear vector autoregression models often
               outperform a variety of (1) adaptive and nonadaptive univariate
               models, (2) nonadaptive multivariate models, (3) adaptive
               nonlinear models, and (4) professionally available survey
               predictions. Further, model selection based on the in-sample
               Schwarz information criterion apparently fails to offer a
               convenient shortcut to true out-of-sample performance measures.",
  journal   = "Rev. Econ. Stat.",
  publisher = "MIT Press",
  volume    =  79,
  number    =  4,
  pages     = "540--550",
  month     =  nov,
  year      =  1997
}

@MISC{Brown1998-bx,
  title   = "The Dow Theory: William Peter Hamilton's Track Record Reconsidered",
  author  = "Brown, Stephen J and Goetzmann, William N and Kumar, Alok",
  journal = "The Journal of Finance",
  volume  =  53,
  number  =  4,
  pages   = "1311--1333",
  year    =  1998
}

@ARTICLE{Bansal1993-sy,
  title    = "No Arbitrage and Arbitrage Pricing: A New Approach",
  author   = "Bansal, Ravi and Viswanathan, S",
  abstract = "ABSTRACT We argue that arbitrage-pricing theories (APT) imply the
              existence of a low-dimensional nonnegative nonlinear pricing
              kernel. In contrast to standard constructs of the APT, we do not
              assume a linear factor structure on the payoffs. This allows us
              to price both primitive and derivative securities.
              Semi-nonparametric techniques are used to estimate the pricing
              kernel and test the theory. Empirical results using size-based
              portfolio returns and yields on bonds reject the nested capital
              asset-pricing model and linear APT and support the nonlinear APT.
              Diagnostics show that the nonlinear model is more capable of
              explaining variations in small firm returns.",
  journal  = "J. Finance",
  volume   =  48,
  number   =  4,
  pages    = "1231--1262",
  month    =  sep,
  year     =  1993
}

@ARTICLE{Desai1996-uz,
  title    = "A comparison of neural networks and linear scoring models in the
              credit union environment",
  author   = "Desai, Vijay S and Crook, Jonathan N and Overstreet, George A",
  abstract = "The purpose of the present paper is to explore the ability of
              neural networks such as multilayer perceptrons and modular neural
              networks, and traditional techniques such as linear discriminant
              analysis and logistic regression, in building credit scoring
              models in the credit union environment. Also, since funding and
              small sample size often preclude the use of customized credit
              scoring models at small credit unions, we investigate the
              performance of generic models and compare them with customized
              models. Our results indicate that customized neural networks
              offer a very promising avenue if the measure of performance is
              percentage of bad loans correctly classified. However, if the
              measure of performance is percentage of good and bad loans
              correctly classified, logistic regression models are comparable
              to the neural networks approach. The performance of generic
              models was not as good as the customized models, particularly
              when it came to correctly classifying bad loans. Although we
              found significant differences in the results for the three credit
              unions, our modular neural network could not accommodate these
              differences, indicating that more innovative architectures might
              be necessary for building effective generic models.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  95,
  number   =  1,
  pages    = "24--37",
  month    =  nov,
  year     =  1996,
  keywords = "Neural networks; Bamking; Credit scoring"
}

@MISC{Sirignano_undated-nf,
  title   = "Universal Features of Price Formation in Financial Markets:
             Perspectives From Deep Learning",
  author  = "Sirignano, Justin and Cont, Rama",
  journal = "SSRN Electronic Journal"
}

@ARTICLE{Arak1985-rp,
  title     = "The real rate of interest: Inferences from the new {UK} indexed
               gilts",
  author    = "Arak, Marcelle and Kreicher, Lawrence",
  journal   = "Int. Econ. Rev.",
  publisher = "JSTOR",
  pages     = "399--408",
  year      =  1985
}

@MISC{Armitage2004-sw,
  title   = "Returns after personal tax on {UK} equity and gilts, 1919--1998",
  author  = "Armitage, Seth",
  journal = "The European Journal of Finance",
  volume  =  10,
  number  =  1,
  pages   = "23--43",
  year    =  2004
}

@MISC{Goodhart1990-wh,
  title   = "{UK} Indexed Gilts: A Case Study of Financial Indexation",
  author  = "Goodhart, Charles",
  journal = "The Future of Financial Systems and Services",
  pages   = "87--103",
  year    =  1990
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Oliver2019-bb,
  title   = "`The capital market is dead': the difficult birth of index‚Äêlinked
             gilts in the {UK}",
  author  = "Oliver, Michael J and Rutterford, Janette",
  journal = "The Economic History Review",
  year    =  2019
}

@BOOK{Agrawal2018-bk,
  title     = "Prediction Machines: The Simple Economics of Artificial
               Intelligence",
  author    = "Agrawal, Ajay and Gans, Joshua and Goldfarb, Avi",
  abstract  = "``What does AI mean for your business? Read this book to find
               out.'' -- Hal Varian, Chief Economist, GoogleArtificial
               intelligence does the seemingly impossible, magically bringing
               machines to life--driving cars, trading stocks, and teaching
               children. But facing the sea change that AI will bring can be
               paralyzing. How should companies set strategies, governments
               design policies, and people plan their lives for a world so
               different from what we know? In the face of such uncertainty,
               many analysts either cower in fear or predict an impossibly
               sunny future.But in Prediction Machines, three eminent
               economists recast the rise of AI as a drop in the cost of
               prediction. With this single, masterful stroke, they lift the
               curtain on the AI-is-magic hype and show how basic tools from
               economics provide clarity about the AI revolution and a basis
               for action by CEOs, managers, policy makers, investors, and
               entrepreneurs.When AI is framed as cheap prediction, its
               extraordinary potential becomes clear:Prediction is at the heart
               of making decisions under uncertainty. Our businesses and
               personal lives are riddled with such decisions.Prediction tools
               increase productivity--operating machines, handling documents,
               communicating with customers.Uncertainty constrains strategy.
               Better prediction creates opportunities for new business
               structures and strategies to compete.Penetrating, fun, and
               always insightful and practical, Prediction Machines follows its
               inescapable logic to explain how to navigate the changes on the
               horizon. The impact of AI will be profound, but the economic
               framework for understanding it is surprisingly simple.",
  publisher = "Harvard Business Press",
  month     =  apr,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Agrawal2014-rt,
  title     = "Some Simple Economics of Crowdfunding",
  author    = "Agrawal, Ajay and Catalini, Christian and Goldfarb, Avi",
  abstract  = "Executive SummaryIt is not surprising that the financing of
               early-stage creative projects and ventures is typically
               geographically localized since these types of funding decisions
               are usually predicated on personal relationships and due
               diligence requiring face-to-face interactions in response to
               high levels of risk, uncertainty, and information asymmetry. So,
               to economists, the recent rise of crowdfunding?raising capital
               from many people through an online platform?which offers little
               opportunity for careful due diligence and involves not only
               friends and family but also many strangers from near and far, is
               initially startling. On the eve of launching equity-based
               crowdfunding, a new market for early-stage finance in the United
               States, we provide a preliminary exploration of its underlying
               economics. We highlight the extent to which economic theory, in
               particular transaction costs, reputation, and market design, can
               explain the rise of nonequity crowdfunding and offer a framework
               for speculating on how equity-based crowdfunding may unfold. We
               conclude by articulating open questions related to how
               crowdfunding may affect social welfare and the rate and
               direction of innovation.",
  journal   = "Innovation Policy and the Economy",
  publisher = "The University of Chicago Press",
  volume    =  14,
  pages     = "63--97",
  month     =  jan,
  year      =  2014
}

@BOOK{Agrawal2019-hu,
  title     = "The Economics of Artificial Intelligence: An Agenda",
  author    = "Agrawal, Ajay and Gans, Joshua and Goldfarb, Avi",
  abstract  = "Advances in artificial intelligence (AI) highlight the potential
               of this technology to affect productivity, growth, inequality,
               market power, innovation, and employment. This volume seeks to
               set the agenda for economic research on the impact of AI. It
               covers four broad themes: AI as a general purpose technology;
               the relationships between AI, growth, jobs, and inequality;
               regulatory responses to changes brought on by AI; and the
               effects of AI on the way economic research is conducted. It
               explores the economic influence of machine learning, the branch
               of computational statistics that has driven much of the recent
               excitement around AI, as well as the economic impact of robotics
               and automation and the potential economic consequences of a
               still-hypothetical artificial general intelligence. The volume
               provides frameworks for understanding the economic impact of AI
               and identifies a number of open research questions.
               Contributors: Daron Acemoglu, Massachusetts Institute of
               Technology Philippe Aghion, Coll{\`e}ge de France Ajay Agrawal,
               University of Toronto Susan Athey, Stanford University James
               Bessen, Boston University School of Law Erik Brynjolfsson, MIT
               Sloan School of Management Colin F. Camerer, California
               Institute of Technology Judith Chevalier, Yale School of
               Management Iain M. Cockburn, Boston University Tyler Cowen,
               George Mason University Jason Furman, Harvard Kennedy School
               Patrick Francois, University of British Columbia Alberto
               Galasso, University of Toronto Joshua Gans, University of
               Toronto Avi Goldfarb, University of Toronto Austan Goolsbee,
               University of Chicago Booth School of Business Rebecca
               Henderson, Harvard Business School Ginger Zhe Jin, University of
               Maryland Benjamin F. Jones, Northwestern University Charles I.
               Jones, Stanford University Daniel Kahneman, Princeton University
               Anton Korinek, Johns Hopkins University Mara Lederman,
               University of Toronto Hong Luo, Harvard Business School John
               McHale, National University of Ireland Paul R. Milgrom, Stanford
               University Matthew Mitchell, University of Toronto Alexander
               Oettl, Georgia Institute of Technology Andrea Prat, Columbia
               Business School Manav Raj, New York University Pascual Restrepo,
               Boston University Daniel Rock, MIT Sloan School of Management
               Jeffrey D. Sachs, Columbia University Robert Seamans, New York
               University Scott Stern, MIT Sloan School of Management Betsey
               Stevenson, University of Michigan Joseph E. Stiglitz. Columbia
               University Chad Syverson, University of Chicago Booth School of
               Business Matt Taddy, University of Chicago Booth School of
               Business Steven Tadelis, University of California, Berkeley
               Manuel Trajtenberg, Tel Aviv University Daniel Trefler,
               University of Toronto Catherine Tucker, MIT Sloan School of
               Management Hal Varian, University of California, Berkeley",
  publisher = "University of Chicago Press",
  year      =  2019,
  language  = "en"
}

@BOOK{Agrawal2017-md,
  title     = "What to Expect from Artificial Intelligence",
  author    = "Agrawal, Ajay and Goldfarb, Avi and Gans, Joshua",
  publisher = "MIT Sloan Management Review",
  year      =  2017,
  language  = "en"
}

@ARTICLE{Giles2004-ye,
  title     = "Calculating a standard error for the Gini coefficient: some
               further results",
  author    = "Giles, David E A",
  journal   = "Oxf. Bull. Econ. Stat.",
  publisher = "Wiley Online Library",
  volume    =  66,
  number    =  3,
  pages     = "425--433",
  year      =  2004
}

@ARTICLE{Ogwang2000-vb,
  title   = "A Convenient Method of Computing the Gini Index and its Standard
             Error",
  author  = "Ogwang, Tomson",
  journal = "Oxf. Bull. Econ. Stat.",
  volume  =  62,
  number  =  1,
  pages   = "123--129",
  month   =  feb,
  year    =  2000
}

@MISC{Modarres2006-al,
  title   = "A Cautionary Note on Estimating the Standard Error of the Gini
             Index of Inequality*",
  author  = "Modarres, Reza and Gastwirth, Joseph L",
  journal = "Oxford Bulletin of Economics and Statistics",
  volume  =  68,
  number  =  3,
  pages   = "385--390",
  year    =  2006
}

@MISC{Ogwang2014-ec,
  title   = "A Convenient Method of Decomposing the Gini Index by Population
             Subgroups",
  author  = "Ogwang, Tomson",
  journal = "Journal of Official Statistics",
  volume  =  30,
  number  =  1,
  pages   = "91--105",
  year    =  2014
}

@ARTICLE{Wang2006-qv,
  title    = "The relationships between sentiment, returns and volatility",
  author   = "Wang, Yaw-Huei and Keswani, Aneel and Taylor, Stephen J",
  abstract = "Previous papers that test whether sentiment is useful for
              predicting volatility ignore whether lagged returns information
              might also be useful for this purpose. By doing so, these papers
              potentially overestimate the role of sentiment in predicting
              volatility. In this paper we test whether sentiment is useful for
              volatility forecasting purposes. We find that most of our
              sentiment measures are caused by returns and volatility rather
              than vice versa. In addition, we find that lagged returns cause
              volatility. All sentiment variables have extremely limited
              forecasting power once returns are included as a forecasting
              variable.",
  journal  = "Int. J. Forecast.",
  volume   =  22,
  number   =  1,
  pages    = "109--123",
  month    =  jan,
  year     =  2006,
  keywords = "Causality; Investor surveys; Market based sentiment measures;
              Realized volatility; Stock index returns"
}

@MISC{Labidi_undated-sq,
  title   = "Investor Sentiment and Aggregate Volatility Pricing",
  author  = "Labidi, Chiraz and Yaakoubi, Soumaya",
  journal = "SSRN Electronic Journal"
}

@ARTICLE{Borovkova2015-bv,
  title     = "News, volatility and jumps: the case of natural gas futures",
  author    = "Borovkova, Svetlana and Mahakena, Diego",
  abstract  = "We investigate the impact of news sentiment on the price
               dynamics of natural gas futures. We propose a Local News
               Sentiment Level model, based on the Local Level model of Durbin
               and Koopman [Time Series Analysis by State Space Methods, 2001],
               to construct a running series of news sentiment from irregularly
               observed news items? sentiments. We construct several return and
               variation measures to proxy for the fine dynamics of the natural
               gas futures prices. We employ event studies, Granger causality
               tests and several state-of-the-art volatility models to assess
               the effect of news on the returns, price jumps and the
               volatility. We find significant relationships between news
               sentiment and the dynamic characteristics of natural gas futures
               prices. Our findings are, among others, that the arrival of news
               in non-trading periods causes overnight returns, that news
               sentiment is Granger caused by negative semi-volatility and that
               news sentiment is more sensitive to negative than to positive
               jumps. In addition to that we find strong evidence that news
               sentiment severely Granger causes price jumps and conclude that
               market participants trade futures as some function of aggregated
               news. We augment volatility models with news sentiment measures
               and conduct an out-of-sample volatility forecasting study. The
               first class of models is the generalized autoregressive
               conditional heteroskedasticity models the second class is the
               high-frequency-based volatility models of Shephard and Shephard
               [J. Appl. Econ., 2010, 25, 197?231] and Noureldin et al. [J.
               Appl. Econ., 2012, 27(6), 907?933]. We adapt both models to
               account for asymmetric volatility, leverage and time to maturity
               effects. By augmenting all models with news sentiment variables,
               we find that including news sentiment in volatility models
               significantly improves volatility forecasts.",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  volume    =  15,
  number    =  7,
  pages     = "1217--1242",
  month     =  jul,
  year      =  2015
}

@ARTICLE{Atkins2018-dg,
  title    = "Financial news predicts stock market volatility better than close
              price",
  author   = "Atkins, Adam and Niranjan, Mahesan and Gerding, Enrico",
  abstract = "The behaviour of time series data from financial markets is
              influenced by a rich mixture of quantitative information from the
              dynamics of the system, captured in its past behaviour, and
              qualitative information about the underlying fundamentals
              arriving via various forms of news feeds. Pattern recognition of
              financial data using an effective combination of these two types
              of information is of much interest nowadays, and is addressed in
              several academic disciplines as well as by practitioners. Recent
              literature has focused much effort on the use of news-derived
              information to predict the direction of movement of a stock, i.e.
              posed as a classification problem, or the precise value of a
              future asset price, i.e. posed as a regression problem. Here, we
              show that information extracted from news sources is better at
              predicting the direction of underlying asset volatility movement,
              or its second order statistics, rather than its direction of
              price movement. We show empirical results by constructing machine
              learning models of Latent Dirichlet Allocation to represent
              information from news feeds, and simple na{\"\i}ve Bayes
              classifiers to predict the direction of movements. Empirical
              results show that the average directional prediction accuracy for
              volatility, on arrival of new information, is 56\%, while that of
              the asset close price is no better than random at 49\%. We
              evaluate these results using a range of stocks and stock indices
              in the US market, using a reliable news source as input. We
              conclude that volatility movements are more predictable than
              asset price movements when using financial news as machine
              learning input, and hence could potentially be exploited in
              pricing derivatives contracts via quantifying volatility.",
  journal  = "The Journal of Finance and Data Science",
  volume   =  4,
  number   =  2,
  pages    = "120--137",
  month    =  jun,
  year     =  2018,
  keywords = "Machine learning; Natural language processing; Volatility
              forecasting; Technical analysis; Computational finance"
}

@ARTICLE{Simonian2018-ge,
  title     = "{INVITED} {EDITORIAL} {COMMENT}: Order from Chaos: How Data
               Science Is Revolutionizing Investment Practice",
  author    = "Simonian, Joseph and de Prado, Marcos L{\'o}pez and Fabozzi,
               Frank J",
  abstract  = "1. Joseph Simonian 1. is the director of quantitative research
               at Natixis Investment Managers in Boston, MA.
               (joseph.simonian\{at\}natixis.com) 2. Marcos L{\'o}pez de Prado
               1. is a principal and the head of machine learning at AQR
               Capital Management LLC in Greenwich, CT, and a lecturer at",
  journal   = "The Journal of Portfolio Management",
  publisher = "Institutional Investor Journals Umbrella",
  volume    =  45,
  number    =  1,
  pages     = "1--4",
  month     =  oct,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Simonian2019-gk,
  title     = "Triumph of the Empiricists: The Birth of Financial Data Science",
  author    = "Simonian, Joseph and Fabozzi, Frank J",
  abstract  = "1. Joseph Simonian 1. is the director of quantitative research
               at Natixis Investment Managers in Boston, MA.
               (joseph.simonian\{at\}natixis.com) 2. Frank J. Fabozzi 1. is a
               professor of finance at EDHEC Business School in Nice, France,
               and the editor of The Journal of Portfolio Management .
               (frank.fabozzi\{at\}edhec.edu) 1. To order reprints of this
               article, please contact David Rowe at
               d.rowe\{at\}pageantmedia.com or 646-891-2157. The authors
               situate financial data science within the broader history of
               econometrics and argue that its ascendance marks a reorientation
               of the field toward a more empirical and pragmatic stance. They
               also argue that owing to the unique nature of financial
               information, financial data science should be considered a field
               in its own right and not just an application of data science
               methods to finance.",
  journal   = "The Journal of Financial Data Science",
  publisher = "Institutional Investor Journals Umbrella",
  volume    =  1,
  number    =  1,
  pages     = "10--13",
  month     =  jan,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Harvey2015-ci,
  title   = "Backtesting",
  author  = "Harvey, Campbell and Liu, Yan",
  journal = "Journal of Portfolio Management",
  year    =  2015
}

@ARTICLE{Van_Gelderen2019-ss,
  title     = "Factor Investing from Concept to Implementation",
  author    = "van Gelderen, Eduard and Huij, Joop and Kyosev, Georgi",
  abstract  = "1. Eduard van Gelderen 1. is the chief investment officer at PSP
               Investments in Montreal, QC, Canada.
               (evangelderen\{at\}investpsp.ca) 2. Joop Huij 1. is an associate
               professor at the Rotterdam School of Management in Rotterdam,
               the Netherlands, and the head of factor investing at Robeco in
               Rotterdam, the Netherlands. (jhuij\{at\}rsm.nl) 3. Georgi Kyosev
               1. is a PhD candidate at the Rotterdam School of Management in
               Rotterdam, the Netherlands, and a researcher at Robeco in
               Rotterdam, the Netherlands. (kyosev\{at\}rsm.nl) 1. To order
               reprints of this article, please contact David Rowe at
               d.rowe\{at\}pageantmedia.com or 646-891-2157. Mutual funds
               following factor investing strategies based on equity asset
               pricing anomalies, such as the small-cap, value, and momentum
               effects, earn significantly higher alphas than traditional
               actively managed mutual funds. The authors report that a
               buy-and-hold strategy for a random factor fund yields 110 basis
               points per annum in excess of the return earned by the average
               traditional actively managed mutual fund. However, they find
               that the actual returns that investors earn by investing in
               factor mutual funds are significantly lower because investors
               dynamically reallocate their funds both across factors and
               factor managers. Although factor funds have attracted
               significant fund flows over their sample period, it appears that
               fund flows have been driven by factor funds earning high past
               returns and not by the funds providing factor exposures. The
               authors argue that, rather than timing factors and factor
               managers, investors would be better off by using a buy-and-hold
               strategy and selecting a multifactor manager.",
  journal   = "The Journal of Portfolio Management",
  publisher = "Institutional Investor Journals Umbrella",
  volume    =  45,
  number    =  3,
  pages     = "125--140",
  month     =  feb,
  year      =  2019,
  language  = "en"
}

@MISC{Macilwain1997-xr,
  title   = "Faith, hope and tact needed for {US} ratification",
  author  = "Macilwain, Colin",
  journal = "Nature",
  volume  =  390,
  number  =  6661,
  pages   = "650--650",
  year    =  1997
}

@MISC{Cheng2019-jx,
  title   = "Energy efficiency, carbon dioxide emission efficiency, and related
             abatement costs in regional China: a synthesis of input--output
             analysis and {DEA}",
  author  = "Cheng, Yu and Lv, Kangjuan and Wang, Jian and Xu, Hao",
  journal = "Energy Efficiency",
  volume  =  12,
  number  =  4,
  pages   = "863--877",
  year    =  2019
}

@MISC{Ward2017-gt,
  title   = "Financial Management Effectiveness and Board Gender Diversity in
             {Member-Governed}, Community Financial Institutions",
  author  = "Ward, Anne Marie and Forker, John",
  journal = "Journal of Business Ethics",
  volume  =  141,
  number  =  2,
  pages   = "351--366",
  year    =  2017
}

@ARTICLE{Ozili2019-of,
  title     = "Non-performing loans in European systemic and non-systemic banks",
  author    = "Ozili, Peterson K",
  abstract  = "Purpose The distinction between systemic banks (GSIBs) and
               non-systemic banks (non-GSIBs) is driven by policy reasons. This
               study aims to examine the behaviour of non-performing loans in
               European GSIBs and non-GSIBs from 2004 to
               2013.Design/methodology/approach The author uses regression
               methodology to analyse the association between non-performing
               loans (NPLs) and the state of the economy.Findings The author
               finds that more profitable banks witness higher NPLs regardless
               of them being systemic or non-systemic. Secondly, GSIBs have
               fewer NPLs during economic booms and during periods of increased
               lending, while non-GSIBs experience higher NPLs during periods
               of increased lending. The author also observes that European
               non-GSIBs that exceed regulatory capital requirement also
               experience higher NPLs. In the post-crisis period, there is a
               significant and negative relationship between NPLs and the
               economic cycle for GSIBs in the post-financial crisis period and
               a significant and positive relationship between NPLs, loan
               supply and bank profitability for GSIBs in the post-financial
               crisis period; on the other hand, there is a significant and
               negative relationship between NPLs and regulatory capital ratios
               for non-GSIBs in the post-financial crisis period and a
               significant and positive relationship between NPLs and bank
               profitability for non-GSIBs in the post-financial crisis period.
               The findings have implications.Originality/value To the best of
               the author's knowledge, the literature on the determinants of
               NPL has not empirically examined the behaviour of NPLs in
               European GSIBs and non-GSIBs. This paper examines this issue to
               provide insights to help policymakers and academics understand
               the peculiarities of NPLs in Europe.",
  journal   = "Journal of Financial Economic Policy",
  publisher = "Emerald Publishing Limited",
  volume    =  12,
  number    =  3,
  pages     = "409--424",
  month     =  jan,
  year      =  2019,
  keywords  = "credit risk; nonperforming loans; systemic banks; systemic risk;
               impaired loans; asset quality; European banks; Europe; bank
               profitability."
}

@ARTICLE{Ciukaj2020-jl,
  title     = "Determinants of the non-performing loan ratio in the European
               Union banking sectors with a high level of impaired loans",
  author    = "Ciukaj, Rados{\l}aw and Kil, Krzysztof",
  abstract  = "AbstractIn the article an attempt is made to identify the
               quality of credit exposure determinants of banks in European
               Union countries that were characterized by a high level of
               impaired loans at the end of 2017 (Bulgaria, Croatia, Cyprus,
               Italy, Ireland, Greece, Portugal). Using the static panel-based
               approach the non-performing loan (NPL) determinants for the
               period from 2011 to 2017 were analyzed. The results showed that
               the high level of NPLs can be explained mainly by both
               macroeconomic and microeconomic factors. In particular, it has
               been shown that in the surveyed countries supervisory
               authorities should pay special attention to smaller banks with
               high dynamics of new loans and a low return on assets due to the
               fact that these entities are characterized by a higher NPL
               ratio. A higher level of NPL is also affected by a high
               concentration of the banking sector and higher interest rates on
               newly granted loans. As a result of research it was also shown
               that the majority of NPL determinants are the same in all types
               of banks, regardless of the business model and the scope of
               banking supervision. The differences were noticeable in
               characteristics regarding the housing market as well as the
               profitability of operations and lending dynamics of the analyzed
               entities.",
  journal   = "Economics and Business Review",
  publisher = "Sciendo",
  volume    =  6,
  number    =  1,
  pages     = "22--45",
  month     =  mar,
  year      =  2020,
  keywords  = "Business and Economics; Political Economics; Political
               Economics, other; Business Management; Business Management,
               other; Mathematics and Statistics for Economists; Mathematics;
               Statitistics",
  language  = "en"
}

@ARTICLE{Bell2019-qj,
  title    = "Fixed and Random effects models: making an informed choice",
  author   = "Bell, Andrew and Fairbrother, Malcolm and Jones, Kelvyn",
  abstract = "This paper assesses modelling choices available to researchers
              using multilevel (including longitudinal) data. We present key
              features, capabilities, and limitations of fixed (FE) and random
              (RE) effects models, including the within-between RE model,
              sometimes misleadingly labelled a `hybrid' model. We show the
              latter is unambiguously a RE model, and the most general of the
              three models, encompassing strengths of both FE and RE. As such,
              and because it allows for important extensions---notably random
              slopes---we argue it should be the starting point of all
              multilevel analyses. Simulations reveal the extent to which these
              models cope with mis-specification, showing (1) failing to
              include random slopes can lead to anti-conservative standard
              errors, and (2) mis-specifying non-Normal random effects as
              Normally distributed can introduce small biases to
              variance/random-effect estimates, but not fixed-part estimates.
              We conclude with advice for applied researchers, supporting good
              methodological decision-making in multilevel/longitudinal data
              analysis. See also earlier publication:
              https://www.researchgate.net/publication/233756428\_Explaining\_Fixed\_Effects\_Random\_Effects\_Modeling\_of\_Time-Series\_Cross-Sectional\_and\_Panel\_Data",
  volume   =  53,
  number   = "2)",
  pages    = "1051--1074",
  month    =  mar,
  year     =  2019
}

@ARTICLE{Wooldridge2019-nr,
  title    = "Correlated random effects models with unbalanced panels",
  author   = "Wooldridge, Jeffrey M",
  abstract = "I propose some strategies for allowing unobserved heterogeneity
              to be correlated withobserved covariates and sample selection for
              unbalanced panels. The methods are extensions of the
              Chamberlain--Mundlak approach for balanced panels when
              explanatory variables are strictly exogenous conditional on
              unobserved effects. A byproduct is fully robust Hausman tests for
              unbalanced panels. Even for nonlinear models, in many cases the
              estimators can be implemented using standard software. The
              framework suggests straightforward tests for sample selection
              that is correlated with unobserved shocks while allowing
              selection to be correlated with the observed covariates and
              unobserved heterogeneity.",
  journal  = "J. Econom.",
  volume   =  211,
  number   =  1,
  pages    = "137--150",
  month    =  jul,
  year     =  2019,
  keywords = "Correlated random effects; Panel data; Unbalanced panel; Hausman
              test"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Singel2011-cf,
  title     = "Google Catches Bing Copying; Microsoft Says 'So What?'",
  author    = "Singel, Ryan and Harrison, Sara and Thompson, Nicholas and
               Barber, Gregory and Martineau, Paris and Simonite, Tom and
               Finley, Klint",
  abstract  = "Google accused Microsoft Tuesday of copying its search results,
               an accusation to which Microsoft responded to with a blase, ``So
               What?'' Google's anti-webspam engineer Matt Cutts accused
               Microsoft on stage at a Bing-sponsored event of copying Google's
               results by watching what people search for using the Internet
               Explorer 8 toolbar and click on at Google.com,
               \textbackslash[‚Ä¶\textbackslash]",
  journal   = "Wired",
  publisher = "WIRED",
  month     =  feb,
  year      =  2011
}

@UNPUBLISHED{Van_Zundert2017-xz,
  title    = "{Volatility-Adjusted} Momentum",
  author   = "van Zundert, Jeroen",
  abstract = "Motivated by standard portfolio theory, this paper incorporates
              ex-ante volatility estimates in the construction of
              winner-minus-loser stock momentum portfolio. I find that over the
              1927-2015 period this leads to an increase in the Sharpe ratio
              from 0.34 to 1.14 and strongly reduced crash risk. This result is
              driven, in part, by the under weighting of high-volatility loser
              stocks, which tend to perform well, and cannot be attributed to
              small caps. In an out-of-sample test on USD-denominated corporate
              bonds, similar improvements are found.",
  month    =  dec,
  year     =  2017,
  keywords = "momentum, cross-section, volatility, stocks, corporate bonds"
}

@UNPUBLISHED{Hollo2012-hr,
  title    = "{CISS} - A Composite Indicator of Systemic Stress in the
              Financial System",
  author   = "Hollo, Daniel and Kremer, Manfred and Lo Duca, Marco",
  abstract = "This paper introduces a new indicator of contemporaneous stress
              in the financial system named Composite Indicator of Systemic
              Stress (CISS). Its specific statistical design is shaped
              according to standard definitions of systemic risk. The main
              methodological innovation of the CISS is the application of basic
              portfolio theory to the aggregation of five market-specific
              subindices created from a total of 15 individual financial stress
              measures. The aggregation accordingly takes into account the
              time-varying cross-correlations between the subindices. As a
              result, the CISS puts relatively more weight on situations in
              which stress prevails in several market segments at the same
              time, capturing the idea that financial stress is more systemic
              and thus more dangerous for the economy as a whole if financial
              instability spreads more widely across the whole financial
              system. Applied to euro area data, we determine within a
              threshold VAR model a systemic crisis-level of the CISS at which
              financial stress tends to depress real economic activity.",
  month    =  mar,
  year     =  2012,
  keywords = "Financial system, financial stability, systemic risk, financial
              stress index, macro-financial, linkages"
}

@ARTICLE{French2019-nk,
  title     = "The effectiveness of smartphone apps in improving financial
               capability",
  author    = "French, Declan and McKillop, Donal and Stewart, Elaine",
  abstract  = "This study is the first to assess whether smartphone apps can be
               utilised to improve financially capable behaviours. In this
               study four smartphone apps, packaged together under the title
               ?Money Matters?, were provided to working-age members (16?65
               years) of the largest credit union in Northern Ireland (Derry
               Credit Union). The smartphone apps consisted of a loan interest
               comparison app, an expenditure comparison app, a cash calendar
               app, and a debt management app. The assessment methodology used
               was a Randomised Control Trial (RCT) with the U.K. Financial
               Capability Outcome Frameworks used to set the context for the
               assessment. For those receiving the apps (the treatment group)
               statistically significant improvements were found in a number of
               measures designed to gauge ?financial knowledge, understanding
               and basic skills? and ?attitudes and motivations?. These
               improvements translated into better financially capable
               behaviours; those receiving the apps were more likely to keep
               track of their income and expenditure and proved to be more
               resilient when faced with a financial shock.",
  journal   = "The European Journal of Finance",
  publisher = "Routledge",
  pages     = "1--17",
  month     =  jul,
  year      =  2019
}

@UNPUBLISHED{Aziz2019-ug,
  title    = "Machine Learning in Finance: A Topic Modeling Approach",
  author   = "Aziz, Saqib and Dowling, Michael M and Hammami, Helmi and
              Piepenbrink, Anke",
  abstract = "We provide a first comprehensive structuring of the literature
              applying machine learning to finance. We use a probabilistic
              topic modeling approach to make sense of this diverse body of
              research spanning across the disciplines of finance, economics,
              computer sciences, and decision sciences. Through the topic
              modelling approach, a Latent Dirichlet Allocation technique, we
              are able to extract the 14 coherent research topics that are the
              focus of the 5,204 academic articles we analyze from the years
              1990 to 2018. We first describe and structure these topics, and
              then further show how the topic focus has evolved over the last
              two decades. Our study thus provides a structured topography for
              finance researchers seeking to integrate machine learning
              research approaches in their exploration of finance phenomena. We
              also showcase the benefits to finance researchers of the method
              of probabilistic modeling of topics for deep comprehension of a
              body of literature, especially when that literature has diverse
              multi-disciplinary actors.",
  month    =  feb,
  year     =  2019,
  keywords = "topic modeling, machine learning, structuring finance research,
              textual analysis, Latent Dirichlet Allocation, multi-disciplinary"
}

@MISC{Coccorese2019-dg,
  title   = "Are mergers among cooperative banks worth a dime? Evidence on
             efficiency effects of {M\&As} in Italy",
  author  = "Coccorese, Paolo and Ferri, Giovanni",
  journal = "Economic Modelling",
  year    =  2019
}

@ARTICLE{Brannen2005-ko,
  title     = "Mixing Methods: The Entry of Qualitative and Quantitative
               Approaches into the Research Process",
  author    = "Brannen, Julia",
  abstract  = "Qualitative and quantitative research are often presented as two
               fundamentally different paradigms through which we study the
               social world. These paradigms act as lightning conductors to
               which sets of epistemological assumptions, theoretical
               approaches and methods are attracted. Each is seen to be
               incompatible with the other. These paradigmatic claims have a
               tendency to resurface from time to time, manifesting themselves
               in the effects of different cultural traditions upon
               intellectual styles of research. There are pressures to view
               research in terms of this divide but perhaps more pressures to
               ignore such a divide.",
  journal   = "Int. J. Soc. Res. Methodol.",
  publisher = "Routledge",
  volume    =  8,
  number    =  3,
  pages     = "173--184",
  month     =  jul,
  year      =  2005
}

@ARTICLE{Brannen2005-ye,
  title     = "Individualisation, Choice and Structure: A Discussion of Current
               Trends in Sociological Analysis",
  author    = "Brannen, Julia and Nilsen, Ann",
  abstract  = "In this paper we seek to explore a tendency in current
               sociological thought to highlight notions of choice and autonomy
               in writings about contemporary Western societies. We wish to
               draw attention to some of the consequences of leaving out
               discussions of the structural aspects of societies and people's
               lives, for individuals as well as for the development and
               application of sociological theory and its ability to understand
               the connection between history and individual biography. Our
               discussion is based on qualitative research that we have
               conducted in recent years, and draws on focus groups with young
               people in Norway and Britain. From this critique we seek to
               demonstrate how concepts that take account of context and
               structure as well individual subjectivities can create a better
               ?fit? with complex and diverse realties.",
  journal   = "Sociol. Rev.",
  publisher = "SAGE Publications Ltd",
  volume    =  53,
  number    =  3,
  pages     = "412--428",
  month     =  aug,
  year      =  2005
}

@ARTICLE{Kozak2018-qx,
  title    = "Interpreting Factor Models",
  author   = "Kozak, Serhiy and Nagel, Stefan and Santosh, Shrihari",
  abstract = "ABSTRACT We argue that tests of reduced-form factor models and
              horse races between ?characteristics? and ?covariances? cannot
              discriminate between alternative models of investor beliefs.
              Since asset returns have substantial commonality, absence of
              near-arbitrage opportunities implies that the stochastic discount
              factor can be represented as a function of a few dominant sources
              of return variation. As long as some arbitrageurs are present,
              this conclusion applies even in an economy in which all
              cross-sectional variation in expected returns is caused by
              sentiment. Sentiment-investor demand results in substantial
              mispricing only if arbitrageurs are exposed to factor risk when
              taking the other side of these trades.",
  journal  = "J. Finance",
  volume   =  73,
  number   =  3,
  pages    = "1183--1223",
  month    =  jun,
  year     =  2018
}

@ARTICLE{Beaver2007-pu,
  title    = "Delisting returns and their effect on accounting-based market
              anomalies",
  author   = "Beaver, William and McNichols, Maureen and Price, Richard",
  abstract = "We show that tests of market efficiency are sensitive to the
              inclusion of delisting firm-years. When included, trading
              strategy returns based on anomaly variables can increase (for
              strategies based on earnings, cash flows and the book-to-market
              ratio) or decrease (for a strategy based on accruals). This is
              due to the disproportionate number of delisting firm-years in the
              lowest decile of these variables. Delisting firm-years are most
              often excluded because the researcher does not correctly
              incorporate delisting returns, because delisting return data are
              missing or because other research design choices implicitly
              exclude them.",
  journal  = "J. Account. Econ.",
  volume   =  43,
  number   =  2,
  pages    = "341--368",
  month    =  jul,
  year     =  2007,
  keywords = "Accounting; Anomalies; Delisting returns; Accruals"
}

@BOOK{Gelman2020-vc,
  title     = "Regression and Other Stories",
  author    = "Gelman, Andrew and Hill, Jennifer and Vehtari, Aki",
  abstract  = "Most textbooks on regression focus on theory and the simplest of
               examples. Real statistical problems, however, are complex and
               subtle. This is not a book about the theory of regression. It is
               about using regression to solve real problems of comparison,
               estimation, prediction, and causal inference. Unlike other
               books, it focuses on practical issues such as sample size and
               missing data and a wide range of goals and techniques. It jumps
               right in to methods and computer code you can use immediately.
               Real examples, real stories from the authors' experience
               demonstrate what regression can do and its limitations, with
               practical advice for understanding assumptions and implementing
               methods for experiments and observational studies. They make a
               smooth transition to logistic regression and GLM. The emphasis
               is on computation in R and Stan rather than derivations, with
               code available online. Graphics and presentation aid
               understanding of the models and model fitting.",
  publisher = "Cambridge University Press",
  month     =  jul,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Elliott2008-dk,
  title    = "Economic Forecasting",
  author   = "Elliott, Graham and Timmermann, Allan",
  abstract = "Economic Forecasting by Graham Elliott and Allan Timmermann.
              Published in volume 46, issue 1, pages 3-56 of Journal of
              Economic Literature, March 2008, Abstract: Forecasts guide
              decisions in all areas of economics and finance and their value
              can only be understood in relation to, and in the context...",
  journal  = "J. Econ. Lit.",
  volume   =  46,
  number   =  1,
  pages    = "3--56",
  month    =  mar,
  year     =  2008
}

@MISC{Fletcher2010-ew,
  title   = "Arbitrage and the Evaluation of Linear Factor Models in {UK} Stock
             Returns",
  author  = "Fletcher, Jonathan",
  journal = "Financial Review",
  volume  =  45,
  number  =  2,
  pages   = "449--468",
  year    =  2010
}

@ARTICLE{Fletcher2007-zd,
  title     = "Can Asset Pricing Models Price Idiosyncratic Risk in {U.K}.
               Stock Returns?",
  author    = "Fletcher, Jonathan",
  abstract  = "Abstract I examine how well different linear factor models and
               consumption-based asset pricing models price idiosyncratic risk
               in U.K. stock returns. Correctly pricing idiosyncratic risk is a
               significant challenge for many of the models I consider. For
               some consumption-based models, there is a clear tradeoff in the
               performance of the models between correctly pricing systematic
               risk and idiosyncratic risk. Linear factor models do a better
               job in most cases in pricing systematic risk than
               consumption-based models but the reverse is true for
               idiosyncratic risk.",
  journal   = "Financial Review",
  publisher = "Wiley Online Library",
  volume    =  42,
  number    =  4,
  pages     = "507--535",
  month     =  nov,
  year      =  2007
}

@ARTICLE{Gerlein2016-co,
  title     = "Evaluating machine learning classification for financial
               trading: An empirical approach",
  author    = "Gerlein, Eduardo A and McGinnity, Martin and Belatreche, Ammar
               and Coleman, Sonya",
  abstract  = "Technical and quantitative analysis in financial trading use
               mathematical and statistical tools to help investors decide on
               the optimum moment to initiate and close orders. While these
               traditional approaches have served their purpose to some extent,
               new techniques arising from the field of computational
               intelligence such as machine learning and data mining have
               emerged to analyse financial information. While the main
               financial engineering research has focused on complex
               computational models such as Neural Networks and Support Vector
               Machines, there are also simpler models that have demonstrated
               their usefulness in applications other than financial trading,
               and are worth considering to determine their advantages and
               inherent limitations when used as trading analysis tools. This
               paper analyses the role of simple machine learning models to
               achieve profitable trading through a series of trading
               simulations in the FOREX market. It assesses the performance of
               the models and how particular setups of the models produce
               systematic and consistent predictions for profitable trading.
               Due to the inherent complexities of financial time series the
               role of attribute selection, periodic retraining and training
               set size are discussed in order to obtain a combination of those
               parameters not only capable of generating positive cumulative
               returns for each one of the machine learning models but also to
               demonstrate how simple algorithms traditionally precluded from
               financial forecasting for trading applications presents similar
               performances as their more complex counterparts. The paper
               discusses how a combination of attributes in addition to
               technical indicators that has been used as inputs of the machine
               learning-based predictors such as price related features,
               seasonality features and lagged values used in classical time
               series analysis are used to enhance the classification
               capabilities that impacts directly into the final profitability.",
  journal   = "Expert Syst. Appl.",
  publisher = "Elsevier",
  volume    =  54,
  pages     = "193--207",
  month     =  jul,
  year      =  2016,
  keywords  = "Trading; Financial forecasting; Computer intelligence; Data
               mining; Machine learning; FOREX markets"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Subrahmanyam2010-bu,
  title     = "The cross-section of expected stock returns: what have we learnt
               from the past twenty-five years of research?",
  author    = "Subrahmanyam, Avanidhar",
  abstract  = "‚Ä¶ merits and demerits of these approaches need to be better
               understood from an empirical standpoint, as ‚Ä¶ it has not become
               the norm to use conditional betas in asset pricing regressions ‚Ä¶
               any control for market friction in their study documenting that
               growth in assets predicts returns ‚Ä¶",
  journal   = "European Financial Management",
  publisher = "Wiley Online Library",
  volume    =  16,
  number    =  1,
  pages     = "27--42",
  year      =  2010
}

@ARTICLE{Zhang2015-fp,
  title    = "Cross-validation for selecting a model selection procedure",
  author   = "Zhang, Yongli and Yang, Yuhong",
  abstract = "While there are various model selection methods, an unanswered
              but important question is how to select one of them for data at
              hand. The difficulty is due to that the targeted behaviors of the
              model selection procedures depend heavily on uncheckable or
              difficult-to-check assumptions on the data generating process.
              Fortunately, cross-validation (CV) provides a general tool to
              solve this problem. In this work, results are provided on how to
              apply CV to consistently choose the best method, yielding new
              insights and guidance for potentially vast amount of application.
              In addition, we address several seemingly widely spread
              misconceptions on CV.",
  journal  = "J. Econom.",
  volume   =  187,
  number   =  1,
  pages    = "95--112",
  month    =  jul,
  year     =  2015,
  keywords = "Cross-validation; Cross-validation paradox; Data splitting ratio;
              Adaptive procedure selection; Information criterion; LASSO; MCP;
              SCAD"
}

@ARTICLE{Freckleton2002-pj,
  title    = "On the misuse of residuals in ecology: regression of residuals
              vs. multiple regression",
  author   = "Freckleton, Robert P",
  abstract = "Summary 1 ?Residuals from linear regressions are used frequently
              in statistical analysis, often for the purpose of controlling for
              unwanted effects in multivariable datasets. This paper criticizes
              the practice, building upon recent critiques. 2 ?Regression of
              residuals is often used as an alternative to multiple regression,
              often with the aim of controlling for confounding variables. When
              correlations exist between independent variables, as is generally
              the case with ecological datasets, this procedure leads to biased
              parameter estimates. Standard multiple regression, by contrast,
              yields unbiased parameter estimates. 3 ?In multiple regression
              parameters are estimated controlling for the effects of the other
              variables in the model, and thus multiple regression achieves
              what residual regression claims to do. 4 ?Several measures of
              correlation exist that differ in the way that variance is
              partitioned among independent variables. These can be estimated
              multiply, or sequentially if reasons exist for estimating effects
              of variables in a hierarchical manner.",
  journal  = "J. Anim. Ecol.",
  volume   =  71,
  number   =  3,
  pages    = "542--545",
  month    =  may,
  year     =  2002
}

@ARTICLE{Raymer2007-qt,
  title    = "Spontaneous knotting of an agitated string",
  author   = "Raymer, Dorian M and Smith, Douglas E",
  abstract = "It is well known that a jostled string tends to become knotted;
              yet the factors governing the ``spontaneous'' formation of
              various knots are unclear. We performed experiments in which a
              string was tumbled inside a box and found that complex knots
              often form within seconds. We used mathematical knot theory to
              analyze the knots. Above a critical string length, the
              probability P of knotting at first increased sharply with length
              but then saturated below 100\%. This behavior differs from that
              of mathematical self-avoiding random walks, where P has been
              proven to approach 100\%. Finite agitation time and jamming of
              the string due to its stiffness result in lower probability, but
              P approaches 100\% with long, flexible strings. We analyzed the
              knots by calculating their Jones polynomials via computer
              analysis of digital photos of the string. Remarkably, almost all
              were identified as prime knots: 120 different types, having
              minimum crossing numbers up to 11, were observed in 3,415 trials.
              All prime knots with up to seven crossings were observed. The
              relative probability of forming a knot decreased exponentially
              with minimum crossing number and M{\"o}bius energy, mathematical
              measures of knot complexity. Based on the observation that long,
              stiff strings tend to form a coiled structure when confined, we
              propose a simple model to describe the knot formation based on
              random ``braid moves'' of the string end. Our model can
              qualitatively account for the observed distribution of knots and
              dependence on agitation time and string length.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  104,
  number   =  42,
  pages    = "16432--16437",
  month    =  oct,
  year     =  2007,
  language = "en"
}

@ARTICLE{Gelman2005-zz,
  title     = "Analysis of variance---why it is more important than ever",
  author    = "Gelman, Andrew",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  33,
  number    =  1,
  pages     = "1--53",
  month     =  feb,
  year      =  2005,
  keywords  = "ANOVA; Bayesian inference; fixed effects; hierarchical model;
               linear regression; multilevel model; random effects; variance
               components",
  language  = "en"
}

@ARTICLE{Angrist1995-mi,
  title     = "{Split-Sample} Instrumental Variables Estimates of the Return to
               Schooling",
  author    = "Angrist, Joshua D and Krueger, Alan B",
  abstract  = "[This article reevaluates recent instrumental variables (IV)
               estimates of the returns to schooling in light of the fact that
               two-stage least squares is biased in the same direction as
               ordinary least squares (OLS) even in very large samples. We
               propose a split-sample instrumental variables (SSIV) estimator
               that is not biased toward OLS. SSIV uses one-half of a sample to
               estimate parameters of the first-stage equation. Estimated
               first-stage parameters are then used to construct fitted values
               and second-stage parameter estimates in the other half sample.
               SSIV is biased toward 0, but this bias can be corrected. The
               splt-sample estimators confirm and reinforce some previous
               findings on the returns to schooling but fail to confirm
               others.]",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "[American Statistical Association, Taylor \& Francis, Ltd.]",
  volume    =  13,
  number    =  2,
  pages     = "225--235",
  year      =  1995
}

@ARTICLE{Burgess2010-ci,
  title     = "Bayesian methods for meta-analysis of causal relationships
               estimated using genetic instrumental variables",
  author    = "Burgess, Stephen and Thompson, Simon G and {CRP CHD Genetics
               Collaboration} and Burgess, S and Thompson, S G and Andrews, G
               and Samani, N J and Hall, A and Whincup, P and Morris, R and
               Lawlor, D A and Davey Smith, G and Timpson, N and Ebrahim, S and
               Ben-Shlomo, Y and Davey Smith, G and Timpson, N and Brown, M and
               Ricketts, S and Sandhu, M and Reiner, A and Psaty, B and Lange,
               L and Cushman, M and Hung, J and Thompson, P and Beilby, J and
               Warrington, N and Palmer, L J and Nordestgaard, B G and
               Tybjaerg-Hansen, A and Zacho, J and Wu, C and Lowe, G and
               Tzoulaki, I and Kumari, M and Sandhu, M and Yamamoto, J F and
               Chiodini, B and Franzosi, M and Hankey, G J and Jamrozik, K and
               Palmer, L and Rimm, E and Pai, J and Psaty, B and Heckbert, S
               and Bis, J and Anand, S and Engert, J and Collins, R and Clarke,
               R and Melander, O and Berglund, G and Ladenvall, P and
               Johansson, L and Jansson, J-H and Hallmans, G and Hingorani, A
               and Humphries, S and Rimm, E and Manson, J and Pai, J and
               Watkins, H and Clarke, R and Hopewell, J and Saleheen, D and
               Frossard, R and Danesh, J and Sattar, N and Robertson, M and
               Shepherd, J and Schaefer, E and Hofman, A and Witteman, J C M
               and Kardys, I and Ben-Shlomo, Y and Davey Smith, G and Timpson,
               N and de Faire, U and Bennet, A and Sattar, N and Ford, I and
               Packard, C and Kumari, M and Manson, J and Lawlor, Debbie A and
               Davey Smith, George and Anand, S and Collins, R and Casas, J P
               and Danesh, J and Davey Smith, G and Franzosi, M and Hingorani,
               A and Lawlor, D A and Manson, J and Nordestgaard, B G and
               Samani, N J and Sandhu, M and Smeeth, L and Wensley, F and
               Anand, S and Bowden, J and Burgess, S and Casas, J P and Di
               Angelantonio, E and Engert, J and Gao, P and Shah, T and Smeeth,
               L and Thompson, S G and Verzilli, C and Walker, M and Whittaker,
               J and Hingorani, A and Danesh, J",
  abstract  = "Genetic markers can be used as instrumental variables, in an
               analogous way to randomization in a clinical trial, to estimate
               the causal relationship between a phenotype and an outcome
               variable. Our purpose is to extend the existing methods for such
               Mendelian randomization studies to the context of multiple
               genetic markers measured in multiple studies, based on the
               analysis of individual participant data. First, for a single
               genetic marker in one study, we show that the usual ratio of
               coefficients approach can be reformulated as a regression with
               heterogeneous error in the explanatory variable. This can be
               implemented using a Bayesian approach, which is next extended to
               include multiple genetic markers. We then propose a hierarchical
               model for undertaking a meta-analysis of multiple studies, in
               which it is not necessary that the same genetic markers are
               measured in each study. This provides an overall estimate of the
               causal relationship between the phenotype and the outcome, and
               an assessment of its heterogeneity across studies. As an
               example, we estimate the causal relationship of blood
               concentrations of C-reactive protein on fibrinogen levels using
               data from 11 studies. These methods provide a flexible framework
               for efficient estimation of causal relationships derived from
               multiple studies. Issues discussed include weak instrument bias,
               analysis of binary outcome data such as disease risk, missing
               genetic data, and the use of haplotypes.",
  journal   = "Stat. Med.",
  publisher = "Wiley Online Library",
  volume    =  29,
  number    =  12,
  pages     = "1298--1311",
  month     =  may,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Kleibergen2003-va,
  title    = "Bayesian and classical approaches to instrumental variable
              regression",
  author   = "Kleibergen, Frank and Zivot, Eric",
  abstract = "We establish relationships between certain Bayesian and classical
              approaches to instrumental variable regression. We determine the
              form of priors that lead to posteriors for structural parameters
              that have similar properties as classical 2SLS and LIML and in
              doing so provide some new insight, especially in the context of
              weak instruments, to the small sample behavior of Bayesian and
              classical procedures. Using a reduced rank restriction on a
              multivariate linear model, we determine the priors that give rise
              to posteriors that are identical in functional form to the
              sampling densities of the classical 2SLS and LIML estimators.",
  journal  = "J. Econom.",
  volume   =  114,
  number   =  1,
  pages    = "29--72",
  month    =  may,
  year     =  2003,
  keywords = "Diffuse prior; Instrumental variables; Posterior distributions;
              Reduced rank; Weak instruments"
}

@ARTICLE{Welch1947-zj,
  title    = "The generalisation of student's problems when several different
              population variances are involved",
  author   = "Welch, B L",
  journal  = "Biometrika",
  volume   =  34,
  number   = "1-2",
  pages    = "28--35",
  year     =  1947,
  keywords = "POPULATION",
  language = "en"
}

@ARTICLE{Gabry2019-jm,
  title    = "Visualization in Bayesian workflow",
  author   = "Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt,
              Michael and Gelman, Andrew",
  abstract = "Abstract Bayesian data analysis is about more than just computing
              a posterior distribution, and Bayesian visualization is about
              more than trace plots of Markov chains. Practical Bayesian data
              analysis, like all data analysis, is an iterative process of
              model building, inference, model checking and evaluation, and
              model expansion. Visualization is helpful in each of these stages
              of the Bayesian workflow and it is indispensable when drawing
              inferences from the types of modern, high dimensional models that
              are used by applied researchers.",
  journal  = "J. R. Stat. Soc. A",
  volume   =  182,
  number   =  2,
  pages    = "389--402",
  month    =  feb,
  year     =  2019
}

@MISC{Cheng2019-wx,
  title   = "Energy efficiency, carbon dioxide emission efficiency, and related
             abatement costs in regional China: a synthesis of input--output
             analysis and {DEA}",
  author  = "Cheng, Yu and Lv, Kangjuan and Wang, Jian and Xu, Hao",
  journal = "Energy Efficiency",
  volume  =  12,
  number  =  4,
  pages   = "863--877",
  year    =  2019
}

@ARTICLE{Kuosmanen2018-rg,
  title    = "Shadow Prices and Marginal Abatement Costs: Convex Quantile
              Regression Approach",
  author   = "Kuosmanen, Timo and Zhou, Xun",
  abstract = "Shadow pricing environmental bads is critically important for
              efficient environmental policy and management. However, most
              empirical studies grossly overestimate the marginal abatement
              costs for three reasons. First, assuming downscaling of
              production as the only abatement option ignores abatement through
              increasing the input use. Second, estimating shadow prices on the
              frontier ignores the impact of inefficiency. Third, estimating
              the frontier by deterministic methods ignores the upward bias due
              to noise in empirical data. To address these problems, a novel
              data-driven approach is developed. Instead of projecting
              inefficient units to the frontier, we estimate the shadow prices
              locally based on the actual level of performance using convex
              quantile regression. Compared to the traditional approaches,
              convex quantile regression is more robust to noise, the choice of
              the direction vector, and heteroscedasticity. Application to the
              U.S. electric power plants provides empirical evidence and
              demonstrates the advantages of the proposed approach.",
  month    =  nov,
  year     =  2018
}

@ARTICLE{Halkos2014-hf,
  title     = "Measuring the effect of Kyoto protocol agreement on countries'
               environmental efficiency in {CO2} emissions: an application of
               conditional full frontiers",
  author    = "Halkos, George E and Tzeremes, Nickolaos G",
  abstract  = "This paper applies the probabilistic approach developed by
               Daraio and Simar (J Prod Anal 24:93--121, 2005, Advanced robust
               and nonparametric methods in efficiency analysis. Springer
               Science, New York, 2007a, J Prod Anal 28:13--32, 2007b) in order
               to develop conditional and unconditional data envelopment
               analysis (DEA) models for the measurement of countries'
               environmental efficiency levels for a sample of 110 countries in
               2007. In order to capture the effect of countries compliance
               with the Kyoto protocol agreement (KPA) policies, we condition
               first the years since a country has signed the KPA until 2007
               and secondly the obliged percentage level of countries' emission
               reductions. Particularly, various DEA models have been applied
               alongside with bootstrap techniques in order to determine the
               effect of KPA on countries' environmental efficiencies. The
               study illustrates how the recent developments in efficiency
               analysis and statistical inference can be applied when
               evaluating environmental performance issues. The results
               indicate a nonlinear relationship between countries' obliged
               percentage levels of emission reductions and their environmental
               efficiency levels. Finally, a similar nonlinear relationship is
               also recorded between the duration which a country has signed
               the KPA and its environmental efficiency levels.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  41,
  number    =  3,
  pages     = "367--382",
  month     =  jun,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Malikov2016-ga,
  title     = "A Cost System Approach to the Stochastic Directional Technology
               Distance Function with Undesirable Outputs: The Case of us Banks
               in 2001--2010",
  author    = "Malikov, Emir and Kumbhakar, Subal C and Tsionas, Mike G",
  abstract  = "This paper offers a methodology to address the endogeneity of
               inputs in the directional technology distance function
               (DTDF)-based formulation of banking technology which explicitly
               accommodates the presence of undesirable nonperforming
               loans---an inherent characteristic of the bank's production due
               to its exposure to credit risk. Specifically, we model
               nonperforming loans as an undesirable output in the bank's
               production process. Since the stochastic DTDF describing banking
               technology is likely to suffer from the endogeneity of inputs,
               we propose addressing this problem by considering a system
               consisting of the DTDF and the first-order conditions from the
               bank's cost minimization problem. The first-order conditions
               also allow us to identify the `cost-optimal' directional vector
               for the banking DTDF, thus eliminating the uncertainty
               associated with an ad hoc choice of the direction. We apply our
               cost system approach to the data on large US commercial banks
               for the 2001--2010 period, which we estimate via Bayesian Markov
               chain Monte Carlo methods subject to theoretical regularity
               conditions. We document dramatic distortions in banks'
               efficiency, productivity growth and scale elasticity estimates
               when the endogeneity of inputs is assumed away and/or the DTDF
               is fitted in an arbitrary direction. Copyright \copyright{} 2015
               John Wiley \& Sons, Ltd.",
  journal   = "J. Appl. Econ.",
  publisher = "Wiley Online Library",
  volume    =  31,
  number    =  7,
  pages     = "1407--1429",
  month     =  nov,
  year      =  2016,
  keywords  = "UndesirableOutputs;Methodology;quality of financial
               intermediation"
}

@ARTICLE{Anderson1952-pi,
  title     = "Asymptotic Theory of Certain ``Goodness of Fit'' Criteria Based
               on Stochastic Processes",
  author    = "Anderson, T W and Darling, D A",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Math. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  23,
  number    =  2,
  pages     = "193--212",
  month     =  jun,
  year      =  1952,
  keywords  = "0*",
  language  = "en"
}

@ARTICLE{Blyth1972-wp,
  title     = "On Simpson's Paradox and the {Sure-Thing} Principle",
  author    = "Blyth, Colin R",
  abstract  = "Abstract This paradox is the possibility of P(A|B) <P(A|B') even
               though P(A|B)$\geq$P(A| B') both under the additional condition
               C and under the complement C' of that condition. Details are
               given on why this can happen and how extreme the inequalities
               can be. An example shows that Savage's sure-thing principle (?If
               you would definitely prefer g to f, either knowing that the
               event C obtained, or knowing that C did not obtain, then you
               definitely prefer g to f.?) is not applicable to alternatives f
               and g that involve sequential operations.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  67,
  number    =  338,
  pages     = "364--366",
  month     =  jun,
  year      =  1972
}

@ARTICLE{Wagner1982-vx,
  title     = "Simpson's Paradox in Real Life",
  author    = "Wagner, Clifford H",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  36,
  number    =  1,
  pages     = "46--48",
  month     =  feb,
  year      =  1982
}

@ARTICLE{Simpson1951-km,
  title    = "The Interpretation of Interaction in Contingency Tables",
  author   = "Simpson, E H",
  abstract = "SUMMARY The definition of second order interaction in a (2 ? 2 ?
              2) table given by Bartlett is accepted, but it is shown by an
              example that the vanishing of this second order interaction does
              not necessarily justify the mechanical procedure of forming the
              three component 2 ? 2 tables and testing each of these for
              significance by standard methods.*",
  journal  = "J. R. Stat. Soc. Series B Stat. Methodol.",
  volume   =  13,
  number   =  2,
  pages    = "238--241",
  month    =  jul,
  year     =  1951
}

@ARTICLE{Minsky1986-wo,
  title     = "The evolution of financial institutions and the performance of
               the economy",
  author    = "Minsky, Hyman P",
  journal   = "J. Econ. Issues",
  publisher = "Taylor \& Francis",
  volume    =  20,
  number    =  2,
  pages     = "345--353",
  year      =  1986
}

@BOOK{Dempster2018-uz,
  title     = "{High-Performance} Computing in Finance: Problems, Methods, and
               Solutions",
  author    = "Dempster, M A H and Kanniainen, Juho and Keane, John and
               Vynckier, Erik",
  abstract  = "High-Performance Computing (HPC) delivers higher computational
               performance to solve problems in science, engineering and
               finance. There are various HPC resources available for different
               needs, ranging from cloud computing-- that can be used without
               much expertise and expense -- to more tailored hardware, such as
               Field-Programmable Gate Arrays (FPGAs) or D-Wave's quantum
               computer systems. High-Performance Computing in Finance is the
               first book that provides a state-of-the-art introduction to HPC
               for finance, capturing both academically and practically
               relevant problems.",
  publisher = "CRC Press",
  month     =  feb,
  year      =  2018,
  language  = "en"
}

@MISC{Fox2015-ge,
  title     = "Big Data, simulations and {HPC} convergence, iBig Data
               benchmarking'': 6th International Workshop",
  author    = "Fox, G and {Others}",
  publisher = "WBDB",
  year      =  2015
}

@ARTICLE{Levene1961-ja,
  title     = "Robust tests for equality of variances",
  author    = "Levene, Howard",
  journal   = "Contributions to probability and statistics. Essays in honor of
               Harold Hotelling",
  publisher = "Stanford University Press",
  pages     = "279--292",
  year      =  1961
}

@UNPUBLISHED{Bailey2016-cn,
  title    = "The Probability of Backtest Overfitting",
  author   = "Bailey, David H and Borwein, Jonathan and Lopez de Prado, Marcos
              and Zhu, Qiji Jim",
  abstract = "Many investment firms and portfolio managers rely on backtests
              (ie, simulations of performance based on historical market data)
              to select investment strategies and allocate capital. Standard
              statistical techniques designed to prevent regression
              overfitting, such as hold-out, tend to be unreliable and
              inaccurate in the context of investment backtests. We propose a
              general framework to assess the probability of backtest
              overfitting (PBO).We illustrate this framework with specific
              generic, model-free and nonparametric implementations in the
              context of investment simulations; we call these implementations
              combinatorially symmetric cross-validation (CSCV). We show that
              CSCV produces reasonable estimates of PBO for several useful
              examples.",
  journal  = "Journal of",
  month    =  sep,
  year     =  2016,
  keywords = "backtest, overfitting, investment strategy, Sharpe ratio
              optimization, performance degradation"
}

@ARTICLE{Balakrishnan2017-ym,
  title     = "Banks' financial reporting frequency and asset quality",
  author    = "Balakrishnan, Karthik and Ertan, Aytekin",
  journal   = "The Accounting Review",
  publisher = "American Accounting Association",
  volume    =  93,
  number    =  3,
  pages     = "1--24",
  year      =  2017
}

@ARTICLE{Rodano2018-oq,
  title     = "Lending Standards over the Credit Cycle",
  author    = "Rodano, Giacomo and Serrano-Velarde, Nicolas and Tarantino,
               Emanuele",
  abstract  = "Abstract. We analyze how firms' segmentation into credit classes
               affects the lending standards applied by banks to small and
               medium enterprises over the cycle.",
  journal   = "Rev. Financ. Stud.",
  publisher = "Narnia",
  volume    =  31,
  number    =  8,
  pages     = "2943--2982",
  month     =  aug,
  year      =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Laeven2013-br,
  title     = "Systemic banking crises database",
  author    = "Laeven, L and Valencia, F",
  abstract  = "The paper presents a comprehensive database on systemic banking
               crises during 1970-- 2011. It proposes a methodology to date
               banking crises based on policy indices, and examines the
               robustness of this approach. The paper also presents information
               on the costs ‚Ä¶",
  journal   = "IMF Economic Review",
  publisher = "Springer",
  year      =  2013
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Suhonen2017-xg,
  title     = "Quantifying backtest overfitting in alternative beta strategies",
  author    = "Suhonen, A and Lennkh, M and Perez, F",
  abstract  = "‚Ä¶ IIJ on Twitter; Visit IIJ on Facebook; YouTube. Article.
               Quantifying Backtest Overfitting in Alternative Beta Strategies.
               Antti Suhonen, Matthias Lennkh and Fabrice Perez. The Journal of
               Portfolio Management Winter 2017, 43 (2) 90-104; DOI:
               https://doi.org/10.3905/jpm.2017.43.2.090 ‚Ä¶",
  journal   = "The Journal of Portfolio",
  publisher = "jpm.iijournals.com",
  year      =  2017
}

@ARTICLE{Karray2013-qv,
  title     = "Bank size and efficiency in developing countries: intermediation
               approach versus value added approach and impact of
               non-traditional activities",
  author    = "Karray, Sameh Charfeddine and eddine Chichti, Jamel",
  journal   = "Asian Economic and Financial Review",
  publisher = "Asian Economic and Social Society",
  volume    =  3,
  number    =  5,
  pages     = "593",
  year      =  2013
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Meyer2003-kn,
  title     = "A test for linear versus convex regression function using
               shape‚Äêrestricted regression",
  author    = "Meyer, Mary C",
  abstract  = "Abstract. An unbiased test for the appropriateness of the simple
               linear regression model is presented. The null hypothesis is
               that the underlying regression fu",
  journal   = "Biometrika",
  publisher = "Narnia",
  volume    =  90,
  number    =  1,
  pages     = "223--232",
  month     =  mar,
  year      =  2003
}

@UNPUBLISHED{Kinateder2016-mb,
  title    = "Basel {II} Versus {III}: A Comparative Assessment of Minimum
              Capital Requirements for Internal Model Approaches",
  author   = "Kinateder, Harald",
  abstract = "In this paper, we provide a comparative assessment of the minimum
              capital requirement (MCR) in three prominent versions of the
              Basel regulatory framework: Basel II, the 2010 version of Basel
              III and the current 2013 version of Basel III. For this purpose,
              we use Cantelli's inequality to compute theoretical MCR violation
              levels for different unconditional distributional specifications,
              accounting for skewness and kurtosis. Cantelli's inequality
              allows us to perform a quantitative comparison of various Basel
              accords without exact knowledge of the future return process.
              Therefore, our results are not biased, due to the specific choice
              of the sample data and/or uncertainty about the underlying return
              process as well as the ``true'' risk model. We find that under
              weak distributional specifications (ie, normal tails), the MCR
              under the 2013 version of Basel III is only marginally higher
              than under Basel II. However, this difference increases
              (decreases) for risk models equipped with heavy-tailed (normal)
              innovations. In contrast to this, we document that under the 2010
              version of Basel III the MCR violation levels during a stress
              period are adequate, even when using a risk model with weak
              distributional specifications. However, we also show that the MCR
              under the 2010 version of Basel III is too conservative in calm
              periods.",
  month    =  jan,
  year     =  2016,
  keywords = "Basel III, Cantelli's inequality, expected shortfall (ES), market
              risk management, minimum capital requirement (MCR), stressed
              value-at-risk (sVaR)"
}

@ARTICLE{Lin2018-fg,
  title    = "Systemic risk, financial markets, and performance of financial
              institutions",
  author   = "Lin, Edward M H and Sun, Edward W and Yu, Min-Teh",
  abstract = "This paper studies the exposure and contribution of financial
              institutions to systemic risks in financial markets. We employ
              three popular indicators of a financial institution's exposure to
              systemic risks: the systemic risk index (SRISK) and marginal
              expected shortfall (MES) of Brownlees and Engle (Volatility,
              correlation and tails for systemic risk measurement, Social
              Science Research Network, Rochester, NY, 2012) and the
              conditional Value-at-Risk (CoVaR) of Adrian and Brunnermeier
              (2011). We use a primary database of Taiwan financial
              institutions for our empirical study. A panel contains data of
              stock market returns and balance sheets of 31 Taiwan financial
              institutions for 2005--2014. We focus on systemic risk analysis
              so as to understand the dynamics of volatility, interdependency,
              and risk during the recent financial crisis. We then report the
              time series dynamics and cross sectional rankings of these
              systemic risk measures. The main results indicate that although
              these three measures differ in their definition of the
              contributions to systemic risk, all are quite similar in
              identifying systemically important financial institutions
              (SIFIs). Moreover, we find empirical evidence that systemic risk
              contributions are closely related to certain institution
              characteristic factors. The results of the Granger causality
              tests prove that a systemic risk measure is a great alternative
              tool for monitoring early warning signals of distress in the real
              economy.",
  journal  = "Ann. Oper. Res.",
  volume   =  262,
  number   =  2,
  pages    = "579--603",
  month    =  mar,
  year     =  2018
}

@ARTICLE{Geraci2018-el,
  title     = "Measuring Interconnectedness between Financial Institutions with
               Bayesian {Time-Varying} Vector Autoregressions",
  author    = "Geraci, Marco Valerio and Gnabo, Jean-Yves",
  abstract  = "We propose a market-based framework that exploits time-varying
               parameter vector autoregressions to estimate the dynamic network
               of financial spillover effects. We apply it to financials in the
               Standard \& Poor's 500 index and estimate interconnectedness at
               the sectoral and institutional levels. At the sectoral level, we
               uncover two main events in terms of interconnectedness: the
               Long-Term Capital Management crisis and the 2008 financial
               crisis. After these crisis events, we find a gradual decrease in
               interconnectedness, not observable using the classical
               rolling-window approach. At the institutional level, our
               framework delivers more stable interconnectedness rankings than
               other comparable market-based measures.",
  journal   = "Journal of Financial and Quantitative Analysis",
  publisher = "Cambridge University Press",
  volume    =  53,
  number    =  3,
  pages     = "1371--1390",
  month     =  jun,
  year      =  2018
}

@ARTICLE{Chen2018-xt,
  title     = "On Exactitude in Financial Regulation: {Value-at-Risk}, Expected
               Shortfall, and Expectiles",
  author    = "Chen, James Ming",
  abstract  = "This article reviews two leading measures of financial risk and
               an emerging alternative. Embraced by the Basel accords,
               value-at-risk and expected shortfall are the leading measures of
               financial risk. Expectiles offset the weaknesses of
               value-at-risk (VaR) and expected shortfall. Indeed, expectiles
               are the only elicitable law-invariant coherent risk measures.
               After reviewing practical concerns involving backtesting and
               robustness, this article more closely examines regulatory
               applications of expectiles. Expectiles are most readily
               evaluated as a special class of quantiles. For ease of
               regulatory implementation, expectiles can be defined exclusively
               in terms of VaR, expected shortfall, and the thresholds at which
               those competing risk measures are enforced. Moreover, expectiles
               are in harmony with gain/loss ratios in financial risk
               management. Expectiles may address some of the flaws in VaR and
               expected shortfall---subject to the reservation that no risk
               measure can achieve exactitude in regulation.",
  journal   = "Risks",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  6,
  number    =  2,
  pages     = "61",
  month     =  jun,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Bernardi2015-sb,
  title     = "Bayesian Tail Risk Interdependence Using Quantile Regression",
  author    = "Bernardi, Mauro and Gayraud, Ghislaine and Petrella, Lea",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Bayesian Anal.",
  publisher = "International Society for Bayesian Analysis",
  volume    =  10,
  number    =  3,
  pages     = "553--603",
  month     =  sep,
  year      =  2015,
  keywords  = "Bayesian quantile regression; time-varying conditional quantile;
               risk measures, state space models",
  language  = "en"
}

@ARTICLE{Strumbelj2010-ph,
  title    = "Online bookmakers' odds as forecasts: The case of European soccer
              leagues",
  author   = "{\v S}trumbelj, E and {\v S}ikonja, M Robnik",
  abstract = "In this paper we examine the effectiveness of using bookmaker
              odds as forecasts by analyzing 10,699 matches from six major
              European soccer leagues and the corresponding odds from 10
              different online bookmakers. We show that the odds from some
              bookmakers are better forecasts than those of others, and provide
              empirical evidence that (a) the effectiveness of using bookmaker
              odds as forecasts has increased over time, and (b) bookmakers
              offer more effective forecasts for some soccer leagues for than
              others.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  3,
  pages    = "482--488",
  month    =  jul,
  year     =  2010,
  keywords = "Sports forecasting; Brier score; Statistical tests; Soccer;
              Betting"
}

@ARTICLE{Dixon1997-ly,
  title    = "Modelling Association Football Scores and Inefficiencies in the
              Football Betting Market",
  author   = "Dixon, Mark J and Coles, Stuart G",
  abstract = "A parametric model is developed and fitted to English league and
              cup football data from 1992 to 1995. The model is motivated by an
              aim to exploit potential inefficiencies in the association
              football betting market, and this is examined using bookmakers'
              odds from 1995 to 1996. The technique is based on a Poisson
              regression model but is complicated by the data structure and the
              dynamic nature of teams' performances. Maximum likelihood
              estimates are shown to be computationally obtainable, and the
              model is shown to have a positive return when used as the basis
              of a betting strategy.",
  journal  = "J. R. Stat. Soc. Ser. C Appl. Stat.",
  volume   =  46,
  number   =  2,
  pages    = "265--280",
  month    =  jan,
  year     =  1997
}

@ARTICLE{Dixon2004-ij,
  title    = "The value of statistical forecasts in the {UK} association
              football betting market",
  author   = "Dixon, Mark J and Pope, Peter F",
  abstract = "In this paper, we evaluate the economic significance of
              statistical forecasts of UK Association Football match outcomes
              in relation to betting market prices. We present a detailed
              comparison of odds set by different bookmakers in relation to
              forecast model predictions, and analyse the potential for
              arbitrage across firms. We also examine extreme odds biases. A
              detailed re-examination of match result odds and a new
              examination of correct score odds for the period 1993 to 1996
              suggest that the market is inefficient.",
  journal  = "Int. J. Forecast.",
  volume   =  20,
  number   =  4,
  pages    = "697--711",
  month    =  oct,
  year     =  2004,
  keywords = "Statistical forecast; Score odd; Firm"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Goddard2004-ik,
  title     = "Forecasting football results and the efficiency of fixed‚Äêodds
               betting",
  author    = "Goddard, J and Asimakopoulos, I",
  abstract  = "‚Ä¶ In the case of association football (soccer) most previous
               research has focused on model - ling ‚Ä¶ The next section reviews
               the literature on modelling and fore- casting football match
               results ‚Ä¶ at any reasonable significance level, suggesting that
               the ordered probit model provides a ‚Ä¶",
  journal   = "J. Forecast.",
  publisher = "Wiley Online Library",
  year      =  2004
}

@ARTICLE{Hvattum2010-dh,
  title    = "Using {ELO} ratings for match result prediction in association
              football",
  author   = "Hvattum, Lars Magnus and Arntzen, Halvard",
  abstract = "Sports betting markets are becoming increasingly competitive.
              These markets are of interest when testing new ideas for
              quantitative prediction models. This paper examines the value of
              assigning ratings to teams based on their past performance in
              order to predict match results in association football. The ELO
              rating system is used to derive covariates that are then used in
              ordered logit regression models. In order to make informed
              statements about the relative merit of the ELO-based predictions
              compared to those from a set of six benchmark prediction methods,
              both economic and statistical measures are used. The results of
              large-scale computational experiments are presented.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  3,
  pages    = "460--470",
  month    =  jul,
  year     =  2010,
  keywords = "Sports forecasting; Loss function; Evaluating forecasts; Rating;
              Ordered logit"
}

@ARTICLE{Strumbelj2014-gy,
  title    = "On determining probability forecasts from betting odds",
  author   = "{\v S}trumbelj, Erik",
  abstract = "We show that the probabilities determined from betting odds using
              Shin's model are more accurate forecasts than those determined
              using basic normalization or regression models. We also provide
              empirical evidence that some bookmakers are significantly
              different sources of probabilities in terms of forecasting
              accuracy, and that betting exchange odds are not always the best
              source, especially in smaller markets. The advantage of using
              Shin probabilities and the differences between bookmakers
              decrease with an increasing market size.",
  journal  = "Int. J. Forecast.",
  volume   =  30,
  number   =  4,
  pages    = "934--943",
  month    =  oct,
  year     =  2014,
  keywords = "Sports forecasting; Probability forecasting; Fixed-odds; Betting
              exchange; Shin's model; Betfair; Calibration"
}

@ARTICLE{Constantinou2013-ai,
  title   = "Profiting from arbitrage and odds biases of the European football
             gambling market",
  author  = "Constantinou, Anthony and Fenton, Norman",
  journal = "Journal of Gambling Business and Economics",
  year    =  2013
}

@ARTICLE{Constantinou2013-tl,
  title    = "Profiting from an inefficient association football gambling
              market: Prediction, risk and uncertainty using Bayesian networks",
  author   = "Constantinou, Anthony Costa and Fenton, Norman Elliott and Neil,
              Martin",
  abstract = "We present a Bayesian network (BN) model for forecasting
              Association Football match outcomes. Both objective and
              subjective information are considered for prediction, and we
              demonstrate how probabilities transform at each level of model
              component, whereby predictive distributions follow hierarchical
              levels of Bayesian inference. The model was used to generate
              forecasts for each match of the 2011/2012 English Premier League
              (EPL) season, and forecasts were published online prior to the
              start of each match. Profitability, risk and uncertainty are
              evaluated by considering various unit-based betting procedures
              against published market odds. Compared to a previously published
              successful BN model, the model presented in this paper is less
              complex and is able to generate even more profitable returns.",
  journal  = "Knowledge-Based Systems",
  volume   =  50,
  pages    = "60--86",
  month    =  sep,
  year     =  2013,
  keywords = "Bayesian networks; Expert systems; Football betting; Football
              forecasts; Subjective information"
}

@ARTICLE{Constantinou2019-fm,
  title    = "Dolores: a model that predicts football match outcomes from all
              over the world",
  author   = "Constantinou, Anthony C",
  abstract = "The paper describes Dolores, a model designed to predict football
              match outcomes in one country by observing football matches in
              multiple other countries. The model is a mixture of two methods:
              (a) dynamic ratings and (b) Hybrid Bayesian Networks. It was
              developed as part of the international special issue competition
              Machine Learning for Soccer. Unlike past academic literature
              which tends to focus on a single league or tournament, Dolores is
              trained with a single dataset that incorporates match outcomes,
              with missing data (as part of the challenge), from 52 football
              leagues from all over the world. The challenge involved using a
              single model to predict 206 future match outcomes from 26
              different leagues, played from March 31 to April 9 in 2017.
              Dolores ranked 2nd in the competition with a predictive error
              0.94\% higher than the top and 116.78\% lower than the bottom
              participants. The paper extends the assessment of the model in
              terms of profitability against published market odds. Given that
              the training dataset incorporates a number of challenges as part
              of the competition, the results suggest that the model
              generalised well over multiple leagues, divisions, and seasons.
              Furthermore, while detailed historical performance for each team
              helps to maximise predictive accuracy, Dolores provides empirical
              proof that a model can make a good prediction for a match outcome
              between teams x and y even when the prediction is derived from
              historical match data that neither x nor y participated in. While
              this agrees with past studies in football and other sports, this
              paper extends the empirical evidence to historical training data
              that does not just include match results from a single
              competition but contains results spanning different leagues and
              divisions from 35 different countries. This implies that we can
              still predict, for example, the outcome of English Premier League
              matches, based on training data from Japan, New Zealand, Mexico,
              South Africa, Russia, and other countries in addition to data
              from the English Premier league.",
  journal  = "Mach. Learn.",
  volume   =  108,
  number   =  1,
  pages    = "49--75",
  month    =  jan,
  year     =  2019
}

@ARTICLE{Bernardo2019-sz,
  title    = "Semi-strong inefficiency in the fixed odds betting market:
              Underestimating the positive impact of head coach replacement in
              the main European soccer leagues",
  author   = "Bernardo, Giovanni and Ruberti, Massimo and Verona, Roberto",
  abstract = "In this paper we analyse the efficiency of the sports betting
              market, seeking to ascertain whether the market is efficient in
              the case of fixed odds provided by bookmakers in the four major
              European soccer leagues under the semi-strong efficiency
              hypothesis. By examining the trends of odds in the event of a
              major change in expectations about team results, i.e. when the
              head coach of a team is replaced, we attempt to verify the
              argument that a profitable strategy for the bettor is likely to
              be possible. In this case, the market under consideration would
              be inefficient. Analysing the average effect of head coach
              replacement, we find a positive impact on team performance. Based
              on this information, we build a betting strategy to find out
              whether the bookmakers' odds absorb this change in expectations
              about the winning probability of involved teams. Comparing our
              strategy result with a distribution generated in a Monte Carlo
              experiment, we conclude that the betting market is inefficient in
              its semi-strong form.",
  journal  = "Q. Rev. Econ. Finance",
  volume   =  71,
  pages    = "239--246",
  month    =  feb,
  year     =  2019,
  keywords = "Sports betting market; Fixed-odds bets; Semi-strong efficiency
              hypothesis; Monte Carlo experiment"
}

@ARTICLE{Angelini2019-mw,
  title    = "Efficiency of online football betting markets",
  author   = "Angelini, Giovanni and De Angelis, Luca",
  abstract = "This paper evaluates the efficiency of online betting markets for
              European (association) football leagues. The existing literature
              shows mixed empirical evidence regarding the degree to which
              betting markets are efficient. We propose a forecast-based
              approach for formally testing the efficiency of online betting
              markets. By considering the odds proposed by 41 bookmakers on 11
              European major leagues over the last 11 years, we find evidence
              of differing degrees of efficiency among markets. We show that,
              if the best odds are selected across bookmakers, eight markets
              are efficient while three show inefficiencies that imply profit
              opportunities for bettors. In particular, our approach allows the
              estimation of the odds thresholds that could be used to set
              profitable betting strategies both ex post and ex ante.",
  journal  = "Int. J. Forecast.",
  volume   =  35,
  number   =  2,
  pages    = "712--721",
  month    =  apr,
  year     =  2019,
  keywords = "Market efficiency; Sports forecasting; Probability forecasting;
              Favourite--longshot bias; Betting markets"
}

@ARTICLE{Fry2017-fs,
  title     = "Bubbles, {Blind-Spots} and Brexit",
  author    = "Fry, John and Brint, Andrew",
  abstract  = "In this paper we develop a well-established financial model to
               investigate whether bubbles were present in opinion polls and
               betting markets prior to the UK's vote on EU membership on 23
               June 2016. The importance of our contribution is threefold.
               Firstly, our continuous-time model allows for irregularly spaced
               time series---a common feature of polling data. Secondly, we
               build on qualitative comparisons that are often made between
               market cycles and voting patterns. Thirdly, our approach is
               theoretically elegant. Thus, where bubbles are found we suggest
               a suitable adjustment. We find evidence of bubbles in polling
               data. This suggests they systematically over-estimate the
               proportion voting for remain. In contrast, bookmakers' odds
               appear to show none of this bubble-like over-confidence.
               However, implied probabilities from bookmakers' odds appear
               remarkably unresponsive to polling data that nonetheless
               indicates a close-fought vote.",
  journal   = "Risks",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  5,
  number    =  3,
  pages     = "37",
  month     =  jul,
  year      =  2017,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Feddersen2017-ez,
  title     = "Sentiment bias and asset prices: Evidence from sports betting
               markets and social media",
  author    = "Feddersen, A and Humphreys, B R and Soebbing, B P",
  abstract  = "Previous research using attendance‚Äêbased proxies for sentiment
               bias in sports betting markets confirmed the presence of
               investor sentiment in these markets. We use data from social
               media (Facebook ``Likes'') to proxy for sentiment bias and
               analyze variation in ‚Ä¶",
  journal   = "Econ. Inq.",
  publisher = "Wiley Online Library",
  year      =  2017
}

@ARTICLE{Gentzkow2019-ly,
  title    = "Text as Data",
  author   = "Gentzkow, Matthew and Kelly, Bryan and Taddy, Matt",
  abstract = "Text as Data by Matthew Gentzkow, Bryan Kelly and Matt Taddy.
              Published in volume 57, issue 3, pages 535-74 of Journal of
              Economic Literature, September 2019, Abstract: An ever-increasing
              share of human interaction, communication, and culture is
              recorded as digital text. We provide an introduction t...",
  journal  = "J. Econ. Lit.",
  volume   =  57,
  number   =  3,
  pages    = "535--574",
  month    =  sep,
  year     =  2019
}

@ARTICLE{Gregory2013-xu,
  title    = "Constructing and Testing Alternative Versions of the
              {Fama-French} and Carhart Models in the {UK}",
  author   = "Gregory, Alan and Tharyan, Rajesh and Christidis, Angela",
  abstract = "Abstract This paper constructs and tests alternative versions of
              the Fama?French and Carhart models for the UK market with the
              purpose of providing guidance for researchers interested in asset
              pricing and event studies. We conduct a comprehensive analysis of
              such models, forming risk factors using approaches advanced in
              the recent literature including value-weighted factor components
              and various decompositions of the risk factors. We also test
              whether such factor models can at least explain the returns of
              large firms. We find that versions of the four-factor model using
              decomposed and value-weighted factor components are able to
              explain the cross-section of returns in large firms or in
              portfolios without extreme momentum exposures. However, we do not
              find that risk factors are consistently and reliably priced.",
  journal  = "J. Bus. Finance Account.",
  volume   =  40,
  number   = "1-2",
  pages    = "172--214",
  month    =  jan,
  year     =  2013
}

@ARTICLE{Fischer2006-gy,
  title     = "Carbon Abatement Costs: Why the Wide Range of Estimates?",
  author    = "Fischer, Carolyn and Morgenstern, Richard D",
  abstract  = "[Estimates of marginal abatement costs for reducing carbon
               emissions derived from major economic-energy models vary widely.
               Controlling for policy regimes we use meta-analysis to examine
               the importance of structural modeling choices in explaining
               differences in estimates. The analysis indicates that particular
               assumptions about perfectly foresighted consumers and Armington
               trade elasticities generate lower estimates of marginal
               abatement costs. Other choices are associated with higher cost
               estimates, including perfectly mobile capital, inclusion of a
               backstop technology, and greater disaggregation among regions
               and sectors. Some features, such as greater technological
               detail, seem less significant. Understanding the importance of
               key modeling assumptions, as well as the way the models are used
               to estimate abatement costs, can help guide the development of
               consistent modeling practices for policy evaluation.]",
  journal   = "Energy J.",
  publisher = "International Association for Energy Economics",
  volume    =  27,
  number    =  2,
  pages     = "73--86",
  year      =  2006
}

@ARTICLE{Bohringer2003-zl,
  title     = "Economic and environmental impacts of the Kyoto Protocol",
  author    = "B{\"o}hringer, Christoph and Vogt, Carsten",
  journal   = "Canadian Journal of Economics/Revue canadienne d'{\'e}conomique",
  publisher = "Wiley Online Library",
  volume    =  36,
  number    =  2,
  pages     = "475--496",
  year      =  2003
}

@ARTICLE{Rubin2015-hn,
  title    = "The cost of {CO2} capture and storage",
  author   = "Rubin, Edward S and Davison, John E and Herzog, Howard J",
  abstract = "The objective of this paper is to assess the current costs of CO2
              capture and storage (CCS) for new fossil fuel power plants and to
              compare those results to the costs reported a decade ago in the
              IPCC Special Report on Carbon Dioxide Capture and Storage
              (SRCCS). Toward that end, we employed a similar methodology based
              on review and analysis of recent cost studies for the major CCS
              options identified in the SRCCS, namely, post-combustion CO2
              capture at supercritical pulverized coal (SCPC) and natural gas
              combined cycle (NGCC) power plants, plus pre-combustion capture
              at coal-based integrated gasification combined cycle (IGCC) power
              plants. We also report current costs for SCPC plants employing
              oxy-combustion for CO2 capture---an option that was still in the
              early stages of development at the time of the SRCCS. To compare
              current CCS cost estimates to those in the SRCCS, we adjust all
              costs to constant 2013 US dollars using cost indices for power
              plant capital costs, fuel costs and other O\&M costs. On this
              basis, we report changes in capital cost, levelized cost of
              electricity, and mitigation costs for each power plant system
              with and without CCS. We also discuss the outlook for future CCS
              costs.",
  journal  = "Int. J. Greenhouse Gas Control",
  volume   =  40,
  pages    = "378--400",
  month    =  sep,
  year     =  2015,
  keywords = "Carbon capture and storage; Carbon sequestration; Power plant
              costs; CCS costs; Economic analysis"
}

@ARTICLE{Rubin2007-kl,
  title    = "Cost and performance of fossil fuel power plants with {CO2}
              capture and storage",
  author   = "Rubin, Edward S and Chen, Chao and Rao, Anand B",
  abstract = "CO2 capture and storage (CCS) is receiving considerable attention
              as a potential greenhouse gas (GHG) mitigation option for fossil
              fuel power plants. Cost and performance estimates for CCS are
              critical factors in energy and policy analysis. CCS cost studies
              necessarily employ a host of technical and economic assumptions
              that can dramatically affect results. Thus, particular studies
              often are of limited value to analysts, researchers, and industry
              personnel seeking results for alternative cases. In this paper,
              we use a generalized modeling tool to estimate and compare the
              emissions, efficiency, resource requirements and current costs of
              fossil fuel power plants with CCS on a systematic basis. This
              plant-level analysis explores a broader range of key assumptions
              than found in recent studies we reviewed for three major plant
              types: pulverized coal (PC) plants, natural gas combined cycle
              (NGCC) plants, and integrated gasification combined cycle (IGCC)
              systems using coal. In particular, we examine the effects of
              recent increases in capital costs and natural gas prices, as well
              as effects of differential plant utilization rates, IGCC
              financing and operating assumptions, variations in plant size,
              and differences in fuel quality, including bituminous,
              sub-bituminous and lignite coals. Our results show higher power
              plant and CCS costs than prior studies as a consequence of recent
              escalations in capital and operating costs. The broader range of
              cases also reveals differences not previously reported in the
              relative costs of PC, NGCC and IGCC plants with and without CCS.
              While CCS can significantly reduce power plant emissions of CO2
              (typically by 85--90\%), the impacts of CCS energy requirements
              on plant-level resource requirements and multi-media
              environmental emissions also are found to be significant, with
              increases of approximately 15--30\% for current CCS systems. To
              characterize such impacts, an alternative definition of the
              ``energy penalty'' is proposed in lieu of the prevailing use of
              this term.",
  journal  = "Energy Policy",
  volume   =  35,
  number   =  9,
  pages    = "4444--4454",
  month    =  sep,
  year     =  2007,
  keywords = "CO capture and storage cost; Comparative power plant economics;
              Carbon sequestration energy requirements"
}

@ARTICLE{Weyant1999-au,
  title   = "Kyoto special issue, Introduction and Overview",
  author  = "Weyant, J P and Hill, J N",
  journal = "Energy J.",
  year    =  1999
}

@ARTICLE{Zhang1998-go,
  title     = "Economic modelling approaches to cost estimates for the control
               of carbon dioxide emissions",
  author    = "Zhang, Zhongxiang and Folmer, Henk",
  journal   = "Energy Econ.",
  publisher = "Elsevier",
  volume    =  20,
  number    =  1,
  pages     = "101--120",
  year      =  1998
}

@ARTICLE{Viguier2003-sz,
  title    = "The costs of the Kyoto Protocol in the European Union",
  author   = "Viguier, Laurent L and Babiker, Mustafa H and Reilly, John M",
  abstract = "We estimate reference CO2 emission projections in the European
              Union, and quantify the economic impacts of the Kyoto commitment
              on Member States. We consider the case where each EU member
              individually meets a CO2 emissions target, applying a
              country-wide cap and trade system to meet the target but without
              trade among countries. We use a version of the MIT Emissions
              Prediction and Policy Analysis (EPPA) model, here disaggregated
              to separately include 9 European Community countries and
              commercial and household transportation sectors. We compare our
              results with that of four energy-economic models that have
              provided detailed analyses of European climate change policy. In
              the absence of specific additional climate policy measures, the
              EPPA reference projections of carbon emissions increase by 14\%
              from 1990 levels. The EU-wide target under the Kyoto Protocol to
              the Framework Convention on Climate Change is a reduction in
              emissions to 8\% below 1990 levels. EPPA emissions projections
              are similar to other recent modeling results, but there are
              underlying differences in energy and carbon intensities among the
              projections. If EU countries were to individually meet the EU
              allocation of the Community-wide carbon cap specified in the
              Kyoto Protocol, we find using EPPA that carbon prices vary from
              $91 in the United Kingdom to $385 in Denmark; welfare costs range
              from 0.6\% to 5\%.",
  journal  = "Energy Policy",
  volume   =  31,
  number   =  5,
  pages    = "459--481",
  month    =  apr,
  year     =  2003,
  keywords = "Kyoto Protocol; European Union; Computable general equilibrium
              model"
}

@ARTICLE{Fang2018-lz,
  title     = "A stable systemic risk ranking in China's banking sector: Based
               on principal component analysis",
  author    = "Fang, Libing and Xiao, Binqing and Yu, Honghai and You, Qixing",
  journal   = "Physica A: Statistical Mechanics and its Applications",
  publisher = "Elsevier",
  volume    =  492,
  pages     = "1997--2009",
  year      =  2018
}

@ARTICLE{Berg2017-gz,
  title     = "An analysis of the consistency of banks' internal ratings",
  author    = "Berg, Tobias and Koziol, Philipp",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  78,
  pages     = "27--41",
  year      =  2017
}

@ARTICLE{Kostakis2012-ma,
  title     = "Higher co-moments and asset pricing on London Stock Exchange",
  author    = "Kostakis, Alexandros and Muhammad, Kashif and Siganos, Antonios",
  abstract  = "This study examines the asset pricing implications of
               preferences over the higher moments of returns' distributions.
               We show that in a market populated by risk-averse, prudent and
               temperate investors, firms whose returns exhibit negative
               coskewness or positive cokurtosis should yield higher premia
               relative to counterpart firms with positive coskewness and
               negative cokurtosis respectively. These theoretical predictions
               are empirically tested using a comprehensive dataset of shares
               listed on the London Stock Exchange during the period
               1986--2008. Our empirical results confirm that coskewness and
               cokurtosis premia are genuinely priced in the UK market, over
               and above what covariance risk, size, value and momentum factors
               can explain. We also show that a theoretically motivated, higher
               co-moment asset pricing model has significant explanatory
               ability over the cross-section of coskewness and cokurtosis
               portfolio returns.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  36,
  number    =  3,
  pages     = "913--922",
  month     =  mar,
  year      =  2012,
  keywords  = "Asset pricing; Coskewness; Cokurtosis; London Stock Exchange"
}

@ARTICLE{Fletcher2005-ng,
  title     = "An examination of alternative {CAPM-based} models in {UK} stock
               returns",
  author    = "Fletcher, Jonathan and Kihanda, Joseph",
  abstract  = "We evaluate the performance of unconditional and conditional
               versions of seven stochastic discount factor models in UK stock
               returns between January 1975 and December 2001. We find that the
               conditional four-moment capital asset pricing model (CAPM) has
               the best performance among the models we consider in terms of
               the lowest [Hansen, L.P., Jagannathan, R., 1997. Assessing
               specification errors in stochastic discount factor models.
               Journal of Finance 52, 591--607] distance measure and explaining
               the time-series predictability of industry portfolio excess
               returns. Conditional models also do a better job than
               unconditional models. However we find that the superior
               performance of the conditional four-moment CAPM, and conditional
               models in general, arises in part due to overfitting the data.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  29,
  number    =  12,
  pages     = "2995--3014",
  month     =  dec,
  year      =  2005,
  keywords  = "Conditional models; Four-moment CAPM"
}

@TECHREPORT{Anginer2019-ph,
  title       = "Bank Regulation and Supervision Ten Years after the Global
                 Financial Crisis",
  author      = "Anginer, Deniz and Bertay, Ata Can and Cull, Robert J and
                 Demirguc-Kunt, Asli and Mare, Davide Salvatore",
  abstract    = "This paper summarizes the latest update of the World Bank Bank
                 Regulation and Supervision Survey. The paper explores and
                 summarizes the evolution in bank capital regulations,
                 capitalization of banks, market discipline, and supervisory
                 power since the global financial crisis. It shows that
                 regulatory capital increased, but some elements of capital
                 regulations became laxer. Market discipline may have
                 deteriorated as the financial safety nets became more generous
                 after the crisis. Bank supervision became stricter and more
                 complex compared with the pre--global financial crisis period.
                 However, supervisory capacity did not increase in proportion
                 to the extent and complexity of new bank regulations. The
                 paper documents the importance of defining bank regulatory
                 capital narrowly, as the quality of capital matters in
                 reducing bank risk. This is particularly true for large banks,
                 because they have more discretion in the computation of risk
                 weights and are better able to issue a variety of capital
                 instruments.",
  number      = "WPS9044",
  pages       = "1--63",
  institution = "The World Bank",
  month       =  oct,
  year        =  2019,
  language    = "en"
}

@BOOK{Cousin2011-kg,
  title     = "Banking in China: Second Edition",
  author    = "Cousin, V",
  abstract  = "Chinese banks have been making headlines recently, but what lies
               beneath? Banking in China appears different. What explains the
               current arrangement? What can we expect from such a banking
               industry in the future? This book answers these two questions in
               a fully revised second edition and contributes to a new
               understanding of Chinese banks.",
  publisher = "Springer",
  month     =  may,
  year      =  2011,
  language  = "en"
}

@ARTICLE{Francis2012-yx,
  title    = "Capital requirements and bank behavior in the {UK}: Are there
              lessons for international capital standards?",
  author   = "Francis, William B and Osborne, Matthew",
  abstract = "The financial crisis prompted widespread interest in developing a
              better understanding of how capital regulation drives bank
              behavior. This paper uses a unique, comprehensive database of
              regulatory capital requirements on all UK banks to examine their
              effects on capital, lending and balance sheet management
              behavior. We find that capital requirements that include
              firm-specific, time-varying add-ons set by supervisors affect
              banks' desired capital ratios and that resulting adjustments to
              capital and lending depend on the gap between actual and target
              ratios. We use these results to measure the effects of a capital
              regime that includes features similar to those embedded in the UK
              framework. Our results suggest that countercyclical capital
              requirements may be less effective in slowing credit activity
              when banks can readily satisfy them with lower-quality
              (lower-costing) capital elements versus higher-quality common
              equity. Given the size of the UK banking sector and the global
              nature of many of the largest institutions in the UK banking
              sector, the results have implications for the ongoing debate
              surrounding the design and calibration of international capital
              standards.",
  journal  = "Journal of Banking \& Finance",
  volume   =  36,
  number   =  3,
  pages    = "803--816",
  month    =  mar,
  year     =  2012,
  keywords = "Bank capital channel; Regulatory capital requirements; Bank
              capital ratios; Bank credit supply; Countercyclical capital
              policy; Macroprudential tools"
}

@ARTICLE{Gagliardini2016-vd,
  title    = "{Time-Varying} Risk Premium in Large {Cross-Sectional} Equity
              Data Sets",
  author   = "Gagliardini, Patrick and Ossola, Elisa and Scaillet, Olivier",
  abstract = "We develop an econometric methodology to infer the path of risk
              premia from a large unbalanced panel of individual stock returns.
              We estimate the time-varying risk premia implied by conditional
              linear asset pricing models where the conditioning includes both
              instruments common to all assets and asset-specific instruments.
              The estimator uses simple weighted two-pass cross-sectional
              regressions, and we show its consistency and asymptotic normality
              under increasing cross-sectional and time series dimensions. We
              address consistent estimation of the asymptotic variance by hard
              thresholding, and testing for asset pricing restrictions induced
              by the no-arbitrage assumption. We derive the restrictions given
              by a continuum of assets in a multi-period economy under an
              approximate factor structure robust to asset repackaging. The
              empirical analysis on returns for about ten thousand U.S. stocks
              from July 1964 to December 2009 shows that risk premia are large
              and volatile in crisis periods. They exhibit large positive and
              negative strays from time-invariant estimates, follow the
              macroeconomic cycles, and do not match risk premia estimates on
              standard sets of portfolios. The asset pricing restrictions are
              rejected for a conditional four-factor model capturing market,
              size, value, and momentum effects.",
  journal  = "Econometrica",
  volume   =  84,
  number   =  3,
  pages    = "985--1046",
  year     =  2016
}

@INCOLLECTION{Desmettre2015-us,
  title     = "10 Computational Challenges in Finance",
  booktitle = "{FPGA} Based Accelerators for Financial Applications",
  author    = "Desmettre, Sascha and Korn, Ralf",
  editor    = "De Schryver, Christian",
  abstract  = "With the growing use of both highly developed mathematical
               models and complicated derivative products at financial markets,
               the demand for high computational power and its efficient use
               via fast algorithms and sophisticated hard- and software
               concepts became a hot topic in mathematics and computer science.
               The combination of the necessity to use numerical methods such
               as Monte Carlo simulation, of the demand for a high accuracy of
               the resulting prices and risk measures, of online availability
               of prices, and the need for repeatedly performing those
               calculations for different input parameters as a kind of
               sensitivity analysis emphasizes this even more. In this survey,
               we describe the mathematical background of some of the most
               challenging computational tasks in financial mathematics. Among
               the examples are the pricing of exotic options by Monte Carlo
               methods, the calibration problem to obtain the input parameters
               for financial market models, and various risk management and
               measurement tasks.",
  publisher = "Springer International Publishing",
  pages     = "1--31",
  year      =  2015,
  address   = "Cham"
}

@ARTICLE{Camacho2018-jd,
  title    = "Markov-switching dynamic factor models in real time",
  author   = "Camacho, Maximo and Perez-Quiros, Gabriel and Poncela, Pilar",
  abstract = "We extend the Markov-switching dynamic factor model to account
              for some of the specificities of the day-to-day monitoring of
              economic developments from macroeconomic indicators, such as
              mixed sampling frequencies and ragged-edge data. First, we
              evaluate the theoretical gains of using data that are available
              promptly for computing probabilities of recession in real time.
              Second, we show how to estimate the model that deals with
              unbalanced panels of data and mixed frequencies, and examine the
              benefits of this extension through several Monte Carlo
              simulations. Finally, we assess its empirical reliability for the
              computation of real-time inferences of the US business cycle, and
              compare it with the alternative method of forecasting the
              probabilities of recession from balanced panels.",
  journal  = "Int. J. Forecast.",
  volume   =  34,
  number   =  4,
  pages    = "598--611",
  month    =  oct,
  year     =  2018,
  keywords = "Business cycles; Output growth; Time series"
}

@ARTICLE{Li2019-iq,
  title    = "State-dependent size and value premium: evidence from a
              regime-switching asset pricing model",
  author   = "Li, Bingxin and Piqueira, Natalia",
  abstract = "The conditional CAPM suggests that the market beta and market
              risk premium should vary over time. In this paper, we provide
              evidence that the size and value premiums are also state
              dependent. We develop a joint Markov regime-switching model for
              the CAPM and the VIX index to study the regime variation of CAPM
              parameters and to investigate the regime-dependent size and value
              anomalies. Stock returns from a two-state regime-switching model
              exhibit the pattern of amplified size (or value) premium in the
              low VIX state and reversed premium in the high VIX state. A
              three-state regime-switching model further confirms that the
              value premiums might be driven by extreme market conditions.
              These findings have important implications for market timing and
              portfolio selection decisions.",
  journal  = "Journal of Asset Management",
  volume   =  20,
  number   =  3,
  pages    = "229--249",
  month    =  may,
  year     =  2019
}

@ARTICLE{Sharpe1963-iz,
  title     = "A Simplified Model for Portfolio Analysis",
  author    = "Sharpe, William F",
  abstract  = "This paper describes the advantages of using a particular model
               of the relationships among securities for practical applications
               of the Markowitz portfolio analysis technique. A computer
               program has been developed to take full advantage of the model:
               2,000 securities can be analyzed at an extremely low cost?as
               little as 2\% of that associated with standard quadratic
               programming codes. Moreover, preliminary evidence suggests that
               the relatively few parameters used by the model can lead to very
               nearly the same results obtained with much larger sets of
               relationships among securities. The possibility of low-cost
               analysis, coupled with a likelihood that a relatively small
               amount of information need be sacrificed make the model an
               attractive candidate for initial practical applications of the
               Markowitz technique.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  9,
  number    =  2,
  pages     = "277--293",
  month     =  jan,
  year      =  1963
}

@BOOK{Hyndman2018-ix,
  title     = "Forecasting: principles and practice",
  author    = "Hyndman, Rob J and Athanasopoulos, George",
  abstract  = "Forecasting is required in many situations. Stocking an
               inventory may require forecasts of demand months in advance.
               Telecommunication routing requires traffic forecasts a few
               minutes ahead. Whatever the circumstances or time horizons
               involved, forecasting is an important aid in effective and
               efficient planning.This textbook provides a comprehensive
               introduction to forecasting methods and presents enough
               information about each method for readers to use them sensibly.",
  publisher = "OTexts",
  month     =  may,
  year      =  2018,
  language  = "en"
}

@BOOK{Fabozzi2014-vz,
  title     = "The Basics of Financial Econometrics: Tools, Concepts, and Asset
               Management Applications",
  author    = "Fabozzi, Frank J and Focardi, Sergio M and Rachev, Svetlozar T
               and Arshanapalli, Bala G",
  abstract  = "An accessible guide to the growing field of financial
               econometrics As finance and financial products have become more
               complex, financial econometrics has emerged as a fast-growing
               field and necessary foundation for anyone involved in
               quantitative finance. The techniques of financial econometrics
               facilitate the development and management of new financial
               instruments by providing models for pricing and risk assessment.
               In short, financial econometrics is an indispensable component
               to modern finance. The Basics of Financial Econometrics covers
               the commonly used techniques in the field without using
               unnecessary mathematical/statistical analysis. It focuses on
               foundational ideas and how they are applied. Topics covered
               include: regression models, factor analysis, volatility
               estimations, and time series techniques. Covers the basics of
               financial econometrics---an important topic in quantitative
               finance Contains several chapters on topics typically not
               covered even in basic books on econometrics such as model
               selection, model risk, and mitigating model risk Geared towards
               both practitioners and finance students who need to understand
               this dynamic discipline, but may not have advanced mathematical
               training, this book is a valuable resource on a topic of growing
               importance.",
  publisher = "John Wiley \& Sons",
  month     =  mar,
  year      =  2014,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kim1999-yy,
  title     = "Has the {US} economy become more stable? A Bayesian approach
               based on a Markov-switching model of the business cycle",
  author    = "Kim, Chang-Jin and Nelson, Charles R",
  abstract  = "‚Ä¶ duration of a regime before a structural break occurs---is
               given by E() 1/(1 q00) ‚Ä¶ assume that data YÀúT have arisen from
               model I defined in section II, according to a probability
               function (marginal likelihood) m(YÀúT0 j). Within the Bayesian
               framework, the Bayes factor has ‚Ä¶",
  journal   = "Rev. Econ. Stat.",
  publisher = "MIT Press",
  volume    =  81,
  number    =  4,
  pages     = "608--616",
  year      =  1999
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Kim2010-an,
  title     = "A Bayesian regime-switching time-series model",
  author    = "Kim, Jaehee and Cheon, Sooyoung",
  abstract  = "This article provides a new Bayesian approach for AR (2)
               time‚Äêseries models with multiple regime ‚Äê switching points. Our
               formulation of the regime ‚Äê switching model involves a binary
               discrete variable that indicates the regime change. This
               variable is specified to be detected ‚Ä¶",
  journal   = "J. Time Ser. Anal.",
  publisher = "Wiley Online Library",
  volume    =  31,
  number    =  5,
  pages     = "365--378",
  year      =  2010
}

@ARTICLE{Kim1998-sb,
  title     = "Business Cycle Turning Points, A New Coincident Index, and Tests
               of Duration Dependence Based on a Dynamic Factor Model With
               Regime Switching",
  author    = "Kim, Chang-Jin and Nelson, Charles R",
  abstract  = "The synthesis of the dynamic factor model of Stock and Watson
               (1989) and the regime-switching model of Hamilton (1989)
               proposed by Diebold and Rudebusch (1996) potentially encompasses
               both features of the business cycle identified by Burns and
               Mitchell (1946): (1) comovement among economic variables through
               the cycle and (2) nonlinearity in its evolution. However,
               maximum-likelihood estimation has required approximation. Recent
               advances in multimove Gibbs sampling methodology open the way to
               approximation-free inference in such non-Gaussian, nonlinear
               models. This paper estimates the model for U.S. data and
               attempts to address three questions: Are both features of the
               business cycle empirically relevant? Might the implied new index
               of coincident indicators be a useful one in practice? Do the
               resulting estimates of regime switches show evidence of duration
               dependence? The answers to all three would appear to be yes.",
  journal   = "Rev. Econ. Stat.",
  publisher = "MIT Press",
  volume    =  80,
  number    =  2,
  pages     = "188--201",
  month     =  may,
  year      =  1998
}

@ARTICLE{Zhang2020-lq,
  title    = "Financial markets under the global pandemic of {COVID-19}",
  author   = "Zhang, Dayong and Hu, Min and Ji, Qiang",
  abstract = "The rapid spread of coronavirus (COVID-19) has dramatic impacts
              on financial markets all over the world. It has created an
              unprecedented level of risk, causing investors to suffer
              significant loses in a very short period of time. This paper aims
              to map the general patterns of country-specific risks and
              systemic risks in the global financial markets. It also analyses
              the potential consequence of policy interventions, such as the
              US' decision to implement a zero-percent interest rate and
              unlimited quantitative easing (QE), and to what extent these
              policies may introduce further uncertainties into global
              financial markets.",
  journal  = "Finance Research Letters",
  pages    = "101528",
  month    =  apr,
  year     =  2020,
  keywords = "Coronavirus; Financial markets; Pandemic; Quantitative easing;
              Systemic risk"
}

@ARTICLE{Willems2017-bx,
  title     = "In-store proximity marketing: experimenting with digital
               point-of-sales communication",
  author    = "Willems, Kim",
  abstract  = "Purpose The authors present an exploratory study on the
               effectiveness of in-store marketing communication appeals via
               digital signage applying the construal level theory (CLT) in a
               field experiment. According to this theory, the authors
               hypothesize that shoppers will on the one hand respond more
               favorably to messages focusing on the desirability of the
               offering, when they are further distanced from the actual
               purchase decision. On the other hand, the authors expect more
               favorable responses toward messages containing feasibility
               appeals, positioned closer by to the purchase decision. The
               purpose of this paper is to determine appropriate location-based
               content for in-store proximity
               marketing.Design/methodology/approach A field experiment was
               conducted in a Belgian coffee bar, examining temporal distance
               effects in a natural retail/service environment. A 2$\times$2
               between-subjects experimental design is implemented (i.e. low vs
               high temporal distance$\times$concrete/cost vs
               abstract/brand-oriented appeal), examining the impact on
               marketing communication effectiveness.Findings Overall, the
               authors find some initial support for CLT on effectiveness
               measures regarding purchase intentions and actual purchase, but
               not in terms of self-reported noticing of the screen and the ad,
               nor in terms of (un)aided ad recall.Research
               limitations/implications This experiment is a pilot study and
               such finds itself confronted with a limited number of
               observations.Originality/value The study is among the first to
               examine how message content (beyond price promotion) can be
               adapted to in-store locations.",
  journal   = "International Journal of Retail \& Distribution Management",
  publisher = "Emerald Publishing Limited",
  volume    =  45,
  number    = "7/8",
  pages     = "910--927",
  month     =  jan,
  year      =  2017
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Levesque2017-cr,
  title     = "Proximity marketing as an enabler of mass customization and
               personalization in a customer service experience",
  booktitle = "Managing Complexity",
  author    = "Levesque, Nataly and Boeck, Harold",
  abstract  = "‚Ä¶ go back to the beginning of the 1920 to review the first
               research about positioning ‚Ä¶ Proximity Marketing implies that
               firms must dispatch their advertising contents to targeted
               geographic locations where ‚Ä¶ is a totally new field on which, to
               our knowledge, no academic reflection of ‚Ä¶",
  publisher = "Springer",
  pages     = "405--420",
  year      =  2017
}

@ARTICLE{Shugan2004-ie,
  title     = "The Impact of Advancing Technology on Marketing and Academic
               Research",
  author    = "Shugan, Steven M",
  abstract  = "Academic research in marketing often and rightfully tends to
               either build on well-established past research topics or follow
               well-established practices in industry. However, as technology
               advances, it might be possible to foresee some more enduring
               trends and focus research on future issues rather than on past
               issues. One approach would be to study emerging technologies
               with rapidly declining costs. Each of these emerging
               technologies spawns myriad applications that have the potential
               to dramatically impact existing markets. Interesting research
               topics include the study of the impact of these applications on
               different market participants (e.g., final consumers, the
               seller, the seller of complementary services, intermediaries,
               information providers, competitors, other industries). Research
               topics also include the optimal structure for products and
               services, given these new applications, as well as which
               intermediary should offer particular services. Research topics
               also include the interactive ability to rapidly customize
               marketing strategy by identifying individuals at particular
               points in time and under particular demand conditions. Five of
               these technologies include enhanced search services, biometrics
               and smart cards, enhanced computational speed, M-commerce, and
               GPS tracking.",
  journal   = "Marketing Science",
  publisher = "INFORMS",
  volume    =  23,
  number    =  4,
  pages     = "469--475",
  month     =  nov,
  year      =  2004
}

@ARTICLE{Webster2013-ip,
  title     = "Elevating marketing: marketing is dead! Long live marketing!",
  author    = "Webster, Frederick E and Lusch, Robert F",
  abstract  = "Marketing must be elevated to a higher level of consciousness. A
               consciousness that grows beyond solving small, immediate
               problems to addressing long-term, large problems that goes
               beyond individual customer satisfaction and short-term financial
               performance to encompass the total value creation system. The
               discipline, in theory and practice, must move beyond a narrow
               focus on customers to a broader concern for them as
               citizen-consumers. This necessitates a recommitment of marketing
               to its fundamental purpose in society, which is improving the
               standard of living for all citizens by co-creating value at all
               levels within a socio-economic system. An elevated (systems)
               concept of marketing must focus on micro, meso and macro systems
               with an understanding of the purpose and shared vision for each
               system, a clear identification of responsibilities, and a focus
               on resource effectiveness and efficiency.",
  journal   = "Journal of the Academy of Marketing Science",
  publisher = "Springer",
  volume    =  41,
  number    =  4,
  pages     = "389--399",
  month     =  jul,
  year      =  2013
}

@ARTICLE{Zhongzhi_Lawrence_He2010-sb,
  title     = "Dynamic Factors and Asset Pricing",
  author    = "{Zhongzhi (Lawrence) He} and Huh, Sahn-Wook and Lee, Bong-Soo",
  abstract  = "[This study develops an econometric model that incorporates
               features of price dynamics across assets as well as through
               time. With the dynamic factors extracted via the Kalman filter,
               we formulate an asset pricing model, termed the dynamic factor
               pricing model (DFPM). We then conduct asset pricing tests in the
               in-sample and out-of-sample contexts. Our analyses show that the
               ex ante factors are a key component in asset pricing and
               forecasting. By using the ex ante factors, the DFPM improves
               upon the explanatory and predictive power of other competing
               models, including unconditional and conditional versions of the
               Fama and French (1993) 3-factor model. In particular, the DFPM
               can explain and better forecast the momentum portfolio returns,
               which are mostly missed by alternative models.]",
  journal   = "The Journal of Financial and Quantitative Analysis",
  publisher = "Cambridge University Press",
  volume    =  45,
  number    =  3,
  pages     = "707--737",
  year      =  2010
}

@ARTICLE{Ang2012-gq,
  title     = "Testing conditional factor models",
  author    = "Ang, Andrew and Kristensen, Dennis",
  abstract  = "Using nonparametric techniques, we develop a methodology for
               estimating and testing conditional alphas and betas and long-run
               alphas and betas, which are the averages of conditional alphas
               and betas, respectively, across time. The estimators and tests
               can be implemented for a single asset or jointly across
               portfolios. The traditional Gibbons, Ross, and Shanken (1989)
               test arises as a special case of no time variation in the alphas
               and factor loadings and homoskedasticity. As applications of the
               methodology, we estimate conditional CAPM and multifactor models
               on book-to-market and momentum decile portfolios. We reject the
               null that long-run alphas are equal to zero even though there is
               substantial variation in the conditional factor loadings of
               these portfolios.",
  journal   = "J. financ. econ.",
  publisher = "Elsevier",
  volume    =  106,
  number    =  1,
  pages     = "132--156",
  month     =  oct,
  year      =  2012,
  keywords  = "Nonparametric estimator; Time-varying beta; Conditional alpha;
               Book-to-market premium; Value and momentum"
}

@ARTICLE{Malkiel2005-nc,
  title     = "Market Efficiency versus Behavioral Finance",
  author    = "Malkiel, Burton and Mullainathan, Sendhil and Stangle, Bruce",
  abstract  = "Two prominent economists?one the author of A Random Walk Down
               Wall Street and the other a leading scholar in behavioral
               finance?debate the current validity of the efficient markets
               hypothesis (EMH). For over 30 years, the idea that capital
               markets are efficient and that stock prices reflect all publicly
               available information dominated academic thinking. But the
               bubble of the late 1990s and recent advances in behavioral
               finance have forced a re-thinking. Behavioralists argue that
               markets are at least ?weakly? predictable. They also point to
               evidence that small investors ?typically day
               traders?consistently lose money as a result of ?loss
               aversion,??overconfidence,? and other behaviors that are not
               part of the focus of EMH (though, as Malkiel notes, there is
               room for irrational investors in an EMH world provided there are
               enough rational investors to counteract and correct them).
               Proponents of EMH, of course, argue that neither individual
               investors nor active fund managers reliably outperform markets,
               so there is no point paying for active management. Yet these
               seemingly disparate views lead to important areas of common
               ground. In particular, both camps agree that individual
               investors should stick to broad-based, low-cost index funds. And
               retirement accounts?either 401Ks or social security
               accounts?should have limited investment selections in order to
               minimize the possibility that behavioral problems lead to
               investor mismanagement.",
  journal   = "J Appl Corporate Finance",
  publisher = "Wiley Online Library",
  volume    =  17,
  number    =  3,
  pages     = "124--136",
  month     =  jun,
  year      =  2005
}

@ARTICLE{Lim2011-rc,
  title     = "{THE} {EVOLUTION} {OF} {STOCK} {MARKET} {EFFICIENCY} {OVER}
               {TIME}: A {SURVEY} {OF} {THE} {EMPIRICAL} {LITERATURE}",
  author    = "Lim, Kian-Ping and Brooks, Robert",
  abstract  = "Abstract This paper provides a systematic review of the
               weak-form market efficiency literature that examines return
               predictability from past price changes, with an exclusive focus
               on the stock markets. Our survey shows that the bulk of the
               empirical studies examine whether the stock market under study
               is or is not weak-form efficient in the absolute sense, assuming
               that the level of market efficiency remains unchanged throughout
               the estimation period. However, the possibility of time-varying
               weak-form market efficiency has received increasing attention in
               recent years. We categorize these emerging studies based on the
               research framework adopted, namely non-overlapping sub-period
               analysis, time-varying parameter model and rolling estimation
               window. An encouraging development is that the documented
               empirical evidence of evolving stock return predictability can
               be rationalized within the framework of the adaptive markets
               hypothesis.",
  journal   = "J. Econ. Surv.",
  publisher = "Wiley Online Library",
  volume    =  25,
  number    =  1,
  pages     = "69--108",
  month     =  feb,
  year      =  2011
}

@ARTICLE{Kim2011-bs,
  title     = "Stock return predictability and the adaptive markets hypothesis:
               Evidence from century-long {U.S}. data",
  author    = "Kim, Jae H and Shamsuddin, Abul and Lim, Kian-Ping",
  abstract  = "This paper provides strong evidence of time-varying return
               predictability of the Dow Jones Industrial Average index from
               1900 to 2009. Return predictability is found to be driven by
               changing market conditions, consistent with the implication of
               the adaptive markets hypothesis. During market crashes, no
               statistically significant return predictability is observed, but
               return predictability is associated with a high degree of
               uncertainty. In times of economic or political crises, stock
               returns have been highly predictable with a moderate degree of
               uncertainty in predictability. We find that return
               predictability has been smaller during economic bubbles than in
               normal times. We also find evidence that return predictability
               is associated with stock market volatility and economic
               fundamentals.",
  journal   = "Journal of Empirical Finance",
  publisher = "Elsevier",
  volume    =  18,
  number    =  5,
  pages     = "868--879",
  month     =  dec,
  year      =  2011,
  keywords  = "Economic bubbles; Economic crises; Adaptive markets hypothesis;
               Market efficiency; U.S. stock market"
}

@ARTICLE{Taylor2014-pm,
  title     = "The rise and fall of technical trading rule success",
  author    = "Taylor, Nick",
  abstract  = "The purpose of this paper is to examine the performance of an
               important set of momentum-based technical trading rules (TTRs)
               applied to all members of the Dow Jones Industrial Average
               (DJIA) stock index over the period 1928--2012. Using a set of
               econometric models that permit time-variation in risk-adjusted
               returns to TTR portfolios, the results reveal that profits
               evolve slowly over time, are confined to particular episodes
               primarily from the mid-1960s to mid-1980s, and rely on the
               ability of investors to short-sell stocks. These findings are
               demonstrated to be consistent with theoretical models that
               predict a relationship between TTR performance and market
               conditions.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  40,
  pages     = "286--302",
  month     =  mar,
  year      =  2014,
  keywords  = "Technical trading rules; Short-selling; Market conditions"
}

@ARTICLE{Mai2019-de,
  title    = "Deep learning models for bankruptcy prediction using textual
              disclosures",
  author   = "Mai, Feng and Tian, Shaonan and Lee, Chihoon and Ma, Ling",
  abstract = "This study introduces deep learning models for corporate
              bankruptcy forecasting using textual disclosures. Although
              textual data are common, it is rarely considered in the financial
              decision support models. Deep learning uses layers of neural
              networks to extract features from textual data for prediction. We
              construct a comprehensive bankruptcy database of 11,827 U.S.
              public companies and show that deep learning models yield
              superior prediction performance in forecasting bankruptcy using
              textual disclosures. When textual data are used in conjunction
              with traditional accounting-based ratio and market-based
              variables, deep learning models can further improve the
              prediction accuracy. We also investigate the effectiveness of two
              deep learning architectures. Interestingly, our empirical results
              show that simpler models such as averaging embedding are more
              effective than convolutional neural networks. Our results provide
              the first large-sample evidence for the predictive power of
              textual disclosures.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  274,
  number   =  2,
  pages    = "743--758",
  month    =  apr,
  year     =  2019,
  keywords = "Decision support systems; Deep learning; Bankruptcy prediction;
              Machine learning; Textual data"
}

@ARTICLE{Huang2019-jg,
  title    = "Analysing Systemic Risk in the Chinese Banking System",
  author   = "Huang, Qiubin and De Haan, Jakob and Scholtens, Bert",
  abstract = "Abstract We examine systemic risk in the Chinese banking system
              by estimating the conditional value at risk (CoVaR), the marginal
              expected shortfall (MES), the systemic impact index (SII) and the
              vulnerability index (VI) for 16 listed banks in China for the
              2007?2014 period. We find that these measures show different
              patterns, capturing different aspects of systemic risk of Chinese
              banks. However, rankings of banks based on these measures are
              significantly correlated. The time-series results for the CoVaR
              and MES measures suggest that systemic risk in the Chinese
              banking system decreased after the global financial crisis but
              started rising in 2014.",
  journal  = "Pac Econ Rev",
  volume   =  24,
  number   =  2,
  pages    = "348--372",
  month    =  may,
  year     =  2019
}

@ARTICLE{Friedman2001-tf,
  title     = "Greedy function approximation: A gradient boosting machine",
  author    = "Friedman, Jerome H",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  29,
  number    =  5,
  pages     = "1189--1232",
  month     =  oct,
  year      =  2001,
  keywords  = "Function estimation; boosting; decision trees; robust
               nonparametric regression"
}

@BOOK{Hastie2009-uq,
  title     = "The Elements of Statistical Learning: Data Mining, Inference,
               and Prediction, Second Edition",
  author    = "Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome",
  abstract  = "During the past decade there has been an explosion in
               computation and information technology. With it have come vast
               amounts of data in a variety of fields such as medicine,
               biology, finance, and marketing. The challenge of understanding
               these data has led to the development of new tools in the field
               of statistics, and spawned new areas such as data mining,
               machine learning, and bioinformatics. Many of these tools have
               common underpinnings but are often expressed with different
               terminology. This book describes the important ideas in these
               areas in a common conceptual framework. While the approach is
               statistical, the emphasis is on concepts rather than
               mathematics. Many examples are given, with a liberal use of
               color graphics. It is a valuable resource for statisticians and
               anyone interested in data mining in science or industry. The
               book's coverage is broad, from supervised learning (prediction)
               to unsupervised learning. The many topics include neural
               networks, support vector machines, classification trees and
               boosting---the first comprehensive treatment of this topic in
               any book. This major new edition features many topics not
               covered in the original, including graphical models, random
               forests, ensemble methods, least angle regression \& path
               algorithms for the lasso, non-negative matrix factorization, and
               spectral clustering. There is also a chapter on methods for
               ``wide'' data (p bigger than n), including multiple testing and
               false discovery rates. Trevor Hastie, Robert Tibshirani, and
               Jerome Friedman are professors of statistics at Stanford
               University. They are prominent researchers in this area: Hastie
               and Tibshirani developed generalized additive models and wrote a
               popular book of that title. Hastie co-developed much of the
               statistical modeling software and environment in R/S-PLUS and
               invented principal curves and surfaces. Tibshirani proposed the
               lasso and is co-author of the very successful An Introduction to
               the Bootstrap. Friedman is the co-inventor of many data-mining
               tools including CART, MARS, projection pursuit and gradient
               boosting.",
  publisher = "Springer Science \& Business Media",
  month     =  aug,
  year      =  2009,
  language  = "en"
}

@BOOK{Murphy2012-xc,
  title     = "Machine Learning: A Probabilistic Perspective",
  author    = "Murphy, Kevin P",
  abstract  = "A comprehensive introduction to machine learning that uses
               probabilistic models and inference as a unifying
               approach.Today's Web-enabled deluge of electronic data calls for
               automated methods of data analysis. Machine learning provides
               these, developing methods that can automatically detect patterns
               in data and then use the uncovered patterns to predict future
               data. This textbook offers a comprehensive and self-contained
               introduction to the field of machine learning, based on a
               unified, probabilistic approach.The coverage combines breadth
               and depth, offering necessary background material on such topics
               as probability, optimization, and linear algebra as well as
               discussion of recent developments in the field, including
               conditional random fields, L1 regularization, and deep learning.
               The book is written in an informal, accessible style, complete
               with pseudo-code for the most important algorithms. All topics
               are copiously illustrated with color images and worked examples
               drawn from such application domains as biology, text processing,
               computer vision, and robotics. Rather than providing a cookbook
               of different heuristic methods, the book stresses a principled
               model-based approach, often using the language of graphical
               models to specify models in a concise and intuitive way. Almost
               all the models described have been implemented in a MATLAB
               software package---PMTK (probabilistic modeling toolkit)---that
               is freely available online. The book is suitable for upper-level
               undergraduates with an introductory-level college math
               background and beginning graduate students.",
  publisher = "MIT Press",
  month     =  sep,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Friedman2000-as,
  title     = "Additive logistic regression: a statistical view of boosting
               (With discussion and a rejoinder by the authors)",
  author    = "Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  28,
  number    =  2,
  pages     = "337--407",
  month     =  apr,
  year      =  2000,
  keywords  = "classification; tree; nonparametric estimation; stagewise
               fitting; machine learning",
  language  = "en"
}

@MISC{Mayr2018-ym,
  title   = "Boosting for statistical modelling-A non-technical introduction",
  author  = "Mayr, Andreas and Hofner, Benjamin",
  journal = "Statistical Modelling",
  volume  =  18,
  number  = "3-4",
  pages   = "365--384",
  year    =  2018
}

@MISC{Johnson2008-pa,
  title     = "A study of the {NIPS} feature selection challenge",
  author    = "Johnson, Nicholas",
  publisher = "Submitted",
  year      =  2008
}

@ARTICLE{Annaert2013-hl,
  title    = "Are extreme returns priced in the stock market? European evidence",
  author   = "Annaert, Jan and De Ceuster, Marc and Verstegen, Kurt",
  abstract = "This paper revisits some recently found evidence in the
              literature on the cross-section of stock returns for a carefully
              constructed dataset of euro area stocks. First, we confirm recent
              results for US data and find evidence of a negative
              cross-sectional relation between extreme positive returns and
              average returns after controlling for characteristics such as
              momentum, book-to-market, size, liquidity and short term return
              reversal. We argue that this is the case because these stocks
              have lottery-like characteristics, which is attractive to certain
              investors. Also, these stocks tend to be very volatile so that
              arbitrageurs are discouraged from correcting potential
              mispricing. As a consequence, these stocks are often overpriced
              and hence face lower expected returns. Second, when we control
              for extreme returns, the recently found negative relationship
              between idiosyncratic risk and future returns is less robust. In
              our models, after adding maximum returns, the relationship is
              insignificant and sometimes even positive. We also find that
              idiosyncratic skewness and coskewness play an important role for
              asset pricing, as predicted by several theoretical models.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  9,
  pages    = "3401--3411",
  month    =  sep,
  year     =  2013,
  keywords = "Extreme returns; Cross-section of expected returns; Lottery-like
              payoffs; Skewness; Idiosyncratic volatility puzzle"
}

@ARTICLE{Wu2013-nq,
  title    = "Corporate social responsibility in the banking industry: Motives
              and financial performance",
  author   = "Wu, Meng-Wen and Shen, Chung-Hua",
  abstract = "The current study investigates the association between corporate
              social responsibility (CSR) and financial performance (FP), and
              discusses the driving motives of banks to engage in CSR. Three
              motives, namely, strategic choices, altruism, and greenwashing,
              suggest that the relationship between CSR and FP is positive,
              non-negative, and non-existent, respectively. We obtained our
              sample, which covered 2003--2009, from the Ethical Investment
              Research Service (EIRIS) databank and Bankscope database. The
              data consists of 162 banks in 22 countries. We then classified
              the banks into four types based on their degree of engagement in
              CSR. This study proposes the use of an extended version of the
              Heckman two-step regression, in which the first step adopts a
              multinomial logit model, and the second step estimates the
              performance equation with the inverse Mills ratio generated by
              the first step. The empirical results show that CSR positively
              associates with FP in terms of return on assets, return on
              equity, net interest income, and non-interest income. In
              contrast, CSR negatively associates with non-performing loans.
              Hence, strategic choice is the primary motive of banks to engage
              in CSR.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  9,
  pages    = "3529--3547",
  month    =  sep,
  year     =  2013,
  keywords = "Corporate social responsibility; Strategic motive; Altruistic
              motive; Extended Heckman two-stage model"
}

@ARTICLE{Nyberg2013-br,
  title    = "Predicting bear and bull stock markets with dynamic binary time
              series models",
  author   = "Nyberg, Henri",
  abstract = "Despite the voluminous empirical research on the potential
              predictability of stock returns, much less attention has been
              paid to the predictability of bear and bull stock markets. In
              this study, the aim is to predict U.S. bear and bull stock
              markets with dynamic binary time series models. Based on the
              analysis of the monthly U.S. data set, bear and bull markets are
              predictable in and out of sample. In particular, substantial
              additional predictive power can be obtained by allowing for a
              dynamic structure in the binary response model. Probability
              forecasts of the state of the stock market can also be utilized
              to obtain optimal asset allocation decisions between stocks and
              bonds. It turns out that the dynamic probit models yield much
              higher portfolio returns than the buy-and-hold trading strategy
              in a small-scale market timing experiment.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  9,
  pages    = "3351--3363",
  month    =  sep,
  year     =  2013,
  keywords = "Bear markets; Turning point; Probit model; Asset allocation;
              Out-of-sample forecasts"
}

@ARTICLE{Ederington2013-uf,
  title    = "The cross-sectional relation between conditional
              heteroskedasticity, the implied volatility smile, and the
              variance risk premium",
  author   = "Ederington, Louis H and Guan, Wei",
  abstract = "This paper estimates how the shape of the implied volatility
              smile and the size of the variance risk premium relate to
              parameters of GARCH-type time-series models measuring how
              conditional volatility responds to return shocks. Markets in
              which return shocks lead to large increases in conditional
              volatility tend to have larger variance risk premia than markets
              in which the impact on conditional volatility is slight. Markets
              in which negative (positive) return shocks lead to larger
              increases in future volatility than positive (negative) return
              shocks tend to have downward (upward) sloping implied volatility
              smiles. Also, differences in how volatility responds to return
              shocks as measured by GARCH-type models explain much, but not
              all, of the variations in excess kurtosis and multi-period
              skewness across different markets.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  9,
  pages    = "3388--3400",
  month    =  sep,
  year     =  2013,
  keywords = "Implied volatility; Volatility smile; Variance risk premium;
              GARCH; Conditional heteroskedasticity"
}

@ARTICLE{Rue2000-zb,
  title    = "Prediction and Retrospective Analysis of Soccer Matches in a
              League",
  author   = "Rue, Havard and Salvesen, Oyvind",
  abstract = "A common discussion subject for the male part of the population
              in particular is the prediction of the next week-end's soccer
              matches, especially for the local team. Knowledge of offensive
              and defensive skills is valuable in the decision process before
              making a bet at a bookmaker. We take an applied statistician's
              approach to the problem, suggesting a Bayesian dynamic
              generalized linear model to estimate the time-dependent skills of
              all teams in a league, and to predict the next week-end's soccer
              matches. The problem is more intricate than it may appear at
              first glance, as we need to estimate the skills of all teams
              simultaneously as they are dependent. It is now possible to deal
              with such inference problems by using the Markov chain Monte
              Carlo iterative simulation technique. We show various
              applications of the proposed model based on the English Premier
              League and division 1 in 1997?1998: prediction with application
              to betting, retrospective analysis of the final ranking, the
              detection of surprising matches and how each team's properties
              vary during the season.",
  journal  = "J Royal Statistical Soc D",
  volume   =  49,
  number   =  3,
  pages    = "399--418",
  month    =  sep,
  year     =  2000
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Karlis2003-qu,
  title     = "Analysis of sports data by using bivariate Poisson models",
  author    = "Karlis, D and Ntzoufras, I",
  abstract  = "Models based on the bivariate Poisson distribution are used for
               modelling sports data. Independent Poisson distributions are
               usually adopted to model the number of goals of two competing
               teams. We replace the independence assumption by considering a
               bivariate ‚Ä¶",
  journal   = "J. R. Stat. Soc.",
  publisher = "Wiley Online Library",
  year      =  2003
}

@ARTICLE{Audas2002-nv,
  title    = "The impact of managerial change on team performance in
              professional sports",
  author   = "Audas, Rick and Dobson, Stephen and Goddard, John",
  abstract = "Over a quarter-century of match-level data are used to examine
              the effect of managerial change on team performance in English
              (association) football, using ordered probit regression. On
              average, teams that changed their manager within-season are found
              to under-perform over the following 3 months. Managerial change
              also increases the variance of the non-systematic component of
              performance in the short term. The high incidence of
              within-season managerial change in English football may be a
              consequence of team owners gambling that an increased variance
              may help produce an improvement in performance sufficient to
              stave off the threat of relegation.",
  journal  = "J. Econ. Bus.",
  volume   =  54,
  number   =  6,
  pages    = "633--650",
  month    =  nov,
  year     =  2002,
  keywords = "Professional team sports; Managerial change; Ordered probit"
}

@ARTICLE{Forrest2005-oa,
  title    = "Odds-setters as forecasters: The case of English football",
  author   = "Forrest, David and Goddard, John and Simmons, Robert",
  abstract = "Sets of odds issued by bookmakers may be interpreted as
              incorporating implicit probabilistic forecasts of sporting
              events. Employing a sample of nearly 10000 English football
              (soccer) games, we compare the effectiveness of forecasts based
              on published odds and forecasts made using a benchmark
              statistical model incorporating a large number of quantifiable
              variables relevant to match outcomes. The experts' views,
              represented by the published odds, are shown to be increasingly
              effective over a 5-year period. Bootstraps performed on the
              statistical model fail to outperform the expert judges. The trend
              towards odds-setters displaying greater expertise as forecasters
              coincided with a period during which intensifying competition is
              likely to have increased the financial penalties for bookmakers
              of imprecise odds-setting. In the context of a financially
              pressured environment, the main findings of this paper challenge
              the consensus that subjective forecasting by experts will
              normally be inferior to forecasts from statistical models.",
  journal  = "Int. J. Forecast.",
  volume   =  21,
  number   =  3,
  pages    = "551--564",
  month    =  jul,
  year     =  2005,
  keywords = "Football; Odds; Ordered probit; Comparative forecasting---causal,
              judgement; Bootstrap-evaluation"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Constantinou2012-mx,
  title    = "pi-football: A Bayesian network model for forecasting Association
              Football match outcomes",
  author   = "Constantinou, Anthony C and Fenton, Norman E and Neil, Martin",
  abstract = "A Bayesian network is a graphical probabilistic model that
              represents the conditional dependencies among uncertain
              variables, which can be both objective and subjective. We present
              a Bayesian network model for forecasting Association Football
              matches in which the subjective variables represent the factors
              that are important for prediction but which historical data fails
              to capture. The model (pi-football) was used to generate
              forecasts about the outcomes of the English Premier League (EPL)
              matches during season 2010/11 (but is easily extended to any
              football league). Forecasts were published online prior to the
              start of each match. We show that:(a)using an appropriate measure
              of forecast accuracy, the subjective information improved the
              model such that posterior forecasts were on par with bookmakers'
              performance;(b)using a standard profitability measure with
              discrepancy levels at ‚©æ5\%, the model generates profit under
              maximum, mean, and common bookmakers' odds, even allowing for the
              bookmakers' built-in profit margin.Hence, compared with other
              published football forecast models, pi-football not only appears
              to be exceptionally accurate, but it can also be used to `beat
              the bookies'.",
  journal  = "Knowledge-Based Systems",
  volume   =  36,
  pages    = "322--339",
  month    =  dec,
  year     =  2012,
  keywords = "Bayesian reasoning; Football betting; Football predictions;
              Soccer predictions; Subjective information"
}

@ARTICLE{Koopman2015-hh,
  title    = "A dynamic bivariate Poisson model for analysing and forecasting
              match results in the English Premier League",
  author   = "Koopman, Siem Jan and Lit, Rutger",
  abstract = "Summary We develop a statistical model for the analysis and
              forecasting of football match results which assumes a bivariate
              Poisson distribution with intensity coefficients that change
              stochastically over time. The dynamic model is a novelty in the
              statistical time series analysis of match results in team sports.
              Our treatment is based on state space and importance sampling
              methods which are computationally efficient. The out-of-sample
              performance of our methodology is verified in a betting strategy
              that is applied to the match outcomes from the 2010?2011 and
              2011?2012 seasons of the English football Premier League. We show
              that our statistical modelling framework can produce a
              significant positive return over the bookmaker's odds.",
  journal  = "J. R. Stat. Soc. A",
  volume   =  178,
  number   =  1,
  pages    = "167--186",
  month    =  jan,
  year     =  2015
}

@ARTICLE{Stekler2010-af,
  title    = "Issues in sports forecasting",
  author   = "Stekler, H O and Sendor, David and Verlander, Richard",
  abstract = "A large amount of effort is spent on forecasting the outcomes of
              sporting events, but few papers have focused exclusively on the
              characteristics of sports forecasts. Instead, many papers have
              been written about the efficiency of sports betting markets. As
              it turns out, it is possible to derive a considerable amount of
              information about the forecasts and the forecasting process from
              studies that have tested the markets for economic efficiency.
              Moreover, the huge number of observations provided by betting
              markets makes it possible to obtain robust tests of various
              forecasting hypotheses. This paper is concerned with a number of
              forecasting topics in horse racing and several team sports. The
              first topic involves the type of forecast that is made: picking a
              winner or predicting whether a particular team will beat the
              point spread. Different evaluation procedures will be examined
              and alternative forecasting methods (models, experts, and the
              market) compared. The paper also examines the evidence with
              regard to the existence of biases in the forecasts, and concludes
              by discussing the applicability of these results to forecasting
              in general.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  3,
  pages    = "606--621",
  month    =  jul,
  year     =  2010,
  keywords = "Sports forecasting; Betting markets; Efficiency; Bias; Sports
              models"
}

@ARTICLE{Flepp2017-ay,
  title    = "The liquidity advantage of the quote-driven market: Evidence from
              the betting industry",
  author   = "Flepp, Raphael and N{\"u}esch, Stephan and Franck, Egon",
  abstract = "Even though betting exchanges are considered to be the superior
              business model in the betting industry due to less operational
              risk and lower information costs, bookmakers continue to be
              successful. We explain the puzzling coexistence of these two
              market structures with the advantage of guaranteed liquidity in
              the bookmaker market. Using matched panel data of over 1.8
              million bookmaker and betting exchange odds for 17,410 soccer
              matches played worldwide, we find that the bookmaker offers
              higher odds and bettor returns than the betting exchange when
              liquidity at the betting exchange is low.",
  journal  = "Q. Rev. Econ. Finance",
  volume   =  64,
  pages    = "306--317",
  month    =  may,
  year     =  2017,
  keywords = "Market structure; Liquidity; Betting industry"
}

@ARTICLE{Buhagiar2018-wa,
  title    = "Why do some soccer bettors lose more money than others?",
  author   = "Buhagiar, Ranier and Cortis, Dominic and Newall, Philip W S",
  abstract = "Why do some soccer bettors lose more money than others? In an
              efficient prediction market, each gambler should break-even
              before costs (but losing a constant amount after costs,
              reflecting the bookmaker's margin). Previous empirical studies
              across numerous sports betting markets show that bets on
              longshots tend to lose more than bets on favourites
              (favourite-longshot bias). We use 163,992 soccer odds from ten
              European leagues to test plausible hypotheses around why some
              soccer bettors lose more money than others. Are soccer bettors
              with above average losses simply biased, or are their losses
              driven by betting on events that are inherently unpredictable? We
              confirm the existence of favourite-longshot bias in soccer in
              this sample, but find another surprising feature of betting on
              longshots. As measured by the Brier score, bookmakers' odds were
              better predictors of longshots than favourites, suggesting
              another potential channel whereby bettors' preference for betting
              on longshots may cost them dearly.",
  journal  = "Journal of Behavioral and Experimental Finance",
  volume   =  18,
  pages    = "85--93",
  month    =  jun,
  year     =  2018,
  keywords = "Favourite-longshot bias; Longshot bias; Brier score; Market
              efficiency; Odds; Sports betting markets; European soccer;
              Betting"
}

@ARTICLE{Geenens2014-nl,
  title    = "On the decisiveness of a game in a tournament",
  author   = "Geenens, Gery",
  abstract = "In sport tournaments in which teams are matched two at a time, it
              is useful for a variety of reasons to be able to quantify how
              important a particular game is. The need for such quantitative
              information has been addressed in the literature by several more
              or less simple measures of game importance. In this paper, we
              point out some of the drawbacks of those measures and we propose
              a different approach, which rather targets how decisive a game is
              with respect to the final victory. We give a definition of this
              idea of game decisiveness in terms of the uncertainty about the
              eventual winner prevailing in the tournament at the time of the
              game. As this uncertainty is strongly related to the notion of
              entropy of a probability distribution, our decisiveness measure
              is based on entropy-related concepts. We study the suggested
              decisiveness measure on two real tournaments, the 1988 NBA
              Championship Series and the UEFA 2012 European Football
              Championship (Euro 2012), and we show how well it agrees with
              what intuition suggests. Finally, we also use our decisiveness
              measure to objectively analyse the recent UEFA decision to expand
              the European Football Championship from 16 to 24 nations in the
              future, in terms of the overall attractiveness of the
              competition.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  232,
  number   =  1,
  pages    = "156--168",
  month    =  jan,
  year     =  2014,
  keywords = "OR in sports; Applied probability; Uncertainty modelling; Game
              importance; Entropy"
}

@ARTICLE{Warner1988-cc,
  title     = "Stock prices and top management changes",
  author    = "Warner, Jerold B and Watts, Ross L and Wruck, Karen H",
  journal   = "J. financ. econ.",
  publisher = "North-Holland",
  volume    =  20,
  pages     = "461--492",
  year      =  1988
}

@ARTICLE{Bekaert2000-fl,
  title     = "Asymmetric Volatility and Risk in Equity Markets",
  author    = "Bekaert, Geert and Wu, Guojun",
  abstract  = "Abstract. It appears that volatility in equity markets is
               asymmetric: returns and conditional volatility are negatively
               correlated. We provide a unified framew",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford Academic",
  volume    =  13,
  number    =  1,
  pages     = "1--42",
  month     =  jan,
  year      =  2000
}

@ARTICLE{Campbell1992-jb,
  title    = "No news is good news: An asymmetric model of changing volatility
              in stock returns",
  author   = "Campbell, John Y and Hentschel, Ludger",
  abstract = "It seems plausible that an increase in stock market volatility
              raises required stock returns, and thus lowers stock prices. We
              develop a formal model of this volatility feedback effect using a
              simple model of changing variance (a quadratic generalized
              autoregressive conditionally heteroskedastic, or QGARCH, model).
              Our model is asymmetric and helps to explain the negative
              skewness and excess kurtosis of U.S. monthly and daily stock
              returns over the period 1926--1988. We find that volatility
              feedback normally has little effect on returns, but it can be
              important during periods of high volatility.",
  journal  = "J. financ. econ.",
  volume   =  31,
  number   =  3,
  pages    = "281--318",
  month    =  jun,
  year     =  1992
}

@ARTICLE{Bollerslev2006-yx,
  title     = "Leverage and Volatility Feedback Effects in {High-Frequency}
               Data",
  author    = "Bollerslev, Tim and Litvinova, Julia and Tauchen, George",
  abstract  = "Abstract. We examine the relationship between volatility and
               past and future returns using high-frequency aggregate equity
               index data. Cons",
  journal   = "Journal of Financial Econometrics",
  publisher = "Oxford Academic",
  volume    =  4,
  number    =  3,
  pages     = "353--384",
  month     =  jul,
  year      =  2006
}

@ARTICLE{Bekaert2014-ol,
  title    = "The {VIX}, the variance premium and stock market volatility",
  author   = "Bekaert, Geert and Hoerova, Marie",
  abstract = "We decompose the squared VIX index, derived from US S\&P500
              options prices, into the conditional variance of stock returns
              and the equity variance premium. We evaluate a plethora of
              state-of-the-art volatility forecasting models to produce an
              accurate measure of the conditional variance. We then examine the
              predictive power of the VIX and its two components for stock
              market returns, economic activity and financial instability. The
              variance premium predicts stock returns while the conditional
              stock market variance predicts economic activity and has a
              relatively higher predictive power for financial instability than
              does the variance premium.",
  journal  = "J. Econom.",
  volume   =  183,
  number   =  2,
  pages    = "181--192",
  month    =  dec,
  year     =  2014,
  keywords = "Option implied volatility; Realized volatility; VIX; Variance
              risk premium; Risk aversion; Stock return predictability;
              Risk--return trade-off; Economic uncertainty; Financial
              instability"
}

@ARTICLE{Poon2003-pv,
  title    = "Forecasting Volatility in Financial Markets: A Review",
  author   = "Poon, Ser-Huang and Granger, Clive W J",
  abstract = "Forecasting Volatility in Financial Markets: A Review by
              Ser-Huang Poon and Clive W.J. Granger. Published in volume 41,
              issue 2, pages 478-539 of Journal of Economic Literature, June
              2003, Abstract: Financial market volatility is an important input
              for investment, option pricing, and financial...",
  journal  = "J. Econ. Lit.",
  volume   =  41,
  number   =  2,
  pages    = "478--539",
  month    =  jun,
  year     =  2003
}

@ARTICLE{Bekiros2017-ow,
  title    = "The asymmetric relationship between returns and implied
              volatility: Evidence from global stock markets",
  author   = "Bekiros, Stelios and Jlassi, Mouna and Naoui, Kamel and Uddin,
              Gazi Salah",
  abstract = "We investigate the asymmetric relationship between returns and
              implied volatility for 20 developed and emerging international
              markets. In particular we examine how the sign and size of return
              innovations affect the expectations of daily changes in
              volatility. Our empirical findings indicate that the conditional
              contemporaneous return-volatility relationship varies not only
              based on the sign of the expected returns but also upon their
              magnitude, according to recent results from the behavioral
              finance literature. We find evidence of an asymmetric and reverse
              return-volatility relationship in many advanced, Asian,
              Latin-American, European and South African markets. We show that
              the US market displays the highest reaction to price falls, Asian
              markets present the lowest sensitivity to volatility
              expectations, while the Euro area is characterized by a
              homogeneous response both in terms of direction and impact. These
              results may be safely attributed to cultural and societal
              characteristics. An extensive quantile regression analysis
              demonstrates that the detected asymmetric pattern varies
              particularly across the extreme distribution tails i.e., in the
              highest/lowest quantile ranges. Indeed, the classical feedback
              and leverage hypotheses appear not plausible, whilst behavioral
              theories emerge as the new paradigm in real-world applications.",
  journal  = "Journal of Financial Stability",
  volume   =  30,
  pages    = "156--174",
  month    =  jun,
  year     =  2017,
  keywords = "Implied volatility; Quantile regression; Behavioral bias;
              Predictability"
}

@MISC{Adrian2019-zr,
  title        = "A Monitoring Framework for Global Financial Stability",
  booktitle    = "{IMF}",
  author       = "Adrian, Tobias and He, Dong and Liang, Nellie and Natalucci,
                  Fabio M",
  abstract     = "This paper describes the conceptual framework that guides
                  assessments of financial stability risks for multilateral
                  surveillance, as currently presented in the Global Financial
                  Stability Report (GFSR). The framework emphasizes consistency
                  in measuring financial vulnerabilities across countries and
                  over time and offers a summary statistic to quantify
                  aggregate financial stability risks. The two parts of the
                  empirical approach---a matrix of specific vulnerabilities and
                  a summary measure of financial stability risks---are distinct
                  but highly complementary for monitoring and policymaking.",
  month        =  aug,
  year         =  2019,
  howpublished = "\url{https://www.imf.org/en/Publications/Staff-Discussion-Notes/Issues/2019/08/23/A-Monitoring-Framework-for-Global-Financial-Stability-46645}",
  note         = "Accessed: 2020-3-5"
}

@ARTICLE{Chung2012-ig,
  title     = "Have we underestimated the likelihood and severity of zero lower
               bound events?",
  author    = "Chung, Hess and Laforte, Jean-Philippe and Reifschneider, David
               and Williams, John C",
  journal   = "J. Money Credit Bank.",
  publisher = "Wiley Online Library",
  volume    =  44,
  pages     = "47--82",
  year      =  2012
}

@ARTICLE{Kim2018-ve,
  title    = "Mining big data using parsimonious factor, machine learning,
              variable selection and shrinkage methods",
  author   = "Kim, Hyun Hak and Swanson, Norman R",
  abstract = "A number of recent studies in the economics literature have
              focused on the usefulness of factor models in the context of
              prediction using ``big data'' (see Bai and Ng, 2008; Dufour and
              Stevanovic, 2010; Forni, Hallin, Lippi, \& Reichlin, 2000; Forni
              et al., 2005; Kim and Swanson, 2014a; Stock and Watson, 2002b,
              2006, 2012, and the references cited therein). We add to this
              literature by analyzing whether ``big data'' are useful for
              modelling low frequency macroeconomic variables, such as
              unemployment, inflation and GDP. In particular, we analyze the
              predictive benefits associated with the use of principal
              component analysis (PCA), independent component analysis (ICA),
              and sparse principal component analysis (SPCA). We also evaluate
              machine learning, variable selection and shrinkage methods,
              including bagging, boosting, ridge regression, least angle
              regression, the elastic net, and the non-negative garotte. Our
              approach is to carry out a forecasting ``horse-race'' using
              prediction models that are constructed based on a variety of
              model specification approaches, factor estimation methods, and
              data windowing methods, in the context of predicting 11
              macroeconomic variables that are relevant to monetary policy
              assessment. In many instances, we find that various of our
              benchmark models, including autoregressive (AR) models, AR models
              with exogenous variables, and (Bayesian) model averaging, do not
              dominate specifications based on factor-type dimension reduction
              combined with various machine learning, variable selection, and
              shrinkage methods (called ``combination'' models). We find that
              forecast combination methods are mean square forecast error
              (MSFE) ``best'' for only three variables out of 11 for a forecast
              horizon of h=1, and for four variables when h=3 or 12. In
              addition, non-PCA type factor estimation methods yield MSFE-best
              predictions for nine variables out of 11 for h=1, although PCA
              dominates at longer horizons. Interestingly, we also find
              evidence of the usefulness of combination models for
              approximately half of our variables when h>1. Most importantly,
              we present strong new evidence of the usefulness of factor-based
              dimension reduction when utilizing ``big data'' for
              macroeconometric forecasting.",
  journal  = "Int. J. Forecast.",
  volume   =  34,
  number   =  2,
  pages    = "339--354",
  month    =  apr,
  year     =  2018,
  keywords = "Prediction; Independent component analysis; Sparse principal
              component analysis; Bagging; Boosting; Bayesian model averaging;
              Ridge regression; Least angle regression; Elastic net and
              non-negative garotte"
}

@INCOLLECTION{Jacquier2011-ie,
  title     = "Bayesian Methods In Finance",
  booktitle = "The Oxford Handbook of Bayesian Econometrics",
  author    = "Jacquier, Eric and Polson, Nicholas",
  editor    = "Geweke, John and Koop, Gary and Van Dijk, Herman",
  abstract  = "This article looks at the usefulness of Bayesian methods in
               finance. It covers all the major topics in finance. It discusses
               the predictability of the mean of asset returns, central to
               finance, as it relates to the efficiency of financial markets.
               It reviews the economic relevance of predictability and its
               impact on optimal allocation. It also describes the Markov chain
               Monte Carlo (MCMC) and particle filtering algorithms that are
               important in modern Bayesian financial econometrics. MCMC
               algorithms have resulted in a tremendous growth in the use of
               stochastic volatility models in financial econometrics. This
               article also contains some major contributions of Bayesian
               econometrics to the literature on empirical asset pricing. Many
               of the other themes in modern Bayesian econometrics, including
               the use of shrinkage and the interaction between theory and
               econometrics are discussed. This article ends up with the
               discussion of a promising recent development in finance:
               filtering with parameter learning.",
  publisher = "Oxford University Press",
  month     =  sep,
  year      =  2011,
  keywords  = "optimal allocation; particle filtering algorithms; stochastic
               volatility models; empirical asset pricing"
}

@ARTICLE{Hui2013-nn,
  title     = "The Effect of {In-Store} Travel Distance on Unplanned Spending:
               Applications to Mobile Promotion Strategies",
  author    = "Hui, Sam K and Inman, J Jeffrey and Huang, Yanliu and Suher,
               Jacob",
  abstract  = "Typically, shoppers? paths only cover less than half of the
               areas in a grocery store. Given that shoppers often use physical
               products in the store as external memory cues, encouraging
               shoppers to travel more of the store may increase unplanned
               spending. Estimating the direct effect of in-store travel
               distance on unplanned spending, however, is complicated by the
               difficulty of collecting in-store path data and the endogeneity
               of in-store travel distance. To address both issues, the authors
               collect a novel data set using in-store radio frequency
               identification tracking and develop an instrumental variable
               approach to account for endogeneity. Their analysis reveals that
               the elasticity of unplanned spending on travel distance is 57\%
               higher than the uncorrected ordinary least squares estimate.
               Simulations based on the authors? estimates suggest that
               strategically promoting three product categories through mobile
               promotion could increase unplanned spending by 16.1\%, compared
               with the estimated effect of a benchmark strategy based on
               relocating three destination categories (7.2\%). Furthermore,
               the authors conduct a field experiment to assess the
               effectiveness of mobile promotions and find that a coupon that
               required shoppers to travel farther from their planned path
               resulted in a substantial increase in unplanned spending
               ($21.29) over a coupon for an unplanned category near their
               planned path ($13.83). The results suggest that targeted mobile
               promotions aimed at increasing in-store path length can increase
               unplanned spending.",
  journal   = "J. Mark.",
  publisher = "SAGE Publications Inc",
  volume    =  77,
  number    =  2,
  pages     = "1--16",
  month     =  mar,
  year      =  2013
}

@ARTICLE{Anatolyev2005-eq,
  title     = "A Trading Approach to Testing for Predictability",
  author    = "Anatolyev, Stanislav and Gerko, Alexander",
  abstract  = "We propose a market timing test for conditional mean
               independence of financial returns. The new excess predictability
               (EP) test statistic has an interpretation of a properly
               normalized return of a certain trading strategy. We discuss
               similarities of the EP test to the popular directional accuracy
               (DA) test of Pesaran and Timmermann. Power properties of the EP
               test are advantageous, and size properties are comparable to
               those of the DA test. We illustrate application of the test
               using weekly data on the S\&P500 index.",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  23,
  number    =  4,
  pages     = "455--461",
  month     =  oct,
  year      =  2005
}

@MISC{Rubia2013-hl,
  title   = "On downside risk predictability through liquidity and trading
             activity: A dynamic quantile approach",
  author  = "Rubia, Antonio and Sanchis-Marco, Lidia",
  journal = "International Journal of Forecasting",
  volume  =  29,
  number  =  1,
  pages   = "202--219",
  year    =  2013
}

@MISC{Arrieta-ibarra2015-zf,
  title   = "Testing for Predictability in Financial Returns Using Statistical
             Learning Procedures",
  author  = "Arrieta-ibarra, Imanol and Lobato, Ignacio N",
  journal = "Journal of Time Series Analysis",
  volume  =  36,
  number  =  5,
  pages   = "672--686",
  year    =  2015
}

@ARTICLE{Feng2020-nt,
  title    = "Taming the Factor Zoo: A Test of New Factors",
  author   = "Feng, Guanhao and Giglio, Stefano and Xiu, Dacheng",
  abstract = "ABSTRACT We propose a model selection method to systematically
              evaluate the contribution to asset pricing of any new factor,
              above and beyond what a high-dimensional set of existing factors
              explains. Our methodology accounts for model selection mistakes
              that produce a bias due to omitted variables, unlike standard
              approaches that assume perfect variable selection. We apply our
              procedure to a set of factors recently discovered in the
              literature. While most of these new factors are shown to be
              redundant relative to the existing factors, a few have
              statistically significant explanatory power beyond the hundreds
              of factors proposed in the past.",
  journal  = "J. Finance",
  volume   =  73,
  pages    = "19",
  month    =  feb,
  year     =  2020
}

@MISC{Morey2016-xu,
  title   = "The fallacy of placing confidence in confidence intervals",
  author  = "Morey, Richard D and Hoekstra, Rink and Rouder, Jeffrey N and Lee,
             Michael D and Wagenmakers, Eric-Jan",
  journal = "Psychonomic Bulletin \& Review",
  volume  =  23,
  number  =  1,
  pages   = "103--123",
  year    =  2016
}

@ARTICLE{Crowder2002-fh,
  title    = "Dynamic modelling and prediction of English Football League
              matches for betting",
  author   = "Crowder, Martin and Dixon, Mark and Ledford, Anthony and
              Robinson, Mike",
  abstract = "Summary. We focus on modelling the 92 soccer teams in the English
              Football Association League over the years 1992?1997 using
              refinements of the independent Poisson model of Dixon and Coles.
              Our framework assumes that each team has attack and defence
              strengths that evolve through time (rather than remaining
              constant) according to some unobserved bivariate stochastic
              process. Estimation of the teams' attack and defence capabilities
              is undertaken via a novel approach involving an approximation
              that is computationally convenient and fast. The results of this
              approximation compare very favourably with results obtained
              through the Dixon and Coles approach. We note that the full model
              (i.e. the model before the above approximation is made) may be
              implemented using Markov chain Monte Carlo procedures, and that
              this approach is vastly more computationally expensive. We focus
              on the probabilities of home win, draw or away win because these
              outcomes constitute the primary betting market. These
              probabilities are estimated for games played between any two of
              the 92 teams and the predictions are compared with the actual
              results.",
  journal  = "Journal of the Royal Statistical Society: Series D (The
              Statistician)",
  volume   =  51,
  number   =  2,
  pages    = "157--168",
  month    =  jun,
  year     =  2002
}

@ARTICLE{Joseph2006-jv,
  title    = "Predicting football results using Bayesian nets and other machine
              learning techniques",
  author   = "Joseph, A and Fenton, N E and Neil, M",
  abstract = "Bayesian networks (BNs) provide a means for representing,
              displaying, and making available in a usable form the knowledge
              of experts in a given field. In this paper, we look at the
              performance of an expert constructed BN compared with other
              machine learning (ML) techniques for predicting the outcome (win,
              lose, or draw) of matches played by Tottenham Hotspur Football
              Club. The period under study was 1995--1997 -- the expert BN was
              constructed at the start of that period, based almost exclusively
              on subjective judgement. Our objective was to determine
              retrospectively the comparative accuracy of the expert BN
              compared to some alternative ML models that were built using data
              from the two-year period. The additional ML techniques considered
              were: MC4, a decision tree learner; Naive Bayesian learner; Data
              Driven Bayesian (a BN whose structure and node probability tables
              are learnt entirely from data); and a K-nearest neighbour
              learner. The results show that the expert BN is generally
              superior to the other techniques for this domain in predictive
              accuracy. The results are even more impressive for BNs given
              that, in a number of key respects, the study assumptions place
              them at a disadvantage. For example, we have assumed that the BN
              prediction is `incorrect' if a BN predicts more than one outcome
              as equally most likely (whereas, in fact, such a prediction would
              prove valuable to somebody who could place an `each way' bet on
              the outcome). Although the expert BN has now long been irrelevant
              (since it contains variables relating to key players who have
              retired or left the club) the results here tend to confirm the
              excellent potential of BNs when they are built by a reliable
              domain expert. The ability to provide accurate predictions
              without requiring much learning data are an obvious bonus in any
              domain where data are scarce. Moreover, the BN was relatively
              simple for the expert to build and its structure could be used
              again in this and similar types of problems.",
  journal  = "Knowledge-Based Systems",
  volume   =  19,
  number   =  7,
  pages    = "544--553",
  month    =  nov,
  year     =  2006,
  keywords = "Bayesian nets; Machine learning; Football"
}

@ARTICLE{Suzuki2010-fx,
  title     = "A Bayesian approach for predicting match outcomes: The 2006
               (Association) Football World Cup",
  author    = "Suzuki, A K and Salasar, L E B and Leite, J G and Louzada-Neto,
               F",
  abstract  = "AbstractIn this paper we propose a Bayesian methodology for
               predicting match outcomes. The methodology is illustrated on the
               2006 Soccer World Cup. As prior information, we make use of the
               specialists? opinions and the FIFA ratings. The method is
               applied to calculate the win, draw and loss probabilities at
               each match and also to simulate the whole competition in order
               to estimate classification probabilities in group stage and
               winning tournament chances for each team. The prediction
               capability of the proposed methodology is determined by the
               DeFinetti measure and by the percentage of correct forecasts.",
  journal   = "J. Oper. Res. Soc.",
  publisher = "Taylor \& Francis",
  volume    =  61,
  number    =  10,
  pages     = "1530--1539",
  month     =  oct,
  year      =  2010
}

@ARTICLE{Gai2018-kb,
  title    = "A survey on {FinTech}",
  author   = "Gai, Keke and Qiu, Meikang and Sun, Xiaotong",
  abstract = "As a new term in the financial industry, FinTech has become a
              popular term that describes novel technologies adopted by the
              financial service institutions. This term covers a large scope of
              techniques, from data security to financial service deliveries.
              An accurate and up-to-date awareness of FinTech has an urgent
              demand for both academics and professionals. This work aims to
              produce a survey of FinTech by collecting and reviewing
              contemporary achievements, by which a theoretical data-driven
              FinTech framework is proposed. Five technical aspects are
              summarized and involved, which include security and privacy, data
              techniques, hardware and infrastructure, applications and
              management, and service models. The main findings of this work
              are fundamentals of forming active FinTech solutions.",
  journal  = "Journal of Network and Computer Applications",
  volume   =  103,
  pages    = "262--273",
  month    =  feb,
  year     =  2018,
  keywords = "FinTech; Cloud computing; Cyber security; Big data; Financial
              computing; Data-driven framework"
}

@ARTICLE{Arner2015-ld,
  title     = "The evolution of Fintech: A new post-crisis paradigm",
  author    = "Arner, Douglas W and Barberis, Janos and Buckley, Ross P",
  journal   = "Geo. J. Int'l L.",
  publisher = "HeinOnline",
  volume    =  47,
  pages     = "1271",
  year      =  2015
}

@ARTICLE{Ryan2018-hv,
  title   = "Fintech isn't so different from traditional banking: Trading off
             aggregation of soft information for transaction processing
             efficiency",
  author  = "Ryan, Stephen G and Zhu, Chenqi",
  journal = "Available at SSRN 3212902",
  year    =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Schultz2010-wd,
  title   = "Mispricing of dual-class shares: Profit opportunities, arbitrage,
             and trading‚òÜ",
  author  = "Schultz, Paul and Shive, Sophie",
  journal = "Journal of Financial Economics",
  volume  =  98,
  number  =  3,
  pages   = "524--549",
  year    =  2010
}

@ARTICLE{Cattelan2013-rj,
  title     = "Dynamic {Bradley--Terry} modelling of sports tournaments",
  author    = "Cattelan, Manuela and Varin, Cristiano and Firth, David",
  journal   = "J. R. Stat. Soc. Ser. C Appl. Stat.",
  publisher = "Wiley Online Library",
  volume    =  62,
  number    =  1,
  pages     = "135--150",
  year      =  2013
}

@ARTICLE{Groll2016-zy,
  title  = "Who's the {Favourite?--A} Bivariate Poisson Model for the {UEFA}
            European Football Championship 2016",
  author = "Groll, Andreas and Kneib, Thomas and Mayr, Andreas and Schauberger,
            Gunther",
  year   =  2016
}

@ARTICLE{Lieberson1972-hx,
  title    = "Leadership and organizational performance: a study of large
              corporations",
  author   = "Lieberson, S and O'Connor, J F",
  journal  = "Am. Sociol. Rev.",
  volume   =  37,
  number   =  2,
  pages    = "117--130",
  month    =  apr,
  year     =  1972,
  language = "en"
}

@ARTICLE{Strauss2020-hz,
  title   = "Corporate secular stagnation: Empirical evidence on the advanced
             economy investment slowdown",
  author  = "Strauss, Ilan and Yang, Jangho",
  journal = "INET Oxford Working Paper No. 2019-16",
  year    =  2020
}

@ARTICLE{Constantinou2018-ua,
  title    = "Things to know about Bayesian networks: Decisions under
              uncertainty, part 2",
  author   = "Constantinou, Anthony C and Fenton, Norman",
  abstract = "Bayesian networks help us model and understand the many variables
              that inform our decision-making processes. Anthony C.
              Constantinou and Norman Fenton explain how they work, how they
              are built and the pitfalls to avoid along the way",
  journal  = "Significance",
  volume   =  15,
  number   =  2,
  pages    = "19--23",
  month    =  apr,
  year     =  2018
}

@ARTICLE{Goodell2013-ti,
  title    = "{US} presidential elections and implied volatility: The role of
              political uncertainty",
  author   = "Goodell, John W and V{\"a}h{\"a}maa, Sami",
  abstract = "This paper focuses on the effects of political uncertainty and
              the political process on implied stock market volatility during
              US presidential election cycles. Using monthly Iowa Electronic
              Markets data over five elections, we document that stock market
              uncertainty, as measured by the VIX volatility index, increases
              along with positive changes in the probability of success of the
              eventual winner. The association between implied volatility and
              the election probability of the eventual winner is positive even
              after controlling for changes in overall election uncertainty.
              These findings indicate that the presidential election process
              engenders market anxiety as investors form and revise their
              expectations regarding future macroeconomic policy.",
  journal  = "Journal of Banking \& Finance",
  volume   =  37,
  number   =  3,
  pages    = "1108--1117",
  month    =  mar,
  year     =  2013,
  keywords = "Presidential elections; Political uncertainty; Implied
              volatility; VIX"
}

@ARTICLE{Boussemart2019-wl,
  title    = "Decomposing banking performance into economic and credit risk
              efficiencies",
  author   = "Boussemart, Jean-Philippe and Leleu, Herv{\'e} and Shen, Zhiyang
              and Vardanyan, Michael and Zhu, Ning",
  abstract = "This paper proposes a non-parametric approach of a banking
              production technology that decomposes performance into economic
              and credit risk efficiencies. The basis of our approach is to
              separate the production technology into two sub-technologies. The
              former is the production of non-interest income and loans from a
              set of traditional inputs. The latter is attached to the
              production of interest income from loans where an explicit
              distinction between good and non-performing loans is introduced.
              Economic efficiency comes from the production of good outputs,
              namely interest and non-interest income, while credit risk
              management efficiency is related to the minimization of the
              non-performing loans that can be considered as an unintended or
              bad output. The model is applied to Chinese financial data
              covering 30 banks from 2005 to 2012 and different scenarios are
              considered. The results indicate that income could be increased
              by an average rate of 16\% while non-performing loans could be
              decreased by an average rate of 33\%. According to our results,
              banking managers could strike a balance between economic
              performance and credit risk management and make more appropriate
              decisions in line with their preferences.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  277,
  number   =  2,
  pages    = "719--726",
  month    =  sep,
  year     =  2019,
  keywords = "Data Envelopment Analysis; Credit risk; Economic efficiency;
              Banking performance; Non-performing loans"
}

@MISC{Dobson2004-ls,
  title   = "Modelling and Forecasting Match Results in the English Premier
             League and Football League",
  author  = "Dobson, Stephen and Goddard, John",
  journal = "Economics, Management and Optimization in Sports",
  pages   = "59--77",
  year    =  2004
}

@MISC{Jones2015-wl,
  title   = "The spillover effect from {FDI} in the English Premier League",
  author  = "Jones, Andrew and Cook, Mark",
  journal = "Soccer \& Society",
  volume  =  16,
  number  =  1,
  pages   = "116--139",
  year    =  2015
}

@MISC{Capobianco2019-uj,
  title   = "Can Machine Learning Predict Soccer Match Results?",
  author  = "Capobianco, Giovanni and Di Giacomo, Umberto and Mercaldo,
             Francesco and Nardone, Vittoria and Santone, Antonella",
  journal = "Proceedings of the 11th International Conference on Agents and
             Artificial Intelligence",
  year    =  2019
}

@PHDTHESIS{Ulmer2013-sb,
  title  = "Predicting soccer match results in the English Premier League",
  author = "Ulmer, Ben and Fernandez, Matthew and Peterson, Michael",
  year   =  2013,
  school = "Ph. D. thesis, Doctoral dissertation, Ph. D. dissertation, Stanford"
}

@ARTICLE{Lang2011-dw,
  title     = "The Sugar Daddy Game: How Wealthy Investors Change Competition
               in Professional Team Sports",
  author    = "Lang, Markus and Grossmann, Martin and Theiler, Philipp",
  abstract  = "[Professional sports leagues have witnessed the appearance of
               sugar daddies --- people who invest enormous amounts of money in
               clubs and become their owners. This paper presents a contest
               model of a professional sports league that incorporates this
               phenomenon. We analyze how the appearance of a sugar daddy
               alters competitive balance and social welfare with respect to a
               league with purely profit-maximizing club owners. We further
               show that the welfare effect of revenue sharing in a sugar daddy
               league is ambiguous and depends on the degree of redistribution
               and on whether the sugar daddy invests in a small or a large
               club.]",
  journal   = "J. Inst. Theor. Econ.",
  publisher = "Mohr Siebeck GmbH \& Co. KG",
  volume    =  167,
  number    =  4,
  pages     = "557--577",
  year      =  2011
}

@ARTICLE{Corona2019-vz,
  title    = "Bayesian forecasting of {UEFA} Champions League under alternative
              seeding regimes",
  author   = "Corona, Francisco and Forrest, David and Tena, J D and Wiper,
              Michael",
  abstract = "The evaluation of seeding rules requires the use of probabilistic
              forecasting models both for individual matches and for the
              tournament. Prior papers have employed a match-level forecasting
              model and then used a Monte Carlo simulation of the tournament
              for estimating outcome probabilities, thus allowing an outcome
              uncertainty measure to be attached to each proposed seeding
              regime, for example. However, this approach does not take into
              account the uncertainty that may surround parameter estimates in
              the underlying match-level forecasting model. We propose a
              Bayesian approach for addressing this problem, and illustrate it
              by simulating the UEFA Champions League under alternative seeding
              regimes. We find that changes in 2015 tended to increase the
              uncertainty over progression to the knock-out stage, but made
              limited difference to which clubs would contest the final.",
  journal  = "Int. J. Forecast.",
  volume   =  35,
  number   =  2,
  pages    = "722--732",
  month    =  apr,
  year     =  2019,
  keywords = "OR in sports; Seeding; Football; Monte Carlo simulation; Bayesian"
}

@ARTICLE{Pawlowski2018-wg,
  title     = "Perceived game uncertainty, suspense and the demand for sport",
  author    = "Pawlowski, Tim and Nalbantis, Georgios and Coates, Dennis",
  journal   = "Econ. Inq.",
  publisher = "Wiley Online Library",
  volume    =  56,
  number    =  1,
  pages     = "173--192",
  year      =  2018
}

@MISC{Malkiel1970-jz,
  title   = "{EFFICIENT} {CAPITAL} {MARKETS}: A {REVIEW} {OF} {THEORY} {AND}
             {EMPIRICAL} {WORK*}",
  author  = "Malkiel, Burton G and Fama, Eugene F",
  journal = "The Journal of Finance",
  volume  =  25,
  number  =  2,
  pages   = "383--417",
  year    =  1970
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Trapero2019-fd,
  title     = "Empirical safety stock estimation based on kernel and {GARCH}
               models",
  author    = "Trapero, J R and Cardos, M and Kourentzes, N",
  abstract  = "Supply chain risk management has drawn the attention of
               practitioners and academics alike. One source of risk is demand
               uncertainty. Demand forecasting and safety stock levels are
               employed to address this risk. Most previous work has focused on
               point demand forecasting ‚Ä¶",
  journal   = "Omega",
  publisher = "Elsevier",
  year      =  2019
}

@ARTICLE{Kourentzes2019-ad,
  title   = "tsutils: Time Series Exploration, Modelling and Forecasting",
  author  = "Kourentzes, N",
  journal = "R package version 0.9. 0. URL: https://CRAN. R-project",
  year    =  2019
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Ben_Taieb2020-ui,
  title     = "Hierarchical probabilistic forecasting of electricity demand
               with smart meter data",
  author    = "Ben Taieb, S and Taylor, J W and {others}",
  abstract  = "Decisions regarding the supply of electricity across a power
               grid must take into consideration the inherent uncertainty in
               demand. Optimal decision-making requires probabilistic forecasts
               for demand in a hierarchy with various levels of aggregation,
               such as ‚Ä¶",
  journal   = "Journal of the American",
  publisher = "Taylor \& Francis",
  year      =  2020
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Athanasopoulos2020-gs,
  title     = "Hierarchical forecasting",
  author    = "Athanasopoulos, G and Gamakumara, P and {others}",
  abstract  = "Accurate forecasts of macroeconomic variables are crucial inputs
               into the decisions of economic agents and policy makers.
               Exploiting inherent aggregation structures of such variables, we
               apply forecast reconciliation methods to generate forecasts that
               are coherent ‚Ä¶",
  journal   = "in the Era of Big Data",
  publisher = "Springer",
  year      =  2020
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Makridakis2019-fx,
  title     = "Forecasting in social settings: the state of the art",
  author    = "Makridakis, S and Hyndman, R J and Petropoulos, F",
  abstract  = "This paper provides a non-systematic review of the progress of
               forecasting in social settings. It is aimed at someone outside
               the field of forecasting who wants to understand and appreciate
               the results of the M4 Competition, and forms a survey paper
               regarding the state ‚Ä¶",
  journal   = "International Journal of",
  publisher = "Elsevier",
  year      =  2019
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Montero-Manso2020-or,
  title     = "{FFORMA}: Feature-based forecast model averaging",
  author    = "Montero-Manso, P and Athanasopoulos, G and {others}",
  abstract  = "We propose an automated method for obtaining weighted forecast
               combinations using time series features. The proposed approach
               involves two phases. First, we use a collection of time series
               to train a meta-model for assigning weights to various possible
               forecasting ‚Ä¶",
  journal   = "International Journal of",
  publisher = "Elsevier",
  year      =  2020
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Wang2019-nk,
  title     = "A new tidy data structure to support exploration and modeling of
               temporal data",
  author    = "Wang, E and Cook, D and Hyndman, R J",
  abstract  = "Mining temporal data for information is often inhibited by a
               multitude of formats: regular or irregular time intervals, point
               events that need aggregating, multiple observational units or
               repeated measurements on multiple individuals, and heterogeneous
               data types. This work ‚Ä¶",
  journal   = "Journal of Computational and",
  publisher = "Taylor \& Francis",
  year      =  2019
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Feng2018-zb,
  title     = "Deep factor alpha",
  author    = "Feng, Guanhao and Polson, Nicholas G and Xu, Jianeng",
  abstract  = "‚Ä¶ 1 arXiv:1805.01104v1 [stat.ME] 3 May 2018 Page 2. 1
               Introduction Deep factor alpha provides a framework based on
               deep learning for searching for (non-linear) factors in
               empirical asset pricing. Factor regression is central to
               understanding risk-return trade-offs in finance ‚Ä¶",
  journal   = "arXiv preprint arXiv:1805. 01104",
  publisher = "pdfs.semanticscholar.org",
  volume    =  2,
  year      =  2018
}

@ARTICLE{Weiner1981-ul,
  title     = "A Model of Corporate Performance as a Function of Environmental,
               Organizational, and Leadership Influences",
  author    = "Weiner, Nan and Mahoney, Thomas A",
  abstract  = "In a model of corporate performance incorporating environmental,
               organizational, and leadership variables, three performance
               dimensions?profit, profitability, and stock price?are examined
               for a sample of 193 manufacturing companies over a 19 year
               period. This research evaluates the work of Lieberson and
               O?Connor, a cornerstone for those arguing that the top
               leadership position of an organization is unimportant.",
  journal   = "AMJ",
  publisher = "Academy of Management",
  volume    =  24,
  number    =  3,
  pages     = "453--470",
  month     =  sep,
  year      =  1981
}

@ARTICLE{Virany1992-sf,
  title     = "Executive Succession and Organization Outcomes in Turbulent
               Environments: An Organization Learning Approach",
  author    = "Virany, Beverly and Tushman, Michael L and Romanelli, Elaine",
  abstract  = "This paper explores executive succession as an important
               mechanism for organization learning and, thus, for organization
               adaptation. We argue that executive succession can fundamentally
               alter the knowledge, skills and interaction processes of the
               senior management team. These revised skills and communication
               processes improve the team's ability to recognize and act on
               changing environmental conditions. Especially in turbulent
               environments, succession may be critical for improving or
               sustaining the performance of the firm. We explore continuity
               and change of CEOs and their executive teams as associated with
               first- and second-order organization learning, which are
               differentially important under stable versus turbulent
               environmental conditions. We also link these organization
               learning ideas to the nature of organization evolution. A series
               of hypotheses link executive-team succession and strategic
               reorientation to subsequent organization performance. Results in
               a study of 59 minicomputer firms, all founded between 1968 and
               1971, indicate that succession exerts a positive influence on
               organization performance. We also show that it is important to
               distinguish between CEO succession and executive-team change,
               which independently improve subsequent organization performance.
               The positive impact of succession is accentuated when it
               coincides with strategic reorientation. Finally we examined how
               longer term patterns in succession and reorientation affect
               organization performance. We discovered two modes of
               organization adaption in this turbulent industry. The most
               typical mode combines CEO succession, sweeping executive-team
               changes, and strategic reorientations. A more rare, and over the
               long-term more effective, adaptational mode involves strategic
               reorientation and executive-team change, but no succession of
               the CEO. Consistently high-performing organizations are managed
               to sustain a relatively high level of learning (through turnover
               of senior executives and strategic reorientation), and at the
               same time to maintain links with established organizational
               competencies (through retention of the CEO).",
  journal   = "Organization Science",
  publisher = "INFORMS",
  volume    =  3,
  number    =  1,
  pages     = "72--91",
  month     =  feb,
  year      =  1992
}

@ARTICLE{Munkin2008-tz,
  title    = "Bayesian analysis of the ordered probit model with endogenous
              selection",
  author   = "Munkin, Murat K and Trivedi, Pravin K",
  abstract = "This paper presents a Bayesian analysis of an ordered probit
              model with endogenous selection. The model can be applied when
              analyzing ordered outcomes that depend on endogenous covariates
              that are discrete choice indicators modeled by a multinomial
              probit model. The model is illustrated by analyzing the effects
              of different types of medical insurance plans on the level of
              hospital utilization, allowing for potential endogeneity of
              insurance status. The estimation is performed using the Markov
              chain Monte Carlo (MCMC) methods to approximate the posterior
              distribution of the parameters in the model.",
  journal  = "J. Econom.",
  volume   =  143,
  number   =  2,
  pages    = "334--348",
  month    =  apr,
  year     =  2008,
  keywords = "Treatment effects; MCMC; Discontinuity regression"
}

@ARTICLE{Stegmueller2013-hy,
  title     = "Modeling Dynamic Preferences: A Bayesian Robust Dynamic Latent
               Ordered Probit Model",
  author    = "Stegmueller, Daniel",
  abstract  = "Much politico-economic research on individuals' preferences is
               cross-sectional and does not model dynamic aspects of preference
               or attitude formation. I present a Bayesian dynamic panel model,
               which facilitates the analysis of repeated preferences using
               individual-level panel data. My model deals with three problems.
               First, I explicitly include feedback from previous preferences
               taking into account that available survey measures of
               preferences are categorical. Second, I model individuals'
               initial conditions when entering the panel as resulting from
               observed and unobserved individual attributes. Third, I capture
               unobserved individual preference heterogeneity both via standard
               parametric random effects and a robust alternative based on
               Bayesian nonparametric density estimation. I use this model to
               analyze the impact of income and wealth on preferences for
               government intervention using the British Household Panel Study
               from 1991 to 2007.",
  journal   = "Polit. Anal.",
  publisher = "Cambridge University Press",
  volume    =  21,
  number    =  3,
  pages     = "314--333",
  year      =  2013
}

@ARTICLE{Gurmu2011-mf,
  title     = "Bayesian Approach to {Zero-Inflated} Bivariate Ordered Probit
               Regression Model, with an Application to Tobacco Use",
  author    = "Gurmu, Shiferaw and Dagne, Getachew A",
  abstract  = "This paper presents a Bayesian analysis of bivariate ordered
               probit regression model with excess of zeros. Specifically, in
               the context of joint modeling of two ordered outcomes, we
               develop zero-inflated bivariate ordered probit model and carry
               out estimation using Markov Chain Monte Carlo techniques. Using
               household tobacco survey data with substantial proportion of
               zeros, we analyze the socioeconomic determinants of individual
               problem of smoking and chewing tobacco. In our illustration, we
               find strong evidence that accounting for excess zeros provides
               good fit to the data. The example shows that the use of a model
               that ignores zero-inflation masks differential effects of
               covariates on nonusers and users.",
  journal   = "J. Probab. Stat.",
  publisher = "Hindawi",
  volume    =  2012,
  month     =  dec,
  year      =  2011,
  language  = "en"
}

@MISC{Xu2019-pc,
  title   = "Addressing spatial heterogeneity of injury severity using Bayesian
             multilevel ordered probit model",
  author  = "Xu, Xuecai and Huang, Dong and Guo, Fengjun",
  journal = "Research in Transportation Economics",
  pages   = "100748",
  year    =  2019
}

@ARTICLE{Wang2009-rm,
  title     = "Baysian inference for ordered response data with a dynamic
               spatial-ordered probit model",
  author    = "Wang, Xiaokun and Kockelman, Kara M",
  journal   = "J. Reg. Sci.",
  publisher = "Wiley Online Library",
  volume    =  49,
  number    =  5,
  pages     = "877--913",
  year      =  2009
}

@ARTICLE{Yoon2011-ha,
  title    = "Bayesian inference for an adaptive Ordered Probit model: an
              application to Brain Computer Interfacing",
  author   = "Yoon, Ji Won and Roberts, Stephen J and Dyson, Mathew and Gan,
              John Q",
  abstract = "This paper proposes an algorithm for adaptive, sequential
              classification in systems with unknown labeling errors, focusing
              on the biomedical application of Brain Computer Interfacing
              (BCI). The method is shown to be robust in the presence of label
              and sensor noise. We focus on the inference and prediction of
              target labels under a nonlinear and non-Gaussian model. In order
              to handle missing or erroneous labeling, we model observed labels
              as a noisy observation of a latent label set with multiple
              classes ($\geq$ 2). Whilst this paper focuses on the method's
              application to BCI systems, the algorithm has the potential to be
              applied to many application domains in which sequential missing
              labels are to be imputed in the presence of uncertainty. This
              dynamic classification algorithm combines an Ordered Probit model
              and an Extended Kalman Filter (EKF). The EKF estimates the
              parameters of the Ordered Probit model sequentially with time. We
              test the performance of the classification approach by processing
              synthetic datasets and real experimental EEG signals with
              multiple classes (2, 3 and 4 labels) for a Brain Computer
              Interfacing (BCI) experiment.",
  journal  = "Neural Netw.",
  volume   =  24,
  number   =  7,
  pages    = "726--734",
  month    =  sep,
  year     =  2011,
  language = "en"
}

@PHDTHESIS{Constantinou2013-yr,
  title  = "Bayesian networks for prediction, risk assessment and decision
            making in an inefficient association football gambling market",
  author = "Constantinou, Anthony Costa",
  year   =  2013,
  school = "Queen Mary, University of London"
}

@ARTICLE{Oreski2012-qx,
  title    = "Hybrid system with genetic algorithm and artificial neural
              networks and its application to retail credit risk assessment",
  author   = "Oreski, Stjepan and Oreski, Dijana and Oreski, Goran",
  abstract = "The databases of the banks around the world have accumulated
              large quantities of information about clients and their financial
              and payment history. These databases can be used for the credit
              risk assessment, but they are commonly high dimensional.
              Irrelevant features in a training dataset may produce less
              accurate results of classification analysis. Data preprocessing
              is required to prepare the data for classification to increase
              the predictive accuracy. Feature selection is a preprocessing
              technique commonly used on high dimensional data and its purposes
              include reducing dimensionality, removing irrelevant and
              redundant features, facilitating data understanding, reducing the
              amount of data needed for learning, improving predictive accuracy
              of algorithms, and increasing interpretability of models. In this
              paper we investigate the extent to which the total data, owned by
              a bank, can be a good basis for predicting the borrower's ability
              to repay the loan on time. We propose a feature selection
              technique for finding an optimum feature subset that enhances the
              classification accuracy of neural network classifiers.
              Experiments were conducted on the credit dataset collected at a
              Croatian bank to assess the accuracy of our technique. We found
              that the hybrid system with genetic algorithm is competitive and
              can be used as feature selection technique to discover the most
              significant features in determining risk of default.",
  journal  = "Expert Syst. Appl.",
  volume   =  39,
  number   =  16,
  pages    = "12605--12617",
  month    =  nov,
  year     =  2012,
  keywords = "Classification; Credit scoring; Neural network; Genetic
              algorithm; Feature selection"
}

@ARTICLE{Hammerschmid2018-so,
  title     = "Regime shifts and stock return predictability",
  author    = "Hammerschmid, Regina and Lohre, Harald",
  abstract  = "Identifying economic regimes is useful in a world of
               time-varying risk premia. We apply regime switching models to
               common factors proxying for the macroeconomic regime and show
               that the ensuing regime factor is relevant in forecasting the
               equity risk premium. Moreover, the relevance of this regime
               factor is preserved in the presence of fundamental variables and
               technical indicators which are known to predict equity risk
               premia. Based on multiple predictive regressions and pooled
               forecasts, the macroeconomic regime factor is deemed
               complementary relative to the fundamental and technical
               information sets. Finally, these forecasts exhibit significant
               out-of-sample predictability that ultimately translates into
               considerable utility gains in a mean-variance portfolio
               strategy.",
  journal   = "International Review of Economics \& Finance",
  publisher = "Elsevier",
  volume    =  56,
  pages     = "138--160",
  month     =  jul,
  year      =  2018,
  keywords  = "Return predictability; Regime switching; Predictive regressions"
}

@ARTICLE{Goodell2020-nn,
  title    = "Election uncertainty, economic policy uncertainty and financial
              market uncertainty: A prediction market analysis",
  author   = "Goodell, John W and McGee, Richard J and McGroarty, Frank",
  abstract = "We examine the relationship between election uncertainty,
              economic policy uncertainty, and financial market uncertainty in
              a prediction-market analysis, covering seven US presidential
              election campaigns. We argue theoretically that changes in the
              incumbent party re-election probability should be a key driver of
              changes in policy uncertainty. Consistent with this theory, we
              find that a large portion of changes in financial uncertainty in
              the final stages of election campaign seasons is explained by
              changes in the probability of the incumbent party getting
              re-elected. Our findings suggest that the incumbent-party
              election probability, derived from prediction markets, is an
              important measure of economic policy uncertainty in the days
              leading up to US elections.",
  journal  = "Journal of Banking \& Finance",
  volume   =  110,
  pages    = "105684",
  month    =  jan,
  year     =  2020,
  keywords = "Political uncertainty; Economic policy uncertainty; Financial
              uncertainty; Election uncertainty; Prediction markets"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Pastor2012-eq,
  title    = "Uncertainty about Government Policy and Stock Prices",
  author   = "P{\'a}stor, LÃÜubosÃÜ and Veronesi, Pietro",
  abstract = "ABSTRACT We analyze how changes in government policy affect stock
              prices. Our general equilibrium model features uncertainty about
              government policy and a government whose decisions have both
              economic and noneconomic motives. The model makes numerous
              empirical predictions. Stock prices should fall at the
              announcement of a policy change, on average. The price decline
              should be large if uncertainty about government policy is large,
              and also if the policy change is preceded by a short or shallow
              economic downturn. Policy changes should increase volatilities
              and correlations among stocks. The jump risk premium associated
              with policy decisions should be positive, on average.",
  journal  = "J. Finance",
  volume   =  67,
  number   =  4,
  pages    = "1219--1264",
  month    =  aug,
  year     =  2012
}

@ARTICLE{Liu2015-sb,
  title    = "Economic policy uncertainty and stock market volatility",
  author   = "Liu, Li and Zhang, Tao",
  abstract = "This paper investigates the predictability of economic policy
              uncertainty (EPU) to stock market volatility. Our in-sample
              evidence suggests that higher EPU leads to significant increases
              in market volatility. Out-of-sample findings show that
              incorporating EPU as an additional predictive variable into the
              existing volatility prediction models significantly improves
              forecasting ability of these models. The improvement is robust to
              the model specifications.",
  journal  = "Finance Research Letters",
  volume   =  15,
  pages    = "99--105",
  month    =  nov,
  year     =  2015,
  keywords = "Economic policy uncertainty; Realized volatility; Predictability"
}

@ARTICLE{Orlov2015-rc,
  title     = "The effect of clubs' bargaining power on football player's
               transfer value",
  author    = "Orlov, Denis and {Others}",
  journal   = "Applied Econometrics",
  publisher = "Publishing House`` SINERGIA PRESS''",
  volume    =  39,
  number    =  3,
  pages     = "45--64",
  year      =  2015
}

@BOOK{Sebastian2015-uh,
  title     = "Model Risk In Financial Markets: From Financial Engineering To
               Risk Management",
  author    = "Sebastian, Tunaru Radu",
  abstract  = "The financial systems in most developed countries today build up
               a large amount of model risk on a daily basis. However, this is
               not particularly visible as the financial risk management agenda
               is still dominated by the subprime-liquidity crisis, the
               sovereign crises, and other major political events. Losses
               caused by model risk are hard to identify and even when they are
               internally identified, as such, they are most likely to be
               classified as normal losses due to market evolution.Model Risk
               in Financial Markets: From Financial Engineering to Risk
               Management seeks to change the current perspective on model
               innovation, implementation and validation. This book presents a
               wide perspective on model risk related to financial markets,
               running the gamut from financial engineering to risk management,
               from financial mathematics to financial statistics. It combines
               theory and practice, both the classical and modern concepts
               being introduced for financial modelling. Quantitative finance
               is a relatively new area of research and much has been written
               on various directions of research and industry applications. In
               this book the reader gradually learns to develop a critical view
               on the fundamental theories and new models being proposed.",
  publisher = "World Scientific",
  month     =  jun,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Schrimpf2010-he,
  title    = "International stock return predictability under model uncertainty",
  author   = "Schrimpf, Andreas",
  abstract = "This paper examines return predictability when the investor is
              uncertain about the right state variables. A novel feature of the
              model averaging approach used in this paper is to account for
              finite-sample bias of the coefficients in the predictive
              regressions. Drawing on an extensive international dataset, we
              find that interest-rate related variables are usually among the
              most prominent predictive variables, whereas valuation ratios
              perform rather poorly. Yet, predictability of market excess
              returns weakens substantially, once model uncertainty is
              accounted for. We document notable differences in the degree of
              in-sample and out-of-sample predictability across different stock
              markets. Overall, these findings suggest that return
              predictability is neither a uniform, nor a universal feature
              across international capital markets.",
  journal  = "J. Int. Money Finance",
  volume   =  29,
  number   =  7,
  pages    = "1256--1282",
  month    =  nov,
  year     =  2010,
  keywords = "Stock return predictability; Bayesian model averaging; Model
              uncertainty; International stock markets"
}

@ARTICLE{Kelly2019-zo,
  title    = "Characteristics are covariances: A unified model of risk and
              return",
  author   = "Kelly, Bryan T and Pruitt, Seth and Su, Yinan",
  abstract = "We propose a new modeling approach for the cross section of
              returns. Our method, Instrumented Principal Component Analysis
              (IPCA), allows for latent factors and time-varying loadings by
              introducing observable characteristics that instrument for the
              unobservable dynamic loadings. If the characteristics/expected
              return relationship is driven by compensation for exposure to
              latent risk factors, IPCA will identify the corresponding latent
              factors. If no such factors exist, IPCA infers that the
              characteristic effect is compensation without risk and allocates
              it to an ``anomaly'' intercept. Studying returns and
              characteristics at the stock-level, we find that five IPCA
              factors explain the cross section of average returns
              significantly more accurately than existing factor models and
              produce characteristic-associated anomaly intercepts that are
              small and statistically insignificant. Furthermore, among a large
              collection of characteristics explored in the literature, only
              ten are statistically significant at the 1\% level in the IPCA
              specification and are responsible for nearly 100\% of the model's
              accuracy.",
  journal  = "J. financ. econ.",
  volume   =  134,
  number   =  3,
  pages    = "501--524",
  month    =  dec,
  year     =  2019,
  keywords = "Cross section of returns; Latent factors; Anomaly; Factor model;
              Conditional betas; PCA; BARRA"
}

@INCOLLECTION{Patel2016-bg,
  title     = "A Probabilistic Framework for Deep Learning",
  booktitle = "Advances in Neural Information Processing Systems 29",
  author    = "Patel, Ankit B and Nguyen, Minh Tan and Baraniuk, Richard",
  editor    = "Lee, D D and Sugiyama, M and Luxburg, U V and Guyon, I and
               Garnett, R",
  publisher = "Curran Associates, Inc.",
  pages     = "2558--2566",
  year      =  2016
}

@ARTICLE{Greenland2016-ko,
  title    = "Statistical tests, {P} values, confidence intervals, and power: a
              guide to misinterpretations",
  author   = "Greenland, Sander and Senn, Stephen J and Rothman, Kenneth J and
              Carlin, John B and Poole, Charles and Goodman, Steven N and
              Altman, Douglas G",
  abstract = "Misinterpretation and abuse of statistical tests, confidence
              intervals, and statistical power have been decried for decades,
              yet remain rampant. A key problem is that there are no
              interpretations of these concepts that are at once simple,
              intuitive, correct, and foolproof. Instead, correct use and
              interpretation of these statistics requires an attention to
              detail which seems to tax the patience of working scientists.
              This high cognitive demand has led to an epidemic of shortcut
              definitions and interpretations that are simply wrong, sometimes
              disastrously so-and yet these misinterpretations dominate much of
              the scientific literature. In light of this problem, we provide
              definitions and a discussion of basic statistics that are more
              general and critical than typically found in traditional
              introductory expositions. Our goal is to provide a resource for
              instructors, researchers, and consumers of statistics whose
              knowledge of statistical theory and technique may be limited but
              who wish to avoid and spot misinterpretations. We emphasize how
              violation of often unstated analysis protocols (such as selecting
              analyses for presentation based on the P values they produce) can
              lead to small P values even if the declared test hypothesis is
              correct, and can lead to large P values even if that hypothesis
              is incorrect. We then provide an explanatory list of 25
              misinterpretations of P values, confidence intervals, and power.
              We conclude with guidelines for improving statistical
              interpretation and reporting.",
  journal  = "Eur. J. Epidemiol.",
  volume   =  31,
  number   =  4,
  pages    = "337--350",
  month    =  apr,
  year     =  2016,
  keywords = "Confidence intervals; Hypothesis testing; Null testing; P value;
              Power; Significance tests; Statistical testing",
  language = "en"
}

@ARTICLE{Biau2008-rr,
  title    = "Consistency of Random Forests and Other Averaging Classifiers",
  author   = "Biau, G{\'e}rard and Devroye, Luc and Lugosi, G{\'a}bor",
  journal  = "J. Mach. Learn. Res.",
  volume   =  9,
  number   = "Sep",
  pages    = "2015--2033",
  year     =  2008
}

@ARTICLE{Kim2004-tq,
  title    = "Managing loan customers using misclassification patterns of
              credit scoring model",
  author   = "Kim, Yoon Seong and Sohn, So Young",
  abstract = "A number of credit scoring models have been developed to evaluate
              credit risk of new loan applicants and existing loan customers,
              respectively. This study proposes a method to manage existing
              customers by using misclassification patterns of credit scoring
              model. We divide two groups of customers, the currently good and
              bad credit customers, into two subgroups, respectively, according
              to whether their credit status is misclassified or not by the
              neural network model. In addition, we infer the characteristics
              of each subgroup and propose management strategies corresponding
              to each subgroup.",
  journal  = "Expert Syst. Appl.",
  volume   =  26,
  number   =  4,
  pages    = "567--573",
  month    =  may,
  year     =  2004,
  keywords = "Credit scoring; Misclassification; Segmentation"
}

@ARTICLE{Tkac2016-tl,
  title    = "Artificial neural networks in business: Two decades of research",
  author   = "Tk{\'a}{\v c}, Michal and Verner, Robert",
  abstract = "In recent two decades, artificial neural networks have been
              extensively used in many business applications. Despite the
              growing number of research papers, only few studies have been
              presented focusing on the overview of published findings in this
              important and popular area. Moreover, the majority of these
              reviews were introduced more than 15 years ago. The aim of this
              work is to expand the range of earlier surveys and provide a
              systematic overview of neural network applications in business
              between 1994 and 2015. We have covered a total of 412 articles
              and classified them according to the year of publication,
              application area, type of neural network, learning algorithm,
              benchmark method, citations and journal. Our investigation
              revealed that most of the research has aimed at financial
              distress and bankruptcy problems, stock price forecasting, and
              decision support, with special attention to classification tasks.
              Besides conventional multilayer feedforward network with gradient
              descent backpropagation, various hybrid networks have been
              developed in order to improve the performance of standard models.
              Even though neural networks have been established as well-known
              method in business, there is enormous space for additional
              research in order to improve their functioning and increase our
              understanding of this influential area.",
  journal  = "Appl. Soft Comput.",
  volume   =  38,
  pages    = "788--804",
  month    =  jan,
  year     =  2016,
  keywords = "Business; Finance; Neural networks; Review"
}

@UNPUBLISHED{Farboodi2018-no,
  title    = "Where Has All the Big Data Gone?",
  author   = "Farboodi, Maryam and Matray, Adrien and Veldkamp, Laura",
  abstract = "As ever more technology is deployed to process and transmit
              financial data, this could benefit society, by allowing capital
              to be allocated more efficiently. Recent work supports this
              notion. Bai, Philippon and Savov (2016) document an improvement
              in the ability of S\&P 500 equity prices to predict firms' future
              earnings. We show that most of this ``price informativeness''
              rise can be attributed to a size composition effect as S\&P 500
              firms are getting larger. In contrast, the average public firm's
              price information is deteriorating. Do these facts imply that big
              data failed to price assets more efficiently? To answer this
              question, we formulate a model of data-processing choices. We
              find that big data growth, in conjunction with a change in the
              relative size of firms, can trigger a decline in informativeness
              for smaller firms. The model also reveals how big data growth can
              masquerade itself as size composition. The implication is that
              ever-growing reams of financial data may be helping price assets
              more accurately. But this might not deliver financial efficiency
              benefits for the vast majority of firms.",
  month    =  apr,
  year     =  2018,
  keywords = "big data, information acquisition, efficient financial markets,
              price informativeness"
}

@ARTICLE{Claessens2017-lc,
  title     = "Global Banking: Recent Developments and Insights from Research",
  author    = "Claessens, Stijn",
  abstract  = "Abstract. Following recent crises, cross-border capital flows
               have declined considerably, and many advanced countries' banks
               are retrenching. At the same time,",
  journal   = "Rev Financ",
  publisher = "Oxford Academic",
  volume    =  21,
  number    =  4,
  pages     = "1513--1555",
  month     =  jul,
  year      =  2017
}

@ARTICLE{Vandenbussche2015-xd,
  title    = "Macroprudential Policies and Housing Prices: A New Database and
              Empirical Evidence for Central, Eastern, and Southeastern Europe:
              {MONEY}, {CREDIT} {AND} {BANKING}",
  author   = "Vandenbussche, J{\'e}r{\^o}me and Vogel, Ursula and Detragiache,
              Enrica",
  abstract = "Several countries in Central, Eastern, and Southeastern Europe
              used a rich set of prudential instruments during the recent
              credit and housing boom and bust cycles. We construct a
              comprehensive database of these policy measures covering 16
              countries at a quarterly frequency. We use this database to
              investigate whether the policy measures had an impact on housing
              price inflation. The measures that appeared to be effective were
              capital measures (minimum capital adequacy ratio, maximum ratio
              of lending to households to share capital) and nonstandard
              liquidity measures (marginal reserve requirements (MRRs) on
              foreign funding, MRRs linked to credit growth).",
  journal  = "J. Money Credit Bank.",
  volume   =  47,
  number   = "S1",
  pages    = "343--377",
  month    =  mar,
  year     =  2015
}

@ARTICLE{Andries2017-gr,
  title    = "Effects of Macroprudential Policy on Systemic Risk and Bank Risk
              Taking",
  author   = "Andries, Alin Marius and Melnic, Florentina and Nistor Mutu,
              Simona",
  abstract = "Using an international sample of 95 banks from 21 European and
              North American countries spanning from 2008 to 2014, this paper
              assesses the effectiveness of a large set of general and housing
              macro-prudential policies in controlling banks' systemic
              importance and risk-taking incentives. Empirical findings
              indicate that tightening the general capital requirements, sector
              specific capital buffers, along with housing countercyclical
              capital requirements and Debt-Service-to-Income lending criteria
              significantly reduce banks' contribution to systemic risk and
              their individual risk-taking. A similar effect has been obtained
              for loosening real estate loans loss provisioning. Furthermore,
              the nexus between macroprudential policies and banks' risk is
              shaped through several channels like bank size, the share of
              foreign bank assets, banking sector competition and the
              independence of supervisory authority.",
  journal  = "Czech Journal of Economics and Finance",
  month    =  aug,
  year     =  2017,
  keywords = "macroprudential policy, systemic risk, bank risk taking"
}

@INCOLLECTION{Griffin2011-tr,
  title     = "Flexible and Nonparametric Modeling",
  booktitle = "The Oxford Handbook of Bayesian Econometrics",
  author    = "Griffin, Jim and Quintana, Fernando and Steel, Mark",
  editor    = "Geweke, John and Koop, Gary and Van Dijk, Herman",
  abstract  = "This article is divided into two parts. The first part considers
               flexible parametric models while the latter is nonparametric. It
               gives applications to regional growth data and semi parametric
               estimation of binomial proportions. It reviews methods for
               flexible mean regression, using either basis functions or
               Gaussian processes. This article also discusses Dirichlet
               processes and describes various posterior simulation algorithms
               for Bayesian nonparametric models. Usefulness is shown in
               empirical illustrations. Various applications as a function of
               income and as a cost function for electricity distribution are
               discussed. This article lists some freely available software
               that can accommodate many of the methods discussed. It provides
               a detailed discussion of both theory and computation for
               flexible treatment of distributions or functional forms or both.",
  publisher = "Oxford University Press",
  month     =  sep,
  year      =  2011,
  keywords  = "flexible parametric models; Gaussian processes; Dirichlet
               processes; nonparametric models"
}

@INCOLLECTION{Geweke2011-vc,
  title     = "Introduction",
  booktitle = "The Oxford Handbook of Bayesian Econometrics",
  author    = "Geweke, John and Koop, Gary and Van Dijk, Herman",
  editor    = "Geweke, John and Koop, Gary and Van Dijk, Herman",
  abstract  = "This article deals with substantial computation component and
               discusses powerful computers and simulation algorithms that lead
               to model development where Bayesian econometric methods are
               predominant. It includes flexible or nonparametric models. It
               also distinguishes econometrics from statistics by its
               combination of economic theory with statistics. This article
               addresses principles, methods, and applications in different
               parts. It deals with a set of issues namely, the use of
               computationally intensive posterior simulation algorithms,
               heterogeneity, and problems caused by proliferation of
               parameters. The models discussed in this article have increased
               range and level of complication. They are strongly infused with
               economic theory and decision-theoretic issues. This article
               covers a broad range of the methods and models used by Bayesian
               econometricians in a wide variety of fields.",
  publisher = "Oxford University Press",
  month     =  sep,
  year      =  2011,
  keywords  = "computation component; simulation algorithms; nonparametric
               models; economic theory; Bayesian econometricians"
}

@ARTICLE{Granger1992-du,
  title    = "Forecasting stock market prices: Lessons for forecasters",
  author   = "Granger, Clive W J",
  abstract = "In recent years a variety of models which apparently forecast
              changes in stock market prices have been introduced. Some of
              these are summarised and interpreted. Nonlinear models are
              particularly discussed, with a switching regime, from
              forecastable to non-forecastable, the switch depending on
              volatility levels, relative earnings/price ratios, size of
              company, and calendar effects. There appear to be benefits from
              disaggregation and for searching for new causal variables. The
              possible lessons for forecasters are emphasised and the relevance
              for the Efficient Market Hypothesis is discussed.",
  journal  = "Int. J. Forecast.",
  volume   =  8,
  number   =  1,
  pages    = "3--13",
  month    =  jun,
  year     =  1992,
  keywords = "Forecastability; Stock returns; Non-linear models; Efficient
              markets"
}

@MISC{Pierdzioch2012-tz,
  title   = "Forecasting stock prices: Do forecasters herd?",
  author  = "Pierdzioch, Christian and R{\"u}lke, Jan-Christoph",
  journal = "Economics Letters",
  volume  =  116,
  number  =  3,
  pages   = "326--329",
  year    =  2012
}

@MISC{Cowles1933-ye,
  title   = "Can Stock Market Forecasters Forecast?",
  author  = "Cowles, Alfred",
  journal = "Econometrica",
  volume  =  1,
  number  =  3,
  pages   = "309",
  year    =  1933
}

@MISC{Umstead1977-gt,
  title   = "{FORECASTING} {STOCK} {MARKET} {PRICES}",
  author  = "Umstead, David A",
  journal = "The Journal of Finance",
  volume  =  32,
  number  =  2,
  pages   = "427--441",
  year    =  1977
}

@ARTICLE{noauthor_2010-aa,
  title   = "Editorial Board",
  journal = "Int. J. Forecast.",
  volume  =  26,
  number  =  2,
  pages   = "iii",
  month   =  apr,
  year    =  2010
}

@ARTICLE{Geweke2010-vq,
  title    = "Comment",
  author   = "Geweke, John",
  abstract = "The article by Zellner and Ando proposes methods for coping with
              the excess kurtosis that is often observed in disturbances in
              applications of the seemingly unrelated regressions (SUR) model.
              This is an important topic which is of particular relevance in
              forecasting. However, the proposed methods do not address the
              problem: the direct Monte Carlo (DMC) algorithm is incorrect and
              the proposed variant of the Student-t distribution cannot account
              for thick tails in the distribution of disturbances in the SUR
              model.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "435--438",
  month    =  apr,
  year     =  2010
}

@ARTICLE{Zellner2010-th,
  title   = "Rejoinder",
  author  = "Zellner, Arnold and Ando, Tomohiro",
  journal = "Int. J. Forecast.",
  volume  =  26,
  number  =  2,
  pages   = "439--442",
  month   =  apr,
  year    =  2010
}

@ARTICLE{noauthor_2010-oo,
  title   = "International Institute of Forecasters and {SAS\textregistered{}}
             research grants",
  journal = "Int. J. Forecast.",
  volume  =  26,
  number  =  2,
  pages   = "443",
  month   =  apr,
  year    =  2010
}

@ARTICLE{Griffiths2010-se,
  title    = "Predictive densities for models with stochastic regressors and
              inequality constraints: Forecasting local-area wheat yield",
  author   = "Griffiths, William E and Newton, Lisa S and O'Donnell,
              Christopher J",
  abstract = "Forecasts from regression models are frequently made conditional
              on a set of values for the regressor variables. We describe and
              illustrate how to obtain forecasts when some of those regressors
              are stochastic and their values have not yet been realized. The
              forecasting device is a Bayesian predictive density which
              accommodates variability from an unknown error term, uncertainty
              from unknown coefficients, and uncertainty from unknown
              stochastic regressors. We illustrate how the predictive density
              of a forecast changes as more regressors are observed and
              therefore fewer are unobserved. An example where the local-area
              wheat yield depends on the rainfall during three
              periods--germination, growing and flowering--is used to
              illustrate the methods. Both a noninformative prior and a prior
              with inequality restrictions on the regression coefficients are
              considered. The results show how the predictive density changes
              as more rainfall information becomes available.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "397--412",
  month    =  apr,
  year     =  2010,
  keywords = "Bayesian forecasting; Inequality restrictions; Random regressors;
              Rainfall distributions; Truncated distributions"
}

@ARTICLE{Jochmann2010-he,
  title    = "Bayesian forecasting using stochastic search variable selection
              in a {VAR} subject to breaks",
  author   = "Jochmann, Markus and Koop, Gary and Strachan, Rodney W",
  abstract = "This paper builds a model which has two extensions over a
              standard VAR. The first of these is stochastic search variable
              selection, which is an automatic model selection device that
              allows coefficients in a possibly over-parameterized VAR to be
              set to zero. The second extension allows for an unknown number of
              structural breaks in the VAR parameters. We investigate the
              in-sample and forecasting performance of our model in an
              application involving a commonly-used US macroeconomic data set.
              In a recursive forecasting exercise, we find moderate
              improvements over a standard VAR, although most of these
              improvements are due to the use of stochastic search variable
              selection rather than to the inclusion of breaks.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "326--347",
  month    =  apr,
  year     =  2010,
  keywords = "Vector autoregressive model; Predictive density;
              Over-parameterization; Structural break; Shrinkage"
}

@ARTICLE{Rodriguez2010-ek,
  title    = "Mixed frequency models: Bayesian approaches to estimation and
              prediction",
  author   = "Rodriguez, Abel and Puggioni, Gavino",
  abstract = "We describe Bayesian models for economic and financial time
              series that use regressors sampled at higher frequencies than the
              outcome of interest. The models are developed within the
              framework of dynamic linear models, which provides a high level
              of flexibility and allows direct interpretation of the results.
              The problem of the collinearity of intraperiod observations is
              solved using model selection and model averaging approaches.
              Bayesian approaches to model selection automatically adjust for
              multiple comparisons, while predictions based on model averaging
              allow us to account for both model and parameter uncertainty when
              predicting future observations. A novel aspect of the models
              presented here is the introduction of new formulations for the
              prior distribution on the model space that allow us to favor
              sparse models where the significant coefficients cluster on
              adjacent lags of the high frequency predictor. We illustrate our
              approach by predicting the gross national product of the United
              States using the term structure of interest rates.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "293--311",
  month    =  apr,
  year     =  2010,
  keywords = "Mixed frequency data; Model selection; Model averaging; Interest
              rates; Gross national product"
}

@ARTICLE{Giordani2010-gx,
  title    = "Forecasting macroeconomic time series with locally adaptive
              signal extraction",
  author   = "Giordani, Paolo and Villani, Mattias",
  abstract = "We introduce a non-Gaussian dynamic mixture model for
              macroeconomic forecasting. The locally adaptive signal extraction
              and regression (LASER) model is designed to capture relatively
              persistent AR processes (signal) which are contaminated by high
              frequency noise. The distributions of the innovations in both
              noise and signal are modeled robustly using mixtures of normals.
              The mean of the process and the variances of the signal and noise
              are allowed to shift either suddenly or gradually at unknown
              locations and unknown numbers of times. The model is then capable
              of capturing movements in the mean and conditional variance of a
              series, as well as in the signal-to-noise ratio. Four versions of
              the model are estimated by Bayesian methods and used to forecast
              a total of nine quarterly macroeconomic series from the US,
              Sweden and Australia. We observe that allowing for infrequent and
              large parameter shifts while imposing normal and homoskedastic
              errors often leads to erratic forecasts, but that the model
              typically forecasts well if it is made more robust by allowing
              for non-normal errors and time varying variances. Our main
              finding is that, for the nine series we analyze, specifications
              with infrequent and large shifts in error variances outperform
              both fixed parameter specifications and smooth, continuous shifts
              when it comes to interval coverage.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "312--325",
  month    =  apr,
  year     =  2010,
  keywords = "Bayesian inference; Forecast evaluation; Regime switching; State
              space modeling; Dynamic mixture models"
}

@ARTICLE{Beechey2010-mb,
  title    = "Forecasting inflation in an inflation-targeting regime: A role
              for informative steady-state priors",
  author   = "Beechey, Meredith and {\"O}sterholm, P{\"a}r",
  abstract = "Inflation targeting as a monetary-policy regime is widely
              associated with an explicit numerical target for the rate of
              inflation. This paper investigates whether the forecasting
              performance of Bayesian autoregressive models can be improved by
              incorporating information about the target. We compare a
              mean-adjusted specification, which allows an informative prior on
              the distribution for the steady state of the process, to
              traditional methodology. We find that the out-of-sample forecasts
              of the mean-adjusted autoregressive model outperform those of the
              traditional specification, often by non-trivial amounts, for five
              early adopters of inflation targeting. It is also noted that as
              the sample lengthens, the posterior distribution of steady-state
              inflation narrows more for countries with explicit point targets.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "248--264",
  month    =  apr,
  year     =  2010,
  keywords = "Monetary policy; Central bank preferences"
}

@ARTICLE{Zellner2010-ip,
  title    = "Bayesian and non-Bayesian analysis of the seemingly unrelated
              regression model with Student-t errors, and its application for
              forecasting",
  author   = "Zellner, Arnold and Ando, Tomohiro",
  abstract = "A description of computationally efficient methods for the
              Bayesian analysis of Student-t seemingly unrelated regression
              (SUR) models with unknown degrees of freedom is given. The method
              combines a direct Monte Carlo (DMC) approach with an importance
              sampling procedure to calculate Bayesian estimation and
              prediction results using a diffuse prior. This approach is
              employed to compute Bayesian posterior densities for the
              parameters, as well as predictive densities for future values of
              variables and the associated moments, intervals and other
              quantities that are useful to both forecasters and others. The
              results obtained using our approach are compared to those yielded
              by the use of DMC for a standard normal SUR model.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "413--434",
  month    =  apr,
  year     =  2010,
  keywords = "Direct Monte Carlo; Heavy tail behavior; Importance sampling;
              Markov chain Monte Carlo"
}

@ARTICLE{Geweke2010-op,
  title    = "Comparing and evaluating Bayesian predictive distributions of
              asset returns",
  author   = "Geweke, John and Amisano, Gianni",
  abstract = "Bayesian inference in a time series model provides exact
              out-of-sample predictive distributions that fully and coherently
              incorporate parameter uncertainty. This study compares and
              evaluates Bayesian predictive distributions from alternative
              models, using as an illustration five alternative models of asset
              returns applied to daily S\&P 500 returns from the period 1976
              through 2005. The comparison exercise uses predictive likelihoods
              and is inherently Bayesian. The evaluation exercise uses the
              probability integral transformation and is inherently
              frequentist. The illustration shows that the two approaches can
              be complementary, with each identifying strengths and weaknesses
              in models that are not evident using the other.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "216--230",
  month    =  apr,
  year     =  2010,
  keywords = "Forecasting; GARCH; Inverse probability transformation; Markov
              mixture; Predictive likelihood; S\&P 500 returns; Stochastic
              volatility"
}

@ARTICLE{Yelland2010-iq,
  title    = "Bayesian forecasting of parts demand",
  author   = "Yelland, Phillip M",
  abstract = "As supply chains for high technology products increase in
              complexity, and as the performance expectations of these supply
              chains also increase, forecasts of parts demands have become
              indispensable to effective operations management in these
              markets. Unfortunately, rapid technological change and an
              abundance of product configurations mean that the demand for
              parts in high-tech products is frequently volatile and hard to
              forecast. The paper describes a Bayesian statistical model which
              was developed to forecast the parts demand for Sun Microsystems,
              Inc., a major vendor of enterprise computer products. The model
              embodies a parametric description of the part's life cycle,
              allowing it to anticipate changes in demand over time.
              Furthermore, using hierarchical priors, the model is able to pool
              demand patterns for a collection of parts, producing calibrated
              forecasts for new parts with little or no demand history. The
              paper discusses the problem addressed by the model, the model
              itself, and a procedure for calibrating it, then compares its
              forecast performance with those of alternatives.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "374--396",
  month    =  apr,
  year     =  2010,
  keywords = "Bayesian methods; Demand forecasting; Forecasting practice; State
              space models; Supply chain"
}

@ARTICLE{Schorfheide2010-id,
  title    = "{DSGE} model-based forecasting of non-modelled variables",
  author   = "Schorfheide, Frank and Sill, Keith and Kryshko, Maxym",
  abstract = "This paper develops and illustrates a simple method of generating
              a DSGE model-based forecast for variables that do not explicitly
              appear in the model (non-core variables). We use auxiliary
              regressions that resemble measurement equations in a dynamic
              factor model to link the non-core variables to the state
              variables of the DSGE model. Predictions for the non-core
              variables are obtained by applying their measurement equations to
              DSGE model-generated forecasts of the state variables. Using a
              medium-scale New Keynesian DSGE model, we apply our approach to
              generate and evaluate recursive forecasts for PCE inflation, core
              PCE inflation, the unemployment rate, and housing starts, along
              with predictions for the seven variables that have been used to
              estimate the DSGE model.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "348--373",
  month    =  apr,
  year     =  2010,
  keywords = "Bayesian methods; DSGE models; Econometric models; Evaluating
              forecasts; Macroeconomic forecasting"
}

@ARTICLE{Hoogerheide2010-pl,
  title    = "Bayesian forecasting of Value at Risk and Expected Shortfall
              using adaptive importance sampling",
  author   = "Hoogerheide, Lennart and van Dijk, Herman K",
  abstract = "An efficient and accurate approach is proposed for forecasting
              the Value at Risk (VaR) and Expected Shortfall (ES) measures in a
              Bayesian framework. This consists of a new adaptive importance
              sampling method for the Quick Evaluation of Risk using Mixture of
              t approximations (QERMit). As a first step, the optimal
              importance density is approximated, after which multi-step `high
              loss' scenarios are efficiently generated. Numerical standard
              errors are compared in simple illustrations and in an empirical
              GARCH model with Student-t errors for daily S\&P 500 returns. The
              results indicate that the proposed QERMit approach outperforms
              alternative approaches, in the sense that it produces more
              accurate VaR and ES estimates given the same amount of computing
              time, or, equivalently, that it requires less computing time for
              the same numerical accuracy.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "231--247",
  month    =  apr,
  year     =  2010,
  keywords = "Value at Risk; Expected Shortfall; Numerical standard error;
              Importance sampling; Mixture of Student- distributions; Variance
              reduction technique"
}

@ARTICLE{Lahiri2010-gb,
  title    = "Learning and heterogeneity in {GDP} and inflation forecasts",
  author   = "Lahiri, Kajal and Sheng, Xuguang",
  abstract = "Using a Bayesian learning model with heterogeneity across agents,
              our study aims to identify the relative importance of alternative
              pathways through which professional forecasters disagree and
              reach consensus on the term structure of inflation and real GDP
              forecasts, resulting in different patterns of forecast accuracy.
              There are two primary sources of forecast disagreement in our
              model: differences in prior beliefs, and differences in the
              interpretation of new public information. Estimated model
              parameters, together with two separate case studies on (i) the
              dynamics of forecast disagreement in the aftermath of the 9/11
              terrorist attack in the US, and (ii) the successful inflation
              targeting experience of Italy after 1997, firmly establish the
              importance of these two pathways to expert disagreement, and help
              to explain the relative forecasting accuracy of these two
              macroeconomic variables.",
  journal  = "Int. J. Forecast.",
  volume   =  26,
  number   =  2,
  pages    = "265--292",
  month    =  apr,
  year     =  2010,
  keywords = "Bayesian learning; Public information; Panel data; Forecast
              disagreement; Forecast horizon; Forecast efficiency; GDP;
              Inflation targeting"
}

@ARTICLE{Frazier2019-dr,
  title    = "Approximate Bayesian forecasting",
  author   = "Frazier, David T and Maneesoonthorn, Worapree and Martin, Gael M
              and McCabe, Brendan P M",
  abstract = "Approximate Bayesian Computation (ABC) has become increasingly
              prominent as a method for conducting parameter inference in a
              range of challenging statistical problems, most notably those
              characterized by an intractable likelihood function. In this
              paper, we focus on the use of ABC not as a tool for parametric
              inference, but as a means of generating probabilistic forecasts;
              or for conducting what we refer to as `approximate Bayesian
              forecasting'. The four key issues explored are: (i) the link
              between the theoretical behavior of the ABC posterior and that of
              the ABC-based predictive; (ii) the use of proper scoring rules to
              measure the (potential) loss of forecast accuracy when using an
              approximate rather than an exact predictive; (iii) the
              performance of approximate Bayesian forecasting in state space
              models; and (iv) the use of forecasting criteria to inform the
              selection of ABC summaries in empirical settings. The primary
              finding of the paper is that ABC can provide a computationally
              efficient means of generating probabilistic forecasts that are
              nearly identical to those produced by the exact predictive, and
              in a fraction of the time required to produce predictions via an
              exact method.",
  journal  = "Int. J. Forecast.",
  volume   =  35,
  number   =  2,
  pages    = "521--539",
  month    =  apr,
  year     =  2019,
  keywords = "Bayesian prediction; Likelihood-free methods; Predictive merging;
              Proper scoring rules; Particle filtering; Jump-diffusion models"
}

@ARTICLE{Domit2019-ej,
  title    = "Forecasting the {UK} economy with a medium-scale Bayesian {VAR}",
  author   = "Domit, S{\'\i}lvia and Monti, Francesca and Sokol, Andrej",
  abstract = "We estimate a Bayesian VAR (BVAR) for the UK economy and assess
              its performance in forecasting GDP growth and CPI inflation in
              real time relative to forecasts from COMPASS, the Bank of
              England's DSGE model, and other benchmarks. We find that the BVAR
              outperformed COMPASS when forecasting both GDP and its
              expenditure components. In contrast, their performances when
              forecasting CPI were similar. We also find that the BVAR density
              forecasts outperformed those of COMPASS, despite under-predicting
              inflation at most forecast horizons. Both models over-predicted
              GDP growth at all forecast horizons, but the issue was less
              pronounced in the BVAR. The BVAR's point and density forecast
              performances are also comparable to those of a Bank of England
              in-house statistical suite for both GDP and CPI inflation, as
              well as to the official Inflation Report projections. Our results
              are broadly consistent with the findings of similar studies for
              other advanced economies.",
  journal  = "Int. J. Forecast.",
  volume   =  35,
  number   =  4,
  pages    = "1669--1678",
  month    =  oct,
  year     =  2019,
  keywords = "Macroeconomic forecasting; Bayesian methods; Vector
              autoregression models; Econometric models; Inflation forecasting"
}

@ARTICLE{Luo2020-ql,
  title    = "Realized volatility forecast with the Bayesian random compressed
              multivariate {HAR} model",
  author   = "Luo, Jiawen and Chen, Langnan",
  abstract = "We develop a Bayesian random compressed multivariate
              heterogeneous autoregressive (BRC-MHAR) model to forecast the
              realized covariance matrices of stock returns. The proposed model
              randomly compresses the predictors and reduces the number of
              parameters. We also construct several competing multivariate
              volatility models with the alternative shrinkage methods to
              compress the parameter's dimensions. We compare the forecast
              performances of the proposed models with the competing models
              based on both statistical and economic evaluations. The results
              of statistical evaluation suggest that the BRC-MHAR models have
              the better forecast precision than the competing models for the
              short-term horizon. The results of economic evaluation suggest
              that the BRC-MHAR models are superior to the competing models in
              terms of the average return, the Shape ratio and the economic
              value.",
  journal  = "Int. J. Forecast.",
  month    =  apr,
  year     =  2020,
  keywords = "Realized volatility forecast; Bayesian random compressed;
              Multivariate HAR model; Forecast precision evaluations; Economic
              evaluations"
}

@ARTICLE{Davig2019-cd,
  title    = "Recession forecasting using Bayesian classification",
  author   = "Davig, Troy and Hall, Aaron Smalter",
  abstract = "We demonstrate the use of a Na{\"\i}ve Bayes model as a recession
              forecasting tool. The approach is closely connected with
              Markov-switching models and logistic regression, but also has
              important differences. In contrast to Markov-switching models,
              our Na{\"\i}ve Bayes model treats National Bureau of Economic
              Research business cycle turning points as data, rather than as
              hidden states to be inferred by the model. Although Na{\"\i}ve
              Bayes and logistic regression are asymptotically equivalent under
              certain distributional assumptions, the assumptions do not hold
              for business cycle data. As a result, Na{\"\i}ve Bayes has a
              larger asymptotic error rate, but converges to the error rate
              more quickly than logistic regression, resulting in more accurate
              recession forecasts with limited data. We show that Na{\"\i}ve
              Bayes outperforms competing models and the Survey of Professional
              Forecasters consistently for real-time recession forecasting up
              to 12 months in advance. These results hold under standard error
              measures, and also under a novel measure that varies the penalty
              on false signals, depending on when they occur within a cycle;
              for example, a false signal in the middle of an expansion is
              penalized more heavily than one that occurs close to a turning
              point.",
  journal  = "Int. J. Forecast.",
  volume   =  35,
  number   =  3,
  pages    = "848--867",
  month    =  jul,
  year     =  2019
}

@ARTICLE{Lahiri2010-bj,
  title   = "Bayesian forecasting in economics",
  author  = "Lahiri, Kajal and Martin, Gael",
  journal = "Int. J. Forecast.",
  volume  =  26,
  number  =  2,
  pages   = "211--215",
  month   =  apr,
  year    =  2010
}

@ARTICLE{Gamson1964-lo,
  title     = "Scapegoating in Baseball",
  author    = "Gamson, William A and Scotch, Norman A",
  journal   = "Am. J. Sociol.",
  publisher = "The University of Chicago Press",
  volume    =  70,
  number    =  1,
  pages     = "69--72",
  month     =  jul,
  year      =  1964
}

@ARTICLE{Baliga1996-pe,
  title     = "{CEO} {DUALITY} {AND} {FIRM} {PERFORMANCE}: {WHAT'S} {THE}
               {FUSS}?",
  author    = "Baliga, B Ram and Moyer, R Charles and Rao, Ramesh S",
  abstract  = "Abstract Rising shareholder activism following poor corporate
               performance and a subsequent drop in shareholder value at many
               major U.S. corporations had rekindled interest in duality and
               corporate governance. Despite limited empirical evidence,
               duality (chairman of the board and CEO are the same individual)
               has been blamed, in many cases, for the poor performance, and
               failure of firms to adapt to a changing environment. In
               examining the relationship between duality and firm performance,
               this study considers the announcement effects of changes in
               duality status, accounting measures of operating performance for
               firms that have changed their duality structure, and long-term
               measures of performance for firms that have had a consistent
               history of a duality structure. Our results suggest that: (1)
               the market is indifferent to changes in a firm's duality status;
               (2) there is little evidence of operating performance changes
               around changes in duality status; and (3) there is only weak
               evidence that duality status affects long-term performance,
               after controlling for other factors that might impact that
               performance.",
  journal   = "Strategic Manage. J.",
  publisher = "John Wiley \& Sons, Ltd",
  volume    =  17,
  number    =  1,
  pages     = "41--53",
  month     =  jan,
  year      =  1996,
  keywords  = "duality; governance; performance"
}

@ARTICLE{Hambrick1992-wi,
  title     = "Top Team Deterioration as Part of the Downward Spiral of Large
               Corporate Bankruptcies",
  author    = "Hambrick, Donald C and D'Aveni, Richard A",
  abstract  = "This exploratory study of 57 large bankruptcies and 57 matched
               survivors examined the top management team (TMT) characteristics
               associated with major corporate failure. Prior research was used
               to guide selection of specific team characteristics for study.
               Not only did the failing firms show significant annual, or
               cross-sectional, divergence from survivors on several indicators
               of TMT composition, but also those divergences became more
               pronounced, even accelerating, over the last five years of the
               bankrupts' lives. The results thus suggest that deterioration of
               the top management team is a central element of the downward
               spiral of large corporate failures. Based upon a limited test of
               causality, the authors propose that a two-way process is at
               work: (1) team deficiencies bring about or aggravate corporate
               deterioration, either through strategic errors or stakeholder
               uneasiness with the flawed team; and (2) corporate deterioration
               brings about team deterioration, through a combination of
               voluntary departures, scapegoating, and limited resources for
               attracting new executive talent.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  38,
  number    =  10,
  pages     = "1445--1466",
  month     =  oct,
  year      =  1992
}

@MISC{Brown1982-cs,
  title   = "Administrative Succession and Organizational Performance: The
             Succession Effect",
  author  = "Brown, M Craig and Craig Brown, M",
  journal = "Administrative Science Quarterly",
  volume  =  27,
  number  =  1,
  pages   = "1",
  year    =  1982
}

@BOOK{Pfeffer2006-az,
  title     = "Hard Facts, Dangerous {Half-Truths}, and Total Nonsense:
               Profiting from Evidence-based Management",
  author    = "Pfeffer, Jeffrey and Sutton, Robert I",
  abstract  = "The best organizations have the best talent. . . Financial
               incentives drive company performance. . . Firms must change or
               die. Popular axioms like these drive business decisions every
               day. Yet too much common management ``wisdom'' isn't wise at
               all---but, instead, flawed knowledge based on ``best practices''
               that are actually poor, incomplete, or outright obsolete. Worse,
               legions of managers use this dubious knowledge to make decisions
               that are hazardous to organizational health.Jeffrey Pfeffer and
               Robert I. Sutton show how companies can bolster performance and
               trump the competition through evidence-based management, an
               approach to decision-making and action that is driven by hard
               facts rather than half-truths or hype. This book guides managers
               in using this approach to dismantle six widely held---but
               ultimately flawed---management beliefs in core areas including
               leadership, strategy, change, talent, financial incentives, and
               work-life balance. The authors show managers how to find and
               apply the best practices for their companies, rather than
               blindly copy what seems to have worked elsewhere.This practical
               and candid book challenges leaders to commit to evidence-based
               management as a way of organizational life---and shows how to
               finally turn this common sense into common practice.",
  publisher = "Harvard Business Press",
  month     =  feb,
  year      =  2006,
  language  = "en"
}

@MISC{Meyer2001-mu,
  title   = "{Performance-Driven} Organizational Change: The Organizational
             Portfolio",
  author  = "Meyer, Marshall W and Donaldson, Lex",
  journal = "Administrative Science Quarterly",
  volume  =  46,
  number  =  4,
  pages   = "776",
  year    =  2001
}

@BOOK{Donaldson1998-ab,
  title     = "{Performance-Driven} Organizational Change: The Organizational
               Portfolio",
  author    = "Donaldson, Lex",
  abstract  = "In this book, Lex Donaldson, one of the leading scholars in the
               field of organization theory, introduces a compelling theory of
               performance-driven organizational change, Organizational
               Portfolio Theory. In explaining why organizations change and
               also why they fail to change, the theory reconceptualizes the
               organization as a portfolio with a number of different causes of
               organizational performance varying over time. The author argues
               that without a performance crisis there is a good chance that
               necessary organizational changes will not be forthcoming, and
               that moreover, the adaptive change induced by the crisis creates
               the capacity for fresh organizational growth.",
  publisher = "SAGE Publications",
  month     =  oct,
  year      =  1998,
  language  = "en"
}

@MISC{Wagner2008-pk,
  title   = "Managerial Succession and Organizational Performance",
  author  = "Wagner, Stefan",
  journal = "SSRN Electronic Journal",
  year    =  2008
}

@ARTICLE{Audas1999-ck,
  title     = "Organizational performance and managerial turnover",
  author    = "Audas, Rick and Dobson, Stephen and Goddard, John",
  journal   = "Manage. Decis. Econ.",
  publisher = "Wiley Online Library",
  volume    =  20,
  number    =  6,
  pages     = "305--318",
  year      =  1999
}

@ARTICLE{Vehtari2015-ve,
  title         = "Practical Bayesian model evaluation using leave-one-out
                   cross-validation and {WAIC}",
  author        = "Vehtari, Aki and Gelman, Andrew and Gabry, Jonah",
  abstract      = "Leave-one-out cross-validation (LOO) and the widely
                   applicable information criterion (WAIC) are methods for
                   estimating pointwise out-of-sample prediction accuracy from
                   a fitted Bayesian model using the log-likelihood evaluated
                   at the posterior simulations of the parameter values. LOO
                   and WAIC have various advantages over simpler estimates of
                   predictive error such as AIC and DIC but are less used in
                   practice because they involve additional computational
                   steps. Here we lay out fast and stable computations for LOO
                   and WAIC that can be performed using existing simulation
                   draws. We introduce an efficient computation of LOO using
                   Pareto-smoothed importance sampling (PSIS), a new procedure
                   for regularizing importance weights. Although WAIC is
                   asymptotically equal to LOO, we demonstrate that PSIS-LOO is
                   more robust in the finite case with weak priors or
                   influential observations. As a byproduct of our
                   calculations, we also obtain approximate standard errors for
                   estimated predictive errors and for comparing of predictive
                   errors between two models. We implement the computations in
                   an R package called 'loo' and demonstrate using models fit
                   with the Bayesian inference package Stan.",
  month         =  jul,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "1507.04544"
}

@ARTICLE{Watanabe2010-ta,
  title    = "Asymptotic Equivalence of Bayes Cross Validation and Widely
              Applicable Information Criterion in Singular Learning Theory",
  author   = "Watanabe, Sumio",
  journal  = "J. Mach. Learn. Res.",
  volume   =  11,
  number   =  116,
  pages    = "3571--3594",
  year     =  2010
}

@ARTICLE{Epifani2008-dy,
  title     = "Case-deletion importance sampling estimators: Central limit
               theorems and related results",
  author    = "Epifani, Ilenia and MacEachern, Steven N and Peruggia, Mario",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Electron. J. Stat.",
  publisher = "The Institute of Mathematical Statistics and the Bernoulli
               Society",
  volume    =  2,
  pages     = "774--806",
  year      =  2008,
  keywords  = "Infinite Variance; Influence; Leverage; Marginal Residual Sum of
               Squares; Markov Chain Monte Carlo; Model Averaging; Moment
               Index; Tail Behavior",
  language  = "en"
}

@ARTICLE{Gelman2014-wp,
  title     = "Understanding predictive information criteria for Bayesian
               models",
  author    = "Gelman, Andrew and Hwang, Jessica and Vehtari, Aki",
  journal   = "Stat. Comput.",
  publisher = "Springer",
  volume    =  24,
  number    =  6,
  pages     = "997--1016",
  year      =  2014
}

@UNPUBLISHED{Farboodi2020-vt,
  title       = "Where Has All the Data Gone?",
  author      = "Farboodi, Maryam and Matray, Adrien and Veldkamp, Laura and
                 Venkateswaran, Venky",
  abstract    = "As financial technology improves and data becomes more
                 abundant, do market prices reflect this data growth? While
                 recent studies documented rises in the information content of
                 prices, we show that, across asset types, there is data
                 divergence. Large, growth stock prices increasingly reflect
                 information about future firm earnings. This is the rise
                 reflected in the previous studies. But over the same time
                 period, the information content of small and value firm prices
                 was flat or declining. Our structural estimation allows us to
                 disentangle these informational trends from changing asset
                 characteristics. These facts pose a new puzzle: Amidst the
                 explosion of data processing, why has this data informed only
                 the prices of a subset of firms, instead of benefiting the
                 market as a whole? Our structural model offers a potential
                 answer: Large growth firms' data grew in value, as big firms
                 got bigger and growth magnified the effect of these changes in
                 size.",
  number      =  26927,
  series      = "Working Paper Series",
  institution = "National Bureau of Economic Research",
  month       =  apr,
  year        =  2020
}

@ARTICLE{Doshi-Velez2017-lp,
  title         = "Towards A Rigorous Science of Interpretable Machine Learning",
  author        = "Doshi-Velez, Finale and Kim, Been",
  abstract      = "As machine learning systems become ubiquitous, there has
                   been a surge of interest in interpretable machine learning:
                   systems that provide explanation for their outputs. These
                   explanations are often used to qualitatively assess other
                   criteria such as safety or non-discrimination. However,
                   despite the interest in interpretability, there is very
                   little consensus on what interpretable machine learning is
                   and how it should be measured. In this position paper, we
                   first define interpretability and describe when
                   interpretability is needed (and when it is not). Next, we
                   suggest a taxonomy for rigorous evaluation and expose open
                   questions towards a more rigorous science of interpretable
                   machine learning.",
  month         =  feb,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1702.08608"
}

@BOOK{Molnar2020-ib,
  title     = "Interpretable Machine Learning",
  author    = "Molnar, Christoph",
  abstract  = "This book is about making machine learning models and their
               decisions interpretable. After exploring the concepts of
               interpretability, you will learn about simple, interpretable
               models such as decision trees, decision rules and linear
               regression. Later chapters focus on general model-agnostic
               methods for interpreting black box models like feature
               importance and accumulated local effects and explaining
               individual predictions with Shapley values and LIME. All
               interpretation methods are explained in depth and discussed
               critically. How do they work under the hood? What are their
               strengths and weaknesses? How can their outputs be interpreted?
               This book will enable you to select and correctly apply the
               interpretation method that is most suitable for your machine
               learning project.",
  publisher = "Lulu.com",
  month     =  feb,
  year      =  2020,
  language  = "en"
}

@ARTICLE{De_Moura2016-ra,
  title     = "A pairs trading strategy based on linear state space models and
               the Kalman filter",
  author    = "de Moura, Carlos Eduardo and Pizzinga, Adrian and Zubelli, Jorge",
  abstract  = "Among many strategies for financial trading, pairs trading has
               played an important role in practical and academic frameworks.
               Loosely speaking, it involves a statistical arbitrage tool for
               identifying and exploiting the inefficiencies of two long-term,
               related financial assets. When a significant deviation from this
               equilibrium is observed, a profit might result. In this paper,
               we propose a pairs trading strategy entirely based on linear
               state space models designed for modelling the spread formed with
               a pair of assets. Once an adequate state space model for the
               spread is estimated, we use the Kalman filter to calculate
               conditional probabilities that the spread will return to its
               long-term mean. The strategy is activated upon large values of
               these conditional probabilities: the spread is bought or sold
               accordingly. Two applications with real data from the US and
               Brazilian markets are offered, and even though they probably
               rely on limited evidence, they already indicate that a very
               basic portfolio consisting of a sole spread outperforms some of
               the main market benchmarks.",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  volume    =  16,
  number    =  10,
  pages     = "1559--1573",
  month     =  oct,
  year      =  2016
}

@ARTICLE{Clegg2018-oj,
  title     = "Pairs trading with partial cointegration",
  author    = "Clegg, Matthew and Krauss, Christopher",
  abstract  = "Partial cointegration is a weakening of cointegration that
               allows for the ?cointegrating? residual to contain a random walk
               and a mean-reverting component. We derive its representation in
               state space, provide a maximum likelihood-based estimation
               routine, and a suitable likelihood ratio test. Then, we explore
               the use of partial cointegration as a means for identifying
               promising pairs and for generating buy and sell signals.
               Specifically, we benchmark partial cointegration against several
               classical pairs trading variants from 1990 until 2015, on a
               survivor bias free data-set of the S\&P 500 constituents. We
               find annualized returns of more than 12\% after transaction
               costs. These results can only partially be explained by common
               sources of systematic risk and are well superior to classical
               distance-based or cointegration-based pairs trading variants on
               our data-set.",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  volume    =  18,
  number    =  1,
  pages     = "121--138",
  month     =  jan,
  year      =  2018
}

@ARTICLE{Fallahpour2016-ce,
  title     = "Pairs trading strategy optimization using the reinforcement
               learning method: a cointegration approach",
  author    = "Fallahpour, Saeid and Hakimian, Hasan and Taheri, Khalil and
               Ramezanifar, Ehsan",
  journal   = "Soft Computing",
  publisher = "Springer",
  volume    =  20,
  number    =  12,
  pages     = "5051--5066",
  year      =  2016
}

@INCOLLECTION{Giordani2011-nz,
  title     = "Bayesian Inference for Time Series State Space Models",
  booktitle = "The Oxford Handbook of Bayesian Econometrics",
  author    = "Giordani, Paolo and Pitt, Michael and Kohn, Robert",
  editor    = "Geweke, John and Koop, Gary and Van Dijk, Herman",
  abstract  = "This article provides a description of time series methods that
               emphasize modern macroeconomics and finance. It discusses a
               variety of posterior simulation algorithms and illustrates their
               use in a range of models. This article introduces the state
               space framework and explains the main ideas behind filtering,
               smoothing, and likelihood computation. It also mentions the
               particle filter as a general approach for estimating state space
               models and gives a brief discussion of its methods. The particle
               filter is a very useful tool in the Bayesian analysis of the
               kinds of complicated nonlinear state space models that are
               increasingly being used in macroeconomics. It also deals with
               conditionally Gaussian state space models and non-Gaussian state
               space models. A discussion of the advantages and disadvantages
               of each algorithm is provided in this article. This aims to help
               with the use of these methods in empirical work.",
  publisher = "Oxford University Press",
  month     =  sep,
  year      =  2011,
  keywords  = "macroeconomics; state space framework; Bayesian analysis;
               Gaussian state space models"
}

@ARTICLE{Chu2019-qu,
  title    = "A new online portfolio selection algorithm based on Kalman Filter
              and anti-correlation",
  author   = "Chu, Gang and Zhang, Wei and Sun, Guofeng and Zhang, Xiaotao",
  abstract = "In this paper, we consider both momentum and reversal in the
              original Anticor algorithm and propose a new online portfolio
              selection algorithm named the Wavelet de-noise Kalman Momentum
              anti-correlation algorithm (W-KACM), which can fully exploit the
              property of the price fluctuation. Our new strategy also employs
              a improved measure of the cyclically adjusted price relative
              called the Wavelet de-noise Kalman Filter price relative (WKFPR).
              WKFPR, unlike the raw price relative that measures only how much
              the price moves from one period to the next, measures how far the
              price deviates from the inherent trend value. To demonstrate the
              effectiveness of our strategy, we extensively simulate on
              previously untested real market datasets, including Chinese stock
              market datasets, and make comparison with AC and KACM algorithms.
              The results of these experiments indicate that our strategy
              significantly outperforms the Anticor and KACM algorithms without
              any additional model or computational complexity.",
  journal  = "Physica A: Statistical Mechanics and its Applications",
  volume   =  536,
  pages    = "120949",
  month    =  dec,
  year     =  2019,
  keywords = "Online portfolio selection; Wavelet de-noise; Kalman Filter;
              Anti-correlation; Mean reversion; Mean aversion"
}

@BOOK{Tsay2014-dx,
  title     = "An Introduction to Analysis of Financial Data with {R}",
  author    = "Tsay, Ruey S",
  abstract  = "A complete set of statistical tools for beginning financial
               analysts from a leading authority Written by one of the leading
               experts on the topic, An Introduction to Analysis of Financial
               Data with R explores basic concepts of visualization of
               financial data. Through a fundamental balance between theory and
               applications, the book supplies readers with an accessible
               approach to financial econometric models and their applications
               to real-world empirical research. The author supplies a hands-on
               introduction to the analysis of financial data using the freely
               available R software package and case studies to illustrate
               actual implementations of the discussed methods. The book begins
               with the basics of financial data, discussing their summary
               statistics and related visualization methods. Subsequent
               chapters explore basic time series analysis and simple
               econometric models for business, finance, and economics as well
               as related topics including: Linear time series analysis, with
               coverage of exponential smoothing for forecasting and methods
               for model comparison Different approaches to calculating asset
               volatility and various volatility models High-frequency
               financial data and simple models for price changes, trading
               intensity, and realized volatility Quantitative methods for risk
               management, including value at risk and conditional value at
               risk Econometric and statistical methods for risk assessment
               based on extreme value theory and quantile regression Throughout
               the book, the visual nature of the topic is showcased through
               graphical representations in R, and two detailed case studies
               demonstrate the relevance of statistics in finance. A related
               website features additional data sets and R scripts so readers
               can create their own simulations and test their comprehension of
               the presented techniques. An Introduction to Analysis of
               Financial Data with R is an excellent book for introductory
               courses on time series and business statistics at the
               upper-undergraduate and graduate level. The book is also an
               excellent resource for researchers and practitioners in the
               fields of business, finance, and economics who would like to
               enhance their understanding of financial data and today's
               financial markets.",
  publisher = "John Wiley \& Sons",
  month     =  aug,
  year      =  2014,
  language  = "en"
}

@BOOK{Spiegelhalter2019-rd,
  title     = "The Art of Statistics: Learning from Data",
  author    = "Spiegelhalter, David",
  abstract  = "'A statistical national treasure' Jeremy Vine, BBC Radio
               2'Required reading for all politicians, journalists, medics and
               anyone who tries to influence people (or is influenced) by
               statistics. A tour de force' Popular ScienceDo busier hospitals
               have higher survival rates? How many trees are there on the
               planet? Why do old men have big ears? David Spiegelhalter
               reveals the answers to these and many other questions -
               questions that can only be addressed using statistical
               science.Statistics has played a leading role in our scientific
               understanding of the world for centuries, yet we are all
               familiar with the way statistical claims can be sensationalised,
               particularly in the media. In the age of big data, as data
               science becomes established as a discipline, a basic grasp of
               statistical literacy is more important than ever. In The Art of
               Statistics, David Spiegelhalter guides the reader through the
               essential principles we need in order to derive knowledge from
               data. Drawing on real world problems to introduce conceptual
               issues, he shows us how statistics can help us determine the
               luckiest passenger on the Titanic, whether serial killer Harold
               Shipman could have been caught earlier, and if screening for
               ovarian cancer is beneficial. 'Shines a light on how we can use
               the ever-growing deluge of data to improve our understanding of
               the world' Nature",
  publisher = "Penguin UK",
  month     =  mar,
  year      =  2019,
  language  = "en"
}

@BOOK{Weyant1999-xx,
  title     = "The costs of the Kyoto Protocol: a multi-model evaluation",
  author    = "Weyant, John and Hill, Jennifer",
  publisher = "International Association for Energy Economics",
  year      =  1999
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Manne1999-cj,
  title     = "The Kyoto Protocol: a cost-effective strategy for meeting
               environmental objectives?",
  author    = "Manne, Alan S and Richels, Richard G",
  abstract  = "This paper has three purposes: 1) to identify the near-term
               costs to the United States of ratifying the Kyoto Protocol ; 2)
               to assess the significance of the Protocol's `` flexibility
               provisions''; and, 3) to evaluate the Kyoto targets in the
               context of the long-term goal of the ‚Ä¶",
  journal   = "Energy J.",
  publisher = "International Association for Energy Economics",
  volume    =  20,
  number    = "Special Issue-The Cost of the Kyoto Protocol: A Multi-Model
               Evaluation",
  year      =  1999
}

@ARTICLE{Barrett1998-jm,
  title     = "Political economy of the Kyoto Protocol",
  author    = "Barrett, S",
  abstract  = "Abstract. The Kyoto Protocol, negotiated in December 1997, is
               the first international treaty to limit emissions of greenhouse
               gases. But Kyoto does not mark th",
  journal   = "Oxf Rev Econ Policy",
  publisher = "Oxford Academic",
  volume    =  14,
  number    =  4,
  pages     = "20--39",
  month     =  dec,
  year      =  1998
}

@ARTICLE{Reilly1999-rw,
  title     = "Multi-gas assessment of the Kyoto Protocol",
  author    = "Reilly, J and Prinn, R and Harnisch, J and Fitzmaurice, J and
               Jacoby, H and Kicklighter, D and Melillo, J and Stone, P and
               Sokolov, A and Wang, C",
  abstract  = "The Kyoto Protocol allows reductions in emissions of several
               `greenhouse' gases to be credited against a CO2-equivalent
               emissions limit, calculated using `global warming potential'
               indices for each gas. Using an integrated global-systems model,
               it is shown that a multi-gas control strategy could greatly
               reduce the costs of fulfilling the Kyoto Protocol compared with
               a CO2-only strategy. Extending the Kyoto Protocol to 2100
               without more severe emissions reductions shows little difference
               between the two strategies in climate and ecosystem effects.
               Under a more stringent emissions policy, the use of global
               warming potentials as applied in the Kyoto Protocol leads to
               considerably more mitigation of climate change for multi-gas
               strategies than for the---supposedly equivalent---CO2-only
               control, thus emphasizing the limits of global warming
               potentials as a tool for political decisions.",
  journal   = "Nature",
  publisher = "Nature Publishing Group",
  volume    =  401,
  number    =  6753,
  pages     = "549--555",
  month     =  oct,
  year      =  1999,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{ONeill2002-kv,
  title     = "Dangerous climate impacts and the Kyoto Protocol",
  author    = "O'Neill, B C and Oppenheimer, M",
  abstract  = "‚Ä¶ Dangerous Climate Impacts and the Kyoto Protocol ‚Ä¶ However,
               both proponents and detractors of the Kyoto Protocol , which was
               designed as an initial step to implement the Framework
               Convention, have begun to demand a definition of long-term
               objectives ‚Ä¶",
  publisher = "science.sciencemag.org",
  year      =  2002
}

@ARTICLE{Buonanno2003-zc,
  title     = "Endogenous induced technical change and the costs of Kyoto",
  author    = "Buonanno, Paolo and Carraro, Carlo and Galeotti, Marzio",
  abstract  = "We present a model for climate change policy analysis which
               accounts for the possibility that technology evolves
               endogenously and that technical change can be induced by
               environmental policy measures. Both the output production
               technology and the emission--output ratio depend upon a stock of
               knowledge, which accumulates through R\&D activities. Two
               versions of this model are studied, one with endogenous
               technical change but exogenous environmental technical change
               and the other with both endogenous and induced technical change.
               A third version also captures technological spillover effects.
               As an application, the model is simulated allowing for trade of
               pollution permits as specified in the Kyoto Protocol and
               assessing the implications in terms of cost efficiency, economic
               growth and R\&D efforts of the three different specifications of
               technical change.",
  journal   = "Res. Energy Econ.",
  publisher = "Elsevier",
  volume    =  25,
  number    =  1,
  pages     = "11--34",
  month     =  feb,
  year      =  2003,
  keywords  = "Climate policy; Environmental modelling; Integrated assessment;
               Technical change"
}

@ARTICLE{McKibbin2004-tt,
  title     = "Estimates of the costs of Kyoto: Marrakesh versus the
               {McKibbin--Wilcoxen} blueprint",
  author    = "McKibbin, Warwick J and Wilcoxen, Peter J",
  abstract  = "In this paper, we update our earlier estimates of the cost of
               the Kyoto Protocol using the G-Cubed model, taking into account
               the new sink allowances from recent negotiations as well as
               allowing for multiple gases and new land clearing estimates. We
               then compare the protocol to an alternative policy outlined in
               McKibbin et al. (Brookings Policy Brief, No. 17. June, The
               Brookings Institution, Washington, 1997; Climate Change Policy
               After Kyoto: A Blueprint for a Realistic Approach, The Brookings
               Institution, Washington, 2002a; J. Econom. Perspect. 16(2)
               (2002b) 107) that does not impose rigid emissions targets. We
               focus particular attention on the sensitivity of compliance
               costs under each policy to unexpected changes in future economic
               conditions. To illustrate the issue, we evaluate the policies
               under two plausible alternative assumptions about a single
               aspect of the future world economy: the rate of productivity
               growth in Russia. We find that moderate growth in Russia would
               raise the cost of the Kyoto Protocol by as much as 50 percent
               but would have little effect on the cost of the alternative
               policy. We conclude that the Kyoto Protocol is inherently
               unstable because unexpected future events could raise compliance
               costs substantially and place enormous pressure on governments
               to abrogate the agreement. The alternative policy would be far
               more stable because it does not subject future governments to
               adverse shocks in compliance costs.",
  journal   = "Energy Policy",
  publisher = "Elsevier",
  volume    =  32,
  number    =  4,
  pages     = "467--479",
  month     =  mar,
  year      =  2004,
  keywords  = "Environmental policy; Climate change; General equilibrium model"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Burniaux2000-pd,
  title     = "A multi-gas assessment of the Kyoto Protocol",
  author    = "Burniaux, Jean-Marc",
  abstract  = "‚Ä¶ Expressed in this way, OECD countries will have to reduce
               their emissions by some 20 to 30 per cent ‚Ä¶ has been undertaken
               using macroeconomic global models to quantify the economic costs
               of implementing the Protocol by (see Weyant and Hill , 1999 ;
               OECD, 1999 ) ‚Ä¶",
  journal   = "OECD Economics Department Working Papers No. 270",
  publisher = "OECD",
  year      =  2000
}

@MISC{noauthor_undated-jc,
  title = "The Kyoto Protocol\_ Regional and Sectoral Contribu.pdf"
}

@MISC{noauthor_undated-qj,
  title = "The Kyoto Protocol, Emissions Trading and the {CDM\_.pdf}"
}

@MISC{noauthor_undated-xw,
  title = "The Kyoto Mechanisms and Russian Climate Politics\_.pdf"
}

@MISC{noauthor_undated-zf,
  title = "The Economics of a Lost Deal\_ Kyoto - The Hague -\_.pdf"
}

@MISC{noauthor_undated-dm,
  title = "The Costs of Kyoto for the {US} Economy\_.pdf"
}

@MISC{noauthor_undated-ln,
  title = "Singapore in a {Post-Kyoto} World\_ Energy, Environme.pdf"
}

@MISC{noauthor_undated-pm,
  title = "{Post-Kyoto} International Climate Policy\_ Implement.pdf"
}

@MISC{noauthor_undated-xh,
  title = "Developing Countries\_ Greenhouse Emissions\_ Uncert.pdf"
}

@MISC{noauthor_undated-bb,
  title = "Climate Politics from Kyoto to Bonn\_ From Little t.pdf"
}

@ARTICLE{B_o_hringer2002-fw,
  title    = "Climate Politics from Kyoto to Bonn: From Little to Nothing?",
  author   = "B {\"o} hringer, Christoph",
  abstract = "Presents a study which assessed how the U.S. withdrawal and the
              amendments of the Bonn climate policy conference will change the
              economic and environmental impacts of the Kyoto Protocol in its
              original form. Analysis of the scenario where the U.S. complies
              with the Kyoto Protocol; Implications of the U.S. withdrawal for
              no emissions trading cases.",
  journal  = "Energy J.",
  volume   =  23,
  number   =  2,
  pages    = "51",
  year     =  2002,
  keywords = "EMISSIONS trading, GOVERNMENT policy, GOVERNMENT policy on
              climate change"
}

@ARTICLE{Lutter2000-ke,
  title    = "Developing Countries' Greenhouse Emissions: Uncertainty and
              Implications for Participation in the Kyoto Protocol",
  author   = "Lutter, Randall",
  abstract = "Developing countries can participate in the Kyoto Protocol to
              limit greenhouse gas emissions by adopting national emissions
              limits. Such limits could offer economic gains to developing
              countries, cost savings to industrialized countries, and
              environmental benefits. They could also address concerns of the
              U.S. Senate. On the other hand, uncertainty about greenhouse gas
              emissions in developing countries is so great that emissions
              limits may impose substantial costs if they turn out to be
              unexpectedly stringent. To manage risks arising from emissions
              limits, developing countries should index any emissions limits to
              variables that predict emissions in the absence of limits. This
              paper presents such an index--similar to one recently adopted by
              Argentina--and develops estimates showing that it could lower the
              risk of economic losses to developing countries from about 40
              percent to about 35 percent. [ABSTRACT FROM AUTHOR]",
  journal  = "Energy J.",
  volume   =  21,
  number   =  4,
  pages    = "93",
  year     =  2000,
  keywords = "INDUSTRIAL policy, GREENHOUSE gas mitigation, INTERNATIONAL
              cooperation"
}

@ARTICLE{Dahl2012-yv,
  title    = "{Post-Kyoto} International Climate Policy: Implementing
              Architectures for Agreement",
  author   = "Dahl, Carol",
  abstract = "The article reviews the book ``Post-Kyoto International Climate
              Policy: Implementing Architectures for Agreement,'' edited
              byJoseph E. Aldy and Robert N. Stavins.",
  journal  = "Energy J.",
  volume   =  33,
  number   =  2,
  pages    = "226--237",
  year     =  2012,
  keywords = "CLIMATOLOGY, NONFICTION, STAVINS, Robert N., POST-Kyoto
              International Climate Policy: Implementing Architectures for
              Agreement (Book)"
}

@ARTICLE{Dahl2012-qt,
  title    = "{Post-Kyoto} International Climate Policy: Summary for Policy
              Makers",
  author   = "Dahl, Carol",
  abstract = "The article reviews the book ``Post-Kyoto International Climate
              Policy: Summary for Policy Makers,'' edited by Joseph E. Aldy and
              Robert N. Stavins.",
  journal  = "Energy J.",
  volume   =  33,
  number   =  2,
  pages    = "238",
  year     =  2012,
  keywords = "CLIMATOLOGY, NONFICTION, STAVINS, Robert N., ALDY, Joseph E.,
              POST-Kyoto International Climate Policy: Summary for Policy
              Makers (Book)"
}

@ARTICLE{Joutz2017-ix,
  title    = "Singapore in a {Post-Kyoto} World: Energy, Environment and the
              Economy",
  author   = "Joutz, Fred",
  journal  = "Energy J.",
  volume   =  38,
  number   =  2,
  pages    = "231--233",
  year     =  2017,
  keywords = "ENERGY industries, GOVERNMENT policy, NONFICTION, DOSHI, Tilak
              K., SINGAPORE in a Post-Kyoto World: Energy, Environment \& the
              Economy (Book)"
}

@ARTICLE{Barker2004-gm,
  title    = "The Costs of Kyoto for the {US} Economy",
  author   = "Barker, Terry and Ekins, Paul",
  abstract = "The high costs for the US economy of mitigating climate change
              have been cited by the Bush administration as one of the reasons
              for rejecting US ratification of the Kyoto Protocol. A range of
              cost estimates are assessed in the IPCC's third report (2001),
              but they are hedged with so many qualifications that it is not
              easy to reach useful conclusions. This paper organises, some of
              the quantitative information on costs of greenhouse gas
              mitigation for the US published before the US rejection of Kyoto.
              The aim is to put them in a wider context, e.g., allowing for
              non-climate benefits, and to draw conclusions that are robust in
              the face of the uncertainties. Important lessons can be drawn for
              how costs can be reduced in any future international commitment
              by the US to reduce emissions. Provided policies are expected,
              gradual and well designed (e.g., through auctioned Annex I
              tradable permits with revenues used to reduce burdensome tax
              rates) the net costs for the US of mitigation are likel",
  journal  = "Energy J.",
  volume   =  25,
  number   =  3,
  pages    = "53--71",
  year     =  2004,
  keywords = "GREENHOUSE gas mitigation, GROSS domestic product, CLIMATE
              change, ECONOMIC conditions in the United States, GAS prices"
}

@ARTICLE{Hourcade2002-cj,
  title    = "The Economics of a Lost Deal: Kyoto - The Hague - Marrakesh",
  author   = "Hourcade, Jean-Charles and Ghersi, Frederic",
  abstract = "Examines the prospects for compromise between competing
              perspectives on four key climate change issues, costs, level of
              domestic action, environmental integrity and developing world
              involvement. Policy issues stemming from uncertainty about
              abatement costs; Conceptual ambiguities about compliance costs;
              Potential financial flows to the developing world.",
  journal  = "Energy J.",
  volume   =  23,
  number   =  3,
  pages    = "1",
  year     =  2002,
  keywords = "COST, CLIMATE change"
}

@ARTICLE{Dahl2002-ql,
  title    = "The Kyoto Mechanisms and Russian Climate Politics (Book)",
  author   = "Dahl, Carol",
  abstract = "Reviews the book 'The Kyoto Mechanisms and Russian Climate
              Politics,' by Arild Moe and Kristian Tangen.",
  journal  = "Energy J.",
  volume   =  23,
  number   =  2,
  pages    = "123",
  year     =  2002,
  keywords = "CLIMATOLOGY, MOE, Arild, TANGEN, Kristian, KYOTO Mechanisms \&
              Russian Climate Politics, The (Book)"
}

@ARTICLE{Painuly2001-ao,
  title    = "The Kyoto Protocol, Emissions Trading and the {CDM}: An Analysis
              from Developing Countries Perspective",
  author   = "Painuly, Jyoti P",
  abstract = "In this paper the Kyoto Protocol is analyzed from the perspective
              of developing countries. The literature on the Protocol's impact
              indicates that Annex B countries will benefit from an emissions
              trading regime and the benefit is highest when non-Annex B
              countries are also included in the trading system. The paper
              addresses the issue of allocation of gains to developing
              countries from the Clean Development Mechanism, when the CDM
              simulates emissions trading. It was found that gains to non-Annex
              B from participation in GHG mitigation might vary from $6 billion
              to $29 billion, about 7\% and 20\% respectively of the global
              gains in an emissions trading system. However, several
              institutional issues related to CDM design and implementation
              will have to be resolved before developing countries can optimize
              their gains. Indirect impacts of the Kyoto Protocol through
              trade, although expected to be significant, have not been
              included. To optimize their gains, non-Annex B countries need to
              actively",
  journal  = "Energy J.",
  volume   =  22,
  number   =  3,
  pages    = "147",
  year     =  2001,
  keywords = "EMISSIONS trading, INTERNATIONAL cooperation on climate change"
}

@ARTICLE{Paltsev2001-lj,
  title    = "The Kyoto Protocol: Regional and Sectoral Contributions to the
              Carbon Leakage",
  author   = "Paltsev, Sergey V",
  abstract = "Carbon dioxide emissions abatement in a group of countries can
              result in increased emissions in non-abating countries. This
              effect has been referred to as carbon leakage. The Kyoto Protocol
              calls for a number of industrialized countries to limit their
              emissions while other countries have no abatement commitments.
              This paper assesses the sectoral and regional determinants of the
              leakage in a static multi-sector, multi-regional computable
              general equilibrium model. In baseline estimates based on our
              model, the Kyoto Protocol leads to a carbon leakage rate of 10
              percent. A decomposition technique is applied which attributes
              increases in CO[sub 2] emissions by non-participating countries
              to specific sectors in the abating countries. This information is
              important for the debate on the tax exemptions for certain
              industries in the participating countries as it provides
              information for the most- and least-leakage contributing sectors
              of the economy. Additional calculations indicate the need f",
  journal  = "Energy J.",
  volume   =  22,
  number   =  4,
  pages    = "53",
  year     =  2001,
  keywords = "GLOBAL warming, CARBON \& the environment, CARBON dioxide"
}

@ARTICLE{Bratt2005-uz,
  title    = "Implementing Kyoto In Canada: The Role Of Nuclear Power",
  author   = "Bratt, Duane",
  abstract = "On December 17, 2002, Canada ratified the Kyoto Protocol,
              committing itself to reducing its greenhouse gas emissions by 6
              percent of 1990 levels. This paper argues that nuclear power must
              be an indispensable component of Canada's Kyoto implementation
              strategy. This is because nuclear power, unlike other
              conventional energy sources of coal, natural gas, and oil, does
              not contribute to the emission of greenhouse gases. Nuclear power
              is frequently criticized for its environmental record (radiation,
              production of waste, reactor safety), but a comparison with the
              other major energy sources reveals the green advantages of
              nuclear power. One potential opportunity is using nuclear power
              in the Alberta oil sands which would contribute to Canada meeting
              its emission reduction targets while limiting the
              economic/political dislocation caused by implementing the Kyoto
              Protocol. The paper concludes by explaining why, despite the
              above advantages, the federal government has failed to properly
              utiliz",
  journal  = "The Energy Journal",
  volume   =  26,
  number   =  1,
  pages    = "107--121",
  year     =  2005,
  keywords = "EMISSIONS (Air pollution), NUCLEAR energy, GREENHOUSE gases,
              COAL, NATURAL gas"
}

@ARTICLE{Elliott2005-vq,
  title     = "Pairs trading",
  author    = "Elliott, Robert J and Van Der Hoek *, John and Malcolm, William
               P",
  abstract  = "'Pairs Trading' is an investment strategy used by many Hedge
               Funds. Consider two similar stocks which trade at some spread.
               If the spread widens short the high stock and buy the low stock.
               As the spread narrows again to some equilibrium value, a profit
               results. This paper provides an analytical framework for such an
               investment strategy. We propose a mean-reverting Gaussian Markov
               chain model for the spread which is observed ill Gaussian noise.
               Predictions from the calibrated model are then compared with
               subsequent observations of the spread to determine appropriate
               investment decisions. The methodology has potential applications
               to generating wealth from any quantities in financial markets
               which are observed to be out of equilibrium.",
  journal   = "Quant. Finance",
  publisher = "Routledge",
  volume    =  5,
  number    =  3,
  pages     = "271--276",
  month     =  jun,
  year      =  2005
}

@BOOK{Minsky2008-hh,
  title     = "Stabilizing an Unstable Economy",
  author    = "Minsky, Hyman",
  abstract  = "``Mr. Minsky long argued markets were crisis prone. His 'moment'
               has arrived.'' -The Wall Street Journal In his seminal work,
               Minsky presents his groundbreaking financial theory of
               investment, one that is startlingly relevant today. He explains
               why the American economy has experienced periods of debilitating
               inflation, rising unemployment, and marked slowdowns-and why the
               economy is now undergoing a credit crisis that he foresaw.
               Stabilizing an Unstable Economy covers: The natural inclination
               of complex, capitalist economies toward instability Booms and
               busts as unavoidable results of high-risk lending practices
               ``Speculative finance'' and its effect on investment and asset
               prices Government's role in bolstering consumption during times
               of high unemployment The need to increase Federal Reserve
               oversight of banks Henry Kaufman, president, Henry Kaufman \&
               Company, Inc., places Minsky's prescient ideas in the context of
               today's financial markets and institutions in a fascinating new
               preface. Two of Minsky's colleagues, Dimitri B. Papadimitriou,
               Ph.D. and president, The Levy Economics Institute of Bard
               College, and L. Randall Wray, Ph.D. and a senior scholar at the
               Institute, also weigh in on Minsky's present relevance in
               today's economic scene in a new introduction. A surge of
               interest in and respect for Hyman Minsky's ideas pervades Wall
               Street, as top economic thinkers and financial writers have
               started using the phrase ``Minsky moment'' to describe America's
               turbulent economy. There has never been a more appropriate time
               to read this classic of economic theory.",
  publisher = "McGraw Hill Professional",
  month     =  apr,
  year      =  2008,
  language  = "en"
}

@ARTICLE{Kuosmanen2009-rb,
  title     = "Data Envelopment Analysis as Nonparametric {Least-Squares}
               Regression",
  author    = "Kuosmanen, Timo and Johnson, Andrew L",
  abstract  = "Data envelopment analysis (DEA) is known as a nonparametric
               mathematical programming approach to productive efficiency
               analysis. In this paper, we show that DEA can be alternatively
               interpreted as nonparametric least-squares regression subject to
               shape constraints on the frontier and sign constraints on
               residuals. This reinterpretation reveals the classic parametric
               programming model by Aigner and Chu [Aigner, D., S. Chu. 1968.
               On estimating the industry production function. Amer. Econom.
               Rev. 58 826?839] as a constrained special case of DEA. Applying
               these insights, we develop a nonparametric variant of the
               corrected ordinary least-squares (COLS) method. We show that
               this new method, referred to as corrected concave nonparametric
               least squares (C2NLS), is consistent and asymptotically
               unbiased. The linkages established in this paper contribute to
               further integration of the econometric and axiomatic approaches
               to efficiency analysis.",
  journal   = "Oper. Res.",
  publisher = "INFORMS",
  volume    =  58,
  number    =  1,
  pages     = "149--160",
  month     =  oct,
  year      =  2009
}

@ARTICLE{Davis2008-gv,
  title    = "Comparing early warning systems for banking crises",
  author   = "Davis, E Philip and Karim, Dilruba",
  abstract = "Despite the extensive literature on prediction of banking crises
              by Early Warning Systems (EWSs), their practical use by policy
              makers is limited, even in the international financial
              institutions. This is a paradox since the changing nature of
              banking risks as more economies liberalise and develop their
              financial systems, as well as ongoing innovation, makes the use
              of EWS for informing policies aimed at preventing crises more
              necessary than ever. In this context, we assess the logit and
              signal extraction EWS for banking crises on a comprehensive
              common dataset. We suggest that logit is the most appropriate
              approach for global EWS and signal extraction for
              country-specific EWS. Furthermore, it is important to consider
              the policy maker's objectives when designing predictive models
              and setting related thresholds since there is a sharp trade-off
              between correctly calling crises and false alarms.",
  journal  = "Journal of Financial Stability",
  volume   =  4,
  number   =  2,
  pages    = "89--120",
  month    =  jun,
  year     =  2008,
  keywords = "Banking crises; Systemic risk; Early warning systems; Logit
              estimation; Signal extraction"
}

@ARTICLE{Assaf2013-fv,
  title     = "Turkish bank efficiency: Bayesian estimation with undesirable
               outputs",
  author    = "Assaf, George and Matousek, Roman and Tsionas, Efthymios G",
  abstract  = "This paper analyzes the productivity and efficiency of Turkish
               banks from 2002 to 2010. We obtained estimates of efficiency,
               productivity growth and efficiency growth using a Bayesian
               stochastic frontier approach and focused on accounting for
               Non-Performing Loans (NPLs) for use in our model. Specifically,
               we introduce NPLs as a bad output in an input distance function,
               and estimate a system of non-linear equations subject to
               endogeneity. We confirm that the productivity growth of Turkish
               banks was positive over the period of this study, which was
               mainly due to the improvement in technology, while efficiency
               growth continued to be negative over the same period.
               Methodologically, we also prove that not accounting for NPLs in
               estimating the frontier model might seriously distort the
               efficiency and productivity results. The study also provides
               measures of shadow prices for NPL and discusses the results in
               terms of several interesting trends in Turkish banking. Finally,
               the paper provides efficiency and productivity comparisons
               between domestic and foreign banks.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier",
  volume    =  37,
  number    =  2,
  pages     = "506--517",
  month     =  feb,
  year      =  2013,
  keywords  = "Turkey; Bank; Non-performing loans; Bayesian distance
               function;3*;UndesirableOutputs;european banking
               efficiency;shadow price;Non performing loans;quality of
               financial intermediation"
}

@MISC{European_Central_Bank2017-fw,
  title        = "Banking supervision - What next?",
  booktitle    = "European Central Bank - Banking Supervision",
  author       = "{European Central Bank}",
  month        =  nov,
  year         =  2017,
  howpublished = "\url{https://www.bankingsupervision.europa.eu/press/speeches/date/2017/html/ssm.sp171114_1.en.html}",
  note         = "Accessed: 2018-5-31"
}

@ARTICLE{Altunbas2007-xz,
  title     = "Examining the Relationships between Capital, Risk and Efficiency
               in European Banking",
  author    = "Altunbas, Yener and Carbo, Santiago and Gardener, Edward P M and
               Molyneux, Philip",
  abstract  = "This paper analyses the relationship between capital, risk and
               efficiency for a large sample of European banks between 1992 and
               2000. In contrast to the established US evidence we do not find
               a positive relationship between inefficiency and bank
               risk-taking. Inefficient European banks appear to hold more
               capital and take on less risk. Empirical evidence is found
               showing the positive relationship between risk on the level of
               capital (and liquidity), possibly indicating regulators'
               preference for capital as a mean of restricting risk-taking
               activities. We also find evidence that the financial strength of
               the corporate sector has a positive influence in reducing bank
               risk-taking and capital levels. There are no major differences
               in the relationships between capital, risk and efficiency for
               commercial and savings banks although there are for co-operative
               banks. In the case of co-operative banks we do find that capital
               levels are inversely related to risks and we find that
               inefficient banks hold lower levels of capital. Some of these
               relationships also vary depending on whether banks are among the
               most or least efficient operators.",
  journal   = "European Financial Management",
  publisher = "Blackwell Publishing Ltd",
  volume    =  13,
  number    =  1,
  pages     = "49--70",
  month     =  jan,
  year      =  2007,
  keywords  = "bank capital; risk; efficiency; credit; European banks; E5; E52;
               G21;european banking efficiency;Mendeley Import (May
               05)/Banking/bcp"
}

@ARTICLE{Spatt2012-xv,
  title   = "Complexity of regulation",
  author  = "Spatt, Chester S",
  journal = "Harvard Business Law Review Online",
  volume  =  3,
  pages   = "1--9",
  year    =  2012
}

@ARTICLE{Averch1962-nt,
  title     = "Behavior of the Firm Under Regulatory Constraint",
  author    = "Averch, Harvey and Johnson, Leland L",
  journal   = "Am. Econ. Rev.",
  publisher = "American Economic Association",
  volume    =  52,
  number    =  5,
  pages     = "1052--1069",
  year      =  1962,
  keywords  = "unintended consequences;BAFA2018"
}

@INCOLLECTION{Johnson2015-vv,
  title     = "An Introduction to {CNLS} and {StoNED} Methods for Efficiency
               Analysis: Economic Insights and Computational Aspects",
  booktitle = "Benchmarking for Performance Evaluation",
  author    = "Johnson, Andrew L and Kuosmanen, Timo",
  abstract  = "This chapter describes the economic insights of the unifying
               framework known as Stochastic semi-Nonparametric Envelopment of
               Data (StoNED), which combines the virtues of the widely used
               neoclassic production models, Data Envelopment Analysis (DEA),
               and Stochastic Frontier Analysis (SFA). Like DEA, StoNED is able
               to estimate an axiomatic production function relaxing the
               functional form specification required in most implementations
               of SFA. However, StoNED is also consistent with the econometric
               models of noise, providing a distinct advantage over standard
               DEA models. Further, StoNED allows for the possibility that
               systematic inefficiency is negligible consistent with
               neoclassical theory, thus providing a unifying framework. StoNED
               is implemented by estimating a conditional mean using convex
               nonparametric least squares (CNLS) followed by using standard
               SFA techniques to estimate the average efficiency and decompose
               the residual. Detailed descriptions of General Algebraic
               Modeling System (GAMS) and matrix laboratory (MATLAB) code will
               aid readers in implementing the StoNED estimator.",
  publisher = "Springer, New Delhi",
  pages     = "117--186",
  year      =  2015,
  language  = "en"
}

@ARTICLE{Merton1936-wb,
  title     = "The Unanticipated Consequences of Purposive Social Action",
  author    = "Merton, Robert K",
  journal   = "Am. Sociol. Rev.",
  publisher = "[American Sociological Association, Sage Publications, Inc.]",
  volume    =  1,
  number    =  6,
  pages     = "894--904",
  year      =  1936,
  keywords  = "unintended consequences;BAFA2018"
}

@ARTICLE{Glass2014-na,
  title    = "Cooperative bank efficiency in Japan: A parametric distance
              function analysis",
  author   = "Glass, J C and McKillop, D G and Quinn, B and Wilson, J",
  abstract = "This study examines the relative performance of Japanese
              cooperative banks between 1998 and 2009, explicitly modelling
              non-performing loans as an undesirable output. Three key findings
              emerge. First, the sector is characterized by increasing returns
              to scale which supports the ongoing amalgamation process within
              the sector. Second, although restricted in product offerings,
              markets and their membership base, Japanese cooperatives secured
              both technical progress (a positive shift in the frontier) and a
              decrease in technical inefficiency (distance from the frontier).
              Third, the analysis highlighted that regulatory pressure to
              reduce non-performing loans will have an adverse impact on both
              output and performance. \copyright{} 2012 \copyright{} 2012
              Taylor \& Francis.",
  journal  = "European Journal of Finance",
  volume   =  20,
  number   =  3,
  pages    = "291--317",
  year     =  2014,
  keywords = "Japanese cooperative banks; efficiency; regulatory compliance;my
              papers;UndesirableOutputs;Mendeley Import (May
              05)/Productivity/Efficiency;Mendeley Import (May 05)"
}

@BOOK{Fare2012-xm,
  title     = "{Multi-Output} Production and Duality: Theory and Applications",
  author    = "F{\"a}re, Rolf and Primont, Daniel",
  abstract  = "Our original reason for writing this book was the desire to
               write down in one place a complete summary of the major results
               in du ality theory pioneered by Ronald W. Shephard in three of
               his books, Cost and Production Functions (1953), Theory of Cost
               and Produc tion Functions (1970), and Indirect Production
               Functions (1974). In this way, newcomers to the field would have
               easy access to these important ideas. In adg,ition, we report a
               few new results of our own. In particular, we show the duality
               relationship between the profit function and the eight
               equivalent representations of technol ogy that were elucidated
               by Shephard. However, in planning the book and discussing it
               with colleagues it became evident that such a book would be more
               useful if it also provided a number of applications of
               Shephard's duality theory to economic problems. Thus, we have
               also attempted to present exam ples of the use of duality theory
               in areas such as efficiency measure ment, index number theory,
               shadow pricing, cost-benefit analysis, and econometric
               estimation. Much of our thinking about duality theory and its
               uses has been influenced by our present and former
               collaborators. They include Charles Blackorby, Shawna Grosskopf,
               Knox Lovell, Robert Russell, and, not surprisingly, Ronald W.
               Shephard. We have also benefit ted over the years from many
               discussions with W. Erwin Diewert.",
  publisher = "Springer Netherlands",
  month     =  sep,
  year      =  2012,
  keywords  = "Mendeley Import (May 05)/Productivity/Efficiency",
  language  = "en"
}

@ARTICLE{Matousek2015-tj,
  title     = "Bank performance and convergence during the financial crisis:
               Evidence from the 'old'European Union and Eurozone",
  author    = "Matousek, R and Rughoo, A and Sarantis, N and Assaf, A G",
  abstract  = "Abstract This paper investigates the process of banking
               integration in the EU15 countries and the Eurozone by testing
               for convergence in bank efficiency among commercial banks. We
               use a two-step approach: First we estimate efficiency by
               applying an innovative",
  journal   = "Journal of Banking \&",
  publisher = "Elsevier",
  year      =  2015,
  keywords  = "european banking efficiency;UndesirableOutputs"
}

@ARTICLE{Glass2010-ds,
  title     = "Irish credit unions: Investigating performance determinants and
               the opportunity cost of regulatory compliance",
  author    = "Glass, J Colin and McKillop, Donal G and Rasaratnam, Syamarlah",
  abstract  = "The study investigates how producer-specific environmental
               factors influence the performance of Irish credit unions. The
               empirical analysis uses a two-stage approach. The first stage
               measures efficiency by a data envelopment analysis (DEA)
               estimator, which explicitly incorporates the production of
               undesirable outputs such as bad loans in the modelling, and the
               second stage uses truncated regression to infer how various
               factors influence the (bias-corrected) estimated efficiency. A
               key finding of the analysis is that 68\% of Irish credit unions
               do not incur an extra opportunity cost in meeting regulatory
               guidance on bad debt. \copyright{} 2009 Elsevier B.V. All rights
               reserved.",
  journal   = "Journal of Banking \& Finance",
  publisher = "Elsevier B.V.",
  volume    =  34,
  number    =  1,
  pages     = "67--76",
  month     =  jan,
  year      =  2010,
  keywords  = "Credit unions; Efficiency; Regulatory compliance;credit
               union;3*;Mendeley Import (May 05)/Banking/bcp;Mendeley Import
               (May 05)/Productivity/Efficiency/OC of regulation"
}

@ARTICLE{Kuosmanen2010-hh,
  title    = "Firm and industry level profit efficiency analysis using absolute
              and uniform shadow prices",
  author   = "Kuosmanen, Timo and Kortelainen, Mika and Sipil{\"a}inen, Timo
              and Cherchye, Laurens",
  abstract = "We discuss the nonparametric approach to profit efficiency
              analysis at the firm and industry levels in the absence of
              complete price information. Two new insights are developed.
              First, we measure profit inefficiency in monetary terms using
              absolute shadow prices. Second, we evaluate all firms using the
              same input--output prices. This allows us to aggregate firm-level
              profit inefficiencies to the overall industry inefficiency.
              Besides the measurement of profit losses, the presented approach
              enables one to recover absolute price information from quantity
              data. We conduct a series of Monte Carlo simulations to study the
              performance of the proposed approach in controlled production
              environments.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  202,
  number   =  2,
  pages    = "584--594",
  month    =  apr,
  year     =  2010,
  keywords = "Data envelopment analysis (DEA); Directional distance function;
              The law of one price;4*"
}

@ARTICLE{Lim2012-yd,
  title     = "Consistency of Multidimensional Convex Regression",
  author    = "Lim, Eunji and Glynn, Peter W",
  abstract  = "Convex regression is concerned with computing the best fit of a
               convex function to a data set of n observations in which the
               independent variable is (possibly) multidimensional. Such
               regression problems arise in operations research, economics, and
               other disciplines in which imposing a convexity constraint on
               the regression function is natural. This paper studies a
               least-squares estimator that is computable as the solution of a
               quadratic program and establishes that it converges almost
               surely to the ?true? function as n ? $\infty$ under modest
               technical assumptions. In addition to this multidimensional
               consistency result, we identify the behavior of the estimator
               when the model is misspecified (so that the ?true? function is
               nonconvex), and we extend the consistency result to settings in
               which the function must be both convex and nondecreasing (as is
               needed for consumer preference utility functions).",
  journal   = "Oper. Res.",
  publisher = "INFORMS",
  volume    =  60,
  number    =  1,
  pages     = "196--208",
  month     =  feb,
  year      =  2012
}

@ARTICLE{Kuosmanen2012-ol,
  title     = "Stochastic non-smooth envelopment of data: semi-parametric
               frontier estimation subject to shape constraints",
  author    = "Kuosmanen, Timo and Kortelainen, Mika",
  abstract  = "The field of productive efficiency analysis is currently divided
               between two main paradigms: the deterministic, nonparametric
               Data Envelopment Analysis (DEA) and the parametric Stochastic
               Frontier Analysis (SFA). This paper examines an encompassing
               semiparametric frontier model that combines the DEA-type
               nonparametric frontier, which satisfies monotonicity and
               concavity, with the SFA-style stochastic homoskedastic composite
               error term. To estimate this model, a new two-stage method is
               proposed, referred to as Stochastic Non-smooth Envelopment of
               Data (StoNED). The first stage of the StoNED method applies
               convex nonparametric least squares (CNLS) to estimate the shape
               of the frontier without any assumptions about its functional
               form or smoothness. In the second stage, the conditional
               expectations of inefficiency are estimated based on the CNLS
               residuals, using the method of moments or pseudolikelihood
               techniques. Although in a cross-sectional setting distinguishing
               inefficiency from noise in general requires distributional
               assumptions, we also show how these can be relaxed in our
               approach if panel data are available. Performance of the StoNED
               method is examined using Monte Carlo simulations.",
  journal   = "J Prod Anal",
  publisher = "Springer US",
  volume    =  38,
  number    =  1,
  pages     = "11--28",
  month     =  aug,
  year      =  2012,
  keywords  = "Mendeley Import (May 05)/Productivity/Efficiency/StoNED;StoNED",
  language  = "en"
}

@ARTICLE{Kuosmanen2004-mb,
  title     = "Shadow Price Approach to Total Factor Productivity Measurement:
               With an Application to Finnish {Grass-Silage} Production",
  author    = "Kuosmanen, Timo and Post, Thierry and Sipil{\"a}inen, Timo",
  abstract  = "This paper explores an intermediate route between the Fisher and
               the Malmquist productivity indexes so as to minimize data
               requirements and assumptions about economic behavior of
               production units and their production technology. Assuming
               quantity data of inputs and outputs and the behavioral
               hypothesis of allocative efficiency, we calculate the exact
               value of the Fisher ideal productivity index using implicit
               shadow prices revealed by the choice of input--output mix. The
               approach is operationalized by means of a nonparametric data
               envelopment analysis (DEA) model. Empirical application to
               Finnish grass silage farms suggests that the Malmquist and the
               Fisher productivity indices yield similar results when averaged
               over firms, but there can be major differences in the results of
               the two approaches at the level of individual firms.",
  journal   = "Journal of Productivity Analysis",
  publisher = "Kluwer Academic Publishers",
  volume    =  22,
  number    = "1-2",
  pages     = "95--121",
  month     =  jul,
  year      =  2004,
  language  = "en"
}

@BOOK{Conover1999-xi,
  title     = "Practical nonparametric statistics",
  author    = "Conover, W J",
  abstract  = "This highly-regarded text serves as a quick reference book which
               offers clear, concise instructions on how and when to use the
               most popular nonparametric procedures. This edition features
               some procedures that have withstood the test of time and are now
               used by many practitioners, such as the Fisher Exact Test for
               two-by-two contingency tables, the Mantel-Haenszel Test for
               combining several contingency tables, the Kaplan-Meier estimates
               of the survival curve, the Jonckheere-Terpstra Test and the Page
               Test for ordered alternatives, and a discussion of the bootstrap
               method.",
  publisher = "Wiley",
  edition   = "3 edition",
  year      =  1999,
  language  = "en"
}

@TECHREPORT{Bholat2016-py,
  title       = "{Non-Performing} Loans: Regulatory and Accounting Treatments
                 of Assets",
  author      = "Bholat, David and Lastra, Rosa M and Markose, Sheri M and
                 Miglionico, Andrea and Sen, Kallol",
  abstract    = "Asset quality is an essential part of sound banking. However,
                 asset quality is difficult for banking regulators and
                 investors to assess in the absence of a comm",
  number      =  594,
  institution = "Bank of England",
  month       =  apr,
  year        =  2016,
  keywords    = "Non-performing loans, impairment, loan loss provisions, bank
                 capital, data standards, credit risk"
}

@ARTICLE{Seijo2011-zv,
  title     = "Nonparametric least squares estimation of a multivariate convex
               regression function",
  author    = "Seijo, Emilio and Sen, Bodhisattva",
  journal   = "Ann. Stat.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  39,
  number    =  3,
  pages     = "1633--1657",
  month     =  jun,
  year      =  2011,
  keywords  = "Consistency; linear program; semidefinite quadratic program;
               shape restricted estimation; subdifferentials",
  language  = "en"
}

@ARTICLE{Murray1995-zk,
  title     = "Measuring Oligopsony Power with Shadow Prices: {{U.S}}. Markets
               for Pulpwood and Sawlogs",
  author    = "Murray, Brian C",
  abstract  = "Empirical estimation of input market power is hindered by
               problems in measuring an input's value of marginal product
               (VMP). By estimating a variable profit function system, however,
               one can infer a factor's VMP through its shadow price. This
               technique is used here to specify a structural equation system,
               which is estimated using time series data for the U.S.
               sawmilling and paper industries, to empirically measure the
               degree of oligopsony power for sawlog and pulpwood inputs
               respectively. Results evaluated at sample means indicate that
               pulpwood markets are more oligopsonistic than sawlog markets,
               though both perform closer to perfect competition than
               monopsony. Time trends for market power differ for each product
               and perfect competition cannot be rejected for sawlogs in later
               years.",
  journal   = "Rev. Econ. Stat.",
  publisher = "The MIT Press",
  volume    =  77,
  number    =  3,
  pages     = "486--498",
  year      =  1995
}

@ARTICLE{Glass2014-bd,
  title    = "Modelling the Performance of Irish Credit Unions, 2002 to 2010",
  author   = "Glass, J Colin and McKillop, Donal G and Quinn, Barry",
  abstract = "This study undertakes a modeling based performance assessment of
              all Irish credit unions between 2002 and 2010, a particularly
              turbulent period in their history. The analysis explicitly
              addresses the current challenges faced by credit unions in that
              the modeling approach used rewards credit unions for reducing
              undesirable outputs (impaired loans and investments) as well as
              for increasing desirable outputs (loans, earning assets and
              members' funds) and decreasing inputs (labour expenditure,
              capital expenditure and fund expenses). The main findings are:
              credit unions are subject to increasing returns to scale;
              technical regression occurred in the years after 2007; there is
              significant scope for an improvement in efficiency through
              expansion of desirable outputs and contraction of undesirable
              outputs and inputs; and that larger credit unions, that are
              better capitalised and pay a higher dividend to members are more
              efficient than their smaller, less capitalised, and lower
              dividend paying counterparts.",
  journal  = "Financial Accountability \& Management",
  volume   =  30,
  number   =  4,
  pages    = "430--453",
  month    =  nov,
  year     =  2014,
  keywords = "credit unions; efficiency; impaired loans and investments;my
              papers;credit union;UndesirableOutputs"
}

@ARTICLE{Lutter2013-fz,
  title     = "Regulatory policy: what role for retrospective analysis and
               review?",
  author    = "Lutter, Randall",
  abstract  = "Given that President Obama's Executive Orders on regulation have
               emphasized the importance of retrospective analysis and review
               of existing federal rules, I survey the state of retrospective
               analysis and review of federal regulations. I first ask how much
               is known about the economic merit of past federal regulatory
               decisions, based on retrospective economic analyses of their
               effects. I review reports of the Office of Management and Budget
               and related literature, but unlike those reports I find only
               five rules, issued by the National Highway Traffic Safety
               Administration (NHTSA), for which retrospective analyses provide
               estimates of both costs and reasonably good proxies for benefits
               (e.g., adverse health effects avoided). Other retrospective
               studies of federal rules estimate are incomplete, estimating
               only the compliance costs, or only the benefits, or only costs
               and measures of effectiveness, like emissions reductions, which
               do not closely relate to people's welfare.I also seek to explain
               differences in the practice of retrospective analysis and review
               between NHTSA, which appears to have the best record of
               retrospective analyses among federal agencies, and the
               Environmental Protection Agency (EPA), an important regulatory
               agency. I find that NHTSA regularly conducts such analyses and
               reviews, while EPA rarely does retrospective analysis and
               presents rulemakings that look like business as usual as being
               the result of a retrospective review. I analyze the role of data
               limitations, statutory authority, and institutional incentives
               in influencing the different behaviors of these two agencies. I
               conclude that differences in data availability and in particular
               the lack of relevant control groups, are an important barrier to
               retrospective analysis at EPA. This data deficiency, combined
               with important restrictions on EPA's ability to consider
               information on net benefits or cost-effectiveness in its
               rule-making, are together the biggest hindrance to generating
               better information about the effects of its rules. I conclude
               with recommendations intended to generate more measurement of
               the actual effects of regulations.",
  journal   = "Journal of Benefit-Cost Analysis",
  publisher = "Cambridge University Press",
  volume    =  4,
  number    =  1,
  pages     = "17--38",
  month     =  jan,
  year      =  2013,
  keywords  = "retrospective requlatory analysis; regulatory review"
}

@ARTICLE{Anderson1954-na,
  title     = "A Test of Goodness of Fit",
  author    = "Anderson, T W and Darling, D A",
  abstract  = "Abstract Some (large sample) significance points are tabulated
               for a distribution-free test of goodness of fit which was
               introduced earlier by the authors. The test, which uses the
               actual observations without grouping, is sensitive to
               discrepancies at the tails of the distribution rather than near
               the median. An illustration is given, using a numerical example
               used previously by Birnbaum in illustrating the Kolmogorov test.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  volume    =  49,
  number    =  268,
  pages     = "765--769",
  month     =  dec,
  year      =  1954
}

@INCOLLECTION{Galema2016-fj,
  title     = "European Bank Efficiency and Performance: The Effects of
               Supranational Versus National Bank Supervision",
  booktitle = "The Palgrave Handbook of European Banking",
  author    = "Galema, Rients and Koetter, Michael",
  editor    = "Beck, Thorsten and Casu, Barbara",
  abstract  = "This chapter explores European bank efficiency and performance.
               First, the authors provide an overview of the key estimation
               methods for efficiency and discuss selected applications to the
               European banking sector. Second, they apply stochastic frontier
               analysis to investigate the extent to which the reallocation of
               supervisory powers is associated with efficiency differences
               between European banks. In doing so, the discussion focuses
               particularly on whether direct supervision by the Single
               Supervisory Mechanism (SSM) as opposed to national competent
               authority (NCA) is related to cost and profit efficiency.",
  publisher = "Palgrave Macmillan, London",
  pages     = "257--292",
  year      =  2016,
  keywords  = "Research Idea;banking union;european banking efficiency",
  language  = "en"
}

@ARTICLE{Carmichael2011-dr,
  title     = "Maintaining Market Position: Team Performance, Revenue And Wage
               Expenditure In The English Premier League",
  author    = "Carmichael, Fiona and McHale, Ian and Thomas, Dennis",
  abstract  = "No abstract is available for this item.",
  journal   = "Bull. Econ. Res.",
  publisher = "Wiley Blackwell",
  volume    =  63,
  number    =  4,
  pages     = "464--479",
  year      =  2011,
  keywords  = "read once;Mendeley Import (May 05)/football;Football Economics"
}

@TECHREPORT{Kuosmanen2013-gw,
  title       = "Green productivity in agriculture: A critical synthesis",
  author      = "Kuosmanen, Timo",
  institution = "OECD",
  year        =  2013
}

@ARTICLE{Stigler1971-ii,
  title     = "The Theory of Economic Regulation",
  author    = "Stigler, George J",
  abstract  = "The potential uses of public resources and powers to improve the
               economic status of economic groups (such as industries and
               occupations) are analyzed to provide a scheme of the demand for
               regulation. The characteristics of the political process which
               allow relatively small groups to obtain such regulation is then
               sketched to provide elements of a theory of supply of
               regulation. A variety of empirical evidence and illustration is
               also presented.",
  journal   = "The Bell Journal of Economics and Management Science",
  publisher = "[Wiley, RAND Corporation]",
  volume    =  2,
  number    =  1,
  pages     = "3--21",
  year      =  1971,
  keywords  = "unintended consequences;BAFA2018"
}

@ARTICLE{Kuosmanen2006-lo,
  title    = "The law of one price in data envelopment analysis: Restricting
              weight flexibility across firms",
  author   = "Kuosmanen, Timo and Cherchye, Laurens and Sipil{\"a}inen, Timo",
  abstract = "The Law of One Price (LoOP) states that all firms face the same
              prices for their inputs and outputs under market equilibrium.
              Taken here as a normative condition for `efficiency prices', this
              law has powerful implications for productive efficiency analysis,
              which have remained unexploited thus far. This paper shows how
              LoOP-based weight restrictions can be incorporated in Data
              Envelopment Analysis (DEA). Utilizing the relation between
              industry-level and firm-level cost efficiency measures, we
              propose to apply a set of input prices that is common for all
              firms and that maximizes the cost efficiency of the industry. Our
              framework allows for firm-specific output weights and for
              variable returns-to-scale, and preserves the linear programming
              structure of the standard DEA. We apply the proposed methodology
              to the evaluation of the research efficiency of economics
              departments of Dutch Universities. This application shows that
              the methodology is computationally tractable for practical
              efficiency analysis, and that it helps in deepening the DEA
              analysis.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  170,
  number   =  3,
  pages    = "735--757",
  month    =  may,
  year     =  2006,
  keywords = "Data envelopment analysis; Law of one price; Industry-level
              efficiency; Weight restrictions; Research efficiency;4*"
}

@ARTICLE{Kuosmanen2008-dr,
  title     = "Representation theorem for convex nonparametric least squares",
  author    = "Kuosmanen, Timo",
  abstract  = "Summary We examine a nonparametric least-squares regression
               model that endogenously selects the functional form of the
               regression function from the family of continuous, monotonic
               increasing and globally concave functions that can be
               nondifferentiable. We show that this family of functions can be
               characterized without a loss of generality by a subset of
               continuous, piece-wise linear functions whose intercept and
               slope coefficients are constrained to satisfy the required
               monotonicity and concavity conditions. This representation
               theorem is useful at least in three respects. First, it enables
               us to derive an explicit representation for the regression
               function, which can be used for assessing marginal properties
               and for the purposes of forecasting and ex post economic
               modelling. Second, it enables us to transform the infinite
               dimensional regression problem into a tractable quadratic
               programming (QP) form, which can be solved by standard QP
               algorithms and solver software. Importantly, the QP formulation
               applies to the general multiple regression setting. Third, an
               operational computational procedure enables us to apply
               bootstrap techniques to draw statistical inference.",
  journal   = "Econom. J.",
  publisher = "Blackwell Publishing Ltd",
  volume    =  11,
  number    =  2,
  pages     = "308--325",
  month     =  jul,
  year      =  2008,
  keywords  = "Concavity; Convexity; Curve fitting; Linear splines; Local
               linear approximation; Nonparametric methods; Regression
               analysis;Mendeley Import (May
               05)/Productivity/Efficiency/StoNED;StoNED"
}

@ARTICLE{Demirguc-Kunt2005-us,
  title     = "{Cross-Country} Empirical Studies of Systemic Bank Distress: A
               Survey",
  author    = "Demirg{\"u}{\c c}-Kunt, Asli and Detragiache, Enrica",
  abstract  = "A rapidly growing empirical literature is studying the causes
               and consequences of bank fragility in contemporary economies.
               The paper reviews the two basic methodologies adopted in
               cross-country empirical studies, the signals approach and the
               multivariate probability model, and their application to study
               the determinants of banking crises. The use of these models to
               provide early warnings for crises is also reviewed, as are
               studies of the economic effects of banking crises and of the
               policies to forestall them. The paper concludes by identifying
               directions for future research.",
  journal   = "Natl. Inst. Econ. Rev.",
  publisher = "SAGE Publications Ltd",
  volume    =  192,
  number    =  1,
  pages     = "68--83",
  month     =  apr,
  year      =  2005
}

@ARTICLE{Arrow1996-lf,
  title    = "Is there a role for benefit-cost analysis in environmental,
              health, and safety regulation?",
  author   = "Arrow, K J and Cropper, M L and Eads, G C and Hahn, R W and Lave,
              L B and Noll, R G and Portney, P R and Russell, M and
              Schmalensee, R and Smith, V K and Stavins, R N",
  journal  = "Science",
  volume   =  272,
  number   =  5259,
  pages    = "221--222",
  month    =  apr,
  year     =  1996,
  language = "en"
}

@ARTICLE{Fare1993-oo,
  title     = "Derivation of Shadow Prices for Undesirable Outputs: A Distance
               Function Approach",
  author    = "F{\"a}re, Rolf and Grosskopf, Shawna and {C. A. Knox Lovell} and
               Yaisawarng, Suthathip",
  abstract  = "Many production activities generate undesirable byproducts in
               conjunction with the desirable outputs they produce. Pittman
               (1983) showed how to adjust productivity calculations, and
               F{\"a}re et al. (1989) showed how to adjust efficiency measures,
               in the presence of undesirable outputs. Here we show how to
               estimate output distance functions as frontiers in order to
               generate shadow values of the undesirable outputs that are
               required to make both types of adjustment. An empirical
               application is provided.",
  journal   = "Rev. Econ. Stat.",
  publisher = "The MIT Press",
  volume    =  75,
  number    =  2,
  pages     = "374--380",
  year      =  1993,
  keywords  = "WIP"
}

@ARTICLE{Wang2013-gi,
  title    = "Scenario-based energy efficiency and productivity in China: A
              non-radial directional distance function analysis",
  author   = "Wang, H and Zhou, P and Zhou, D Q",
  abstract = "Improving energy efficiency and productivity is one of the most
              cost-effective ways for achieving the sustainable development
              target in China. This paper employs non-radial directional
              distance function approach to empirically investigate energy
              efficiency and energy productivity by including CO2 emissions as
              an undesirable output. Three production scenarios, namely energy
              conservation (EC), energy conservation and emission reduction
              (ECER), and energy conservation, emission reduction and economic
              growth (ECEREG), are specified to assess China's energy
              efficiency and productivity growth during the period of Eleventh
              Five-Year Plan. Our empirical results show that there exist
              substantial differences in China's total-factor energy efficiency
              and productivity under different scenarios. Under the ECEREG
              scenario, the national average total-factor energy efficiency
              score was 0.6306 in 2005-2010, while the national average
              total-factor energy productivity increased by 0.27\% annually
              during the period. The main driving force for energy productivity
              growth in China was energy technological change rather than
              energy efficiency change. ?? 2013 Elsevier B.V.",
  journal  = "Energy Econ.",
  volume   =  40,
  pages    = "795--803",
  month    =  nov,
  year     =  2013,
  keywords = "CO2 emissions; Energy efficiency; Energy productivity; Non-radial
              directional distance function"
}

@ARTICLE{Wang2013-cf,
  title    = "A Malmquist {CO2} emission performance index based on a
              metafrontier approach",
  author   = "Wang, Qunwei and Zhang, Huiming and Zhang, Wei",
  abstract = "Considering carbon dioxide (CO2) emitters in different groups
              that may not have the same technology, this paper extends the
              Malmquist emission performance index (MCPI) for measuring changes
              in CO2 emission performance using a nonparametric metafrontier.
              The MCPI can be decomposed into two components: efficiency change
              index and technological change index. Meanwhile, the
              interrelationships between two MCPIs, relative to the
              metafrontier and the group frontier respectively, are also
              examined by the key factor, technology gap ratio (TGR).",
  journal  = "Math. Comput. Model.",
  volume   =  58,
  number   =  5,
  pages    = "1068--1073",
  month    =  sep,
  year     =  2013,
  keywords = "CO2 emission performance; Malmquist index; Metafrontier function;
              Undesirable output; CO emission performance"
}

@ARTICLE{Zhou2010-ge,
  title    = "Total factor carbon emission performance: A Malmquist index
              analysis",
  author   = "Zhou, P and Ang, B W and Han, J Y",
  abstract = "This paper introduces a Malmquist CO2 emission performance index
              (MCPI) for measuring changes in total factor carbon emission
              performance over time. The MCPI is derived by solving several
              data envelopment analysis models. Bootstrapping MCPI is proposed
              to perform statistical inferences on the MCPI results. Using the
              index the emission performance of the world's 18 top CO2 emitters
              from 1997 to 2004 is studied. The results obtained show that the
              total factor carbon emission performance of the countries as a
              whole improved by 24\% over the period and this was mainly driven
              by technological progress. The results of a cross-country
              regression analysis to investigate the determinants of the
              resulting MCPI are presented.",
  journal  = "Energy Econ.",
  volume   =  32,
  number   =  1,
  pages    = "194--201",
  month    =  jan,
  year     =  2010,
  keywords = "Bootstrap; C61; Carbon dioxide emissions; D24; Data envelopment
              analysis; Malmquist index; Q43; Q54; Total factor productivity"
}

@ARTICLE{Bretholt2013-ka,
  title    = "Evolving the latent variable model as an environmental {DEA}
              technology",
  author   = "Bretholt, Abraham and Pan, Jeh-Nan",
  abstract = "This article tests several nonparametric DEA models for their
              ability to accurately decompose CO2 emissions change using a
              Malmquist styled decomposition framework. This production
              oriented activity analysis involves panel data and two data sets
              from the literature for comparison. A new Latent Variable radial
              input-oriented technology is introduced that is closely
              associated with a Koopmans Efficient Slacks Based Model. The
              Latent Variable technology simultaneously reduces inputs and
              undesirable outputs in a single Multiple Objective Linear
              Program. This production theoretic methodology is adapted to
              preserve both scale efficiency and causality within the
              envelopment framework. Finally, the application studies
              demonstrate the internal consistency of the Latent Variable
              reduction coefficients, which overturns previous results and
              paves the way for further research into undesirable
              externalities.",
  journal  = "Omega",
  volume   =  41,
  number   =  2,
  pages    = "315--325",
  month    =  apr,
  year     =  2013,
  keywords = "DEA; Decomposition; Efficiency; Malmquist Index; Multicriteria;
              Optimization; Resource management; Scale efficiency; Slacks based
              model; Undesirable outputs and externalities"
}

@ARTICLE{Agee2014-bt,
  title    = "Non-separable pollution control: Implications for a {CO2}
              emissions cap and trade system",
  author   = "Agee, Mark D and Atkinson, Scott E and Crocker, Thomas D and
              Williams, Jonathan W",
  abstract = "The federal government now confronts considerable political
              pressure to add CO2 to the existing set of criteria air
              pollutants. As with current criteria pollutants, proposals call
              for control of CO2, assuming that the control of each of the
              three criteria pollutants is separable from the others. However,
              control of CO2, SO2, and NOX emissions is most appropriately
              viewed as joint rather than separable based on engineering
              relationships. Empirically, we also find considerable jointness.
              Using a 10-year panel for 77 U.S. electric utilities, which
              comprise the largest sector in terms of energy-related CO2
              emissions, we estimate a multiple-input, multiple-output
              directional distance function combining good inputs (production
              capital, pollution control capital, labor, and energy) and a bad
              input (sulfur burned) to produce good outputs (residential and
              industrial/commercial electricity production) and bad outputs
              (SO2, NOX, and CO2). We find that while utilities do not directly
              control CO2 emissions, considerable jointness exists across SO2,
              NOX, and CO2 emissions. Failure to account for this jointness
              increases the cost of pollution control, making it less
              acceptable to the public and policymakers. We also compute the
              technical efficiency of our set of utilities and find that
              considerable cost savings can be achieved by adopting the best
              technology for production of electricity and reduction of
              pollutants.",
  journal  = "Res. Energy Econ.",
  volume   =  36,
  number   =  1,
  pages    = "64--82",
  month    =  jan,
  year     =  2014,
  keywords = "U.S. electric power generation; CO2, SO2, NOX emissions;
              Efficient cap and trade system design; Directional distance
              function; Technical change; CO; SO; NO emissions"
}

@ARTICLE{Sueyoshi2012-ix,
  title    = "Returns to Scale, Damages to Scale, Marginal Rate of
              Transformation and Rate of Substitution in {DEA} Environmental
              Assessment",
  author   = "Sueyoshi, Toshiyuki and Goto, Mika",
  abstract = "This study discusses a new use of DEA (Data Envelopment Analysis)
              environmental assessment to measure MRT (Marginal Rate of
              Transformation) and RS (Rate of Substitution) between desirable
              and undesirable outputs. To discuss MRT and RS, this study first
              examines a concept of disposability from the perspective of
              corporate strategies to adapt a regulation change on undesirable
              outputs. The concept of disposability is separated into natural
              and managerial disposability. Then, this study explores the
              computational framework of RTS (Returns to Scale) and DTS
              (Damages to Scale). The type of RTS is measured within the
              natural disposability, while the type of DTS is measured within
              the managerial disposability. Considering the two types of
              disposability, this study discusses MRT and RS between desirable
              and undesirable outputs. As an illustrative example, this study
              applies the proposed approach to evaluate the performance of US
              coal-fired power plants. This study finds that the regulation
              policy on NOx and SO2 has been effective on their emission
              controls under US Clean Air Act (CAA). The regulation on CO2, or
              a major source of the global warming and climate change, is still
              insufficient in the United States. Therefore, this study
              recommends that US federal and local governments should regulate
              the amount of CO2 emission under the CAA.",
  journal  = "Energy Econ.",
  volume   =  34,
  number   =  4,
  pages    = "905--917",
  month    =  jul,
  year     =  2012,
  keywords = "Environmental assessment; Returns to Scale; Damages to Scale;
              Rate of Substitution"
}

@ARTICLE{Zhou2014-qe,
  title    = "On estimating shadow prices of undesirable outputs with
              efficiency models: A literature review",
  author   = "Zhou, P and Zhou, X and Fan, L W",
  abstract = "Undesirable outputs (or bads) refer to the byproducts accompanied
              with desirable outputs (or goods) in a production process, e.g.
              sulfur dioxide and carbon dioxide in coal-fired power generation.
              The shadow price of undesirable output, which may be interpreted
              as the opportunity cost of abating one additional unit of
              undesirable output in terms of the loss of desirable output,
              could provide valuable reference information for policy analysis
              and making. A prevalent practice is to use the Shephard or
              directional distance function to derive the shadow price, which
              can be further calculated by parametric or nonparametric
              efficiency models. In application, earlier studies have estimated
              shadow prices at plant, sector and even economy levels. This
              study aims to conduct a systematic review of the studies on
              estimating shadow prices of undesirable outputs with efficiency
              models. We first introduce the methodological framework for
              deriving shadow prices as well as the nonparametric/parametric
              efficiency models for calculating their values. A systematic
              summary of over forty earlier studies in this field is then
              conducted, through which the key features of the existing studies
              are summarized and possible future research directions are
              identified.",
  journal  = "Appl. Energy",
  volume   =  130,
  pages    = "799--806",
  month    =  oct,
  year     =  2014,
  keywords = "CO2 emissions; Shadow price; Undesirable output; Efficiency;
              Distance function; CO emissions"
}

@ARTICLE{Kuosmanen2017-qg,
  title    = "Modeling joint production of multiple outputs in {{StoNED}}:
              Directional distance function approach",
  author   = "Kuosmanen, Timo and Johnson, Andrew",
  abstract = "Estimation of joint production technologies involving multiple
              outputs has proved a vexing challenge. Existing methods are
              unsatisfactory as they either assume away stochastic noise or
              restrict to functional forms that have incorrect output
              curvature. The first contribution of this paper is to develop a
              new probabilistic data generating process that is compatible with
              the directional distance function. The directional distance
              function is a very general functional characterization of
              production technology that has proved useful for modeling joint
              production of multiple outputs. The second contribution of this
              paper is to develop a new estimator of the directional distance
              function that builds upon axiomatic properties and does not
              require any functional form assumptions. The proposed estimator
              is a natural extension of stochastic nonparametric envelopment of
              data (StoNED) framework to multiple output setting. We examine
              the practical aspects and usefulness of the proposed approach in
              the context of incentive regulation of the Finnish electricity
              distribution firms.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  262,
  number   =  2,
  pages    = "792--801",
  month    =  oct,
  year     =  2017,
  keywords = "Data envelopment analysis (DEA); Economies of scope; Efficiency
              analysis; Frontier estimation; Stochastic nonparametric
              envelopment of data (StoNED)"
}

@ARTICLE{Lee2012-qn,
  title    = "Technical efficiency, shadow price of carbon dioxide emissions,
              and substitutability for energy in the Chinese manufacturing
              industries",
  author   = "Lee, Myunghun and Zhang, Ning",
  abstract = "China is the world's largest CO2 producer and energy consumer. In
              this paper, we calculate the maximum technically obtainable CO2
              emissions reduction from the efficient use of inputs and estimate
              the shadow prices of CO2 emissions in order to assess the
              potential cost savings deriving from trading emissions among
              industries by measuring the input distance function for 30
              Chinese manufacturing industries. Our empirical results indicate
              that CO2 emissions could be reduced by as much as 680million tons
              in the aggregate. The shadow prices of CO2 vary from a high of
              $18.82 to a low of zero across industries, with an average of
              $3.13 per ton. Additionally, the estimated indirect Morishima
              elasticities of substitution of capital for fossil fuels indicate
              that the substitutabilities of capital for oil, gas, and coal are
              higher than the substitutability for labor.",
  journal  = "Energy Econ.",
  volume   =  34,
  number   =  5,
  pages    = "1492--1497",
  month    =  sep,
  year     =  2012,
  keywords = "Technical efficiency; CO shadow price; Morishima substitution
              elasticity; Chinese manufacturing industry"
}

@ARTICLE{OBrien1981-vh,
  title    = "A simple test for variance effects in experimental designs",
  author   = "O'Brien, Ralph G",
  abstract = "Although experimental effects are usually assessed through
              contrasts of group means, there are situations in which
              differences among the groups' variances are also of interest.
              Such analyses are infrequently used in behavioral research,
              possibly because the most common methods are not robust to
              nonnormally distributed data. A procedure is presented that
              produces robust tests of the equality of cell variances by
              performing a regular ANOVA using a transformation of the
              dependent variable. Special contrasts (e.g., simple effects,
              subeffects) are discussed and an example is given. (29 ref)",
  journal  = "Psychol. Bull.",
  volume   =  89,
  number   =  3,
  pages    = "570--574",
  year     =  1981
}

@ARTICLE{Zaim2000-sz,
  title    = "Environmental efficiency in carbon dioxide emissions in the
              {{OECD}}: A non-parametric approach",
  author   = "Zaim, O and Taskin, F",
  abstract = "Abstract The role of the environment is an important issue in the
              policy-making and hence, the accurate assessment of the
              environmental conditions is vital. In this paper, an
              environmental efficiency index is developed for the OECD
              countries using non-parametric techniques. The approach adopted
              is based on the assumption that there is just one production
              process behind the production of both goods and pollution
              emissions. The index derived in this work measures the extent of
              the required output sacrifice, due to the transformation of the
              production process, from one where all outputs are strongly
              disposable to the one which is characterized by weak
              disposability of pollutants. Using this index, we first conduct
              cross-section comparisons on the state of each country's
              production process in its treatment of pollution emissions. We
              then trace each country's modification of their production
              processes overtime. The results indicate that if the
              disposability for CO2emissions were strictly restricted as the
              result of an environmental regulation, the total value of output
              loss to the OECD countries as a whole would correspond to 3.7,
              4.8 and 3.5\% of the total OECD GDP for 1980, 1985 and 1990,
              respectively.",
  journal  = "J. Environ. Manage.",
  volume   =  58,
  number   =  2,
  pages    = "95--107",
  month    =  feb,
  year     =  2000,
  keywords = "environmental efficiency index, carbon dioxide emissions,
              non-parametric efficiency measurement."
}

@ARTICLE{Gomes2008-hg,
  title     = "Modelling undesirable outputs with zero sum gains data
               envelopment analysis models",
  author    = "Gomes, E G and Lins, M P E",
  abstract  = "Data envelopment analysis (DEA) literature has proposed
               alternative models for performance assessment in the presence of
               undesirable outputs, such as pollutant emissions, where
               increased outputs imply reduced performance. However, the case
               where global equilibrium of outputs should be imposed has not
               yet been considered. We propose that the zero sum gains DEA
               (ZSG-DEA) models look especially suitable for treating
               equilibrium models, where the sum of the quantities produced by
               all decision-making units can be set as the upper admissible
               bound. This paper uses ZSG-DEA models to evaluate the carbon
               dioxide emission case study, which can be considered part of the
               Kyoto Protocol statement.",
  journal   = "J. Oper. Res. Soc.",
  publisher = "Palgrave Macmillan UK",
  volume    =  59,
  number    =  5,
  pages     = "616--623",
  month     =  may,
  year      =  2008,
  language  = "en"
}

@ARTICLE{Zhou2008-rk,
  title    = "Linear programming models for measuring economy-wide energy
              efficiency performance",
  author   = "Zhou, P and Ang, B W",
  abstract = "Data envelopment analysis (DEA) has recently gained popularity in
              energy efficiency analysis. A common feature of the previously
              proposed DEA models for measuring energy efficiency performance
              is that they treat energy consumption as an input within a
              production framework without considering undesirable outputs.
              However, energy use results in the generation of undesirable
              outputs as by-products of producing desirable outputs. Within a
              joint production framework of both desirable and undesirable
              outputs, this paper presents several DEA-type linear programming
              models for measuring economy-wide energy efficiency performance.
              In addition to considering undesirable outputs, our models treat
              different energy sources as different inputs so that changes in
              energy mix could be accounted for in evaluating energy
              efficiency. The proposed models are applied to measure the energy
              efficiency performances of 21 OECD countries and the results
              obtained are presented.",
  journal  = "Energy Policy",
  volume   =  36,
  number   =  8,
  pages    = "2911--2916",
  month    =  aug,
  year     =  2008,
  keywords = "Energy efficiency; Undesirable outputs; Data envelopment analysis"
}

@ARTICLE{Zhou2012-bs,
  title    = "Measuring economy-wide energy efficiency performance: A
              parametric frontier approach",
  author   = "Zhou, P and Ang, B W and Zhou, D Q",
  abstract = "This paper proposes a parametric frontier approach to estimating
              economy-wide energy efficiency performance from a production
              efficiency point of view. It uses the Shephard energy distance
              function to define an energy efficiency index and adopts the
              stochastic frontier analysis technique to estimate the index. A
              case study of measuring the economy-wide energy efficiency
              performance of a sample of OECD countries using the proposed
              approach is presented. It is found that the proposed parametric
              frontier approach has higher discriminating power in energy
              efficiency performance measurement compared to its nonparametric
              frontier counterparts.",
  journal  = "Appl. Energy",
  volume   =  90,
  number   =  1,
  pages    = "196--200",
  month    =  feb,
  year     =  2012,
  keywords = "Energy efficiency; Distance function; Stochastic frontier
              analysis; Data envelopment analysis"
}

@BOOK{Ancev2017-si,
  title     = "New Directions in Productivity Measurement and Efficiency
               Analysis: Counting the Environment and Natural Resources",
  author    = "Ancev, Tihomir and Azad, M A Samad and Hern{\'a}ndez-Sancho,
               Francesc",
  abstract  = "This book explores novel research perspectives on the
               intersection of environmental/natural resource economics and
               productivity analysis, emphasizing the link between productivity
               and efficiency measurement and environmental impacts. The
               purpose of the book is to present new approaches and methods for
               measuring environmentally adjusted productivity and efficiency,
               and for incorporating natural resources in standard national
               accounting practices. These methods are applicable in many
               contexts, including air and water pollution, climate change,
               green accounting, and environmental regulation",
  publisher = "Edward Elgar Publishing",
  month     =  jun,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Zhou2012-km,
  title    = "Energy and {CO2} emission performance in electricity generation:
              A non-radial directional distance function approach",
  author   = "Zhou, P and Ang, B W and Wang, H",
  abstract = "This paper presents a non-radial directional distance function
              approach to modeling energy and CO2emission performance in
              electricity generation from the production efficiency point of
              view. We first define and construct the environmental production
              technologies for the countries with and without CHP plants,
              respectively. The non-radial direction distance function approach
              is then proposed and several indexes are developed to measure
              energy and CO2 emission performance of electricity generation.
              The directional distance functions established can be computed by
              solving a series of data envelopment analysis models. We then
              conduct an empirical study using the dataset for over one hundred
              countries. It is found that OECD countries have better carbon
              emission performance and integrated energy-carbon performance
              than non-OECD countries in electricity generation, while the
              difference in energy performance is not significant.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  221,
  number   =  3,
  pages    = "625--635",
  month    =  sep,
  year     =  2012,
  keywords = "Data envelopment analysis; Directional distance function; Energy
              efficiency; CO emission performance; Electricity generation"
}

@INCOLLECTION{Stephens1986-qb,
  title     = "Tests based on {EDF} statistics",
  booktitle = "Goodness-of-fit-techniques",
  author    = "Stephens, Michael A",
  publisher = "Routledge",
  pages     = "97--194",
  year      =  1986
}

@ARTICLE{Cifci2018-eu,
  title     = "Reassessing the Links between {GHG} Emissions, Economic Growth,
               and the {{UNFCCC}}: A {Difference-in-Differences} Approach",
  author    = "Cifci, Eren and Oliver, Matthew E",
  abstract  = "International climate agreements such as the Kyoto Protocol of
               1997 and, more recently, the Paris Climate Agreement are fragile
               because, at a national level, political constituencies' value
               systems may conflict with the goal of reducing greenhouse gas
               (GHG) emissions to sustainable levels. Proponents cite climate
               change as the most pressing challenge of our time, contending
               that international cooperation will play an essential role in
               addressing this challenge. Political opponents argue that the
               disproportionate requirements on developed nations to shoulder
               the financial burden will inhibit their economic growth. We find
               empirical evidence that both arguments are likely to be correct.
               We use standard regression techniques to analyze a multi-country
               dataset of GHG emissions, GDP per capita growth, and other
               factors. We estimate that after the Kyoto Protocol (KP) entered
               into force `Annex I' countries reduced GHG emissions on average
               by roughly 1 million metric tons of CO2 equivalent (MTCO2e),
               relative to non-Annex I countries. However, our estimates reveal
               that these countries also experienced an average reduction in
               GDP per capita growth rates of around 1--2 percentage points
               relative to non-Annex I countries.",
  journal   = "Sustain. Sci. Pract. Policy",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  10,
  number    =  2,
  pages     = "334",
  month     =  jan,
  year      =  2018,
  language  = "en"
}

@ARTICLE{De_Angelis2019-yq,
  title     = "Climate Change and Economic Growth: The Role of Environmental
               Policy Stringency",
  author    = "de Angelis, Enrico Maria and Di Giacomo, Marina and Vannoni,
               Davide",
  abstract  = "The paper investigates the relationship between economic growth
               and environmental quality in the context of the Kuznets curve,
               which foresees that growth, while initially causing negative
               externalities for the environment, eventually can be seen also
               as the solution to environmental degradation. The novelty of the
               paper is to analyze the role of environmental policies, and in
               particular the use of market-based and non-market instruments to
               challenge the pollution plague and mitigate climate change. The
               results of fixed effects estimates on a sample of 32 countries
               observed for the period 1992--2012 show the existence of an
               inverted U-shaped relationship between per capita gross domestic
               product (GDP) and per-capita CO2 emissions for the quadratic
               specification, as well as of an N-shaped pattern for the cubic
               specification. Most importantly, the stringency indexes, i.e.,
               the proxies used to account for environmental regulation,
               exhibit negative and strongly significant coefficients,
               suggesting that the policies are effective in reducing
               environmental damages associated with economic growth.",
  journal   = "Sustain. Sci. Pract. Policy",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  11,
  number    =  8,
  pages     = "2273",
  month     =  apr,
  year      =  2019,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Chateau2018-zg,
  title     = "Impacts of Green Growth Policies on Labour Markets and Wage
               Income Distribution",
  author    = "Chateau, Jean and Bibas, Ruben and Lanzi, Elisa",
  abstract  = "This paper explores the consequences on the labour markets of
               structural changes induced by decarbonisation policies. These
               policies are likely going to have consequences on labour-income
               distribution given i) existing rigidities in the labour markets,
               and ii) their ‚Ä¶",
  journal   = "OECD Environmental Working Papers No. 137",
  publisher = "OECD",
  year      =  2018
}

@ARTICLE{Vale2016-dm,
  title    = "The changing climate of climate change economics",
  author   = "Vale, Petterson Molina",
  abstract = "Climate change economics is now four decades old. Much of what it
              has achieved as a field of academic enquiry can be linked back to
              issues of integrated assessment modelling. This paper shows that
              the standard approach is going through a major change in scope as
              of the last five years. The conventional focus on determining
              optimal mitigation paths based on modelling the social cost of
              carbon is being enlarged to embrace promising new waves of
              research. These are: (1) the economics of insurance against
              catastrophic risks; (2) the economics of trade and climate; and
              (3) the economics of climate change adaptation. The paper helps
              to bridge the gap between economics and climate policy by showing
              that the analytical toolkit of climate change economics has
              shifted towards more realistic representations of climatic
              policy.",
  journal  = "Ecol. Econ.",
  volume   =  121,
  pages    = "12--19",
  month    =  jan,
  year     =  2016,
  keywords = "Climate change; Economics; Carbon; Adaptation; Insurance; Trade"
}

@ARTICLE{Manne2004-va,
  title    = "{US} rejection of the Kyoto Protocol: the impact on compliance
              costs and {CO2} emissions",
  author   = "Manne, Alan and Richels, Richard",
  abstract = "Despite the US rejection of the Kyoto Protocol, the meeting of
              the parties to the United Nations Framework Convention on Climate
              Change in July 2001 has increased the likelihood that the
              Protocol will be ratified. This raises a number of issues
              concerning mitigation costs, particularly for the buyers and
              sellers of emission permits. In this paper, we examine how the US
              decision is likely to affect compliance costs for other Annex B
              countries during the first commitment period. We also explore the
              implications for US emissions. Key findings include:
              1.Participating Organisation for Economic Co-operation and
              Development countries may experience a decline in mitigation
              costs, but because of the banking provision contained in the
              Protocol, the decline may not be as great as some would
              suggest;2.If the majority of ``hot air'' is concentrated in a
              small number of countries in Eastern Europe and the former Soviet
              Union, these countries may be able to organize a sellers' cartel
              and extract sizable economic rents; and3.Even in the absence of
              mandatory emission reduction requirements, US emissions in 2010
              may be lower than their business-as-usual baseline because of
              expectations regarding future regulatory requirements.",
  journal  = "Energy Policy",
  volume   =  32,
  number   =  4,
  pages    = "447--454",
  month    =  mar,
  year     =  2004,
  keywords = "Greenhouse gases; Emissions abatement; Kyoto"
}

@ARTICLE{Hovi2012-rp,
  title     = "Why the United States did not become a party to the Kyoto
               Protocol: German, Norwegian, and {US} perspectives",
  author    = "Hovi, Jon and Sprinz, Detlef F and Bang, Guri",
  abstract  = "According to two-level game theory, negotiators tailor
               agreements at the international level to be ratifiable at the
               domestic level. This did not happen in the Kyoto negotiations,
               however, in the US case. We interviewed 26 German, Norwegian,
               and US participants in and observers of the climate negotiations
               concerning their views on three explanations for why the United
               States did not become a party to Kyoto. Explanation 1 argues
               that Kyoto delegations mistakenly thought the Senate was
               bluffing when adopting Byrd?Hagel. Explanation 2 contends that
               Europeans preferred a more ambitious agreement without US
               participation to a less ambitious agreement with US
               participation. Finally, explanation 3 suggests that in Kyoto the
               Clinton?Gore administration gave up on Senate ratification, and
               essentially pushed for an agreement that would provide them a
               climate-friendly face. While all explanations received some
               support from interviewees, explanation 1 and (particularly)
               explanation 3 received considerably more support than
               explanation 2.",
  journal   = "European Journal of International Relations",
  publisher = "SAGE Publications Ltd",
  volume    =  18,
  number    =  1,
  pages     = "129--150",
  month     =  mar,
  year      =  2012
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Brechin2003-ip,
  title    = "Comparative public opinion and knowledge on global climatic
              change and the Kyoto Protocol: the {US} versus the world?",
  author   = "Brechin, Steven R",
  abstract = "Using a variety of public opinion polls over a number of years
              and from a number of countries this paper revisits the questions
              of crossnational public concern for global warming first examined
              over a decade ago. Although the scientific community today speaks
              out on global climatic change in essentially a unified voice
              concerning its anthropogenic causes and potential devastating
              impacts at the global level, it remains the case that many
              citizens of a number of nations still seem to harbor considerable
              uncertainties about the problem itself. Although it could be
              argued that there has been a slight improvement over the last
              decade in the public's understanding regarding the anthropogenic
              causes of global warming, the people of all the nations studied
              remain largely uniformed about the problem. In a recent
              international study on knowledge about global warming, the
              citizens of Mexico led all fifteen countries surveyed in 2001
              with just twenty‚Äêsix percent of the survey respondents correctly
              identifying burning fossil fuels as the primary cause of global
              warming. The citizens of the U.S., among the most educated in the
              world, where somewhere in the middle of the pack, tied with the
              citizens of Brazil at fifteen percent, but slightly lower than
              Cubans. In response to President Bush's withdrawal of the Kyoto
              Protocol in 1991, the U.S. public appears to be far more
              supportive of the action than the citizens of a number of
              European countries where there was considerable outrage about the
              decision.",
  journal  = "Int. J. Sociol. Soc. Policy",
  volume   =  23,
  number   =  10,
  pages    = "106--134",
  year     =  2003
}

@ARTICLE{Nordhaus1999-qk,
  title     = "Requiem for Kyoto: An Economic Analysis of the Kyoto Protocol",
  author    = "Nordhaus, William D and Boyer, Joseph G",
  abstract  = "[This paper uses the newly developed RICE-98 model to analyze
               the economics of the Kyoto Protocol. It analyzes versions of the
               Kyoto Protocol that have different approaches to trading
               emissions rights and compares these with efficient approaches.
               The major conclusions are: (a) the net global cost of the Kyoto
               Protocol is \$716 billion in present value, (b) the United
               States bears almost two-thirds of the global cost; and (c) the
               benefit-cost ratio of the Kyoto Protocol is 1/7. Additionally,
               the emissions strategy is highly cost-ineffective, with the
               global temperature reduction achieved at a cost almost 8 times
               the cost of a strategy which is cost-effective in terms of
               ``where'' and ``when'' efficiency. These conclusions assume that
               trading in carbon permits is allowed among the Annex I
               countries.]",
  journal   = "Energy J.",
  publisher = "International Association for Energy Economics",
  volume    =  20,
  pages     = "93--130",
  year      =  1999
}

@ARTICLE{Wouter_Botzen2012-du,
  title    = "How sensitive is Nordhaus to Weitzman? Climate policy in {DICE}
              with an alternative damage function",
  author   = "Wouter Botzen, W J and van den Bergh, Jeroen C J M",
  abstract = "The damage function in the famous climate-economy model DICE has
              received much criticism. Weitzman (2010) has proposed an
              alternative approach that gives more serious attention to climate
              change impacts for larger temperature increases. We calculate
              optimal climate policy with DICE using this approach. Optimal
              emission abatement trajectories turn out to be very sensitive to
              the damage specification. We summarise the difference between the
              associated optimal abatement costs in NPV terms.",
  journal  = "Econ. Lett.",
  volume   =  117,
  number   =  1,
  pages    = "372--374",
  month    =  oct,
  year     =  2012,
  keywords = "Climate change; DICE model; Stern discounting"
}

@ARTICLE{Markandya2015-st,
  title    = "Analyzing Trade-offs in International Climate Policy Options: The
              Case of the Green Climate Fund",
  author   = "Markandya, A and Antimiani, A and Costantini, V and Martini, C
              and Palma, A and Tommasino, M C",
  abstract = "Summary We investigate the trade-offs between economic growth and
              low carbon targets for developing and developed countries in the
              period up to 2035. Policy options are evaluated with an original
              version of the dynamic CGE model GDynE. Abatement costs appear to
              be strongly detrimental to economic growth for developing
              countries. We investigate options for reducing these costs that
              are consistent with the current negotiations. We show that the
              Green Climate Fund financed through a levy on carbon taxation can
              benefit all parties, and large benefits are associated with
              investment of the Green Climate Fund to foster energy efficiency
              in developing countries.",
  journal  = "World Dev.",
  volume   =  74,
  pages    = "93--107",
  month    =  oct,
  year     =  2015,
  keywords = "climate change policies; Green Climate Fund; developing
              countries; dynamic CGE energy model"
}

@ARTICLE{Maradan2005-lq,
  title     = "Marginal costs of carbon dioxide abatement: empirical evidence
               from cross-country analysis",
  author    = "Maradan, David and Vassiliev, Anatoli and {Others}",
  journal   = "Revue Suisse d Economie et de Statistique",
  publisher = "Citeseer",
  volume    =  141,
  number    =  3,
  pages     = "377",
  year      =  2005
}

@MISC{Du2015-uo,
  title   = "Marginal Abatement Costs of Carbon Dioxide Emissions in China: A
             Parametric Analysis",
  author  = "Du, Limin and Hanley, Aoife and Wei, Chu",
  journal = "Environmental and Resource Economics",
  volume  =  61,
  number  =  2,
  pages   = "191--216",
  year    =  2015
}

@MISC{Wang2011-em,
  title   = "Marginal abatement costs of carbon dioxide in China: A
             nonparametric analysis",
  author  = "Wang, Qunwei and Cui, Qinjun and Zhou, Dequn and Wang, Sisi",
  journal = "Energy Procedia",
  volume  =  5,
  pages   = "2316--2320",
  year    =  2011
}

@MISC{Chen2011-dg,
  title   = "The Abatement of Carbon Dioxide Intensity in China: Factors
             Decomposition and Policy Implications",
  author  = "Chen, Shiyi",
  journal = "The World Economy",
  volume  =  34,
  number  =  7,
  pages   = "1148--1167",
  year    =  2011
}

@ARTICLE{Choi2012-rg,
  title    = "Efficiency and abatement costs of energy-related {CO2} emissions
              in China: A slacks-based efficiency measure",
  author   = "Choi, Yongrok and Zhang, Ning and Zhou, P",
  abstract = "This paper uses nonparametric efficiency analysis technique to
              estimate the energy efficiency, potential emission reductions and
              marginal abatement costs of energy-related CO2 emissions in
              China. We employ a non-radial slacks-based data envelopment
              analysis (DEA) model for estimating the potential reductions and
              efficiency of CO2 emissions for China. The dual model of the
              slacks-based DEA model is then used to estimate the marginal
              abatement costs of CO2 emissions. An empirical study based on
              China's panel data (2001--2010) is carried out and some policy
              implications are also discussed.",
  journal  = "Appl. Energy",
  volume   =  98,
  pages    = "198--208",
  month    =  oct,
  year     =  2012,
  keywords = "CO emissions; Efficiency; Abatement costs; Slacks-based measure
              (SBM); Data Envelopment Analysis (DEA); China"
}

@ARTICLE{Lee2019-vt,
  title    = "Nash marginal abatement cost estimation of air pollutant
              emissions using the stochastic semi-nonparametric frontier",
  author   = "Lee, Chia-Yen and Wang, Ke",
  abstract = "Emissions trading (or cap and trade) is a market-based approach
              providing economic incentives for achieving reductions in the
              emissions of pollutants. Marginal abatement costs (MAC), also
              termed shadow prices of air pollution emissions, provide valuable
              guidelines to support environmental regulatory policies for CO2,
              SO2 and NOx, the key contributors to climate change, smog, and
              acid rain. This study estimates the marginal abatement cost of
              undesirable outputs with respect to the Nash equilibrium on the
              stochastic semi-nonparametric envelopment of data (StoNED) in an
              imperfectly competitive market. Considering an endogenous price
              function of electricity, the mixed complementarity problem (MiCP)
              is formulated to identify the Nash equilibrium in a production
              possibility set. The proposed model addresses the four issues of
              MAC estimation in the existing literature. Applying the proposed
              method to an empirical study of 33 coal-fired power plants
              operating in China in 2013 shows that StoNED provides a robust
              frontier that is not sensitive to the outlier and the proposed
              interval of MAC estimation validates the shadow prices
              corresponding to the Nash equilibrium in an imperfectly
              competitive market.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  273,
  number   =  1,
  pages    = "390--400",
  month    =  feb,
  year     =  2019,
  keywords = "Data envelopment analysis; Marginal abatement costs; Emissions
              trading; Nash equilibrium; Stochastic semi-nonparametric frontier"
}

@ARTICLE{Dafermos2018-nu,
  title    = "Climate Change, Financial Stability and Monetary Policy",
  author   = "Dafermos, Yannis and Nikolaidi, Maria and Galanis, Giorgos",
  abstract = "Using a stock-flow-fund ecological macroeconomic model, we
              analyse (i) the effects of climate change on financial stability
              and (ii) the financial and global warming implications of a green
              quantitative easing (QE) programme. Emphasis is placed on the
              impact of climate change damages on the price of financial assets
              and the financial position of firms and banks. The model is
              estimated and calibrated using global data and simulations are
              conducted for the period 2016--2120. Four key results arise.
              First, by destroying the capital of firms and reducing their
              profitability, climate change is likely to gradually deteriorate
              the liquidity of firms, leading to a higher rate of default that
              could harm both the financial and the non-financial corporate
              sector. Second, climate change damages can lead to a portfolio
              reallocation that can cause a gradual decline in the price of
              corporate bonds. Third, climate-induced financial instability
              might adversely affect credit expansion, exacerbating the
              negative impact of climate change on economic activity. Fourth,
              the implementation of a green corporate QE programme can reduce
              climate-induced financial instability and restrict global
              warming. The effectiveness of this programme depends positively
              on the responsiveness of green investment to changes in bond
              yields.",
  journal  = "Ecol. Econ.",
  volume   =  152,
  pages    = "219--234",
  month    =  oct,
  year     =  2018,
  keywords = "Ecological macroeconomics; Stock-flow consistent modelling;
              Climate change; Financial stability; Green quantitative easing"
}

@ARTICLE{Ciarli2019-fw,
  title    = "Modelling the Evolution of Economic Structure and Climate Change:
              A Review",
  author   = "Ciarli, Tommaso and Savona, Maria",
  abstract = "We discuss how different models assessing climate change
              integrate aspects of structural change that are crucial to
              improve understanding of the relation between changes in the
              environment and in the economy. We identify six related aspects
              of structural change, which have significant impact on climate
              change: sectoral composition, industrial organisation,
              technology, employment, final demand, and institutions. Economic
              models vary substantially with respect to the aspects of
              structural change that they include, and how they model them. We
              review different modelling families and compare these
              differences: integrated assessment models (IAM), computable
              general equilibrium (CGE) models, structural change models (SCM),
              ecological macroeconomics models in the Keynesian tradition (EMK)
              and evolutionary agent based models (EABM). We find that IAM and
              CGE address few of the aspects of structural change identified;
              SCM focus on the sectoral composition; and EKM study final demand
              and employment structure. But all these models are aggregate and
              omit the complexity of the interactions between structural and
              climate change. EABM have explored a larger number of aspects of
              structural change, modelling their emergence from the interaction
              of microeconomic actors, but have not yet exploited their
              potential to study the interactions among interrelated aspects of
              structural and climate change.",
  journal  = "Ecol. Econ.",
  volume   =  158,
  pages    = "51--64",
  month    =  apr,
  year     =  2019,
  keywords = "Structural change; Climate change; Economic modelling"
}

@ARTICLE{Tsigaris2019-zl,
  title    = "The potential impacts of climate change on capital in the 21st
              century",
  author   = "Tsigaris, Panagiotis and Wood, Joel",
  abstract = "An endogenous growth model with a simple climate system is used
              to examine the potential impacts of climate change on the
              capital-to-net income ratio and the net of depreciation share of
              income to capital, a measure of wealth concentration and income
              distribution between capital and labour respectively, over the
              next two centuries. If climate change only directly affects
              production, as usually assumed, the capital-to-net income ratio
              will increase as compared to what it would be in the absence of
              climate change. The capital-to-income ratio will increase even
              further if climate change affects labour productivity. In both
              cases, the increase in the ratio after 2100 is due to the stock
              of capital being depleted at a lower rate than net income is
              falling. However, the capital-to-net income ratio will be lower
              and eventually fall if damage from climate change increases the
              depreciation rate of capital; this decline is marginally reduced
              if climate change impacts both capital and labour productivity.
              In the case where climate change impacts the depreciation of
              capital, the ratio after 2100 is falling because the stock of
              capital is destroyed faster than net-income is falling.
              Furthermore, climate change reduces the net share of income
              accruing to capital in all scenarios with dramatic changes in the
              case of climate change affecting the depreciation of capital.
              Emissions abatement almost completely mitigates these impacts on
              the capital-to-net income ratio and the net share of income to
              capital.",
  journal  = "Ecol. Econ.",
  volume   =  162,
  pages    = "74--86",
  month    =  aug,
  year     =  2019,
  keywords = "Climate change; Inequality; Wealth; Economic growth"
}

@ARTICLE{Stern2013-oz,
  title    = "The Structure of Economic Modeling of the Potential Impacts of
              Climate Change: Grafting Gross Underestimation of Risk onto
              Already Narrow Science Models",
  author   = "Stern, Nicholas",
  abstract = "The Structure of Economic Modeling of the Potential Impacts of
              Climate Change: Grafting Gross Underestimation of Risk onto
              Already Narrow Science Models by Nicholas Stern. Published in
              volume 51, issue 3, pages 838-59 of Journal of Economic
              Literature, September 2013, Abstract: Scientists describe t...",
  journal  = "J. Econ. Lit.",
  volume   =  51,
  number   =  3,
  pages    = "838--859",
  month    =  sep,
  year     =  2013
}

@ARTICLE{Wang2014-ge,
  title    = "Nonparametric quantile frontier estimation under shape
              restriction",
  author   = "Wang, Yongqiao and Wang, Shouyang and Dang, Chuangyin and Ge,
              Wenxiu",
  abstract = "This paper proposes a shape-restricted nonparametric quantile
              regression to estimate the $\tau$-frontier, which acts as a
              benchmark for whether a decision making unit achieves top $\tau$
              efficiency. This method adopts a two-step strategy: first,
              identifying fitted values that minimize an asymmetric absolute
              loss under the nondecreasing and concave shape restriction;
              second, constructing a nondecreasing and concave estimator that
              links these fitted values. This method makes no assumption on the
              error distribution and the functional form. Experimental results
              on some artificial data sets clearly demonstrate its superiority
              over the classical linear quantile regression. We also discuss
              how to enforce constraints to avoid quantile crossings between
              multiple estimated frontiers with different values of $\tau$.
              Finally this paper shows that this method can be applied to
              estimate the production function when one has some prior
              knowledge about the error term.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  232,
  number   =  3,
  pages    = "671--678",
  month    =  feb,
  year     =  2014,
  keywords = "Productivity and competitiveness; Production frontier; Quantile
              regression; Shape restriction; Concavity; Non-crossing"
}

@ARTICLE{Aiolfi2005-fy,
  title    = "Model uncertainty, thick modelling and the predictability of
              stock returns",
  author   = "Aiolfi, Marco and Favero, Carlo A",
  abstract = "Abstract Recent financial research has provided evidence on the
              predictability of asset returns. In this paper we consider the
              results contained in Pesaran and Timmerman (1995), which provided
              evidence on predictability of excess returns in the US stock
              market over the sample 1959?1992. We show that the extension of
              the sample to the nineties weakens considerably the statistical
              and economic significance of the predictability of stock returns
              based on earlier data. We propose an extension of their
              framework, based on the explicit consideration of model
              uncertainty under rich parameterizations for the predictive
              models. We propose a novel methodology to deal with model
              uncertainty based on ?thick? modelling, i.e. on considering a
              multiplicity of predictive models rather than a single predictive
              model. We show that portfolio allocations based on a thick
              modelling strategy systematically outperform thin
              modelling.?Copyright ? 2005 John Wiley \& Sons, Ltd.",
  journal  = "J. Forecast.",
  volume   =  24,
  number   =  4,
  pages    = "233--254",
  month    =  jul,
  year     =  2005
}

@ARTICLE{Hessou2017-ca,
  title     = "Basel {III} capital buffer requirements and credit union
               prudential regulation: Canadian evidence",
  author    = "Hessou, Helyoth and Lai, Van Son",
  abstract  = "Abstract Some Canadian provinces have already adopted Basel III
               rules for the oversight of their administrated credit unions. We
               analyze the importance of the Basel III additional capital
               buffer requirements for credit union prudential regulation.
               Based on a sample of the 100 largest credit unions in Canada
               from 1996 to 2014, we find that Canadian credit union capital
               buffers behave countercyclically over the business cycle.
               Further, credit unions hold a capital buffer bigger than the
               maximum buffer advocated under Basel III which is 5\% of
               risk-weighted assets (RWA). These results suggest that, unlike
               commercial banks worldwide, credit unions, by and large, are
               already in compliance with the new Basel III buffer
               requirements. However, there is evidence that the capital
               buffers of low-capitalized credit unions are procyclical. These
               credit unions increased their RWA during booms but failed to
               build up additional capital accordingly. Hence, weakly
               capitalized credit unions are more likely to adjust their
               capital buffers if they are subject to Basel III capital buffer
               regulation.",
  journal   = "Journal of Financial Stability",
  publisher = "Elsevier",
  volume    =  30,
  number    = "Supplement C",
  pages     = "92--110",
  month     =  jun,
  year      =  2017,
  keywords  = "Capital regulation; Credit union capital; Business cycle
               fluctuations; Countercyclical capital buffer; Conservation
               capital buffer; Basel III"
}

@MISC{Burkner_undated-dl,
  title  = "Ordinal Regression Models in Psychology: A Tutorial",
  author = "B{\"u}rkner, Paul - Christian and B{\"u}rkner, Paul-Christian and
            Vuorre, Matti"
}

@ARTICLE{Liddell2018-wc,
  title    = "Analyzing ordinal data with metric models: What could possibly go
              wrong?",
  author   = "Liddell, Torrin M and Kruschke, John K",
  abstract = "We surveyed all articles in the Journal of Personality and Social
              Psychology (JPSP), Psychological Science (PS), and the Journal of
              Experimental Psychology: General (JEP:G) that mentioned the term
              ``Likert,'' and found that 100\% of the articles that analyzed
              ordinal data did so using a metric model. We present novel
              evidence that analyzing ordinal data as if they were metric can
              systematically lead to errors. We demonstrate false alarms (i.e.,
              detecting an effect where none exists, Type I errors) and
              failures to detect effects (i.e., loss of power, Type II errors).
              We demonstrate systematic inversions of effects, for which
              treating ordinal data as metric indicates the opposite ordering
              of means than the true ordering of means. We show the same
              problems --- false alarms, misses, and inversions --- for
              interactions in factorial designs and for trend analyses in
              regression. We demonstrate that averaging across multiple ordinal
              measurements does not solve or even ameliorate these problems. A
              central contribution is a graphical explanation of how and when
              the misrepresentations occur. Moreover, we point out that there
              is no sure-fire way to detect these problems by treating the
              ordinal values as metric, and instead we advocate use of
              ordered-probit models (or similar) because they will better
              describe the data. Finally, although frequentist approaches to
              some ordered-probit models are available, we use Bayesian methods
              because of their flexibility in specifying models and their
              richness and accuracy in providing parameter estimates. An R
              script is provided for running an analysis that compares
              ordered-probit and metric models.",
  journal  = "J. Exp. Soc. Psychol.",
  volume   =  79,
  pages    = "328--348",
  month    =  nov,
  year     =  2018,
  keywords = "Ordinal data; Likert; Ordered-probit; Bayesian analysis"
}

@ARTICLE{Kruschke2018-br,
  title    = "Bayesian data analysis for newcomers",
  author   = "Kruschke, John K and Liddell, Torrin M",
  abstract = "This article explains the foundational concepts of Bayesian data
              analysis using virtually no mathematical notation. Bayesian ideas
              already match your intuitions from everyday reasoning and from
              traditional data analysis. Simple examples of Bayesian data
              analysis are presented that illustrate how the information
              delivered by a Bayesian analysis can be directly interpreted.
              Bayesian approaches to null-value assessment are discussed. The
              article clarifies misconceptions about Bayesian methods that
              newcomers might have acquired elsewhere. We discuss prior
              distributions and explain how they are not a liability but an
              important asset. We discuss the relation of Bayesian data
              analysis to Bayesian models of mind, and we briefly discuss what
              methodological problems Bayesian data analysis is not meant to
              solve. After you have read this article, you should have a clear
              sense of how Bayesian data analysis works and the sort of
              information it delivers, and why that information is so intuitive
              and useful for drawing conclusions from data.",
  journal  = "Psychon. Bull. Rev.",
  volume   =  25,
  number   =  1,
  pages    = "155--177",
  month    =  feb,
  year     =  2018,
  keywords = "Bayes factor; Bayesian analysis; Bayesian model; Confidence
              interval; Highest density interval; Null hypothesis significance
              test; Region of practical equivalence; Replication crisis; p
              value",
  language = "en"
}

@ARTICLE{Colquhoun2014-qg,
  title         = "An investigation of the false discovery rate and the
                   misinterpretation of {P} values",
  author        = "Colquhoun, David",
  abstract      = "The following proposition is justified from several
                   different points of view. If you use P = 0.05 to suggest
                   that you have made a discovery, you will be wrong at least
                   30 percent of the time. If, as is often the case,
                   experiments are under-powered, you will be wrong most of the
                   time. It is concluded that if you wish to keep your false
                   discovery rate below 5 percent, you need to use a 3-sigma
                   rule, or to insist on P value below 0.001. And never use the
                   word ``significant''.",
  month         =  jul,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "stat.AP",
  eprint        = "1407.5296"
}

@ARTICLE{Li2010-ay,
  title     = "The information content of forward-looking statements in
               corporate filings---A na{\"\i}ve Bayesian machine learning
               approach",
  author    = "Li, Feng",
  journal   = "Journal of Accounting Research",
  publisher = "Wiley Online Library",
  volume    =  48,
  number    =  5,
  pages     = "1049--1102",
  year      =  2010
}

@BOOK{Tufte2001-ea,
  title     = "The visual display of quantitative information",
  author    = "Tufte, Edward R",
  publisher = "Graphics press Cheshire, CT",
  volume    =  2,
  year      =  2001
}

@BOOK{McElreath2020-ag,
  title     = "Statistical Rethinking: A Bayesian Course with Examples in {R}
               and {STAN}",
  author    = "McElreath, Richard",
  abstract  = "Statistical Rethinking: A Bayesian Course with Examples in R and
               Stan builds your knowledge of and confidence in making
               inferences from data. Reflecting the need for scripting in
               today's model-based statistics, the book pushes you to perform
               step-by-step calculations that are usually automated. This
               unique computational approach ensures that you understand enough
               of the details to make reasonable choices and interpretations in
               your own modeling work. The text presents causal inference and
               generalized linear multilevel models from a simple Bayesian
               perspective that builds on information theory and maximum
               entropy. The core material ranges from the basics of regression
               to advanced multilevel models. It also presents measurement
               error, missing data, and Gaussian process models for spatial and
               phylogenetic confounding. The second edition emphasizes the
               directed acyclic graph (DAG) approach to causal inference,
               integrating DAGs into many examples. The new edition also
               contains new material on the design of prior distributions,
               splines, ordered categorical predictors, social relations
               models, cross-validation, importance sampling, instrumental
               variables, and Hamiltonian Monte Carlo. It ends with an entirely
               new chapter that goes beyond generalized linear modeling,
               showing how domain-specific scientific models can be built into
               statistical analyses. Features Integrates working code into the
               main text Illustrates concepts through worked data analysis
               examples Emphasizes understanding assumptions and how
               assumptions are reflected in code Offers more detailed
               explanations of the mathematics in optional sections Presents
               examples of using the dagitty R package to analyze causal graphs
               Provides the rethinking R package on the author's website and on
               GitHub",
  publisher = "CRC Press",
  month     =  mar,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Hanley2002-vn,
  title    = "Choice Modelling Approaches: A Superior Alternative for
              Environmental Valuatioin?",
  author   = "Hanley, Nick and Mourato, Susana and Wright, Robert E",
  abstract = "In this paper, we examine some popular ?choice modelling?
              approaches to environmental valuation, which can be considered as
              alternatives to more familiar valuation techniques based on
              stated preferences such as the contingent valuation method. A
              number of choice modelling methods are consistent with consumer
              theory, and its focus on an attribute-based theory of value
              permits a superior representation of many environmental
              management contexts. However, choice modelling surveys can place
              a severe cognitive burden upon respondents and induce satisficing
              rather than maximising behavioural patterns. In this framework,
              we seek to identify the best available choice modelling
              alternative and investigate its potential to ?solve? some of the
              major biases associated with standard contingent valuation. We
              then discuss its use in the light of policy appraisal needs
              within the EU. An application to the demand for rock climbing in
              Scotland is provided as an illustration.",
  journal  = "J. Econ. Surv.",
  volume   =  15,
  number   =  3,
  pages    = "435--462",
  month    =  dec,
  year     =  2002
}

@BOOK{Greene2010-dm,
  title     = "Modeling Ordered Choices: A Primer",
  author    = "Greene, William H and Hensher, David A",
  abstract  = "It is increasingly common for analysts to seek out the opinions
               of individuals and organizations using attitudinal scales such
               as degree of satisfaction or importance attached to an issue.
               Examples include levels of obesity, seriousness of a health
               condition, attitudes towards service levels, opinions on
               products, voting intentions, and the degree of clarity of
               contracts. Ordered choice models provide a relevant methodology
               for capturing the sources of influence that explain the choice
               made amongst a set of ordered alternatives. The methods have
               evolved to a level of sophistication that can allow for
               heterogeneity in the threshold parameters, in the explanatory
               variables (through random parameters), and in the decomposition
               of the residual variance. This book brings together
               contributions in ordered choice modeling from a number of
               disciplines, synthesizing developments over the last fifty
               years, and suggests useful extensions to account for the wide
               range of sources of influence on choice.",
  publisher = "Cambridge University Press",
  month     =  apr,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Gabry2019-qd,
  title     = "Visualization in Bayesian workflow",
  author    = "Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and
               Betancourt, Michael and Gelman, Andrew",
  journal   = "J. R. Stat. Soc. Ser. A Stat. Soc.",
  publisher = "Wiley",
  volume    =  182,
  number    =  2,
  pages     = "389--402",
  month     =  feb,
  year      =  2019,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

@MISC{Kong2020-bv,
  title   = "Modern Chinese banking networks during the Republican Era",
  author  = "Kong, Lingyu and Ploeckl, Florian",
  journal = "Business History",
  pages   = "1--27",
  year    =  2020
}

@UNPUBLISHED{Gehrig2017-fd,
  title    = "Did the Basel Process of Capital Regulation Enhance the
              Resiliency of European Banks?",
  author   = "Gehrig, Thomas and Iannino, Maria Chiara",
  abstract = "This paper analyses the evolution of the safety and soundness of
              the European banking sector during the various stages of the
              Basel process of capital regulation. In the first part we
              document the evolution of various measures of systemic risk as
              the Basel process unfolds. Most strikingly, we find that the
              exposure to systemic risk as measured by SRISK has been steeply
              rising for the highest quintile, moderately rising for the second
              quintile and remaining roughly stationary for the remaining three
              quintiles of listed European banks. This observation suggests
              that the Basel process has succeeded in containing systemic risk
              for the majority of European banks but not for the largest
              institutions. In the second part we analyse the drivers of
              systemic risk. We find compelling evidence that the increase in
              exposure to systemic risk (SRISK) is intimately tied to the
              implementation of internal models for determining credit risk as
              well as market risk. Based on this evidence, the sub-prime crisis
              found especially the largest and more systemic banks ill-prepared
              and lacking resiliency. This condition has even aggravated during
              the European sovereign crisis. Banking Union has not (yet)
              brought about a significant increase in the safety and soundness
              of the European banking system. Finally, low interest rates
              affect considerably to the contribution to systemic risk across
              the whole spectrum of banks.",
  journal  = "CEPR Discussion Paper No. DP11920",
  month    =  aug,
  year     =  2017,
  keywords = "bank capital, systemic risk, internal risk based models,
              contagion, resilience"
}

@BOOK{Hilpisch2018-cf,
  title     = "Python for Finance: Mastering {Data-Driven} Finance",
  author    = "Hilpisch, Yves",
  abstract  = "The financial industry has recently adopted Python at a
               tremendous rate, with some of the largest investment banks and
               hedge funds using it to build core trading and risk management
               systems. Updated for Python 3, the second edition of this
               hands-on book helps you get started with the language, guiding
               developers and quantitative analysts through Python libraries
               and tools for building financial applications and interactive
               financial analytics.Using practical examples throughout the
               book, author Yves Hilpisch also shows you how to develop a
               full-fledged framework for Monte Carlo simulation-based
               derivatives and risk analytics, based on a large, realistic case
               study. Much of the book uses interactive IPython Notebooks.",
  publisher = "``O'Reilly Media, Inc.''",
  month     =  dec,
  year      =  2018,
  language  = "en"
}

@BOOK{James2013-se,
  title     = "An Introduction to Statistical Learning: with Applications in
               {R}",
  author    = "James, Gareth and Witten, Daniela and Hastie, Trevor and
               Tibshirani, Robert",
  abstract  = "An Introduction to Statistical Learning provides an accessible
               overview of the field of statistical learning, an essential
               toolset for making sense of the vast and complex data sets that
               have emerged in fields ranging from biology to finance to
               marketing to astrophysics in the past twenty years. This book
               presents some of the most important modeling and prediction
               techniques, along with relevant applications. Topics include
               linear regression, classification, resampling methods, shrinkage
               approaches, tree-based methods, support vector machines,
               clustering, and more. Color graphics and real-world examples are
               used to illustrate the methods presented. Since the goal of this
               textbook is to facilitate the use of these statistical learning
               techniques by practitioners in science, industry, and other
               fields, each chapter contains a tutorial on implementing the
               analyses and methods presented in R, an extremely popular open
               source statistical software platform.Two of the authors co-wrote
               The Elements of Statistical Learning (Hastie, Tibshirani and
               Friedman, 2nd edition 2009), a popular reference book for
               statistics and machine learning researchers. An Introduction to
               Statistical Learning covers many of the same topics, but at a
               level accessible to a much broader audience. This book is
               targeted at statisticians and non-statisticians alike who wish
               to use cutting-edge statistical learning techniques to analyze
               their data. The text assumes only a previous course in linear
               regression and no knowledge of matrix algebra.",
  publisher = "Springer Science \& Business Media",
  month     =  jun,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Lock1997-ut,
  title   = "The Regulatory Scheme for Player Representatives in the National
             Football League: The Real Power of Jerry Maguire",
  author  = "Lock, Ethan",
  journal = "Am. Bus. L.J.",
  volume  =  35,
  number  =  2,
  pages   = "[i]--348",
  year    =  1997
}

@ARTICLE{Emrich2014-rf,
  title     = "The intensity of internet use by volunteers: empirical results
               for the internet portal of the German Football Association",
  author    = "Emrich, Eike and Pierdzioch, Christian and Oestmann, Marco",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  14,
  number    =  3,
  pages     = "238--258",
  year      =  2014
}

@ARTICLE{Fahrner2020-ds,
  title     = "Analysing the context-specific relevance of competencies --
               sport management alumni perspectives",
  author    = "Fahrner, Marcel and Sch{\"u}ttoff, Ute",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  20,
  number    =  3,
  pages     = "344--363",
  year      =  2020
}

@ARTICLE{Fahrner2020-yd,
  title     = "Trust within sport {NGB} boards: association with board
               structure and board member characteristics",
  author    = "Fahrner, Marcel and Harris, Spencer",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  0,
  number    =  0,
  pages     = "1--20",
  year      =  2020
}

@ARTICLE{Scheerder2011-oc,
  title     = "Expenditures on Sport Apparel: Creating Consumer Profiles
               through Interval Regression Modelling",
  author    = "Scheerder, Jeroen and Vos, Steven and Taks, Marijke",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  11,
  number    =  3,
  pages     = "251--274",
  year      =  2011
}

@ARTICLE{Huth2019-si,
  title     = "Who invests in financial instruments of sport clubs? An
               empirical analysis of actual and potential individual investors
               of professional European football clubs",
  author    = "Huth, Christopher",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  0,
  number    =  0,
  pages     = "1--20",
  year      =  2019
}

@ARTICLE{Wicker2020-lk,
  title     = "The impact of participation frequency and travel distances for
               different sport participation purposes on subjective well-being:
               the `unhappy commuter' and the happy sport tourist?",
  author    = "Wicker, Pamela",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  volume    =  20,
  number    =  3,
  pages     = "385--402",
  year      =  2020
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Guidotti2018-tj,
  title     = "A Survey of Methods for Explaining Black Box Models",
  author    = "Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore
               and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino",
  abstract  = "In recent years, many accurate decision support systems have
               been constructed as black boxes, that is as systems that hide
               their internal logic to the user. This lack of explanation
               constitutes both a practical and an ethical issue. The
               literature reports many approaches ‚Ä¶",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  volume    =  51,
  number    =  5,
  pages     = "1--42",
  month     =  aug,
  year      =  2018,
  address   = "New York, NY, USA",
  keywords  = "explanations, interpretability, Open the black box, transparent
               models"
}

@ARTICLE{Li2017-cw,
  title         = "Deep Reinforcement Learning: An Overview",
  author        = "Li, Yuxi",
  abstract      = "We give an overview of recent exciting achievements of deep
                   reinforcement learning (RL). We discuss six core elements,
                   six important mechanisms, and twelve applications. We start
                   with background of machine learning, deep learning and
                   reinforcement learning. Next we discuss core RL elements,
                   including value function, in particular, Deep Q-Network
                   (DQN), policy, reward, model, planning, and exploration.
                   After that, we discuss important mechanisms for RL,
                   including attention and memory, unsupervised learning,
                   transfer learning, multi-agent RL, hierarchical RL, and
                   learning to learn. Then we discuss various applications of
                   RL, including games, in particular, AlphaGo, robotics,
                   natural language processing, including dialogue systems,
                   machine translation, and text generation, computer vision,
                   neural architecture design, business management, finance,
                   healthcare, Industry 4.0, smart grid, intelligent
                   transportation systems, and computer systems. We mention
                   topics not reviewed yet, and list a collection of RL
                   resources. After presenting a brief summary, we close with
                   discussions. Please see Deep Reinforcement Learning,
                   arXiv:1810.06339, for a significant update.",
  month         =  jan,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1701.07274"
}

@ARTICLE{Samek2017-rh,
  title         = "Explainable Artificial Intelligence: Understanding,
                   Visualizing and Interpreting Deep Learning Models",
  author        = "Samek, Wojciech and Wiegand, Thomas and M{\"u}ller,
                   Klaus-Robert",
  abstract      = "With the availability of large databases and recent
                   improvements in deep learning methodology, the performance
                   of AI systems is reaching or even exceeding the human level
                   on an increasing number of complex tasks. Impressive
                   examples of this development can be found in domains such as
                   image classification, sentiment analysis, speech
                   understanding or strategic game playing. However, because of
                   their nested non-linear structure, these highly successful
                   machine learning and artificial intelligence models are
                   usually applied in a black box manner, i.e., no information
                   is provided about what exactly makes them arrive at their
                   predictions. Since this lack of transparency can be a major
                   drawback, e.g., in medical applications, the development of
                   methods for visualizing, explaining and interpreting deep
                   learning models has recently attracted increasing attention.
                   This paper summarizes recent developments in this field and
                   makes a plea for more interpretability in artificial
                   intelligence. Furthermore, it presents two approaches to
                   explaining predictions of deep learning models, one method
                   which computes the sensitivity of the prediction with
                   respect to changes in the input and one approach which
                   meaningfully decomposes the decision in terms of the input
                   variables. These methods are evaluated on three
                   classification tasks.",
  month         =  aug,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1708.08296"
}

@ARTICLE{Smilkov2017-co,
  title         = "{SmoothGrad}: removing noise by adding noise",
  author        = "Smilkov, Daniel and Thorat, Nikhil and Kim, Been and
                   Vi{\'e}gas, Fernanda and Wattenberg, Martin",
  abstract      = "Explaining the output of a deep network remains a challenge.
                   In the case of an image classifier, one type of explanation
                   is to identify pixels that strongly influence the final
                   decision. A starting point for this strategy is the gradient
                   of the class score function with respect to the input image.
                   This gradient can be interpreted as a sensitivity map, and
                   there are several techniques that elaborate on this basic
                   idea. This paper makes two contributions: it introduces
                   SmoothGrad, a simple method that can help visually sharpen
                   gradient-based sensitivity maps, and it discusses lessons in
                   the visualization of these maps. We publish the code for our
                   experiments and a website with our results.",
  month         =  jun,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1706.03825"
}

@ARTICLE{Adadi2018-fi,
  title     = "Peeking Inside the {Black-Box}: A Survey on Explainable
               Artificial Intelligence ({XAI})",
  author    = "Adadi, A and Berrada, M",
  abstract  = "At the dawn of the fourth industrial revolution, we are
               witnessing a fast and widespread adoption of artificial
               intelligence (AI) in our daily life, which contributes to
               accelerating the shift towards a more algorithmic society.
               However, even with such unprecedented advancements, a key
               impediment to the use of AI-based systems is that they often
               lack transparency. Indeed, the black-box nature of these systems
               allows powerful predictions, but it cannot be directly
               explained. This issue has triggered a new debate on explainable
               AI (XAI). A research field holds substantial promise for
               improving trust and transparency of AI-based systems. It is
               recognized as the sine qua non for AI to continue making steady
               progress without disruption. This survey provides an entry point
               for interested researchers and practitioners to learn key
               aspects of the young and rapidly growing body of research
               related to XAI. Through the lens of the literature, we review
               the existing approaches regarding the topic, discuss trends
               surrounding its sphere, and present major research trajectories.",
  journal   = "IEEE Access",
  publisher = "ieeexplore.ieee.org",
  volume    =  6,
  pages     = "52138--52160",
  year      =  2018,
  keywords  = "artificial intelligence;AI-based systems;black-box
               nature;explainable AI;XAI;explainable artificial
               intelligence;fourth industrial revolution;Conferences;Machine
               learning;Market research;Prediction algorithms;Machine learning
               algorithms;Biological system modeling;Explainable artificial
               intelligence;interpretable machine learning;black-box models"
}

@INPROCEEDINGS{Gilpin2018-cm,
  title     = "Explaining Explanations: An Overview of Interpretability of
               Machine Learning",
  booktitle = "2018 {IEEE} 5th International Conference on Data Science and
               Advanced Analytics ({DSAA})",
  author    = "Gilpin, L H and Bau, D and Yuan, B Z and Bajwa, A and Specter, M
               and Kagal, L",
  abstract  = "There has recently been a surge of work in explanatory
               artificial intelligence (XAI). This research area tackles the
               important problem that complex machines and algorithms often
               cannot provide insights into their behavior and thought
               processes. XAI allows users and parts of the internal system to
               be more transparent, providing explanations of their decisions
               in some level of detail. These explanations are important to
               ensure algorithmic fairness, identify potential bias/problems in
               the training data, and to ensure that the algorithms perform as
               expected. However, explanations produced by these systems is
               neither standardized nor systematically assessed. In an effort
               to create best practices and identify open challenges, we
               describe foundational concepts of explainability and show how
               they can be used to classify existing literature. We discuss why
               current approaches to explanatory methods especially for deep
               neural networks are insufficient. Finally, based on our survey,
               we conclude with suggested future research directions for
               explanatory artificial intelligence.",
  publisher = "ieeexplore.ieee.org",
  pages     = "80--89",
  month     =  oct,
  year      =  2018,
  keywords  = "artificial intelligence;data analysis;learning (artificial
               intelligence);neural nets;complex machines;algorithmic
               fairness;training data;suggested future research
               directions;explanatory artificial intelligence;explaining
               explanations;machine learning;XAI;potential
               bias-problems;Artificial intelligence;Computational
               modeling;Decision trees;Biological neural
               networks;Taxonomy;Complexity theory;Machine learning
               theories;Models and systems;Deep learning and deep
               analytics;Fairness and transparency in data science"
}

@INPROCEEDINGS{Kim2018-rh,
  title     = "Interpretability Beyond Feature Attribution: Quantitative
               Testing with Concept Activation Vectors ({{TCAV}})",
  booktitle = "Proceedings of the 35th International Conference on Machine
               Learning",
  author    = "Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai,
               Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory",
  editor    = "Dy, Jennifer and Krause, Andreas",
  abstract  = "The interpretation of deep learning models is a challenge due to
               their size, complexity, and often opaque internal state. In
               addition, many systems, such as image classifiers, operate on
               low-level features rather than high-level concepts. To address
               these challenges, we introduce Concept Activation Vectors
               (CAVs), which provide an interpretation of a neural net's
               internal state in terms of human-friendly concepts. The key idea
               is to view the high-dimensional internal state of a neural net
               as an aid, not an obstacle. We show how to use CAVs as part of a
               technique, Testing with CAVs (TCAV), that uses directional
               derivatives to quantify the degree to which a user-defined
               concept is important to a classification result--for example,
               how sensitive a prediction of ``zebra'' is to the presence of
               stripes. Using the domain of image classification as a testing
               ground, we describe how CAVs may be used to explore hypotheses
               and generate insights for a standard image classification
               network as well as a medical application.",
  publisher = "PMLR",
  volume    =  80,
  pages     = "2668--2677",
  series    = "Proceedings of Machine Learning Research",
  year      =  2018,
  address   = "Stockholmsm{\"a}ssan, Stockholm Sweden"
}

@ARTICLE{Corbett-Davies2018-fw,
  title         = "The Measure and Mismeasure of Fairness: A Critical Review of
                   Fair Machine Learning",
  author        = "Corbett-Davies, Sam and Goel, Sharad",
  abstract      = "The nascent field of fair machine learning aims to ensure
                   that decisions guided by algorithms are equitable. Over the
                   last several years, three formal definitions of fairness
                   have gained prominence: (1) anti-classification, meaning
                   that protected attributes---like race, gender, and their
                   proxies---are not explicitly used to make decisions; (2)
                   classification parity, meaning that common measures of
                   predictive performance (e.g., false positive and false
                   negative rates) are equal across groups defined by the
                   protected attributes; and (3) calibration, meaning that
                   conditional on risk estimates, outcomes are independent of
                   protected attributes. Here we show that all three of these
                   fairness definitions suffer from significant statistical
                   limitations. Requiring anti-classification or classification
                   parity can, perversely, harm the very groups they were
                   designed to protect; and calibration, though generally
                   desirable, provides little guarantee that decisions are
                   equitable. In contrast to these formal fairness criteria, we
                   argue that it is often preferable to treat similarly risky
                   people similarly, based on the most statistically accurate
                   estimates of risk that one can produce. Such a strategy,
                   while not universally applicable, often aligns well with
                   policy objectives; notably, this strategy will typically
                   violate both anti-classification and classification parity.
                   In practice, it requires significant effort to construct
                   suitable risk estimates. One must carefully define and
                   measure the targets of prediction to avoid retrenching
                   biases in the data. But, importantly, one cannot generally
                   address these difficulties by requiring that algorithms
                   satisfy popular mathematical formalizations of fairness. By
                   highlighting these challenges in the foundation of fair
                   machine learning, we hope to help researchers and
                   practitioners productively advance the area.",
  month         =  jul,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "1808.00023"
}

@ARTICLE{Ross2017-yp,
  title         = "Right for the Right Reasons: Training Differentiable Models
                   by Constraining their Explanations",
  author        = "Ross, Andrew Slavin and Hughes, Michael C and Doshi-Velez,
                   Finale",
  abstract      = "Neural networks are among the most accurate supervised
                   learning methods in use today, but their opacity makes them
                   difficult to trust in critical applications, especially when
                   conditions in training differ from those in test. Recent
                   work on explanations for black-box models has produced tools
                   (e.g. LIME) to show the implicit rules behind predictions,
                   which can help us identify when models are right for the
                   wrong reasons. However, these methods do not scale to
                   explaining entire datasets and cannot correct the problems
                   they reveal. We introduce a method for efficiently
                   explaining and regularizing differentiable models by
                   examining and selectively penalizing their input gradients,
                   which provide a normal to the decision boundary. We apply
                   these penalties both based on expert annotation and in an
                   unsupervised fashion that encourages diverse models with
                   qualitatively different decision boundaries for the same
                   classification problem. On multiple datasets, we show our
                   approach generates faithful explanations and models that
                   generalize much better when conditions differ between
                   training and test.",
  month         =  mar,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1703.03717"
}

@ARTICLE{Poursabzi-Sangdeh2018-pr,
  title         = "Manipulating and Measuring Model Interpretability",
  author        = "Poursabzi-Sangdeh, Forough and Goldstein, Daniel G and
                   Hofman, Jake M and Vaughan, Jennifer Wortman and Wallach,
                   Hanna",
  abstract      = "With the increased use of machine learning in
                   decision-making scenarios, there has been a growing interest
                   in creating human-interpretable machine learning models.
                   While many such models have been proposed, there have been
                   relatively few experimental studies of whether these models
                   achieve their intended effects, such as encouraging people
                   to follow the model's predictions when the model is correct
                   and to deviate when it makes a mistake. We present a series
                   of randomized, pre-registered experiments comprising 3,800
                   participants in which people were shown functionally
                   identical models that varied only in two factors thought to
                   influence interpretability: the number of input features and
                   the model transparency (clear or black-box). Predictably,
                   participants who were shown a clear model with a small
                   number of features were better able to simulate the model's
                   predictions. However, contrary to what one might expect when
                   manipulating interpretability, we found no improvements in
                   the degree to which participants followed the model's
                   predictions when it was beneficial to do so. Even more
                   surprisingly, increased transparency hampered people's
                   ability to detect when the model makes a sizable mistake and
                   correct for it, seemingly due to information overload. These
                   counterintuitive results suggest that decision scientists
                   creating interpretable models should harbor a healthy
                   skepticism of their intuitions and empirically verify that
                   interpretable models achieve their intended effects.",
  month         =  feb,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1802.07810"
}

@ARTICLE{Papernot2018-qj,
  title         = "Deep k-Nearest Neighbors: Towards Confident, Interpretable
                   and Robust Deep Learning",
  author        = "Papernot, Nicolas and McDaniel, Patrick",
  abstract      = "Deep neural networks (DNNs) enable innovative applications
                   of machine learning like image recognition, machine
                   translation, or malware detection. However, deep learning is
                   often criticized for its lack of robustness in adversarial
                   settings (e.g., vulnerability to adversarial inputs) and
                   general inability to rationalize its predictions. In this
                   work, we exploit the structure of deep learning to enable
                   new learning-based inference and decision strategies that
                   achieve desirable properties such as robustness and
                   interpretability. We take a first step in this direction and
                   introduce the Deep k-Nearest Neighbors (DkNN). This hybrid
                   classifier combines the k-nearest neighbors algorithm with
                   representations of the data learned by each layer of the
                   DNN: a test input is compared to its neighboring training
                   points according to the distance that separates them in the
                   representations. We show the labels of these neighboring
                   points afford confidence estimates for inputs outside the
                   model's training manifold, including on malicious inputs
                   like adversarial examples--and therein provides protections
                   against inputs that are outside the models understanding.
                   This is because the nearest neighbors can be used to
                   estimate the nonconformity of, i.e., the lack of support
                   for, a prediction in the training data. The neighbors also
                   constitute human-interpretable explanations of predictions.
                   We evaluate the DkNN algorithm on several datasets, and show
                   the confidence estimates accurately identify inputs outside
                   the model, and that the explanations provided by nearest
                   neighbors are intuitive and useful in understanding model
                   failures.",
  month         =  mar,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1803.04765"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INCOLLECTION{Alvarez_Melis2018-ss,
  title     = "Towards Robust Interpretability with {Self-Explaining} Neural
               Networks",
  booktitle = "Advances in Neural Information Processing Systems 31",
  author    = "Alvarez Melis, David and Jaakkola, Tommi",
  editor    = "Bengio, S and Wallach, H and Larochelle, H and Grauman, K and
               Cesa-Bianchi, N and Garnett, R",
  abstract  = "Most recent work on interpretability of complex machine learning
               models has focused on estimating a-posteriori explanations for
               previously trained models around specific predictions.
               Self-explaining models where interpretability plays a key role
               already during ‚Ä¶",
  publisher = "Curran Associates, Inc.",
  pages     = "7775--7784",
  year      =  2018
}

@ARTICLE{Rahwan2019-hn,
  title     = "Machine behaviour",
  author    = "Rahwan, Iyad and Cebrian, Manuel and Obradovich, Nick and
               Bongard, Josh and Bonnefon, Jean-Fran{\c c}ois and Breazeal,
               Cynthia and Crandall, Jacob W and Christakis, Nicholas A and
               Couzin, Iain D and Jackson, Matthew O and Jennings, Nicholas R
               and Kamar, Ece and Kloumann, Isabel M and Larochelle, Hugo and
               Lazer, David and McElreath, Richard and Mislove, Alan and
               Parkes, David C and Pentland, Alex 'sandy' and Roberts, Margaret
               E and Shariff, Azim and Tenenbaum, Joshua B and Wellman, Michael",
  abstract  = "Machines powered by artificial intelligence increasingly mediate
               our social, cultural, economic and political interactions.
               Understanding the behaviour of artificial intelligence systems
               is essential to our ability to control their actions, reap their
               benefits and minimize their harms. Here we argue that this
               necessitates a broad scientific research agenda to study machine
               behaviour that incorporates and expands upon the discipline of
               computer science and includes insights from across the sciences.
               We first outline a set of questions that are fundamental to this
               emerging field and then explore the technical, legal and
               institutional constraints on the study of machine behaviour.",
  journal   = "Nature",
  publisher = "nature.com",
  volume    =  568,
  number    =  7753,
  pages     = "477--486",
  month     =  apr,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Miller2017-bj,
  title         = "Explainable {AI}: Beware of Inmates Running the Asylum Or:
                   How {I} Learnt to Stop Worrying and Love the Social and
                   Behavioural Sciences",
  author        = "Miller, Tim and Howe, Piers and Sonenberg, Liz",
  abstract      = "In his seminal book `The Inmates are Running the Asylum: Why
                   High-Tech Products Drive Us Crazy And How To Restore The
                   Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper
                   argues that a major reason why software is often poorly
                   designed (from a user perspective) is that programmers are
                   in charge of design decisions, rather than interaction
                   designers. As a result, programmers design software for
                   themselves, rather than for their target audience, a
                   phenomenon he refers to as the `inmates running the asylum'.
                   This paper argues that explainable AI risks a similar fate.
                   While the re-emergence of explainable AI is positive, this
                   paper argues most of us as AI researchers are building
                   explanatory agents for ourselves, rather than for the
                   intended users. But explainable AI is more likely to succeed
                   if researchers and practitioners understand, adopt,
                   implement, and improve models from the vast and valuable
                   bodies of research in philosophy, psychology, and cognitive
                   science, and if evaluation of these models is focused more
                   on people than on technology. From a light scan of
                   literature, we demonstrate that there is considerable scope
                   to infuse more results from the social and behavioural
                   sciences into explainable AI, and present some key results
                   from these fields that are relevant to explainable AI.",
  month         =  dec,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1712.00547"
}

@INPROCEEDINGS{Dosilovic2018-uk,
  title     = "Explainable artificial intelligence: A survey",
  booktitle = "2018 41st International Convention on Information and
               Communication Technology, Electronics and Microelectronics
               ({MIPRO})",
  author    = "Do{\v s}ilovi{\'c}, F K and Br{\v c}i{\'c}, M and Hlupi{\'c}, N",
  abstract  = "In the last decade, with availability of large datasets and more
               computing power, machine learning systems have achieved
               (super)human performance in a wide variety of tasks. Examples of
               this rapid development can be seen in image recognition, speech
               analysis, strategic game planning and many more. The problem
               with many state-of-the-art models is a lack of transparency and
               interpretability. The lack of thereof is a major drawback in
               many applications, e.g. healthcare and finance, where rationale
               for model's decision is a requirement for trust. In the light of
               these issues, explainable artificial intelligence (XAI) has
               become an area of interest in research community. This paper
               summarizes recent developments in XAI in supervised learning,
               starts a discussion on its connection with artificial general
               intelligence, and gives proposals for further research
               directions.",
  publisher = "ieeexplore.ieee.org",
  pages     = "0210--0215",
  month     =  may,
  year      =  2018,
  keywords  = "learning (artificial
               intelligence);interpretability;healthcare;finance;explainable
               artificial intelligence;XAI;recent developments;supervised
               learning;artificial general intelligence;datasets;computing
               power;machine learning systems;(super)human performance;image
               recognition;speech analysis;strategic game
               planning;state-of-the-art models;transparency;Predictive
               models;Machine learning;Support vector machines;Decision
               trees;Supervised learning;Optimization;explainable artificial
               intelligence;interpretability;explainability;comprehensibility"
}

@ARTICLE{Belinkov2019-hf,
  title     = "Analysis Methods in Neural Language Processing: A Survey",
  author    = "Belinkov, Yonatan and Glass, James",
  abstract  = "The field of natural language processing has seen impressive
               progress in recent years, with neural network models replacing
               many of the traditional systems. A plethora of new models have
               been proposed, many of which are thought to be opaque compared
               to their feature-rich counterparts. This has led researchers to
               analyze, interpret, and evaluate neural networks in novel and
               more fine-grained ways. In this survey paper, we review analysis
               methods in neural language processing, categorize them according
               to prominent research trends, highlight existing limitations,
               and point to potential directions for future work.",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press",
  volume    =  7,
  pages     = "49--72",
  month     =  nov,
  year      =  2019
}

@ARTICLE{Weller2017-nt,
  title         = "Transparency: Motivations and Challenges",
  author        = "Weller, Adrian",
  abstract      = "Transparency is often deemed critical to enable effective
                   real-world deployment of intelligent systems. Yet the
                   motivations for and benefits of different types of
                   transparency can vary significantly depending on context,
                   and objective measurement criteria are difficult to
                   identify. We provide a brief survey, suggesting challenges
                   and related concerns. We highlight and review settings where
                   transparency may cause harm, discussing connections across
                   privacy, multi-agent game theory, economics, fairness and
                   trust.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY",
  eprint        = "1708.01870"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@INPROCEEDINGS{Zhang2019-bc,
  title     = "Interpreting cnns via decision trees",
  booktitle = "Proceedings of the {IEEE} Conference on Computer Vision and
               Pattern Recognition",
  author    = "Zhang, Quanshi and Yang, Yu and Ma, Haotian and Wu, Ying Nian",
  abstract  = "This paper aims to quantitatively explain the rationales of each
               prediction that is made by a pre-trained convolutional neural
               network (CNN). We propose to learn a decision tree, which
               clarifies the specific reason for each prediction made by the
               CNN at the semantic level. Ie ‚Ä¶",
  publisher = "openaccess.thecvf.com",
  pages     = "6261--6270",
  year      =  2019
}

@INPROCEEDINGS{Kolosnjaji2018-wg,
  title     = "Adversarial Malware Binaries: Evading Deep Learning for Malware
               Detection in Executables",
  booktitle = "2018 26th European Signal Processing Conference ({EUSIPCO})",
  author    = "Kolosnjaji, B and Demontis, A and Biggio, B and Maiorca, D and
               Giacinto, G and Eckert, C and Roli, F",
  abstract  = "Machine learning has already been exploited as a useful tool for
               detecting malicious executable files. Data retrieved from
               malware samples, such as header fields, instruction sequences,
               or even raw bytes, is leveraged to learn models that
               discriminate between benign and malicious software. However, it
               has also been shown that machine learning and deep neural
               networks can be fooled by evasion attacks (also known as
               adversarial examples), i.e., small changes to the input data
               that cause misclassification at test time. In this work, we
               investigate the vulnerability of malware detection methods that
               use deep networks to learn from raw bytes. We propose a
               gradient-based attack that is capable of evading a
               recently-proposed deep network suited to this purpose by only
               changing few specific bytes at the end of each mal ware sample,
               while preserving its intrusive functionality. Promising results
               show that our adversarial malware binaries evade the targeted
               network with high probability, even though less than 1 \% of
               their bytes are modified.",
  publisher = "ieeexplore.ieee.org",
  pages     = "533--537",
  month     =  sep,
  year      =  2018,
  keywords  = "invasive software;learning (artificial intelligence);neural
               nets;malware samples;malicious executable files;adversarial
               malware binaries;gradient-based attack;deep network;malware
               detection methods;adversarial examples;evasion attacks;deep
               neural networks;machine learning;malicious software;instruction
               sequences;Malware;Machine learning;Neural networks;Feature
               extraction;Signal processing algorithms;Europe;Signal processing"
}

@ARTICLE{Guidotti2018-zk,
  title         = "Local {Rule-Based} Explanations of Black Box Decision
                   Systems",
  author        = "Guidotti, Riccardo and Monreale, Anna and Ruggieri,
                   Salvatore and Pedreschi, Dino and Turini, Franco and
                   Giannotti, Fosca",
  abstract      = "The recent years have witnessed the rise of accurate but
                   obscure decision systems which hide the logic of their
                   internal decision processes to the users. The lack of
                   explanations for the decisions of black box systems is a key
                   ethical issue, and a limitation to the adoption of machine
                   learning components in socially sensitive and
                   safety-critical contexts. \%Therefore, we need explanations
                   that reveals the reasons why a predictor takes a certain
                   decision. In this paper we focus on the problem of black box
                   outcome explanation, i.e., explaining the reasons of the
                   decision taken on a specific instance. We propose LORE, an
                   agnostic method able to provide interpretable and faithful
                   explanations. LORE first leans a local interpretable
                   predictor on a synthetic neighborhood generated by a genetic
                   algorithm. Then it derives from the logic of the local
                   interpretable predictor a meaningful explanation consisting
                   of: a decision rule, which explains the reasons of the
                   decision; and a set of counterfactual rules, suggesting the
                   changes in the instance's features that lead to a different
                   outcome. Wide experiments show that LORE outperforms
                   existing methods and baselines both in the quality of
                   explanations and in the accuracy in mimicking the black box.",
  month         =  may,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1805.10820"
}

@ARTICLE{Bastani2017-et,
  title         = "Interpreting Blackbox Models via Model Extraction",
  author        = "Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa",
  abstract      = "Interpretability has become incredibly important as machine
                   learning is increasingly used to inform consequential
                   decisions. We propose to construct global explanations of
                   complex, blackbox models in the form of a decision tree
                   approximating the original model---as long as the decision
                   tree is a good approximation, then it mirrors the
                   computation performed by the blackbox model. We devise a
                   novel algorithm for extracting decision tree explanations
                   that actively samples new training points to avoid
                   overfitting. We evaluate our algorithm on a random forest to
                   predict diabetes risk and a learned controller for
                   cart-pole. Compared to several baselines, our decision trees
                   are both substantially more accurate and equally or more
                   interpretable based on a user study. Finally, we describe
                   several insights provided by our interpretations, including
                   a causal issue validated by a physician.",
  month         =  may,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1705.08504"
}

@ARTICLE{Dong2017-ci,
  title         = "Towards Interpretable Deep Neural Networks by Leveraging
                   Adversarial Examples",
  author        = "Dong, Yinpeng and Su, Hang and Zhu, Jun and Bao, Fan",
  abstract      = "Deep neural networks (DNNs) have demonstrated impressive
                   performance on a wide array of tasks, but they are usually
                   considered opaque since internal structure and learned
                   parameters are not interpretable. In this paper, we
                   re-examine the internal representations of DNNs using
                   adversarial images, which are generated by an
                   ensemble-optimization algorithm. We find that: (1) the
                   neurons in DNNs do not truly detect semantic objects/parts,
                   but respond to objects/parts only as recurrent
                   discriminative patches; (2) deep visual representations are
                   not robust distributed codes of visual concepts because the
                   representations of adversarial images are largely not
                   consistent with those of real images, although they have
                   similar visual appearance, both of which are different from
                   previous findings. To further improve the interpretability
                   of DNNs, we propose an adversarial training scheme with a
                   consistent loss such that the neurons are endowed with
                   human-interpretable concepts. The induced interpretable
                   representations enable us to trace eventual outcomes back to
                   influential neurons. Therefore, human users can know how the
                   models make predictions, as well as when and why they make
                   errors.",
  month         =  aug,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1708.05493"
}

@INCOLLECTION{Ras2018-wr,
  title     = "Explanation Methods in Deep Learning: Users, Values, Concerns
               and Challenges",
  booktitle = "Explainable and Interpretable Models in Computer Vision and
               Machine Learning",
  author    = "Ras, Gabri{\"e}lle and van Gerven, Marcel and Haselager, Pim",
  editor    = "Escalante, Hugo Jair and Escalera, Sergio and Guyon, Isabelle
               and Bar{\'o}, Xavier and G{\"u}{\c c}l{\"u}t{\"u}rk, Ya{\u g}mur
               and G{\"u}{\c c}l{\"u}, Umut and van Gerven, Marcel",
  abstract  = "Issues regarding explainable AI involve four components: users,
               laws and regulations, explanations and algorithms. Together
               these components provide a context in which explanation methods
               can be evaluated regarding their adequacy. The goal of this
               chapter is to bridge the gap between expert users and lay users.
               Different kinds of users are identified and their concerns
               revealed, relevant statements from the General Data Protection
               Regulation are analyzed in the context of Deep Neural Networks
               (DNNs), a taxonomy for the classification of existing
               explanation methods is introduced, and finally, the various
               classes of explanation methods are analyzed to verify if user
               concerns are justified. Overall, it is clear that (visual)
               explanations can be given about various aspects of the influence
               of the input on the output. However, it is noted that
               explanation methods or interfaces for lay users are missing and
               we speculate which criteria these methods/interfaces should
               satisfy. Finally it is noted that two important concerns are
               difficult to address with explanation methods: the concern about
               bias in datasets that leads to biased DNNs, as well as the
               suspicion about unfair outcomes.",
  publisher = "Springer International Publishing",
  pages     = "19--36",
  year      =  2018,
  address   = "Cham"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gottesman2019-lc,
  title     = "Guidelines for reinforcement learning in healthcare",
  author    = "Gottesman, Omer and Johansson, Fredrik and Komorowski, Matthieu
               and Faisal, Aldo and Sontag, David and Doshi-Velez, Finale and
               Celi, Leo Anthony",
  abstract  = "Reinforcement learning (RL) is a subfield of AI that provides
               tools to optimize sequences of decisions for long-term outcomes.
               For example, faced with a patient with sepsis, the intensivist
               must decide if and when to initiate and adjust treatments such
               as antibiotics ‚Ä¶",
  journal   = "Nat. Med.",
  publisher = "nature.com",
  volume    =  25,
  number    =  1,
  pages     = "16--18",
  month     =  jan,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Arnold2019-ph,
  title     = "{FactSheets}: Increasing trust in {AI} services through
               supplier's declarations of conformity",
  author    = "Arnold, M and Bellamy, R K E and Hind, M and Houde, S and Mehta,
               S and Mojsilovi{\'c}, A and Nair, R and Ramamurthy, K N and
               Olteanu, A and Piorkowski, D and Reimer, D and Richards, J and
               Tsay, J and Varshney, K R",
  abstract  = "Accuracy is an important concern for suppliers of artificial
               intelligence (AI) services, but considerations beyond accuracy,
               such as safety (which includes fairness and explainability),
               security, and provenance, are also critical elements to engender
               consumers' trust in a service. Many industries use transparent,
               standardized, but often not legally required documents called
               supplier's declarations of conformity (SDoCs) to describe the
               lineage of a product along with the safety and performance
               testing it has undergone. SDoCs may be considered
               multidimensional fact sheets that capture and quantify various
               aspects of the product and its development to make it worthy of
               consumers' trust. In this article, inspired by this practice, we
               propose FactSheets to help increase trust in AI services. We
               envision such documents to contain purpose, performance, safety,
               security, and provenance information to be completed by AI
               service providers for examination by consumers. We suggest a
               comprehensive set of declaration items tailored to AI in the
               Appendix of this article.",
  journal   = "IBM J. Res. Dev.",
  publisher = "ieeexplore.ieee.org",
  volume    =  63,
  number    = "4/5",
  pages     = "6:1--6:13",
  month     =  jul,
  year      =  2019,
  keywords  = "Artificial
               intelligence;Safety;Security;Industries;Standards;Software;Testing"
}

@ARTICLE{Li2018-rb,
  title         = "Deep Reinforcement Learning",
  author        = "Li, Yuxi",
  abstract      = "We discuss deep reinforcement learning in an overview style.
                   We draw a big picture, filled with details. We discuss six
                   core elements, six important mechanisms, and twelve
                   applications, focusing on contemporary work, and in
                   historical contexts. We start with background of artificial
                   intelligence, machine learning, deep learning, and
                   reinforcement learning (RL), with resources. Next we discuss
                   RL core elements, including value function, policy, reward,
                   model, exploration vs. exploitation, and representation.
                   Then we discuss important mechanisms for RL, including
                   attention and memory, unsupervised learning, hierarchical
                   RL, multi-agent RL, relational RL, and learning to learn.
                   After that, we discuss RL applications, including games,
                   robotics, natural language processing (NLP), computer
                   vision, finance, business management, healthcare, education,
                   energy, transportation, computer systems, and, science,
                   engineering, and art. Finally we summarize briefly, discuss
                   challenges and opportunities, and close with an epilogue.",
  month         =  oct,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1810.06339"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Vaughan2017-tm,
  title     = "Making better use of the crowd: how crowdsourcing can advance
               machine learning research",
  author    = "Vaughan, Jennifer Wortman",
  abstract  = "This survey provides a comprehensive overview of the landscape
               of crowdsourcing research, targeted at the machine learning
               community. We begin with an overview of the ways in which
               crowdsourcing can be used to advance machine learning research,
               focusing ‚Ä¶",
  journal   = "J. Mach. Learn. Res.",
  publisher = "JMLR.org",
  volume    =  18,
  number    =  1,
  pages     = "7026--7071",
  month     =  jan,
  year      =  2017,
  keywords  = "model evaluation, mechanical turk, incentives, hybrid
               intelligence, data generation, crowdsourcing, behavioral
               experiments"
}

@ARTICLE{Gunduz2019-fb,
  title     = "Machine Learning in the Air",
  author    = "G{\"u}nd{\"u}z, D and de Kerret, P and Sidiropoulos, N D and
               Gesbert, D and Murthy, C R and van der Schaar, M",
  abstract  = "Thanks to the recent advances in processing speed, data
               acquisition and storage, machine learning (ML) is penetrating
               every facet of our lives, and transforming research in many
               areas in a fundamental manner. Wireless communications is
               another success story - ubiquitous in our lives, from handheld
               devices to wearables, smart homes, and automobiles. While recent
               years have seen a flurry of research activity in exploiting ML
               tools for various wireless communication problems, the impact of
               these techniques in practical communication systems and
               standards is yet to be seen. In this paper, we review some of
               the major promises and challenges of ML in wireless
               communication systems, focusing mainly on the physical layer. We
               present some of the most striking recent accomplishments that ML
               techniques have achieved with respect to classical approaches,
               and point to promising research directions where ML is likely to
               make the biggest impact in the near future. We also highlight
               the complementary problem of designing physical layer techniques
               to enable distributed ML at the wireless network edge, which
               further emphasizes the need to understand and connect ML with
               fundamental concepts in wireless communications.",
  journal   = "IEEE J. Sel. Areas Commun.",
  publisher = "ieeexplore.ieee.org",
  volume    =  37,
  number    =  10,
  pages     = "2184--2199",
  month     =  oct,
  year      =  2019,
  keywords  = "data acquisition;learning (artificial intelligence);radio
               networks;telecommunication computing;machine learning;handheld
               devices;smart homes;ML tools;wireless communication
               problems;wireless communication systems;ML techniques;physical
               layer techniques;wireless network edge;Wireless
               communication;Data models;Machine
               learning;Tools;Computers;Physical layer;Autoencoders;channel
               coding;channel estimation;data-driven methods;distributed
               learning;distributed resource allocation;deep learning;federated
               edge learning;joint source-channel coding;machine
               learning;stochastic approximation;wireless communications"
}

@ARTICLE{Tomsett2018-du,
  title         = "Interpretable to Whom? A Role-based Model for Analyzing
                   Interpretable Machine Learning Systems",
  author        = "Tomsett, Richard and Braines, Dave and Harborne, Dan and
                   Preece, Alun and Chakraborty, Supriyo",
  abstract      = "Several researchers have argued that a machine learning
                   system's interpretability should be defined in relation to a
                   specific agent or task: we should not ask if the system is
                   interpretable, but to whom is it interpretable. We describe
                   a model intended to help answer this question, by
                   identifying different roles that agents can fulfill in
                   relation to the machine learning system. We illustrate the
                   use of our model in a variety of scenarios, exploring how an
                   agent's role influences its goals, and the implications for
                   defining interpretability. Finally, we make suggestions for
                   how our model could be useful to interpretability
                   researchers, system developers, and regulatory bodies
                   auditing machine learning systems.",
  month         =  jun,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "1806.07552"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Herman2017-dr,
  title     = "The promise and peril of human evaluation for model
               interpretability",
  author    = "Herman, Bernease",
  abstract  = "Transparency, user trust, and human comprehension are popular
               ethical motivations for interpretable machine learning. In
               support of these goals, researchers evaluate model explanation
               performance using humans and real world applications. This alone
               presents a ‚Ä¶",
  journal   = "arXiv preprint arXiv:1711. 07414",
  publisher = "deepai.org",
  pages     = "8",
  year      =  2017
}

@MISC{noauthor_undated-zy,
  title        = "Fathers of the Deep Learning revolution receive 2018 {ACM}
                  {A.M}. Turing Award",
  abstract     = "2018 ACM A.M. Turing Award recipients Yoshua Bengio, Geoffrey
                  Hinton, and Yann LeCun achieved conceptual and engineering
                  breakthroughs that made deep neural networks a critical
                  component of computing.",
  howpublished = "\url{https://www.acm.org/media-center/2019/march/turing-award-2018}",
  note         = "Accessed: 2020-7-2"
}

@TECHREPORT{Financial_Stability_Board2017-lu,
  title       = "Artificial intelligence and machine learning in financial
                 services: Market developments and financial stability
                 implications",
  author      = "{Financial Stability Board}",
  institution = "Financial Stability Board",
  year        =  2017
}

@ARTICLE{Raisch2020-ed,
  title     = "Artificial Intelligence and Management: The
               {Automation-Augmentation} Paradox",
  author    = "Raisch, Sebastian and Krakowski, Sebastian",
  abstract  = "Taking three recent business books on artificial intelligence
               (AI) as a starting point, we explore the automation and
               augmentation concepts in the management domain. Whereas
               automation implies that machines take over a human task,
               augmentation means that humans collaborate closely with machines
               to perform a task. Taking a normative stance, the three books
               advise organizations to prioritize augmentation, which they
               relate to superior performance. Using a more comprehensive
               paradox perspective, we argue that, in the management domain,
               augmentation cannot be neatly separated from automation. These
               dual AI applications are interdependent across time and space,
               creating a paradoxical tension. Over-emphasizing either
               augmentation or automation fuels reinforcing cycles with
               negative organizational and societal outcomes. However, if
               organizations adopt a broader perspective comprising both
               automation and augmentation, they could deal with the tension
               and achieve complementarities that benefit business and society.
               Drawing on our insights, we conclude that management scholars
               need to be involved in research on the use of AI in
               organizations. We also argue that a substantial change is
               required in how AI research is currently conducted in order to
               develop meaningful theory and to provide practice with sound
               advice.",
  journal   = "AMRO",
  publisher = "Academy of Management",
  month     =  feb,
  year      =  2020
}

@ARTICLE{Lowe2019-qg,
  title    = "``Artificial intelligence'', or statistics?",
  author   = "Lowe, David",
  abstract = "Professor David Lowe briefly explains the statistical
              underpinnings of AI",
  journal  = "Significance",
  volume   =  16,
  number   =  4,
  pages    = "7--7",
  month    =  aug,
  year     =  2019
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Tarran2019-ro,
  title    = "{THE} {S} {WORD} ‚Ä¶ and what to do about it",
  author   = "Tarran, Brian",
  abstract = "What do we do about significance?",
  journal  = "Significance",
  volume   =  16,
  number   =  4,
  pages    = "14--14",
  month    =  aug,
  year     =  2019
}

@ARTICLE{Sheldon2019-hn,
  title    = "What does it all mean?",
  author   = "Sheldon, Neil",
  abstract = "The misunderstanding of ?significance? is part of a wider problem
              with the way statistical language is used ? and a change is long
              overdue, says Neil Sheldon",
  journal  = "Significance",
  volume   =  16,
  number   =  4,
  pages    = "15--17",
  month    =  aug,
  year     =  2019
}

@ARTICLE{Molgaard2019-jz,
  title    = "Military vital statistics The Spanish flu and the First World War",
  author   = "Molgaard, Craig A",
  abstract = "Craig A. Molgaard explores the origins, spread and impact of the
              deadly influenza pandemic of 1918?1919, and how the outbreak
              intersected with the end of the First World War",
  journal  = "Significance",
  volume   =  16,
  number   =  4,
  pages    = "32--37",
  month    =  aug,
  year     =  2019
}

@ARTICLE{Hahn2019-ws,
  title    = "Statistical intervals, not statistical significance",
  author   = "Hahn, Gerald J and Doganaksoy, Necip and Meeker, William Q",
  abstract = "Convincing practitioners of the inadequacy of significance
              testing can employ a two-step approach. First, explain the
              difference between statistical significance and practical
              importance. Then, at least in many situations, use an appropriate
              statistical interval to quantify the statistical uncertainty. By
              Gerry Hahn, Necip Doganaksoy and Bill Meeker",
  journal  = "Significance",
  volume   =  16,
  number   =  4,
  pages    = "20--22",
  month    =  aug,
  year     =  2019
}

@ARTICLE{Gregory2020-dg,
  title     = "The Role of Artificial Intelligence and Data Network Effects for
               Creating User Value",
  author    = "Gregory, Robert Wayne and Henfridsson, Ola and Kaganer, Evgeny
               and Kyriakou, Harris",
  abstract  = "Some of the world?s most profitable firms own platforms that
               exhibit network effects. A platform exhibits network effects if
               the more people that use it, the more valuable that it becomes
               to each user. Theorizing about the value perceived by users of a
               platform that exhibits network effects has traditionally focused
               on direct and indirect network effects. In this paper, we
               theorize about a third type of network effects?data network
               effects?that has emerged from advances in artificial
               intelligence (AI) and the growing availability of data. A
               platform exhibits data network effects if the more that the
               platform learns from the data it collects on users, the more
               valuable the platform becomes to each user. We argue that there
               is a positive direct relationship between the AI capability of a
               platform and the value perceived in the platform by its users?a
               relationship that is moderated by platform legitimation, data
               stewardship and user-centric design.",
  journal   = "AMRO",
  publisher = "Academy of Management",
  month     =  mar,
  year      =  2020
}

@ARTICLE{Gelman2021-ep,
  title    = "Slamming the sham: A Bayesian model for adaptive adjustment with
              noisy control data",
  author   = "Gelman, Andrew and V{\'a}k{\'a}r, Matthijs",
  abstract = "It is not always clear how to adjust for control data in causal
              inference, balancing the goals of reducing bias and variance. We
              show how, in a setting with repeated experiments, Bayesian
              hierarchical modeling yields an adaptive procedure that uses the
              data to determine how much adjustment to perform. The result is a
              novel analysis with increased statistical efficiency compared
              with the default analysis based on difference estimates. We
              demonstrate this procedure on two real examples, as well as on a
              series of simulated datasets. We show that the increased
              efficiency can have real-world consequences in terms of the
              conclusions that can be drawn from the experiments. We also
              discuss the relevance of this work to causal inference and
              statistical design and analysis more generally.",
  journal  = "Stat. Med.",
  month    =  apr,
  year     =  2021,
  keywords = "Bayesian statistics; Hierarchical model; Parallel experiments;
              Sham data",
  language = "en"
}

@ARTICLE{Barboza2017-rm,
  title    = "Machine learning models and bankruptcy prediction",
  author   = "Barboza, Flavio and Kimura, Herbert and Altman, Edward",
  abstract = "There has been intensive research from academics and
              practitioners regarding models for predicting bankruptcy and
              default events, for credit risk management. Seminal academic
              research has evaluated bankruptcy using traditional statistics
              techniques (e.g. discriminant analysis and logistic regression)
              and early artificial intelligence models (e.g. artificial neural
              networks). In this study, we test machine learning models
              (support vector machines, bagging, boosting, and random forest)
              to predict bankruptcy one year prior to the event, and compare
              their performance with results from discriminant analysis,
              logistic regression, and neural networks. We use data from 1985
              to 2013 on North American firms, integrating information from the
              Salomon Center database and Compustat, analysing more than 10,000
              firm-year observations. The key insight of the study is a
              substantial improvement in prediction accuracy using machine
              learning techniques especially when, in addition to the original
              Altman's Z-score variables, we include six complementary
              financial indicators. Based on Carton and Hofer (2006), we use
              new variables, such as the operating margin, change in
              return-on-equity, change in price-to-book, and growth measures
              related to assets, sales, and number of employees, as predictive
              variables. Machine learning models show, on average,
              approximately 10\% more accuracy in relation to traditional
              models. Comparing the best models, with all predictive variables,
              the machine learning technique related to random forest led to
              87\% accuracy, whereas logistic regression and linear
              discriminant analysis led to 69\% and 50\% accuracy,
              respectively, in the testing sample. We find that bagging,
              boosting, and random forest models outperform the others
              techniques, and that all prediction accuracy in the testing
              sample improves when the additional variables are included. Our
              research adds to the discussion of the continuing debate about
              superiority of computational methods over statistical techniques
              such as in Tsai, Hsu, and Yen (2014) and Yeh, Chi, and Lin
              (2014). In particular, for machine learning mechanisms, we do not
              find SVM to lead to higher accuracy rates than other models. This
              result contradicts outcomes from Danenas and Garsva (2015) and
              Cleofas-Sanchez, Garcia, Marques, and Senchez (2016), but
              corroborates, for instance, Wang, Ma, and Yang (2014), Liang, Lu,
              Tsai, and Shih (2016), and Cano et al. (2017). Our study supports
              the applicability of the expert systems by practitioners as in
              Heo and Yang (2014), Kim, Kang, and Kim (2015) and Xiao, Xiao,
              and Wang (2016).",
  journal  = "Expert Syst. Appl.",
  volume   =  83,
  pages    = "405--417",
  month    =  oct,
  year     =  2017,
  keywords = "Bankruptcy prediction; Machine learning; Support vector machines;
              Boosting; Bagging; Random forest"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Hosaka2019-zt,
  title    = "Bankruptcy prediction using imaged financial ratios and
              convolutional neural networks",
  author   = "Hosaka, Tadaaki",
  abstract = "Convolutional neural networks are being applied to identification
              problems in a variety of fields, and in some areas are showing
              higher discrimination accuracies than conventional methods.
              However, applications of convolutional neural networks to
              financial analyses have only been reported in a small number of
              studies on the prediction of stock price movements. The reason
              for this seems to be that convolutional neural networks are more
              suitable for application to images and less suitable for general
              numerical data including financial statements. Hence, in this
              research, an attempt is made to apply a convolutional neural
              network to the prediction of corporate bankruptcy, which in most
              cases is treated as a two-class classification problem. We use
              the financial statements (balance sheets and profit-and-loss
              statements) of 102 companies that have been delisted from the
              Japanese stock market due to de facto bankruptcy as well as the
              financial statements of 2062 currently listed companies over four
              financial periods. In our proposed method, a set of financial
              ratios are derived from the financial statements and represented
              as a grayscale image. The image generated by this process is
              utilized for training and testing a convolutional neural network.
              Moreover, the size of the dataset is increased using the weighted
              averages to create synthetic data points. A total of 7520 images
              for the bankrupt and continuing enterprises classes are used for
              training the convolutional neural network based on GoogLeNet.
              Bankruptcy predictions through the trained network are shown to
              have a higher performance compared to methods using decision
              trees, linear discriminant analysis, support vector machines,
              multi-layer perceptron, AdaBoost, or Altman's Z‚Ä≤‚Ä≤-score.",
  journal  = "Expert Syst. Appl.",
  volume   =  117,
  pages    = "287--299",
  month    =  mar,
  year     =  2019,
  keywords = "Deep learning; Business failure; Financial statement; Imaging"
}

@ARTICLE{Lopez_Iturriaga2015-em,
  title    = "Bankruptcy visualization and prediction using neural networks: A
              study of {U.S}. commercial banks",
  author   = "L{\'o}pez Iturriaga, F{\'e}lix J and Sanz, Iv{\'a}n Pastor",
  abstract = "We develop a model of neural networks to study the bankruptcy of
              U.S. banks, taking into account the specific features of the
              recent financial crisis. We combine multilayer perceptrons and
              self-organizing maps to provide a tool that displays the
              probability of distress up to three years before bankruptcy
              occurs. Based on data from the Federal Deposit Insurance
              Corporation between 2002 and 2012, our results show that failed
              banks are more concentrated in real estate loans and have more
              provisions. Their situation is partially due to risky expansion,
              which results in less equity and interest income. After drawing
              the profile of distressed banks, we develop a model to detect
              failures and a tool to assess bank risk in the short, medium and
              long term using bankruptcies that occurred from May 2012 to
              December 2013 in U.S. banks. The model can detect 96.15\% of the
              failures in this period and outperforms traditional models of
              bankruptcy prediction.",
  journal  = "Expert Syst. Appl.",
  volume   =  42,
  number   =  6,
  pages    = "2857--2869",
  month    =  apr,
  year     =  2015,
  keywords = "Bankruptcy prediction; Financial crisis; Multilayer perceptron;
              Neural networks; Self-organizing maps"
}

@ARTICLE{Dimitras1996-tj,
  title    = "A survey of business failures with an emphasis on prediction
              methods and industrial applications",
  author   = "Dimitras, A I and Zanakis, S H and Zopounidis, C",
  abstract = "The considerable interest in the prediction of business failures
              is reflected in the large number of studies presented in the
              literature. Various methods have been used to construct
              prediction models. This paper provides a review of the literature
              and a framework for the presentation of this information.
              Articles can be classified according to the country, industrial
              sector and period of data, as well as the financial ratios and
              models or methods employed. Relationships and research trends in
              the prediction of business failure are discussed.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  90,
  number   =  3,
  pages    = "487--513",
  month    =  may,
  year     =  1996,
  keywords = "Business failures; Statistical analysis"
}

@ARTICLE{Hanna2020-bn,
  title     = "News media and investor sentiment during bull and bear markets",
  author    = "Hanna, Alan J and Turner, John D and Walker, Clive B",
  abstract  = "The news media have been described by Shiller ([2000].
               Irrational Exuberance. Princeton, NJ: Princeton University
               Press.) as fundamental propagators of speculative price
               movements. We test whether investors react differently to
               sentiment in bull and bear markets using the tone of reporting
               in the Financial Times as a proxy for sentiment. Our sentiment
               proxy is a daily measure covering the period from 1899 to 2010.
               We find that the tone of the Financial Times influences trading
               volume during bull markets. These findings are consistent with
               noise traders driving trade during speculative booms and
               Shiller?s press-as-propagators hypothesis.",
  journal   = "The European Journal of Finance",
  publisher = "Routledge",
  pages     = "1--19",
  month     =  mar,
  year      =  2020
}

@ARTICLE{Crowder2002-tv,
  title     = "Dynamic Modelling and Prediction of English Football League
               Matches for Betting",
  author    = "Crowder, Martin and Dixon, Mark and Ledford, Anthony and
               Robinson, Mike",
  abstract  = "[We focus on modelling the 92 soccer teams in the English
               Football Association League over the years 1992-1997 using
               refinements of the independent Poisson model of Dixon and Coles.
               Our framework assumes that each team has attack and defence
               strengths that evolve through time (rather than remaining
               constant) according to some unobserved bivariate stochastic
               process. Estimation of the teams' attack and defence
               capabilities is undertaken via a novel approach involving an
               approximation that is computationally convenient and fast. The
               results of this approximation compare very favourably with
               results obtained through the Dixon and Coles approach. We note
               that the full model (i.e. the model before the above
               approximation is made) may be implemented using Markov chain
               Monte Carlo procedures, and that this approach is vastly more
               computationally expensive. We focus on the probabilities of home
               win, draw or away win because these outcomes constitute the
               primary betting market. These probabilities are estimated for
               games played between any two of the 92 teams and the predictions
               are compared with the actual results.]",
  journal   = "Journal of the Royal Statistical Society. Series D (The
               Statistician)",
  publisher = "[Royal Statistical Society, Wiley]",
  volume    =  51,
  number    =  2,
  pages     = "157--168",
  year      =  2002
}

@ARTICLE{Boshnakov2017-rw,
  title    = "A bivariate Weibull count model for forecasting association
              football scores",
  author   = "Boshnakov, Georgi and Kharrat, Tarak and McHale, Ian G",
  abstract = "The paper presents a model for forecasting association football
              scores. The model uses a Weibull inter-arrival-times-based count
              process and a copula to produce a bivariate distribution of the
              numbers of goals scored by the home and away teams in a match. We
              test it against a variety of alternatives, including the simpler
              Poisson distribution-based model and an independent version of
              our model. The out-of-sample performance of our methodology is
              illustrated using, first, calibration curves, then a Kelly-type
              betting strategy that is applied to the pre-match win/draw/loss
              market and to the over--under 2.5 goals market. The new model
              provides an improved fit to the data relative to previous models,
              and results in positive returns to betting.",
  journal  = "Int. J. Forecast.",
  volume   =  33,
  number   =  2,
  pages    = "458--466",
  month    =  apr,
  year     =  2017,
  keywords = "Betting; Calibration; Copula; Counting process; Soccer; Weibull"
}

@ARTICLE{Baboota2019-at,
  title    = "Predictive analysis and modelling football results using machine
              learning approach for English Premier League",
  author   = "Baboota, Rahul and Kaur, Harleen",
  abstract = "The introduction of artificial intelligence has given us the
              ability to build predictive systems with unprecedented accuracy.
              Machine learning is being used in virtually all areas in one way
              or another, due to its extreme effectiveness. One such area where
              predictive systems have gained a lot of popularity is the
              prediction of football match results. This paper demonstrates our
              work on the building of a generalized predictive model for
              predicting the results of the English Premier League. Using
              feature engineering and exploratory data analysis, we create a
              feature set for determining the most important factors for
              predicting the results of a football match, and consequently
              create a highly accurate predictive system using machine
              learning. We demonstrate the strong dependence of our models'
              performances on important features. Our best model using gradient
              boosting achieved a performance of 0.2156 on the ranked
              probability score (RPS) metric for game weeks 6 to 38 for the
              English Premier League aggregated over two seasons (2014--2015
              and 2015--2016), whereas the betting organizations that we
              consider (Bet365 and Pinnacle Sports) obtained an RPS value of
              0.2012 for the same period. Since a lower RPS value represents a
              higher predictive accuracy, our model was not able to outperform
              the bookmaker's predictions, despite obtaining promising results.",
  journal  = "Int. J. Forecast.",
  volume   =  35,
  number   =  2,
  pages    = "741--755",
  month    =  apr,
  year     =  2019,
  keywords = "Machine learning; Feature engineering; Data mining; Predictive
              analysis; Random forest; Support vector machines (SVM); Ranked
              probability score (RPS); Gradient boosting"
}

@ARTICLE{Ogburn2020-kg,
  title    = "Causal inference, social networks and chain graphs",
  author   = "Ogburn, Elizabeth L and Shpitser, Ilya and Lee, Youjin",
  abstract = "Summary Traditionally, statistical inference and causal inference
              on human subjects rely on the assumption that individuals are
              independently affected by treatments or exposures. However,
              recently there has been increasing interest in settings, such as
              social networks, where individuals may interact with one another
              such that treatments may spill over from the treated individual
              to their social contacts and outcomes may be contagious. Existing
              models proposed for causal inference using observational data
              from networks of interacting individuals have two major
              shortcomings. First, they often require a level of granularity in
              the data that is infeasible in practice to collect in most
              settings and, second, the models are high dimensional and often
              too big to fit to the available data. We illustrate and justify a
              parsimonious parameterization for network data with interference
              and contagion. Our parameterization corresponds to a particular
              family of graphical models known as chain graphs. We argue that,
              in some settings, chain graph models approximate the marginal
              distribution of a snapshot of a longitudinal data-generating
              process on interacting units. We illustrate the use of chain
              graphs for causal inference about collective decision making in
              social networks by using data from US Supreme Court decisions
              between 1994 and 2004 and in simulations.",
  journal  = "J. R. Stat. Soc. A",
  volume   =  5,
  pages    = "116",
  month    =  jul,
  year     =  2020
}

@ARTICLE{noauthor_undated-jj,
  title    = "Presidential Address: The Scientific Outlook in Financial
              Economics",
  author   = "Harvey, Campbell R",
  abstract = "Given the competition for top journal space, there is an
              incentive to produce ``significant'' results. With the
              combination of unreported tests, lack of adjustment for multiple
              tests, and direct and indirect p-hacking, many of the results
              being published will fail to hold up in the future. In addition,
              there are basic issues with the interpretation of statistical
              significance. Increasing thresholds may be necessary, but still
              may not be sufficient: if the effect being studied is rare, even
              t > 3 will produce a large number of false positives. Here I
              explore the meaning and limitations of a p-value. I offer a
              simple alternative (the minimum Bayes factor). I present
              guidelines for a robust, transparent research culture in
              financial economics. Finally, I offer some thoughts on the
              importance of risk taking (from the perspective of authors and
              editors) to advance our field. The transcript and presentation
              slides are available here: http://ssrn.com/abstract=2895842.",
  month    =  jul,
  year     =  2017,
  keywords = "P-hacking, Multiple testing, Selection, Data mining, Data
              dredging, Rare incidence, Type I error, Type II error, P-values,
              Minimum Bayes Factor, MBF, SD-MBF, Bayesian P-values"
}

@ARTICLE{Ogburn2020-ux,
  title    = "Causal inference, social networks and chain graphs",
  author   = "Ogburn, Elizabeth L and Shpitser, Ilya and Lee, Youjin",
  abstract = "Summary Traditionally, statistical inference and causal inference
              on human subjects rely on the assumption that individuals are
              independently affected by treatments or exposures. However,
              recently there has been increasing interest in settings, such as
              social networks, where individuals may interact with one another
              such that treatments may spill over from the treated individual
              to their social contacts and outcomes may be contagious. Existing
              models proposed for causal inference using observational data
              from networks of interacting individuals have two major
              shortcomings. First, they often require a level of granularity in
              the data that is infeasible in practice to collect in most
              settings and, second, the models are high dimensional and often
              too big to fit to the available data. We illustrate and justify a
              parsimonious parameterization for network data with interference
              and contagion. Our parameterization corresponds to a particular
              family of graphical models known as chain graphs. We argue that,
              in some settings, chain graph models approximate the marginal
              distribution of a snapshot of a longitudinal data-generating
              process on interacting units. We illustrate the use of chain
              graphs for causal inference about collective decision making in
              social networks by using data from US Supreme Court decisions
              between 1994 and 2004 and in simulations.",
  journal  = "J. R. Stat. Soc. A",
  volume   =  5,
  pages    = "116",
  month    =  jul,
  year     =  2020
}

@INCOLLECTION{Hamilton2010-jh,
  title     = "Regime switching models",
  booktitle = "Macroeconometrics and Time Series Analysis",
  author    = "Hamilton, James D",
  editor    = "Durlauf, Steven N and Blume, Lawrence E",
  abstract  = "Many economic time series occasionally exhibit dramatic breaks
               in their behaviour, associated with events such as financial
               crises (Jeanne and Masson, 2000; Cerra and Saxena, 2005;
               Hamilton, 2005) or abrupt changes in government policy
               (Hamilton, 1988; Sims and Zha, 2006; Davig, 2004). Of particular
               interest to economists is the apparent tendency of many economic
               variables to behave quite differently during economic downturns,
               when underutilization of factors of production rather than their
               long-run tendency to grow governs economic dynamics (Hamilton,
               1989; Chauvet and Hamilton, 2006). Abrupt changes are also a
               prevalent feature of financial data, and the approach described
               below is quite amenable to theoretical calculations for how such
               abrupt changes in fundamentals should show up in asset prices
               (Ang and Bekaert, 2002a; 2000b; Garcia, Luger and Renault, 2003;
               Dai, Singleton and Yang, 2003).",
  publisher = "Palgrave Macmillan UK",
  pages     = "202--209",
  year      =  2010,
  address   = "London"
}

@ARTICLE{Garcia2003-wb,
  title    = "Empirical assessment of an intertemporal option pricing model
              with latent variables",
  author   = "Garcia, Ren{\'e} and Luger, Richard and Renault, Eric",
  abstract = "This paper assesses the empirical performance of an intertemporal
              option pricing model with latent variables which generalizes the
              Black--Scholes and the stochastic volatility formulas. We derive
              a closed-form formula for an equilibrium model with recursive
              preferences where the fundamentals follow a Markov switching
              process. In a simulation experiment based on the model, we show
              that option prices are more informative about preference
              parameters than stock returns. When we estimate the preference
              parameters implicit in S\&P 500 call option prices given our
              model, we find quite reasonable values for the coefficient of
              relative risk aversion and the intertemporal elasticity of
              substitution.",
  journal  = "J. Econom.",
  volume   =  116,
  number   =  1,
  pages    = "49--83",
  month    =  sep,
  year     =  2003,
  keywords = "Stochastic volatility; Black--Scholes implied volatility; Smile
              effect; Equilibrium option pricing; Recursive utility"
}

@ARTICLE{Dai2007-ow,
  title     = "Regime Shifts in a Dynamic Term Structure Model of {U.S}.
               Treasury Bond Yields",
  author    = "Dai, Qiang and Singleton, Kenneth J and Yang, Wei",
  abstract  = "Abstract. This article develops and empirically implements an
               arbitrage-free, dynamic term structure model with ``priced''
               factor and regime-shift risks. The ris",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford Academic",
  volume    =  20,
  number    =  5,
  pages     = "1669--1706",
  month     =  apr,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Andersen2010-ra,
  title    = "Do Bonds Span Volatility Risk in the {U.S}. Treasury Market? A
              Specification Test for Affine Term Structure Models",
  author   = "Andersen, Torben G and Benzoni, Luca",
  abstract = "ABSTRACT We propose using model-free yield quadratic variation
              measures computed from intraday data as a tool for specification
              testing and selection of dynamic term structure models. We find
              that the yield curve fails to span realized yield volatility in
              the U.S. Treasury market, as the systematic volatility factors
              are largely unrelated to the cross-section of yields. We conclude
              that a broad class of affine diffusive, quadratic Gaussian, and
              affine jump-diffusive models cannot accommodate the observed
              yield volatility dynamics. Hence, the Treasury market per se is
              incomplete, as yield volatility risk cannot be hedged solely
              through Treasury securities.",
  journal  = "J. Finance",
  volume   =  65,
  number   =  2,
  pages    = "603--653",
  month    =  apr,
  year     =  2010
}

@ARTICLE{Ang2002-hg,
  title    = "Asymmetric correlations of equity portfolios",
  author   = "Ang, Andrew and Chen, Joseph",
  abstract = "Correlations between U.S. stocks and the aggregate U.S. market
              are much greater for downside moves, especially for extreme
              downside moves, than for upside moves. We develop a new statistic
              for measuring, comparing, and testing asymmetries in conditional
              correlations. Conditional on the downside, correlations in the
              data differ from the conditional correlations implied by a normal
              distribution by 11.6\%. We find that conditional asymmetric
              correlations are fundamentally different from other measures of
              asymmetries, such as skewness and co-skewness. We find that small
              stocks, value stocks, and past loser stocks have more asymmetric
              movements. Controlling for size, we find that stocks with lower
              betas exhibit greater correlation asymmetries, and we find no
              relationship between leverage and correlation asymmetries.
              Correlation asymmetries in the data reject the null hypothesis of
              multivariate normal distributions at daily, weekly, and monthly
              frequencies. However, several empirical models with greater
              flexibility, particularly regime-switching models, perform better
              at capturing correlation asymmetries.",
  journal  = "J. financ. econ.",
  volume   =  63,
  number   =  3,
  pages    = "443--494",
  month    =  mar,
  year     =  2002,
  keywords = "Stock return asymmetries; Correlation; Dispersion; Model bias;
              GARCH; Jump model; Regime-switching"
}

@ARTICLE{Baele2005-pa,
  title     = "Volatility Spillover Effects in European Equity Markets",
  author    = "Baele, Lieven",
  abstract  = "[This paper investigates to what extent globalization and
               regional integration lead to increasing equity market
               interdependence. I focus on Western Europe, as this region has
               gone through a unique period of economic, financial, and
               monetary integration. More specifically, I quantify the
               magnitude and time-varying nature of volatility spillovers from
               the aggregate European (EU) and U.S. market to 13 local European
               equity markets. To account for time-varying integration, I use a
               regime-switching model to allow the shock sensitivites to change
               over time. I find regime switches to be both statistically and
               economically important. Both the EU and U.S. shock spillover
               intensity increased substantially over the 1980s and 1990s,
               though the rise is more pronounced for EU spillovers. Shock
               spillover intensities increased most strongly in the second half
               of the 1980s and the first half of the 1990s. I show that
               increased trade integration, equity market development, and low
               inflation contribute to the increase in EU shock spillover
               intensity. I also find evidence for contagion from the U.S.
               market to a number of local European equity markets during
               periods of high world market volatility.]",
  journal   = "The Journal of Financial and Quantitative Analysis",
  publisher = "Cambridge University Press",
  volume    =  40,
  number    =  2,
  pages     = "373--401",
  year      =  2005
}

@ARTICLE{Guidolin2007-to,
  title     = "Asset allocation under multivariate regime switching",
  author    = "Guidolin, Massimo and Timmermann, Allan",
  abstract  = "This paper studies asset allocation decisions in the presence of
               regime switching in asset returns. We find evidence that four
               separate regimes -- characterized as crash, slow growth, bull
               and recovery states -- are required to capture the joint
               distribution of stock and bond returns. Optimal asset
               allocations vary considerably across these states and change
               over time as investors revise their estimates of the state
               probabilities. In the crash state, buy-and-hold investors
               allocate more of their portfolio to stocks the longer their
               investment horizon, while the optimal allocation to stocks
               declines as a function of the investment horizon in bull
               markets. The joint effects of learning about state probabilities
               and predictability of asset returns from the dividend yield give
               rise to a non-monotonic relationship between the investment
               horizon and the demand for stocks. Out-of-sample forecasting
               experiments confirm the economic importance of accounting for
               the presence of regimes in asset returns.",
  journal   = "J. Econ. Dyn. Control",
  publisher = "Elsevier",
  volume    =  31,
  number    =  11,
  pages     = "3503--3544",
  month     =  nov,
  year      =  2007,
  keywords  = "Regime switching; Portfolio choice; Predictability"
}

@ARTICLE{Kritzman2012-yl,
  title     = "Regime Shifts: Implications for Dynamic Strategies (corrected)",
  author    = "Kritzman, Mark and Page, S{\'e}bastien and Turkington, David",
  abstract  = "Authors? Note: S{\'e}bastien Page, CFA, worked on this article
               while at State Street Global Markets.",
  journal   = "Financial Analysts Journal",
  publisher = "Routledge",
  volume    =  68,
  number    =  3,
  pages     = "22--39",
  month     =  may,
  year      =  2012
}

@ARTICLE{Guidolin2007-is,
  title     = "Size and Value Anomalies under Regime Shifts",
  author    = "Guidolin, Massimo and Timmermann, Allan",
  abstract  = "Abstract. This paper finds strong evidence of time-variations in
               the joint distribution of returns on a stock market portfolio
               and portfolios tracking size- an",
  journal   = "Journal of Financial Econometrics",
  publisher = "Oxford Academic",
  volume    =  6,
  number    =  1,
  pages     = "1--48",
  month     =  dec,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Tu2010-ys,
  title     = "Is Regime Switching in Stock Returns Important in Portfolio
               Decisions?",
  author    = "Tu, Jun",
  abstract  = "The stock market displays regime switching between upturns and
               downturns. This paper provides a Bayesian framework for making
               portfolio decisions that takes this regime switching into
               account, together with asset pricing model uncertainty and
               parameter uncertainty. The findings reveal that the economic
               value of accounting for regimes is substantially independent of
               whether or not model and parameter uncertainties are
               incorporated: the certainty-equivalent losses associated with
               ignoring regime switching are generally above 2\% per year and
               can be as high as 10\%. These results suggest that the more
               realistic regime switching model is fundamentally different from
               the commonly used single-state model, and hence should be
               employed instead in portfolio decisions irrespective of concerns
               about model or parameter uncertainty.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  56,
  number    =  7,
  pages     = "1198--1215",
  month     =  jul,
  year      =  2010
}

@ARTICLE{Hansen1997-dk,
  title     = "Approximate Asymptotic {P} Values for {StructuraS-Change} Tests",
  author    = "Hansen, Bruce E",
  abstract  = "Numerical approximations to the asymptotic distributions of
               recently proposed tests for structural change are presented.
               This enables easy yet accurate calculation of asymptotic p
               values. A GAUSS program is available to perform the
               computations.",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  15,
  number    =  1,
  pages     = "60--67",
  month     =  jan,
  year      =  1997
}

@BOOK{Gelman2013-ij,
  title     = "Bayesian Data Analysis, Third Edition",
  author    = "Gelman, Andrew and Carlin, John B and Stern, Hal S and Dunson,
               David B and Vehtari, Aki and Rubin, Donald B",
  abstract  = "Now in its third edition, this classic book is widely considered
               the leading text on Bayesian methods, lauded for its accessible,
               practical approach to analyzing data and solving research
               problems. Bayesian Data Analysis, Third Edition continues to
               take an applied approach to analysis using up-to-date Bayesian
               methods. The authors---all leaders in the statistics
               community---introduce basic concepts from a data-analytic
               perspective before presenting advanced methods. Throughout the
               text, numerous worked examples drawn from real applications and
               research emphasize the use of Bayesian inference in practice.
               New to the Third Edition Four new chapters on nonparametric
               modeling Coverage of weakly informative priors and
               boundary-avoiding priors Updated discussion of cross-validation
               and predictive information criteria Improved convergence
               monitoring and effective sample size calculations for iterative
               simulation Presentations of Hamiltonian Monte Carlo, variational
               Bayes, and expectation propagation New and revised software code
               The book can be used in three different ways. For undergraduate
               students, it introduces Bayesian inference starting from first
               principles. For graduate students, the text presents effective
               current approaches to Bayesian modeling and computation in
               statistics and related fields. For researchers, it provides an
               assortment of Bayesian methods in applied statistics. Additional
               materials, including data sets used in the examples, solutions
               to selected exercises, and software instructions, are available
               on the book's web page.",
  publisher = "CRC Press",
  month     =  nov,
  year      =  2013,
  language  = "en"
}

@ARTICLE{McKillop2020-nc,
  title    = "Cooperative financial institutions: A review of the literature",
  author   = "McKillop, Donal and French, Declan and Quinn, Barry and Sobiech,
              Anna L and Wilson, John O S",
  abstract = "Financial cooperatives play an important role in the financial
              systems of many countries. They act as a safe haven for deposits
              and are major sources of credit for households and small- and
              medium-sized firms. A not-for-profit orientation (in many cases)
              and a focus on maximising benefits to members have ensured the
              enduring popularity and sustainability of financial cooperatives.
              This is particularly evident since the global financial crisis
              when financial cooperatives continued to extend credit to members
              as many profit-orientated commercial banks restricted credit to
              households and firms. The overarching theme of the first part of
              this review is the structural and behavioural characteristics of
              financial cooperatives. In this part we consider, the origin and
              diffusion of financial cooperatives, network arrangements, the
              business model, relationship banking, balancing the interest of
              members, tax treatment and regulatory framework. The second part
              has performance and contribution to the real economy as the
              overarching theme. In this part we consider, efficiency and
              sustainability, mergers, acquisitions and failures, the benefits
              (and challenges) of FinTech and the contribution of financial
              cooperatives to the real economy including during times of
              crisis. The paper concludes with a summary of what we now know
              (and do not know) about financial cooperatives and provides
              suggestions as to where future research may usefully concentrate.",
  journal  = "International Review of Financial Analysis",
  volume   =  71,
  pages    = "101520",
  month    =  oct,
  year     =  2020,
  keywords = "Cooperative financial institutions; Literature review"
}

@ARTICLE{Van_den_Broeck1994-pi,
  title    = "Stochastic frontier models: A Bayesian perspective",
  author   = "van den Broeck, Julien and Koop, Gary and Osiewalski, Jacek and
              Steel, Mark F J",
  abstract = "A Bayesian approach to estimation, prediction, and model
              comparison in composed error production models is presented. A
              broad range of distributions on the inefficiency term define the
              contending models, which can either be treated separately or
              pooled. Posterior results are derived for the individual
              efficiences as well as for the parameters, and the differences
              with the usual sampling-theory approach are highlighted. The
              required numerical integrations are handled by Monte Carlo
              methods with Importance Sampling, and an empirical example
              illustrates the procedures.",
  journal  = "J. Econom.",
  volume   =  61,
  number   =  2,
  pages    = "273--303",
  month    =  apr,
  year     =  1994,
  keywords = "Composed error models; Efficiency; Model comparison; Mixing of
              models; Prior elicitation"
}

@ARTICLE{Rossi2015-xq,
  title     = "Modeling Covariance Risk in Merton's {ICAPM}",
  author    = "Rossi, Alberto G and Timmermann, Allan",
  abstract  = "Abstract. We propose a new method for constructing the hedge
               component in Merton's ICAPM that uses a daily summary measure of
               economic activity to track time-v",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford Academic",
  volume    =  28,
  number    =  5,
  pages     = "1428--1461",
  month     =  feb,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Fu2014-ct,
  title    = "Bank competition and financial stability in Asia Pacific",
  author   = "Fu, Xiaoqing (maggie) and Lin, Yongjia (rebecca) and Molyneux,
              Philip",
  abstract = "Analysis of the tradeoff between competition and financial
              stability has been at the center of academic and policy debate
              for over two decades and especially since the 2007--2008 global
              financial crises. Here we use information on 14 Asia Pacific
              economies from 2003 to 2010 to investigate the influence of bank
              competition, concentration, regulation and national institutions
              on individual bank fragility as measured by the probability of
              bankruptcy and the bank's Z-score. The results suggest that
              greater concentration fosters financial fragility and that lower
              pricing power also induces bank risk exposure after controlling
              for a variety of macroeconomic, bank-specific, regulatory and
              institutional factors. In terms of regulations and institutions,
              the results show that tougher entry restrictions may benefit bank
              stability, whereas stronger deposit insurance schemes are
              associated with greater bank fragility.",
  journal  = "Journal of Banking \& Finance",
  volume   =  38,
  pages    = "64--77",
  month    =  jan,
  year     =  2014,
  keywords = "Bank competition; Financial stability; Regulation; Banks in Asia
              Pacific"
}

@ARTICLE{Zhang2016-pj,
  title    = "Non-performing loans, moral hazard and regulation of the Chinese
              commercial banking system",
  author   = "Zhang, Dayong and Cai, Jing and Dickinson, David G and Kutan, Ali
              M",
  abstract = "Non-performing loans (NPLs) represent a major obstacle to the
              development of banking sector. One of the key objectives of the
              banking sector reforms in China has therefore been to reduce the
              high level of NPLs. To do so, Chinese regulatory authorities have
              injected significant capital into the banking system and
              scrutinized NPLs since 2003. This paper examines the impact of
              NPLs on bank behavior in China. Using a threshold panel
              regression model and a dataset covering 60 city commercial banks,
              16 state-owned banks and joint-stock banks, and 11 rural
              commercial banks during 2006--2012, we test whether lending
              decisions of Chinese banks exhibit moral hazard. The results
              support the moral hazard hypothesis, suggesting that an increase
              in the NPLs ratio raises riskier lending, potentially causing
              further deterioration of the loan quality and financial system
              instability. Policy implications of findings are evaluated.",
  journal  = "Journal of Banking \& Finance",
  volume   =  63,
  pages    = "48--60",
  month    =  feb,
  year     =  2016,
  keywords = "Non-performing loans; Moral hazard; Lending behavior; Threshold
              panel regression; Bank regulation"
}

@ARTICLE{Zhang2013-kz,
  title    = "Market concentration, risk-taking, and bank performance: Evidence
              from emerging economies",
  author   = "Zhang, Jianhua and Jiang, Chunxia and Qu, Baozhi and Wang, Peng",
  abstract = "This paper investigates the relationship between market
              concentration, risk-taking, and bank performance using a unique
              dataset of the BRIC banks over the period 2003--2010. We find a
              negative association between market concentration and
              performance, in support of the ``quiet life'' hypothesis. We also
              find that banks taking a lower level of risks perform better, in
              favor of prudential practice. Moreover, the BRICs' banking
              sectors were all negatively affected by the 2007--2008 global
              financial crisis with China and Russia being the least and most
              affected, respectively. On average Chinese and Brazilian banks
              outperform Indian and Russian ones, indicating that China and
              Brazil have more favorable institutional infrastructure. These
              results are robust to alternative model specifications and
              estimation techniques. Our analysis may have important policy
              implications for bankers and regulators in the BRICs and other
              developing and transition countries.",
  journal  = "International Review of Financial Analysis",
  volume   =  30,
  pages    = "149--157",
  month    =  dec,
  year     =  2013,
  keywords = "Market concentration; Risk-taking; Bank performance; Stochastic
              frontier analysis; Brazil; Russia; India; China"
}

@ARTICLE{Lin2016-uv,
  title    = "Changes in ownership structure and bank efficiency in Asian
              developing countries: The role of financial freedom",
  author   = "Lin, Kun-Li and Doan, Anh Tuan and Doong, Shuh-Chyi",
  abstract = "This paper investigates the effect of the changes in bank
              ownership on cost efficiency across twelve Asian developing
              economies. We also evaluate how financial freedom shapes the
              effect of the changes in bank ownership on cost efficiency. Using
              stochastic frontier approach to estimate bank efficiency scores
              during the period 2003--2012, we find that foreign presence
              improves bank efficiency, primarily in countries with high
              financial freedom. In addition, our results also show that
              increased government (domestic) ownership of bank appears to
              improve (impede) bank efficiency in countries with more financial
              freedom after financial crisis.",
  journal  = "International Review of Economics \& Finance",
  volume   =  43,
  pages    = "19--34",
  month    =  may,
  year     =  2016,
  keywords = "Ownership changes; Bank efficiency; Financial freedom"
}

@ARTICLE{Doan2018-gl,
  title    = "What drives bank efficiency? The interaction of bank income
              diversification and ownership",
  author   = "Doan, Anh-Tuan and Lin, Kun-Li and Doong, Shuh-Chyi",
  abstract = "This paper examines the relation between income diversification
              and bank efficiency across 83 countries over the period
              2003--2012. We also evaluate how ownership structure affects the
              impact of bank diversification on cost efficiency. Using a
              stochastic frontier approach to estimate bank cost efficiency, we
              find evidence that increased diversification tends to improve
              bank efficiency but the benefits of diversification are offset by
              the increased exposure to volatile non-interest activities. With
              respect to the impact of ownership, we find that state-owned
              banks with fewer volatile income sources are likely to be less
              efficient in terms of income diversification. Our results also
              reveal that more diversified foreign-owned banks tend to be less
              efficient in developed countries, while the increased foreign
              ownership of banks appears to improve the diversification
              benefits in developing countries after the financial crisis. Our
              findings highlight the implications of bank income
              diversification and ownership for efficiency and are relevant to
              bank regulators who are considering additional regulations on
              bank management.",
  journal  = "International Review of Economics \& Finance",
  volume   =  55,
  pages    = "203--219",
  month    =  may,
  year     =  2018,
  keywords = "Income diversification; Ownership structure; Efficiency; Banking"
}

@ARTICLE{Shaban2018-qn,
  title    = "The effects of ownership change on bank performance and risk
              exposure: Evidence from indonesia",
  author   = "Shaban, Mohamed and James, Gregory A",
  abstract = "This study investigates the effects of ownership change on the
              performance and exposure to risk of 60 Indonesian commercial
              banks over the period 2005--2012. We find that state-owned banks
              tend to be less profitable and more exposed to risk than private
              and foreign banks. Domestic investors tend to select the best
              performers for acquisition. Domestic acquisition is generally
              associated with a decrease in the efficiency of the acquired
              banks. Non-regional foreign acquisition is associated with a
              reduction in risk exposure. Acquisition by regional foreign
              investors is associated with performance gains.",
  journal  = "Journal of Banking \& Finance",
  volume   =  88,
  pages    = "483--497",
  month    =  mar,
  year     =  2018,
  keywords = "Bank; Efficiency; Ownership; Governance; M\&A; Foreign
              acquisition; Privatization; Indonesia"
}

@ARTICLE{Dong2017-wg,
  title    = "Governance, efficiency and risk taking in Chinese banking",
  author   = "Dong, Yizhe and Girardone, Claudia and Kuo, Jing-Ming",
  abstract = "We employ a hand-collected unique dataset on banks operating in
              China between 2003 and 2011 to investigate the impact of board
              governance features (size, composition and functioning) on bank
              efficiency and risk taking. Our evidence suggests that board
              characteristics tend to have a greater influence on banks' profit
              and cost efficiency than on loan quality. We find that the
              proportion of female directors on the board appears not only to
              be linked to higher profit and cost efficiency but also to lower
              traditional banking risk. Similarly, board independence is
              associated with higher profit efficiency of banks; while the
              opposite is found for executive directors and in the presence of
              dual leadership of the CEO/chairperson. Among the control
              variables, we found that liquidity negatively affects profit and
              cost efficiency, while positively affecting risk. Interestingly,
              we find some evidence of an incremental effect of specific board
              characteristics on efficiency for banks with more concentrated
              ownership structures and state-owned institutions; while for
              banks with CEO performance-related pay schemes the effect on
              efficiency when significant is usually negative. Our results
              offer useful insights to policy makers in China charged with the
              task of improving the governance mechanisms in banking
              institutions.",
  journal  = "The British Accounting Review",
  volume   =  49,
  number   =  2,
  pages    = "211--229",
  month    =  mar,
  year     =  2017,
  keywords = "Board governance; Bank efficiency; Asset quality; Bank ownership;
              Performance-related compensation; Chinese banking sector"
}

@ARTICLE{Fu2014-ag,
  title    = "Bank efficiency and shareholder value in Asia Pacific",
  author   = "Fu, Xiaoqing (maggie) and Lin, Yongjia (rebecca) and Molyneux,
              Philip",
  abstract = "This paper uses dynamic panel estimation approaches to
              investigate the relationship between shareholder value and
              efficiency for a large sample of commercial banks in 14
              Asia-Pacific economies between 2003 and 2010. In general, the
              results indicate that shareholder value is positively linked to
              improvements in both cost and profit efficiency, and the
              influence varies over time. The findings also suggest that bank
              size, credit losses, and market risk significantly influence bank
              performance. These results are robust to various model
              specifications.",
  journal  = "Journal of International Financial Markets, Institutions and
              Money",
  volume   =  33,
  pages    = "200--222",
  month    =  nov,
  year     =  2014,
  keywords = "Bank efficiency; Shareholder value; Risk-taking; Banks in Asia
              Pacific"
}

@ARTICLE{Fu2016-zw,
  title    = "{BANK} {CAPITAL} {AND} {LIQUIDITY} {CREATION} {IN} {ASIA}
              {PACIFIC}",
  author   = "Fu, Xiaoqing Maggie and Lin, Yongjia Rebecca and Molyneux, Philip",
  abstract = "We calculate liquidity creation for a large sample of commercial
              banks in 14 Asia-Pacific economies from 2005 to 2012. We find
              that banks in this region created \$7.83 trillion in liquidity in
              2012, or approximately 3.24 times the total liquidity created in
              2005. Large banks and those based in Developing Asia created the
              highest liquidity. We also investigate the bicausal relationship
              between liquidity creation and regulatory capital and find that
              the trade-off between the benefits of financial stability induced
              by enhanced capital requirements and those of higher liquidity
              creation is applicable to all of the sample banks, regardless of
              bank size and economic region. (JEL G21, G28)",
  journal  = "Econ. Inq.",
  volume   =  54,
  number   =  2,
  pages    = "966--993",
  month    =  apr,
  year     =  2016
}

@ARTICLE{Zhang2018-ev,
  title     = "Real estate investments and financial stability: evidence from
               regional commercial banks in China",
  author    = "Zhang, Dayong and Cai, Jing and Liu, Jia and Kutan, Ali M",
  abstract  = "ABSTRACTThe 2008 US subprime mortgage crisis demonstrated how
               developments in real estate markets can cause instability in the
               banking sector and raised concerns in many emerging economies
               with significant real estate development and a rapidly growing
               commercial banking sector, particularly in China. There is clear
               evidence that commercial banks in China, especially regional
               commercial banks, have lent significantly to the real estate
               sector. The recent slowdown in the housing market in China and
               the increase in nonperforming loans (NPLs) in China's commercial
               banking sector motivated us to investigate the connection
               between real estate markets and banking stability. This paper
               proposes three testable hypotheses linking the growth of
               investment in real estate and the stability of regional
               commercial banks in China, measured by NPLs. Our empirical
               results reveal a close connection between the growth of
               investment in real estate and the NPLs among regional commercial
               banks, and its sensitivity to real estate market cycles. When
               real estate market activity declines, our results suggest,
               regional commercial banks can find themselves in trouble if they
               have significant exposure to one type of (real estate) asset. In
               addition, we find that regional bank competition plays a
               critical role in defining the relationship between bank
               stability and real estate investment activity.",
  journal   = "null",
  publisher = "Routledge",
  volume    =  24,
  number    =  16,
  pages     = "1388--1408",
  month     =  nov,
  year      =  2018
}

@ARTICLE{Nguyen2016-xe,
  title    = "Efficiency, innovation and competition: evidence from Vietnam,
              China and India",
  author   = "Nguyen, Thanh Pham Thien and Nghiem, Son Hong and Roca, Eduardo
              and Sharma, Parmendra",
  abstract = "Given a number of common features in both the economy and banking
              markets across Vietnam, China and India, this study investigates
              the comparative levels of efficiency, innovation and competition
              and then examines the effect of competition on innovation of
              banks in the three countries over the period 1995--2011. Applying
              for the first time a recently developed stochastic meta-frontier
              framework to the banking sector, this study finds that Indian
              banks are the most cost-efficient, followed by Chinese banks and
              Vietnamese banks. Indian banks are also more innovative in
              reducing the operating costs compared to Chinese and Vietnamese
              banks. Using the Lerner index, the banking markets of India and
              Vietnam are found to be more competitive than that of China. The
              relationship between competition and cost-reducing innovation
              follows an inverse U-shape curve, which is consistent with the
              literature. This study also finds that the optimal Lerner index
              to achieve the greatest innovation in reducing operating costs is
              55 \%. Based on the results of the Lerner index, this study
              recommends that to improve cost-reducing innovation, policy
              makers should promote bank competition in all the three
              countries, but to a greater extent in China than in Vietnam and
              India.",
  journal  = "Empir. Econ.",
  volume   =  51,
  number   =  3,
  pages    = "1235--1259",
  month    =  nov,
  year     =  2016
}

@ARTICLE{Lee2015-td,
  title    = "How does Bank Capital Affect Bank Profitability and Risk?
              Evidence from China's {WTO} Accession",
  author   = "Lee, Chien-Chiang and Ning, Shao-Lin and Lee, Chi-Chuan",
  abstract = "Abstract This paper examines how bank capital affects bank
              profitability and risk in China, and how its impact differed
              before and after the nation entered the WTO. Our study uses the
              dynamic generalized method of moments approach with a panel
              database containing 171 Chinese commercial banks. We find that
              bank capital has significant influence on bank profitability and
              risk, but its impact has declined since China joined the WTO in
              2001. For different sized groups, the impact of capital on
              profitability exhibits a distinct trend. The effects of capital
              on bank risk are different for large and small banks depending on
              the risk variables used for the Chinese banking industry.",
  journal  = "China World Econ.",
  volume   =  23,
  number   =  4,
  pages    = "19--39",
  month    =  jul,
  year     =  2015
}

@ARTICLE{Safiullah2019-dt,
  title    = "Risk-adjusted efficiency and corporate governance: Evidence from
              Islamic and conventional banks",
  author   = "{Safiullah} and Shamsuddin, Abul",
  abstract = "Previous studies have compared the efficiency of Islamic banks
              with their conventional counterparts using a common efficiency
              frontier and ignoring risks, in spite of the two bank groups
              operating under different technological, market and institutional
              conditions. We overcome this issue by estimating efficiency using
              the stochastic meta-frontier model for a large international
              sample, and show that compared to conventional banks, Islamic
              banks are 4 percentage points more cost efficient, but 17
              percentage points less profit efficient on a risk-adjusted basis.
              For both bank types, higher bank risk reduces cost efficiency but
              increases profit efficiency, implying that risks contribute more
              to generating revenues than inflating costs. Having a stronger
              Shariah supervisory board is conducive to improving Islamic
              banks' profit efficiency. Our findings are robust to accounting
              for potential endogeneity in the governance-efficiency
              relationship.",
  journal  = "Journal of Corporate Finance",
  volume   =  55,
  pages    = "105--140",
  month    =  apr,
  year     =  2019,
  keywords = "Cost efficiency; Profit efficiency; Islamic banking; Shariah
              supervisory board; Stochastic meta-frontier."
}

@ARTICLE{Zhou2019-is,
  title    = "{Chair-CEO} generation gap and bank risk-taking",
  author   = "Zhou, Yifan and Kara, Alper and Molyneux, Philip",
  abstract = "Poor bank governance has disastrous consequences for economies as
              the 2007--2009 financial crisis has shown. In the aftermath,
              board diversity is identified as an effective mechanism to
              enhance bank governance. Diversity, creating cognitive conflict
              between board members, is expected to enhance board's
              independence of thought to better perform monitoring and advising
              functions. Age is a key demographic measure and age dissimilarity
              between the chair and the CEO in non-financial firms leads to
              better economic outcomes (Goergen, Limbach, \& Scholz, 2015). In
              this paper, we examine whether chair-CEO age dissimilarity can
              mitigate banks' excessive risk-taking behaviour. Using a unique
              sample of 100 listed banks in Europe between 2005 and 2014, we
              find that age difference between the chair and the CEO reduces
              bank risk-taking. A chair-CEO generational gap --defined as a
              minimum of 20 years' age difference-- has a larger impact in
              reducing risk-taking.",
  journal  = "The British Accounting Review",
  volume   =  51,
  number   =  4,
  pages    = "352--372",
  month    =  jun,
  year     =  2019,
  keywords = "Bank governance; Board diversity; Age difference; Generational
              gap; Bank risk-taking; European banks"
}

@ARTICLE{Zhu2020-br,
  title    = "Ranking Chinese commercial banks based on their expected impact
              on structural efficiency",
  author   = "Zhu, Ning and Hougaard, Jens Leth and Yu, Zhiqian and Wang, Bing",
  abstract = "This paper examines the performance of 16 major Chinese
              commercial banks. In particular, we employ a new way of ranking
              the banks based on their average marginal impact on structural
              efficiency. We find important differences between our ranking
              results and that of the conventional super-efficiency approach.
              We argue that in case of Chinese commercial banks the new way of
              ranking the banks is more in line with expectations based on
              various key performance indicators. The findings further
              reconfirm that the performance of the large commercial banks
              surpass that of the small to medium sized commercial banks.",
  journal  = "Omega",
  volume   =  94,
  pages    = "102049",
  month    =  jul,
  year     =  2020,
  keywords = "Banking; Marginal contribution change; Ranking method; Data
              envelopment analysis; Structural efficiency"
}

@MISC{Kagraoka2016-ru,
  title   = "Common dynamic factors in driving commodity prices: Implications
             of a generalized dynamic factor model",
  author  = "Kagraoka, Yusho",
  journal = "Economic Modelling",
  volume  =  52,
  pages   = "609--617",
  year    =  2016
}

@MISC{Varela_undated-xa,
  title        = "Running financial risk management applications on {FPGA} in
                  the Amazon cloud",
  author       = "Varela, Javier Alejandro and Wehn, Norbert",
  howpublished = "\url{https://ems.eit.uni-kl.de/fileadmin/ems/pdf/Whitepaper-EMS-FPGA-AWS-1.pdf}",
  note         = "Accessed: 2021-1-11"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Coqueret2020-en,
  title     = "Machine Learning in Finance: From Theory to Practice",
  author    = "Coqueret, Guillaume",
  publisher = "ROUTLEDGE JOURNALS, TAYLOR \& FRANCIS LTD 2-4 PARK SQUARE,
               MILTON PARK ‚Ä¶",
  year      =  2020
}

@ARTICLE{Efron1991-rm,
  title    = "Statistical data analysis in the computer age",
  author   = "Efron, B and Tibshirani, R",
  abstract = "Most of our familiar statistical methods, such as hypothesis
              testing, linear regression, analysis of variance, and maximum
              likelihood estimation, were designed to be implemented on
              mechanical calculators. Modern electronic computation has
              encouraged a host of new statistical methods that require fewer
              distributional assumptions than their predecessors and can be
              applied to more complicated statistical estimators. These methods
              allow the scientist to explore and describe data and draw valid
              statistical inferences without the usual concerns for
              mathematical tractability. This is possible because traditional
              methods of mathematical analysis are replaced by specially
              constructed computer algorithms. Mathematics has not disappeared
              from statistical theory. It is the main method for deciding which
              algorithms are correct and efficient tools for automating
              statistical inference.",
  journal  = "Science",
  volume   =  253,
  number   =  5018,
  pages    = "390--395",
  month    =  jul,
  year     =  1991,
  language = "en"
}

@BOOK{Efron2016-bp,
  title     = "Computer Age Statistical Inference",
  author    = "Efron, Bradley and Hastie, Trevor",
  abstract  = "The twenty-first century has seen a breathtaking expansion of
               statistical methodology, both in scope and in influence. 'Big
               data', 'data science', and 'machine learning' have become
               familiar terms in the news, as statistical methods are brought
               to bear upon the enormous data sets of modern science and
               commerce. How did we get here? And where are we going? This book
               takes us on an exhilarating journey through the revolution in
               data analysis following the introduction of electronic
               computation in the 1950s. Beginning with classical inferential
               theories - Bayesian, frequentist, Fisherian - individual
               chapters take up a series of influential topics: survival
               analysis, logistic regression, empirical Bayes, the jackknife
               and bootstrap, random forests, neural networks, Markov chain
               Monte Carlo, inference after model selection, and dozens more.
               The distinctly modern approach integrates methodology and
               algorithms with statistical inference. The book ends with
               speculation on the future direction of statistics and data
               science.",
  publisher = "Cambridge University Press",
  month     =  jul,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Cappiello2006-kt,
  title     = "Asymmetric Dynamics in the Correlations of Global Equity and
               Bond Returns",
  author    = "Cappiello, Lorenzo and Engle, Robert F and Sheppard, Kevin",
  abstract  = "Abstract. This paper proposes a new generalized autoregressive
               conditionally heteroskedastic (GARCH) process, the asymmetric
               generalized dynamic conditional co",
  journal   = "Journal of Financial Econometrics",
  publisher = "Oxford Academic",
  volume    =  4,
  number    =  4,
  pages     = "537--572",
  month     =  sep,
  year      =  2006,
  language  = "en"
}

@UNPUBLISHED{Kearney2020-ac,
  title  = "How Deep is Your Learning? Machine learning stories from the
            enchanted predictability forest of {UK} firm value",
  author = "Kearney, Fearghal and Quinn, Barry and Sketch, Robert",
  year   =  2020
}

@ARTICLE{Lopez_de_Prado2019-ia,
  title     = "A data science solution to the multiple-testing crisis in
               financial research",
  author    = "L{\'o}pez de Prado, Marcos",
  abstract  = "Most discoveries in empirical finance are false, as a
               consequence of selection bias under multiple testing. Although
               many researchers are aware of this problem, the solutions
               proposed in the literature tend to be complex and hard to
               implement. In this article, the author reduces the problem of
               selection bias in the context of investment strategy development
               to two sub-problems: determining the number of essentially
               independent trials and determining the variance across those
               trials. The author explains what data researchers need to report
               to allow others to evaluate the effect that multiple testing has
               had on reported performance. He applies his method to a real
               case of strategy development and estimates the probability that
               a discovered strategy is false.",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  month     =  feb,
  year      =  2019
}

@ARTICLE{Osmundsen2019-gn,
  title     = "{MCMC} for Markov-switching models---Gibbs sampling vs.
               marginalized likelihood",
  author    = "Osmundsen, Kjartan Kloster and Kleppe, Tore Selland and Oglend,
               Atle",
  abstract  = "AbstractThis article proposes a method to estimate
               Markov-switching vector autoregressive models that combines
               (integrated over latent states) marginal likelihood and
               Hamiltonian Monte Carlo. The method is compared to commonly used
               implementations of Gibbs sampling. The proposed method is found
               to be numerically robust, flexible with respect to model
               specification and easy to implement using the Stan software
               package. The methodology is illustrated on a real data
               application exploring time-varying cointegration relationships
               in a data set consisting of crude oil and natural gas prices.",
  journal   = "null",
  publisher = "Taylor \& Francis",
  pages     = "1--22",
  month     =  jan,
  year      =  2019
}

@ARTICLE{noauthor_2013-ft,
  title    = "Risk of Bayesian Inference in Misspecified Models, and the
              Sandwich Covariance Matrix",
  abstract = "It is well known that, in misspecified parametric models, the
              maximum likelihood estimator (MLE) is consistent for the
              pseudo-true value and has an asymptotically normal sampling
              distribution with ?sandwich? covariance matrix. Also, posteriors
              are asymptotically centered at the MLE, normal, and of asymptotic
              variance that is, in general, different than the sandwich matrix.
              It is shown that due to this discrepancy, Bayesian inference
              about the pseudo-true parameter value is, in general, of lower
              asymptotic frequentist risk when the original posterior is
              substituted by an artificial normal posterior centered at the MLE
              with sandwich covariance matrix. An algorithm is suggested that
              allows the implementation of this artificial posterior also in
              models with high dimensional nuisance parameters which cannot
              reasonably be estimated by maximizing the likelihood.",
  journal  = "Econometrica",
  volume   =  81,
  number   =  5,
  pages    = "1805--1849",
  year     =  2013
}

@ARTICLE{Bernardi2013-kn,
  title         = "Bayesian inference for {CoVaR}",
  author        = "Bernardi, Mauro and Gayraud, Ghislaine and Petrella, Lea",
  abstract      = "Recent financial disasters emphasised the need to
                   investigate the consequence associated with the tail
                   co-movements among institutions; episodes of contagion are
                   frequently observed and increase the probability of large
                   losses affecting market participants' risk capital. Commonly
                   used risk management tools fail to account for potential
                   spillover effects among institutions because they provide
                   individual risk assessment. We contribute to analyse the
                   interdependence effects of extreme events providing an
                   estimation tool for evaluating the Conditional Value-at-Risk
                   (CoVaR) defined as the Value-at-Risk of an institution
                   conditioned on another institution being under distress. In
                   particular, our approach relies on Bayesian quantile
                   regression framework. We propose a Markov chain Monte Carlo
                   algorithm exploiting the Asymmetric Laplace distribution and
                   its representation as a location-scale mixture of Normals.
                   Moreover, since risk measures are usually evaluated on time
                   series data and returns typically change over time, we
                   extend the CoVaR model to account for the dynamics of the
                   tail behaviour. Application on U.S. companies belonging to
                   different sectors of the Standard and Poor's Composite Index
                   (S\&P500) is considered to evaluate the marginal
                   contribution to the overall systemic risk of each individual
                   institution",
  month         =  jun,
  year          =  2013,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "1306.2834"
}

@ARTICLE{Brunnermeier2020-ft,
  title     = "Asset Price Bubbles and Systemic Risk",
  author    = "Brunnermeier, Markus and Rother, Simon and Schnabel, Isabel",
  abstract  = "Abstract. We analyze the relationship between asset price
               bubbles and systemic risk, using bank-level data covering almost
               30 years. Banks' systemic risk alrea",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford Academic",
  volume    =  33,
  number    =  9,
  pages     = "4272--4317",
  month     =  feb,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Phillips2018-yb,
  title    = "Financial bubble implosion and reverse regression",
  author   = "Phillips, Peter C B and Shi, Shu Ping",
  abstract = "Expansion and collapse are two key features of a financial asset
              bubble. Bubble expansion may be modeled using a mildly explosive
              process. Bubble implosion may take several different forms
              depending on the nature of the collapse and therefore requires
              some flexibility in modeling. This paper first strengthens the
              theoretical foundation of the real time bubble monitoring
              strategy proposed in Phillips, Shi and Yu (2015a,b, PSY) by
              developing analytics and studying the performance characteristics
              of the testing algorithm under alternative forms of bubble
              implosion which capture various return paths to market normalcy.
              Second, we propose a new reverse sample use of the PSY procedure
              for detecting crises and estimating the date of market recovery.
              Consistency of the dating estimators is established and the limit
              theory addresses new complications arising from the alternative
              forms of bubble implosion and the endogeneity effects present in
              the reverse regression. A real-time version of the strategy is
              provided that is suited for practical implementation. Simulations
              explore the finite sample performance of the strategy for dating
              market recovery. The use of the PSY strategy for bubble
              monitoring and the new procedure for crisis detection are
              illustrated with an application to the Nasdaq stock market.",
  journal  = "Econometric Theory",
  volume   =  34,
  number   =  4,
  pages    = "49",
  month    =  aug,
  year     =  2018,
  language = "en"
}

@ARTICLE{Sims2006-bk,
  title    = "Were There Regime Switches in {U.S}. Monetary Policy?",
  author   = "Sims, Christopher A and Zha, Tao",
  abstract = "Were There Regime Switches in U.S. Monetary Policy? by
              Christopher A. Sims and Tao Zha. Published in volume 96, issue 1,
              pages 54-81 of American Economic Review, March 2006, Abstract: A
              multivariate regime-switching model for monetary policy is
              confronted with U.S. data. The best fit allows time var...",
  journal  = "Am. Econ. Rev.",
  volume   =  96,
  number   =  1,
  pages    = "54--81",
  month    =  mar,
  year     =  2006
}

@ARTICLE{Carpenter2017-tn,
  title    = "Stan: A Probabilistic Programming Language",
  author   = "Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew and Lee,
              Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker,
              Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen",
  abstract = "Stan is a probabilistic programming language for specifying
              statistical models. A Stan program imperatively defines a log
              probability function over parameters conditioned on specified
              data and constants. As of version 2.14.0, Stan provides full
              Bayesian inference for continuous-variable models through Markov
              chain Monte Carlo methods such as the No-U-Turn sampler, an
              adaptive form of Hamiltonian Monte Carlo sampling. Penalized
              maximum likelihood estimates are calculated using optimization
              methods such as the limited memory
              Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a
              platform for computing log densities and their gradients and
              Hessians, which can be used in alternative algorithms such as
              variational Bayes, expectation propagation, and marginal
              inference using approximate integration. To this end, Stan is set
              up so that the densities, gradients, and Hessians, along with
              intermediate quantities of the algorithm such as acceptance
              probabilities, are easily accessible. Stan can be called from the
              command line using the cmdstan package, through R using the rstan
              package, and through Python using the pystan package. All three
              interfaces support sampling and optimization-based inference with
              diagnostics and posterior analysis. rstan and pystan also provide
              access to log probabilities, gradients, Hessians, parameter
              transforms, and specialized plotting.",
  journal  = "Journal of Statistical Software, Articles",
  volume   =  76,
  number   =  1,
  pages    = "1--32",
  year     =  2017,
  keywords = "probabilistic programming; Bayesian inference; algorithmic
              differentiation; Stan"
}

@ARTICLE{Hoffman2014-tm,
  title   = "The {No-U-Turn} sampler: adaptively setting path lengths in
             Hamiltonian Monte Carlo",
  author  = "Hoffman, Matthew D and Gelman, Andrew",
  journal = "J. Mach. Learn. Res.",
  volume  =  15,
  number  =  1,
  pages   = "1593--1623",
  year    =  2014
}

@INCOLLECTION{Rossi2011-ly,
  title     = "Bayesian Applications in Marketing",
  booktitle = "The Oxford Handbook of Bayesian Econometrics",
  author    = "Rossi, Peter and Allenby, Greg",
  editor    = "Geweke, John and Koop, Gary and Van Dijk, Herman",
  abstract  = "This article describes various discrete choice models of
               consumers who may be heterogeneous both in terms of their
               preferences and in their sensitivities to marketing variables
               such as price. It addresses a distinct set of challenges that
               are being posed through the use of hierarchical priors. It
               considers standard statistical approach that generates
               discreteness by applying a censoring function to underlying
               continuous latent variables. This approach generates models that
               can be employed in situations where more descriptive models are
               required. Nonparametric and flexible parametric models involving
               Dirichlet processes and other mixtures are also accepted and
               favored in marketing. This article outlines several utility
               specifications that incorporate discreteness and other important
               aspects of consumer decisions. Computational issues are
               important when dealing with large marketing data sets and this
               article discusses on how to implement posterior simulation
               methods in marketing models.",
  publisher = "Oxford University Press",
  month     =  sep,
  year      =  2011,
  keywords  = "discrete choice models; hierarchical priors; statistical
               approach; censoring function; Dirichlet processes"
}

@ARTICLE{Gelman2020-vd,
  title     = "Statistics as squid ink: How prominent researchers can get away
               with misrepresenting data",
  author    = "Gelman, Andrew and Guzey, Alexey",
  journal   = "Chance (N. Y.)",
  publisher = "Informa UK Limited",
  volume    =  33,
  number    =  2,
  pages     = "25--27",
  month     =  apr,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Fok2006-ma,
  title     = "A Hierarchical Bayes Error Correction Model to Explain Dynamic
               Effects of Price Changes",
  author    = "Fok, Dennis and Horv{\'a}th, Csilla and Paap, Richard and
               Franses, Philip Hans",
  abstract  = "[The authors put forth a sales response model to explain the
               differences in immediate and dynamic effects of promotional
               prices and regular prices on sales. The model consists of a
               vector autoregression that is rewritten in error correction
               format, which allows the authors to disentangle the immediate
               effects from the dynamic effects. In a second level of the
               model, the immediate price elasticities, the cumulative
               promotional price elasticity, and the long-term regular price
               elasticity are correlated with various brand-specific and
               category-specific characteristics. The model is applied to seven
               years of data on weekly sales of 100 different brands in 25
               product categories. The authors find many significant moderating
               effects on the elasticity of price promotions. Brands in
               categories that are characterized by high price differentiation
               and that constitute a lower share of budget are less sensitive
               to price discounts. Deep price discounts increase the immediate
               price sensitivity of customers. The authors also find
               significant effects for the cumulative elasticity. The immediate
               effect of a regular price change is often close to zero. The
               long-term effect of such a regular price decrease usually
               amounts to an increase in sales. This is especially true in
               categories that are characterized by a large price dispersion
               and frequent price promotions and for hedonic, nonperishable
               products.]",
  journal   = "J. Mark. Res.",
  publisher = "American Marketing Association",
  volume    =  43,
  number    =  3,
  pages     = "443--461",
  year      =  2006
}

@UNPUBLISHED{Meuleman2019-lp,
  title    = "Macroprudential Policy and Bank Systemic Risk",
  author   = "Meuleman, Elien and Vander Vennet, Rudi",
  abstract = "This paper investigates the effectiveness of macroprudential
              policy to contain the systemic risk of European banks between
              2000 and 2017. We use a new database (MaPPED) collected by
              experts at the ECB and national central banks with narrative
              information on a broad range of instruments which are tracked
              over their life cycle. Using a dynamic panel framework at a
              monthly frequency enables us to assess the impact of
              macroprudential tools and their design on the banks' systemic
              risk both in the short and the long run. We furthermore decompose
              the systemic risk measure in an individual bank risk component
              and a systemic linkage component. This is of particular interest
              because microprudential policy focuses on the tail risk of an
              individual bank while macroprudential policy targets systemic
              risk by addressing the interlinkages and common exposures across
              banks. On average, all banks benefit from macroprudential tools
              in terms of their individual risk. We find that credit growth
              tools and exposure limits exhibit the most pronounced downward
              effect on the individual risk component. However, we find
              evidence for a risk-shifting effect which is more pronounced for
              retail-oriented banks. The effects are heterogeneous across banks
              with respect to the systemic linkage component. Liquidity tools
              and measures aimed at increasing the resilience of banks decrease
              the systemic linkage of banks. However, these tools appear to be
              most effective for distressed banks. Our results have
              implications for the optimal design of macroprudential
              instruments.",
  month    =  apr,
  year     =  2019,
  keywords = "European banks, macroprudential policy, systemic risk"
}

@ARTICLE{Demirguc-Kunt2008-gz,
  title    = "Banking on the principles: Compliance with Basel Core Principles
              and bank soundness",
  author   = "Demirg{\"u}{\c c}-Kunt, Asl{\i} and Detragiache, Enrica and
              Tressel, Thierry",
  abstract = "This study finds that banks receive more favorable Moody's
              financial strength ratings in countries with better compliance
              with Basel Core Principles related to information provision. The
              results are robust to controlling for broad indexes of
              institutional quality, macroeconomic variables, sovereign
              ratings, and reverse causality. Compliance with other Core
              Principles does not affect ratings robustly. Measuring bank
              soundness through Z-scores yields broadly similar results for
              advanced and emerging markets. Countries aiming to upgrade
              banking regulation and supervision should consider giving
              priority to information provision over other elements of the core
              principles.",
  journal  = "Journal of Financial Intermediation",
  volume   =  17,
  number   =  4,
  pages    = "511--542",
  month    =  oct,
  year     =  2008,
  keywords = "Bank soundness; Regulation and supervision; Basel Core Principles"
}

@ARTICLE{Burkner2017-ga,
  title         = "Advanced Bayesian Multilevel Modeling with the {R} Package
                   brms",
  author        = "B{\"u}rkner, Paul-Christian",
  abstract      = "The brms package allows R users to easily specify a wide
                   range of Bayesian single-level and multilevel models, which
                   are fitted with the probabilistic programming language Stan
                   behind the scenes. Several response distributions are
                   supported, of which all parameters (e.g., location, scale,
                   and shape) can be predicted at the same time thus allowing
                   for distributional regression. Non-linear relationships may
                   be specified using non-linear predictor terms or
                   semi-parametric approaches such as splines or Gaussian
                   processes. To make all of these modeling options possible in
                   a multilevel framework, brms provides an intuitive and
                   powerful formula syntax, which extends the well known
                   formula syntax of lme4. The purpose of the present paper is
                   to introduce this syntax in detail and to demonstrate its
                   usefulness with four examples, each showing other relevant
                   aspects of the syntax.",
  month         =  may,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "1705.11123"
}

@ARTICLE{McKillop2020-om,
  title    = "Cooperative financial institutions: A review of the literature",
  author   = "McKillop, Donal and French, Declan and Quinn, Barry and Sobiech,
              Anna L and Wilson, John O S",
  abstract = "Financial cooperatives play an important role in the financial
              systems of many countries. They act as a safe haven for deposits
              and are major sources of credit for households and small- and
              medium-sized firms. A not-for-profit orientation (in many cases)
              and a focus on maximising benefits to members have ensured the
              enduring popularity and sustainability of financial cooperatives.
              This is particularly evident since the global financial crisis
              when financial cooperatives continued to extend credit to members
              as many profit-orientated commercial banks restricted credit to
              households and firms. The overarching theme of the first part of
              this review is the structural and behavioural characteristics of
              financial cooperatives. In this part we consider, the origin and
              diffusion of financial cooperatives, network arrangements, the
              business model, relationship banking, balancing the interest of
              members, tax treatment and regulatory framework. The second part
              has performance and contribution to the real economy as the
              overarching theme. In this part we consider, efficiency and
              sustainability, mergers, acquisitions and failures, the benefits
              (and challenges) of FinTech and the contribution of financial
              cooperatives to the real economy including during times of
              crisis. The paper concludes with a summary of what we now know
              (and do not know) about financial cooperatives and provides
              suggestions as to where future research may usefully concentrate.",
  journal  = "International Review of Financial Analysis",
  volume   =  71,
  pages    = "101520",
  month    =  oct,
  year     =  2020,
  keywords = "Cooperative financial institutions; Literature review"
}

@ARTICLE{Li2020-ti,
  title    = "Risk spillovers between {FinTech} and traditional financial
              institutions: Evidence from the {U.S}",
  author   = "Li, Jianping and Li, Jingyu and Zhu, Xiaoqian and Yao, Yinhong
              and Casu, Barbara",
  abstract = "In this paper, we propose a novel approach to examine the risk
              spillovers between FinTech firms and traditional financial
              institutions, during a time of fast technological advances. Based
              on the stock returns of U.S. financial and FinTech institutions,
              we estimate pairwise risk spillovers by using the Granger
              causality test across quantiles. We consider the whole
              distribution: the left tail (bearish case), the right tail
              (bullish case) and the center of the distribution and construct
              three types of spillover networks (downside-to-downside,
              upside-to-upside, and center-to-center) and obtain network-based
              spillover indicators. We find that linkages in the network are
              stronger in the bearish case when the risk of spillover is
              higher. FinTech institutions' risk spillover to financial
              institutions positively correlates with financial institutions'
              increase in systemic risk. These results have important policy
              implications, as they underscore the importance of enhancing the
              supervision and regulation of FinTech companies, to maintain
              financial stability.",
  journal  = "International Review of Financial Analysis",
  volume   =  71,
  pages    = "101544",
  month    =  oct,
  year     =  2020,
  keywords = "Financial technology (FinTech); Financial risk; Risk spillover;
              Systemic risk; Financial stability"
}

@TECHREPORT{Gonzalez2020-go,
  title       = "Determinants of {Non-Performing} Loans: Can National Asset
                 Management Companies Help to Alleviate the Problems?",
  author      = "Gonzalez, Brenda Solis",
  abstract    = "Using a novel dataset I examine to what extent the
                 introduction of national Asset Management Companies (AMCs)
                 impacts the effects of bank-specific and macroeconomic
                 determinants of the NPLs ratio for European countries. This
                 study provides evidence on how national AMCs help to alleviate
                 the level of the NPL ratio in countries with high level of
                 non-viable exposures. The results of the dynamic panel data
                 models show that the NPL ratio is lower and less persistent
                 for banks in countries with national AMC since banks are able
                 to clean their balance sheet with lower losses when market
                 prices of NPL are depressed. For countries with national AMC
                 in general the influence of bank-specific factors is lower
                 than during normal conditions. In the case of macroeconomic
                 factors, the results on the size and direction of the impact
                 are mixed. However, these factors remain the key determinants
                 with the unemployment and the lending rate being the leading
                 indicators.",
  publisher   = "Charles University Prague, Faculty of Social Sciences,
                 Institute of Economic Studies",
  number      = "2020/17",
  institution = "Charles University Prague, Faculty of Social Sciences,
                 Institute of Economic Studies",
  month       =  jun,
  year        =  2020,
  keywords    = "Non-performing loans; Asset Management Companies; credit risk;
                 macroeconomic determinants; bank-specific determinants;
                 dynamic panel data"
}

@TECHREPORT{Espinoza2020-qm,
  title       = "Systemic Risk Modeling: How Theory Can Meet Statistics",
  author      = "Espinoza, Raphael A and Segoviano, Miguel A and Yan, Ji",
  abstract    = "We propose a framework to link empirical models of systemic
                 risk to theoretical network/ general equilibrium models used
                 to understand the channels of transmission of systemic risk.
                 The theoretical model allows for systemic risk due to
                 interbank counterparty risk, common asset exposures/fire
                 sales, and a ``Minsky`` cycle of optimism. The empirical model
                 uses stock market and CDS spreads data to estimate a
                 multivariate density of equity returns and to compute the
                 expected equity return for each bank, conditional on a bad
                 macro-outcome. Theses ``cross-sectional'' moments are used to
                 re-calibrate the theoretical model and estimate the importance
                 of the Minsky cycle of optimism in driving systemic risk.",
  publisher   = "International Monetary Fund",
  number      = "2020/054",
  institution = "International Monetary Fund",
  month       =  mar,
  year        =  2020,
  keywords    = "Financial crises;Bank credit;Financial markets;Financial
                 institutions;Macroprudential policies and financial
                 stability;Systemic risk; Minsky effect; CIMDO; Default; WP;
                 interbank; repayment rate; expected shortfall; time t; Minsky"
}

@ARTICLE{Easley2020-du,
  title     = "Microstructure in the Machine Age",
  author    = "Easley, David and L{\'o}pez de Prado, Marcos and O'Hara, Maureen
               and Zhang, Zhibai",
  abstract  = "Abstract. Understanding modern market microstructure phenomena
               requires large amounts of data and advanced mathematical tools.
               We demonstrate how machine learni",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  month     =  jul,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Easley2012-ba,
  title     = "Flow Toxicity and Liquidity in a High-frequency World",
  author    = "Easley, David and L{\'o}pez de Prado, Marcos M and O'Hara,
               Maureen",
  abstract  = "Abstract. Order flow is toxic when it adversely selects market
               makers, who may be unaware they are providing liquidity at a
               loss. We present a new procedure to",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford Academic",
  volume    =  25,
  number    =  5,
  pages     = "1457--1493",
  month     =  mar,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Henrique2019-qe,
  title     = "Literature review: Machine learning techniques applied to
               financial market prediction",
  author    = "Henrique, Bruno Miranda and Sobreiro, Vinicius Amorim and
               Kimura, Herbert",
  journal   = "Expert Syst. Appl.",
  publisher = "Elsevier BV",
  volume    =  124,
  pages     = "226--251",
  month     =  jun,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Lipton2020-bu,
  title         = "A closed-form solution for optimal mean-reverting trading
                   strategies",
  author        = "Lipton, Alexander and de Prado, Marcos Lopez",
  abstract      = "When prices reflect all available information, they
                   oscillate around an equilibrium level. This oscillation is
                   the result of the temporary market impact caused by waves of
                   buyers and sellers. This price behavior can be approximated
                   through an Ornstein-Uhlenbeck (O-U) process. Market makers
                   provide liquidity in an attempt to monetize this
                   oscillation. They enter a long position when a security is
                   priced below its estimated equilibrium level, and they enter
                   a short position when a security is priced above its
                   estimated equilibrium level. They hold that position until
                   one of three outcomes occur: (1) they achieve the targeted
                   profit; (2) they experience a maximum tolerated loss; (3)
                   the position is held beyond a maximum tolerated horizon. All
                   market makers are confronted with the problem of defining
                   profit-taking and stop-out levels. More generally, all
                   execution traders acting on behalf of a client must
                   determine at what levels an order must be fulfilled. Those
                   optimal levels can be determined by maximizing the trader's
                   Sharpe ratio in the context of O-U processes via Monte Carlo
                   experiments. This paper develops an analytical framework and
                   derives those optimal levels by using the method of heat
                   potentials.",
  month         =  mar,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "q-fin.TR",
  eprint        = "2003.10502"
}

@ARTICLE{Lipton2020-sm,
  title     = "A closed-form solution for optimal mean-reverting trading
               strategies",
  author    = "Lipton, Alex and L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Lipton2020-va,
  title     = "Three quant lessons from {COVID-19}",
  author    = "Lipton, Alex and L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2020-vs,
  title     = "Interpretable machine learning: Shapley values (seminar slides)",
  author    = "L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2020-ug,
  title     = "Three machine learning solutions to the bias-variance dilemma
               (seminar slides)",
  author    = "L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2019-tn,
  title     = "Detection of false investment strategies using unsupervised
               learning methods",
  author    = "L{\'o}pez de Prado, Marcos and Lewis, Michael J",
  journal   = "Quant. Finance",
  publisher = "Informa UK Limited",
  volume    =  19,
  number    =  9,
  pages     = "1555--1565",
  month     =  sep,
  year      =  2019,
  language  = "en"
}

@INCOLLECTION{Bailey2020-xv,
  title     = "Do financial gurus produce reliable forecasts?",
  booktitle = "Springer Proceedings in Mathematics \& Statistics",
  author    = "Bailey, David H and Borwein, Jonathan M and Salehipour, Amir and
               L{\'o}pez de Prado, Marcos",
  publisher = "Springer International Publishing",
  pages     = "255--274",
  series    = "Springer proceedings in mathematics \& statistics",
  year      =  2020,
  address   = "Cham"
}

@ARTICLE{Lopez_de_Prado2019-pz,
  title     = "Optimal risk budgeting under a finite investment horizon",
  author    = "L{\'o}pez de Prado, Marcos and Vince, Ralph and Zhu, Qiji Jim",
  abstract  = "The Growth-Optimal Portfolio (GOP) theory determines the path of
               bet sizes that maximize long-term wealth. This multi-horizon
               goal makes it more appealing among practitioners than myopic
               approaches, like Markowitz's mean-variance or risk parity. The
               GOP literature typically considers risk-neutral investors with
               an infinite investment horizon. In this paper, we compute the
               optimal bet sizes in the more realistic setting of risk-averse
               investors with finite investment horizons. We find that, under
               this more realistic setting, the optimal bet sizes are
               considerably smaller than previously suggested by the GOP
               literature. We also develop quantitative methods for determining
               the risk-adjusted growth allocations (or risk budgeting) for a
               given finite investment horizon.",
  journal   = "Risks",
  publisher = "MDPI AG",
  volume    =  7,
  number    =  3,
  pages     = "86",
  month     =  aug,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Lipton2020-ub,
  title     = "Mitigation strategies for {COVID-19}: Lessons from the {K-SEIR}
               model",
  author    = "Lipton, Alex and L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2020-eu,
  title     = "Clustered feature importance (presentation slides)",
  author    = "L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Lipton2020-ei,
  title     = "Three quant lessons from {COVID-19} (presentation slides)",
  author    = "Lipton, Alex and L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2020-wp,
  title     = "Codependence (presentation slides)",
  author    = "L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2020-eb,
  title     = "Clustering (presentation slides)",
  author    = "L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2020-ym,
  title     = "Overfitting: Causes and solutions (seminar slides)",
  author    = "L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Lipton2020-ns,
  title     = "Exit strategies for {COVID-19}: An application of the {K-SEIR}
               model (presentation slides)",
  author    = "Lipton, Alex and L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2020,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2018-hi,
  title     = "A practical solution to the multiple-testing crisis in financial
               research",
  author    = "Lopez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2018,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2018-vw,
  title     = "The myth and reality of financial machine learning (presentation
               slides)",
  author    = "Lopez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2018,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2018-xs,
  title     = "Financial machine learning in 10 minutes (presentation slides)",
  author    = "Lopez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2018,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2018-xi,
  title     = "Advances in financial machine learning: Lecture 1/10",
  author    = "L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2018,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2018-gw,
  title     = "Advances in financial machine learning: Lecture 8/10",
  author    = "L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2018,
  language  = "en"
}

@ARTICLE{Lopez_de_Prado2018-jz,
  title     = "Advances in financial machine learning: Lecture 2/10",
  author    = "L{\'o}pez de Prado, Marcos",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2018,
  language  = "en"
}

@ARTICLE{Apley2020-qg,
  title     = "Visualizing the effects of predictor variables in black box
               supervised learning models",
  author    = "Apley, Daniel W and Zhu, Jingyu",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Wiley",
  volume    =  82,
  number    =  4,
  pages     = "1059--1086",
  month     =  sep,
  year      =  2020,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

@INCOLLECTION{Lopez_de_Prado2020-sr,
  title     = "Machine Learning for Asset Managers",
  booktitle = "Elements in Quantitative Finance",
  author    = "L{\'o}pez de Prado, Marcos",
  abstract  = "Cambridge Core - Finance and Accountancy - Machine Learning for
               Asset Managers",
  publisher = "Cambridge University Press",
  month     =  apr,
  year      =  2020
}

@BOOK{Lopez_de_Prado2018-pr,
  title     = "Advances in Financial Machine Learning",
  author    = "L{\'o}pez de Prado, Marcos",
  abstract  = "Machine learning (ML) is changing virtually every aspect of our
               lives. Today ML algorithms accomplish tasks that until recently
               only expert humans could perform. As it relates to finance, this
               is the most exciting time to adopt a disruptive technology that
               will transform how everyone invests for generations. Readers
               will learn how to structure Big data in a way that is amenable
               to ML algorithms; how to conduct research with ML algorithms on
               that data; how to use supercomputing methods; how to backtest
               your discoveries while avoiding false positives. The book
               addresses real-life problems faced by practitioners on a daily
               basis, and explains scientifically sound solutions using math,
               supported by code and examples. Readers become active users who
               can test the proposed solutions in their particular setting.
               Written by a recognized expert and portfolio manager, this book
               will equip investment professionals with the groundbreaking
               tools needed to succeed in modern finance.",
  publisher = "John Wiley \& Sons",
  month     =  feb,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Berg2020-iy,
  title     = "On the rise of {FinTechs}: Credit scoring using digital
               footprints",
  author    = "Berg, Tobias and Burg, Valentin and Gombovi{\'c}, Ana and Puri,
               Manju",
  abstract  = "Abstract We analyze the information content of a digital
               footprint---that is, information that users leave online simply
               by accessing or registering on a Web site---for predicting
               consumer default. We show that even simple, easily accessible
               variables from a digital footprint match the information content
               of credit bureau scores. A digital footprint complements rather
               than substitutes for credit bureau information and affects
               access to credit and reduces default rates. We discuss the
               implications for financial intermediaries' business models,
               access to credit for the unbanked, and the behavior of
               consumers, firms, and regulators in the digital sphere. (JEL
               G20, G21, G29)",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press (OUP)",
  volume    =  33,
  number    =  7,
  pages     = "2845--2897",
  month     =  jul,
  year      =  2020,
  copyright = "https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model",
  language  = "en"
}

@ARTICLE{Wasserstein2019-jr,
  title     = "Moving to a World Beyond ``p < 0.05''",
  author    = "Wasserstein, Ronald L and Schirm, Allen L and Lazar, Nicole A",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  73,
  number    = "sup1",
  pages     = "1--19",
  month     =  mar,
  year      =  2019
}

@ARTICLE{Strumbelj2014-de,
  title     = "Explaining prediction models and individual predictions with
               feature contributions",
  author    = "{\v S}trumbelj, Erik and Kononenko, Igor",
  journal   = "Knowl. Inf. Syst.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  41,
  number    =  3,
  pages     = "647--665",
  month     =  dec,
  year      =  2014,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@UNPUBLISHED{Lopez_de_Prado2019-hp,
  title    = "Beyond Econometrics: A Roadmap Towards Financial Machine Learning",
  author   = "Lopez de Prado, Marcos",
  abstract = "One of the most exciting recent developments in financial
              research is the availability of new administrative, private
              sector and micro-level datasets that did not exist a few years
              ago. The unstructured nature of many of these observations, along
              with the complexity of the phenomena they measure, means that
              many of these datasets are beyond the grasp of econometric
              analysis. Machine learning (ML) techniques offer the numerical
              power and functional flexibility needed to identify complex
              patterns in a high-dimensional space. However, ML is often
              perceived as a black box, in contrast with the transparency of
              econometric approaches. This article demonstrates that each
              analytical step of the econometric process has a homologous step
              in ML analyses. By clearly stating this correspondence, our goal
              is to facilitate and reconcile the adoption of ML techniques
              among econometricians.",
  journal  = "papers.ssrn.com ‚Ä∫ sol3 ‚Ä∫ paperspapers.ssrn.com ‚Ä∫ sol3 ‚Ä∫ papers",
  month    =  sep,
  year     =  2019,
  keywords = "machine learning, artificial intelligence, econometrics,
              financial economics"
}

@INCOLLECTION{Thorp2011-ih,
  title     = "Understanding the Kelly Criterion",
  booktitle = "The Kelly Capital Growth Investment Criterion",
  author    = "Thorp, Edward O",
  abstract  = "In January 1961, I spoke at the annual meeting of the American
               Mathematical Society on ?Fortune's Formula: The Game of
               Blackjack?. This announced the discovery of favorable card
               counting systems for blackjack. My 1962 book Beat the Dealer
               explained the detailed theory and practice. The ?optimal? way to
               bet in favorable situations was an important feature. In Beat
               the Dealer, I called this, naturally enough, ?The Kelly gambling
               system?, since I learned about it from the 1956 paper by John L.
               Kelly (Claude Shannon, who refereed the Kelly paper, brought it
               to my attention in November of 1960). I have continued to use it
               successfully in gambling and in investing. Since 1966, I've
               called it ?the Kelly Criterion?. The rising tide of theory about
               and practical use of the Kelly Criterion by several leading
               money managers received further impetus from William
               Poundstone's readable book about the Kelly Criterion, Fortune's
               Formula. (As this title came from that of my 1961 talk, I was
               asked to approve the use of the title) . At a value investor's
               conference held in Los Angeles in May, 2007, my son reported
               that ?everyone? said they were using the Kelly Criterion?",
  publisher = "WORLD SCIENTIFIC",
  volume    =  3,
  pages     = "509--523",
  series    = "World Scientific Handbook in Financial Economics Series",
  month     =  feb,
  year      =  2011
}

@INCOLLECTION{Thorp1975-vi,
  title     = "Portfolio choice and the Kelly criterion",
  booktitle = "Stochastic optimization models in finance",
  author    = "Thorp, Edward O",
  publisher = "Elsevier",
  pages     = "599--619",
  year      =  1975
}

@ARTICLE{MacLean2010-eb,
  title   = "Good and bad properties of the Kelly criterion",
  author  = "MacLean, Leonard C and Thorp, Edward O and Ziemba, William T",
  journal = "Risk",
  volume  =  20,
  number  =  2,
  pages   = "1",
  year    =  2010
}

@ARTICLE{Rotando1992-op,
  title     = "The Kelly Criterion and the Stock Market",
  author    = "Rotando, Louis M and Thorp, Edward O",
  journal   = "Am. Math. Mon.",
  publisher = "Taylor \& Francis",
  volume    =  99,
  number    =  10,
  pages     = "922--931",
  month     =  dec,
  year      =  1992
}

@INCOLLECTION{Thorp2011-gd,
  title     = "The Kelly Criterion in Blackjack Sports Betting, and the Stock
               Market",
  booktitle = "The Kelly Capital Growth Investment Criterion",
  author    = "Thorp, Edward O",
  abstract  = "The central problem for gamblers is to find positive expectation
               bets. But the gambler also needs to know how to manage his
               money, i.e., how much to bet. In the stock market (more
               inclusively, the securities markets) the problem is similar but
               more complex. The gambler, who is now an ?investor?, looks for
               ?excess risk adjusted return?. In both these settings, we
               explore the use of the Kelly criterion, which is to maximize the
               expected value of the logarithm of wealth (?maximize expected
               logarithmic utility?). The criterion is known to economists and
               financial theorists by names such as the ?geometric mean
               maximizing portfolio strategy?, maximizing logarithmic utility,
               the growth-optimal strategy, the capital growth criterion, etc.
               The author initiated the practical application of the Kelly
               criterion by using it for card counting in blackjack. We will
               present some useful formulas and methods to answer various
               natural questions about it that arise in blackjack and other
               gambling games. Then we illustrate its recent use in a
               successful casino sports betting system. Finally, we discuss its
               application to the securities markets where it has helped the
               author to make a thirty year total of 80 billion dollars worth
               of ?bets?.",
  publisher = "WORLD SCIENTIFIC",
  volume    =  3,
  pages     = "789--832",
  series    = "World Scientific Handbook in Financial Economics Series",
  month     =  feb,
  year      =  2011
}

@ARTICLE{Browne1996-io,
  title     = "Portfolio Choice and the Bayesian Kelly Criterion",
  author    = "Browne, Sid and Whitt, Ward",
  abstract  = "[We derive optimal gambling and investment policies for cases in
               which the underlying stochastic process has parameter values
               that are unobserved random variables. For the objective of
               maximizing logarithmic utility when the underlying stochastic
               process is a simple random walk in a random environment, we show
               that a state-dependent control is optimal, which is a
               generalization of the celebrated Kelly strategy: the optimal
               strategy is to bet a fraction of current wealth equal to a
               linear function of the posterior mean increment. To approximate
               more general stochastic processes, we consider a continuous-time
               analog involving Brownian motion. To analyze the continuous-time
               problem, we study the diffusion limit of random walks in a
               random environment. We prove that they converge weakly to a
               Kiefer process, or tied-down Brownian sheet. We then find
               conditions under which the discrete-time process converges to a
               diffusion, and analyze the resulting process. We analyze in
               detail the case of the natural conjugate prior, where the
               success probability has a beta distribution, and show that the
               resulting limit diffusion can be viewed as a rescaled Brownian
               motion. These results allow explicit computation of the optimal
               control policies for the continuous-time gambling and investment
               problems without resorting to continuous-time stochastic-control
               procedures. Moreover they also allow an explicit quantitative
               evaluation of the financial value of randomness, the financial
               gain of perfect information and the financial cost of learning
               in the Bayesian problem.]",
  journal   = "Adv. Appl. Probab.",
  publisher = "Applied Probability Trust",
  volume    =  28,
  number    =  4,
  pages     = "1145--1176",
  year      =  1996
}

@ARTICLE{noauthor_undated-dx,
  title    = "Advances in Financial Machine Learning: Numerai's Tournament
              (seminar slides)",
  author   = "Lopez de Prado, Marcos",
  abstract = "Machine learning (ML) is changing virtually every aspect of our
              lives. Today ML algorithms accomplish tasks that until recently
              only expert humans could perform. As it relates to finance, this
              is the most exciting time to adopt a disruptive technology that
              will transform how everyone invests for generations. In this
              course, we discuss scientifically sound ML tools that have been
              successfully applied to the management of large pools of
              funds.This material is part of Cornell University's ORIE 5256
              graduate course at the School of Engineering.",
  month    =  nov,
  year     =  2019,
  keywords = "Machine Learning, Artificial Intelligence, Asset Management"
}

@ARTICLE{Westphal1998-fr,
  title     = "The symbolic management of stockholders: Corporate governance
               reforms and shareholder reactions",
  author    = "Westphal, James D and Zajac, Edward J",
  journal   = "Adm. Sci. Q.",
  publisher = "JSTOR",
  pages     = "127--153",
  year      =  1998
}

@TECHREPORT{Chen2019-ns,
  title       = "The Limits of {P-Hacking}: A Thought Experiment",
  author      = "Chen, Andrew Y",
  abstract    = "Suppose that asset pricing factors are just p-hacked noise.
                 How much p-hacking is required to produce the 300 factors
                 documented by academics? I show that, if 10,000 academics
                 generate 1 factor every minute, it takes 15 million years of
                 p-hacking. This absurd conclusion comes from applying the
                 p-hacking theory to published data. To fit the fat right tail
                 of published t-stats, the p-hacking theory requires that the
                 probability of publishing t-stats < 6.0 is infinitesimal. Thus
                 it takes a ridiculous amount of p-hacking to publish a single
                 t-stat. These results show that p-hacking alone cannot explain
                 the factor zoo.",
  number      = " Finance and Economics Discussion Series 2019-016.",
  institution = "Washington: Board of Governors of the Federal Reserve System",
  month       =  mar,
  year        =  2019,
  keywords    = "Stock return anomalies, Multiple testing, p-hacking,
                 Publication bias"
}

@ARTICLE{Phillips2015-wv,
  title   = "{TESTING} {FOR} {MULTIPLE} {BUBBLES}: {HISTORICAL} {EPISODES} {OF}
             {EXUBERANCE} {AND} {COLLAPSE} {IN} {THE} {S\&P} 500",
  author  = "Phillips, Peter C B and Shi, Shuping and Yu, Jun",
  journal = "Int. Econ. Rev.",
  volume  =  56,
  number  =  4,
  pages   = "1043--1078",
  year    =  2015
}

@MISC{Pavlidis2017-sy,
  title   = "{TESTING} {FOR} {SPECULATIVE} {BUBBLES} {USING} {SPOT} {AND}
             {FORWARD} {PRICES}",
  author  = "Pavlidis, Efthymios G and Paya, Ivan and Peel, David A",
  journal = "International Economic Review",
  volume  =  58,
  number  =  4,
  pages   = "1191--1226",
  year    =  2017
}

@MISC{Phillips2015-dp,
  title   = "{TESTING} {FOR} {MULTIPLE} {BUBBLES}: {LIMIT} {THEORY} {OF}
             {REAL-TIME} {DETECTORS}",
  author  = "Phillips, Peter C B and Shi, Shuping and Yu, Jun",
  journal = "International Economic Review",
  volume  =  56,
  number  =  4,
  pages   = "1079--1134",
  year    =  2015
}

@ARTICLE{Alhamzawi2012-nf,
  title     = "Bayesian adaptive Lasso quantile regression",
  author    = "Alhamzawi, Rahim and Yu, Keming and Benoit, Dries F",
  abstract  = "Recently, variable selection by penalized likelihood has
               attracted much research interest. In this paper, we propose
               adaptive Lasso quantile regression (BALQR) from a Bayesian
               perspective. The method extends the Bayesian Lasso quantile
               regression by allowing different penalization parameters for
               different regression coefficients. Inverse gamma prior
               distributions are placed on the penalty parameters. We treat the
               hyperparameters of the inverse gamma prior as unknowns and
               estimate them along with the other parameters. A Gibbs sampler
               is developed to simulate the parameters from the posterior
               distributions. Through simulation studies and analysis of a
               prostate cancer dataset, we compare the performance of the BALQR
               method proposed with six existing Bayesian and non-Bayesian
               methods. The simulation studies and the prostate cancer data
               analysis indicate that the BALQR method performs well in
               comparison to the other approaches.",
  journal   = "Stat. Modelling",
  publisher = "SAGE Publications India",
  volume    =  12,
  number    =  3,
  pages     = "279--297",
  month     =  jun,
  year      =  2012
}

@ARTICLE{Foroni2013-zl,
  title     = "A survey of econometric methods for mixed-frequency data",
  author    = "Foroni, Claudia and Marcellino, Massimiliano Giuseppe",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2013,
  language  = "en"
}

@ARTICLE{Li2010-as,
  title     = "Bayesian regularized quantile regression",
  author    = "Li, Qing and Xi, Ruibin and Lin, Nan",
  abstract  = "Project Euclid - mathematics and statistics online",
  journal   = "Bayesian Anal.",
  publisher = "International Society for Bayesian Analysis",
  volume    =  5,
  number    =  3,
  pages     = "533--556",
  month     =  sep,
  year      =  2010,
  keywords  = "Quantile regression; Regularization; Gibbs sampler; Bayesian
               analysis; Lasso; Elastic net; Group lasso",
  language  = "en"
}

@ARTICLE{Fan2020-mb,
  title    = "{Factor-Adjusted} Regularized Model Selection",
  author   = "Fan, Jianqing and Ke, Yuan and Wang, Kaizheng",
  abstract = "This paper studies model selection consistency for high
              dimensional sparse regression when data exhibits both
              cross-sectional and serial dependency. Most commonly-used model
              selection methods fail to consistently recover the true model
              when the covariates are highly correlated. Motivated by
              econometric and financial studies, we consider the case where
              covariate dependence can be reduced through the factor model, and
              propose a consistency strategy named Factor-Adjusted Regularized
              Model Selection (FarmSelect). By learning the latent factors and
              idiosyncratic components and using both of them as predictors,
              FarmSelect transforms the problem from model selection with
              highly correlated covariates to that with weakly correlated ones
              via lifting. Model selection consistency, as well as optimal
              rates of convergence, are obtained under mild conditions.
              Numerical studies demonstrate the nice finite sample performance
              in terms of both model selection and out-of-sample prediction.
              Moreover, our method is flexible in the sense that it pays no
              price for weakly correlated and uncorrelated cases. Our method is
              applicable to a wide range of high dimensional sparse regression
              problems. An R-package FarmSelect is also provided for
              implementation.",
  journal  = "J. Econom.",
  volume   =  216,
  number   =  1,
  pages    = "71--85",
  month    =  may,
  year     =  2020,
  keywords = "C52; C58; Correlated covariates; Factor model; Model selection
              consistency; Regularized M-estimator; Time series",
  language = "en"
}

@ARTICLE{Mogliani2020-qu,
  title    = "Bayesian {MIDAS} penalized regressions: Estimation, selection,
              and prediction",
  author   = "Mogliani, Matteo and Simoni, Anna",
  abstract = "We propose a new approach to mixed-frequency regressions in a
              high-dimensional environment that resorts to Group Lasso
              penalization and Bayesian estimation and inference. In
              particular, to improve the prediction properties of the model and
              its sparse recovery ability, we consider a Group Lasso with a
              spike-and-slab prior. Penalty hyper-parameters governing the
              model shrinkage are automatically tuned via an adaptive MCMC
              algorithm. We establish good frequentist asymptotic properties of
              the posterior prediction error, we recover the optimal posterior
              contraction rate, and we show optimality of the posterior
              predictive density. Simulations show that the proposed models
              have good selection and forecasting performance in small samples,
              even when the design matrix presents cross-correlation. When
              applied to forecasting U.S. GDP, our penalized regressions can
              outperform many strong competitors. Results suggest that
              financial variables may have some, although very limited,
              short-term predictive content.",
  journal  = "J. Econom.",
  month    =  aug,
  year     =  2020,
  keywords = "Bayesian MIDAS regressions; Penalized regressions; Predictive
              distribution; Forecasting; Posterior contraction"
}

@ARTICLE{Fan2019-zs,
  title    = "{FarmTest}: Factor-adjusted robust multiple testing with
              approximate false discovery control",
  author   = "Fan, Jianqing and Ke, Yuan and Sun, Qiang and Zhou, Wen-Xin",
  abstract = "Large-scale multiple testing with correlated and heavy-tailed
              data arises in a wide range of research areas from genomics,
              medical imaging to finance. Conventional methods for estimating
              the false discovery proportion (FDP) often ignore the effect of
              heavy-tailedness and the dependence structure among test
              statistics, and thus may lead to inefficient or even inconsistent
              estimation. Also, the commonly imposed joint normality assumption
              is arguably too stringent for many applications. To address these
              challenges, in this paper we propose a Factor-Adjusted Robust
              Multiple Testing (FarmTest) procedure for large-scale
              simultaneous inference with control of the false discovery
              proportion. We demonstrate that robust factor adjustments are
              extremely important in both controlling the FDP and improving the
              power. We identify general conditions under which the proposed
              method produces consistent estimate of the FDP. As a byproduct
              that is of independent interest, we establish an exponential-type
              deviation inequality for a robust U-type covariance estimator
              under the spectral norm. Extensive numerical experiments
              demonstrate the advantage of the proposed method over several
              state-of-the-art methods especially when the data are generated
              from heavy-tailed distributions. The proposed procedures are
              implemented in the R-package FarmTest.",
  journal  = "J. Am. Stat. Assoc.",
  volume   =  114,
  number   =  528,
  pages    = "1880--1893",
  month    =  mar,
  year     =  2019,
  keywords = "Factor adjustment; False discovery proportion; Huber loss;
              Large-scale multiple testing; Robustness",
  language = "en"
}

@ARTICLE{Tibshirani1996-zu,
  title     = "Regression Shrinkage and Selection Via the Lasso",
  author    = "Tibshirani, Robert",
  abstract  = "SUMMARY We propose a new method for estimation in linear models.
               The ?lasso? minimizes the residual sum of squares subject to the
               sum of the absolute value of the coefficients being less than a
               constant. Because of the nature of this constraint it tends to
               produce some coefficients that are exactly 0 and hence gives
               interpretable models. Our simulation studies suggest that the
               lasso enjoys some of the favourable properties of both subset
               selection and ridge regression. It produces interpretable models
               like subset selection and exhibits the stability of ridge
               regression. There is also an interesting relationship with
               recent work in adaptive function estimation by Donoho and
               Johnstone. The lasso idea is quite general and can be applied in
               a variety of statistical models: extensions to generalized
               regression models and tree-based models are briefly described.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "John Wiley \& Sons, Ltd",
  volume    =  58,
  number    =  1,
  pages     = "267--288",
  month     =  jan,
  year      =  1996,
  keywords  = "quadratic programming; regression; shrinkage; subset selection"
}

@ARTICLE{Tibshirani2011-ka,
  title    = "Regression shrinkage and selection via the lasso: a retrospective",
  author   = "Tibshirani, Robert",
  abstract = "Summary.? In the paper I give a brief review of the basic idea
              and some history and then discuss some developments since the
              original paper on regression shrinkage and selection via the
              lasso.",
  journal  = "J. R. Stat. Soc. Series B Stat. Methodol.",
  volume   =  73,
  number   =  3,
  pages    = "273--282",
  month    =  jun,
  year     =  2011,
  keywords = "l1-penalty; Penalization; Regularization"
}

@ARTICLE{Wang2007-zo,
  title    = "Regression coefficient and autoregressive order shrinkage and
              selection via the lasso",
  author   = "Wang, Hansheng and Li, Guodong and Tsai, Chih-Ling",
  abstract = "Summary.? The least absolute shrinkage and selection operator
              (?lasso?) has been widely used in regression shrinkage and
              selection. We extend its application to the regression model with
              autoregressive errors. Two types of lasso estimators are
              carefully studied. The first is similar to the traditional lasso
              estimator with only two tuning parameters (one for regression
              coefficients and the other for autoregression coefficients).
              These tuning parameters can be easily calculated via a
              data-driven method, but the resulting lasso estimator may not be
              fully efficient. To overcome this limitation, we propose a second
              lasso estimator which uses different tuning parameters for each
              coefficient. We show that this modified lasso can produce the
              estimator as efficiently as the oracle. Moreover, we propose an
              algorithm for tuning parameter estimates to obtain the modified
              lasso estimator. Simulation studies demonstrate that the modified
              estimator is superior to the traditional estimator. One empirical
              example is also presented to illustrate the usefulness of lasso
              estimators. The extension of the lasso to the autoregression with
              exogenous variables model is briefly discussed.",
  journal  = "J. R. Stat. Soc. Series B Stat. Methodol.",
  volume   =  69,
  number   =  1,
  pages    = "63--78",
  month    =  feb,
  year     =  2007,
  keywords = "Autoregression with exogenous variables; Lasso; Oracle estimator;
              Regression model with autoregressive errors"
}

@ARTICLE{Tsionas2003-py,
  title     = "Bayesian quantile inference",
  author    = "Tsionas, Efthymios G",
  abstract  = "The paper proposes a Bayesian interpretation of quantile
               regression that is shown to be equivalent to scale mixtures of
               normals leading to a skewed Laplace distribution. This
               representation of the model facilitates Bayesian analysis by
               means of Gibbs sampling with data augmentation, and nests
               regression in the L1 norm as a special case. The new methods are
               applied to an analysis of the patents - R\&D relationship for
               U.S. firms and unit root inference for the dollar-deutschemark
               exchange rate.",
  journal   = "J. Stat. Comput. Simul.",
  publisher = "Taylor \& Francis",
  volume    =  73,
  number    =  9,
  pages     = "659--674",
  month     =  sep,
  year      =  2003
}

@ARTICLE{Benoit2017-im,
  title     = "{bayesQR}: A Bayesian approach to quantile regression",
  author    = "Benoit, Dries F and Van den Poel, Dirk and {Others}",
  journal   = "J. Stat. Softw.",
  publisher = "Foundation for Open Access Statistics",
  volume    =  76,
  number    =  7,
  pages     = "1--32",
  year      =  2017
}

@ARTICLE{Geraci2014-ob,
  title     = "Linear quantile mixed models",
  author    = "Geraci, Marco and Bottai, Matteo",
  journal   = "Stat. Comput.",
  publisher = "Springer",
  volume    =  24,
  number    =  3,
  pages     = "461--479",
  year      =  2014
}

@ARTICLE{Xu2020-lg,
  title    = "Mixed data sampling expectile regression with applications to
              measuring financial risk",
  author   = "Xu, Qifa and Chen, Lu and Jiang, Cuixia and Yu, Keming",
  abstract = "To avoid information loss or measurement error in traditional
              methods dealing with mixed frequency data, we develop a novel
              mixed data sampling expectile regression (MIDAS-ER) model to
              measure financial risk. We construct the MIDAS-ER model by
              introducing a MIDAS structure into expectile regressions. This
              enables us to perform an expectile regression on raw mixed
              frequency data directly. We apply the proposed MIDAS-ER model to
              estimate two popular financial risk measures, namely, Value at
              Risk and Expected Shortfall, with both simulated data and four
              stock indices, and compare the model's performance with those of
              several popular models. The outstanding performance of our model
              demonstrates that high-frequency information helps to improve the
              accuracy of risk measurement. In addition, the numerical results
              also imply that our model can be a significant tool for
              risk-averse investors to control risk losses and for financial
              institutions to implement robust risk management.",
  journal  = "Econ. Model.",
  volume   =  91,
  pages    = "469--486",
  month    =  sep,
  year     =  2020,
  keywords = "Financial risk measure; Mixed frequency data; Value at risk
              (VaR); Expected shortfall (ES); MIDAS expectile regression"
}

@ARTICLE{Hale2019-sr,
  title    = "Monitoring banking system connectedness with big data",
  author   = "Hale, Galina and Lopez, Jose A",
  abstract = "In this paper, we propose a procedure that generates measures of
              connectedness between individual firms and for the system as a
              whole based on information observed only at the firm level; i.e.,
              no explicit linkages are observed. We apply our procedure to
              large U.S. bank holding companies. We show how bank outcome
              variables of interest can be decomposed, including with
              mixed-frequency models, for network analysis to measure
              connectedness across firms. Network analysis of these
              decompositions produces measures that could be of use in
              financial stability monitoring as well as the analysis of
              individual firms' linkages.",
  journal  = "J. Econom.",
  volume   =  212,
  number   =  1,
  pages    = "203--220",
  month    =  sep,
  year     =  2019,
  keywords = "Financial stability; Bank supervision; Network centrality;
              Systemic connectedness"
}

@ARTICLE{Ghysels2016-bp,
  title    = "Mixed Frequency Data Sampling Regression Models: The {R} Package
              midasr",
  author   = "Ghysels, Eric and Kvedaras, Virmantas and Zemlys, Vaidotas",
  abstract = "When modeling economic relationships it is increasingly common to
              encounter data sampled at different frequencies. We introduce the
              R package midasr which enables estimating regression models with
              variables sampled at different frequencies within a MIDAS
              regression framework put forward in work by Ghysels, Santa-Clara,
              and Valkanov (2002). In this article we define a general
              autoregressive MIDAS regression model with multiple variables of
              different frequencies and show how it can be specified using the
              familiar R formula interface and estimated using various
              optimization methods chosen by the researcher. We discuss how to
              check the validity of the estimated model both in terms of
              numerical convergence and statistical adequacy of a chosen
              regression specification, how to perform model selection based on
              a information criterion, how to assess forecasting accuracy of
              the MIDAS regression model and how to obtain a forecast
              aggregation of different MIDAS regression models. We illustrate
              the capabilities of the package with a simulated MIDAS regression
              model and give two empirical examples of application of MIDAS
              regression.",
  journal  = "Journal of Statistical Software, Articles",
  volume   =  72,
  number   =  4,
  pages    = "1--35",
  year     =  2016,
  keywords = "MIDAS; specification test"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Polson2014-dw,
  title     = "The Bayesian bridge",
  author    = "Polson, Nicholas G and Scott, James G and Windle, Jesse",
  abstract  = "We propose the Bayesian bridge estimator for regularized
               regression and classification. Two key mixture representations
               for the Bayesian bridge model are developed: a scale mixture of
               normal distributions with respect to an ¬ª-stable random
               variable; a mixture of Bartlett--Fejer kernels (or triangle
               densities) with respect to a two-component mixture of gamma
               random variables. Both lead to Markov chain Monte Carlo methods
               for posterior simulation, and these methods turn out to have
               complementary domains of maximum efficiency. The first
               representation is a well-known result due to West and is the
               better choice for collinear design matrices. The second
               representation is new and is more efficient for orthogonal
               problems, largely because it avoids the need to deal with
               exponentially tilted stable random variables. It also provides
               insight into the multimodality of the joint posterior
               distribution, which is a feature of the bridge model that is
               notably absent under ridge or lasso-type priors. We prove a
               theorem that extends this representation to a wider class of
               densities representable as scale mixtures of beta distributions,
               and we provide an explicit inversion formula for the mixing
               distribution. The connections with slice sampling and scale
               mixtures of normal distributions are explored. On the practical
               side, we find that the Bayesian bridge model outperforms its
               classical cousin in estimation and prediction across a variety
               of data sets, both simulated and real. We also show that the
               Markov chain Monte Carlo algorithm for fitting the bridge model
               exhibits excellent mixing properties, particularly for the
               global scale parameter. This makes for a favourable contrast
               with analogous Markov chain Monte Carlo algorithms for other
               sparse Bayesian models. All methods described in this paper are
               implemented in the R package BayesBridge. An extensive set of
               simulation results is provided in two on-line supplemental
               files.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "[Royal Statistical Society, Wiley]",
  volume    =  76,
  number    =  4,
  pages     = "713--733",
  year      =  2014
}

@TECHREPORT{Barth2013-hz,
  title       = "Measure It , Improve It",
  author      = "Barth, James R and Jr, Gerard Caprio and Levine, Ross",
  institution = "Milken Institute",
  year        =  2013
}

@ARTICLE{Gelman2019-xn,
  title     = "Why {High-Order} Polynomials Should Not Be Used in Regression
               Discontinuity Designs",
  author    = "Gelman, Andrew and Imbens, Guido",
  abstract  = "It is common in regression discontinuity analysis to control for
               third, fourth, or higher-degree polynomials of the forcing
               variable. There appears to be a perception that such methods are
               theoretically justified, even though they can lead to evidently
               nonsensical results. We argue that controlling for global
               high-order polynomials in regression discontinuity analysis is a
               flawed approach with three major problems: it leads to noisy
               estimates, sensitivity to the degree of the polynomial, and poor
               coverage of confidence intervals. We recommend researchers
               instead use estimators based on local linear or quadratic
               polynomials or other smooth functions.",
  journal   = "J. Bus. Econ. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  37,
  number    =  3,
  pages     = "447--456",
  month     =  jul,
  year      =  2019
}

@ARTICLE{Pozzolo2015-pm,
  title     = "Calibrating Probability with Undersampling for Unbalanced
               Classification",
  author    = "Pozzolo, Andrea Dal and Caelen, Olivier and Johnson, Reid A and
               Bontempi, Gianluca",
  abstract  = "PDF | Undersampling is a popular technique for unbalanced
               datasets to reduce the skew in class distributions. However, it
               is well-known that... | Find, read and cite all the research you
               need on ResearchGate",
  publisher = "unknown",
  month     =  dec,
  year      =  2015
}

@ARTICLE{Calin-Jageman2019-vx,
  title     = "The New Statistics for Better Science: Ask How Much, How
               Uncertain, and What Else is Known",
  author    = "Calin-Jageman, Robert J and Cumming, Geoff",
  abstract  = "The ``New Statistics'' emphasizes effect sizes, confidence
               intervals, meta-analysis, and the use of Open Science practices.
               We present 3 specific ways in which a New Statistics approach
               can help improve scientific practice: by reducing
               over-confidence in small samples, by reducing confirmation bias,
               and by fostering more cautious judgments of consistency. We
               illustrate these points through consideration of the literature
               on oxytocin and human trust, a research area that typifies some
               of the endemic problems that arise with poor statistical
               practice.",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  73,
  number    = "Suppl 1",
  pages     = "271--280",
  month     =  mar,
  year      =  2019,
  keywords  = "Confidence Intervals; Estimation; Meta-Analysis; Open Science;
               the New Statistics",
  language  = "en"
}

@BOOK{Dixon2020-ju,
  title     = "Machine Learning in Finance: From Theory to Practice",
  author    = "Dixon, Matthew F and Halperin, Igor and Bilokon, Paul",
  abstract  = "This book introduces machine learning methods in finance. It
               presents a unified treatment of machine learning and various
               statistical and computational disciplines in quantitative
               finance, such as financial econometrics and discrete time
               stochastic control, with an emphasis on how theory and
               hypothesis tests inform the choice of algorithm for financial
               data modeling and decision making. With the trend towards
               increasing computational resources and larger datasets, machine
               learning has grown into an important skillset for the finance
               industry. This book is written for advanced graduate students
               and academics in financial econometrics, mathematical finance
               and applied statistics, in addition to quants and data
               scientists in the field of quantitative finance. Machine
               Learning in Finance: From Theory to Practice is divided into
               three parts, each part covering theory and applications. The
               first presents supervised learning for cross-sectional data from
               both a Bayesian and frequentist perspective. The more advanced
               material places a firm emphasis on neural networks, including
               deep learning, as well as Gaussian processes, with examples in
               investment management and derivative modeling. The second part
               presents supervised learning for time series data, arguably the
               most common data type used in finance with examples in trading,
               stochastic volatility and fixed income modeling. Finally, the
               third part presents reinforcement learning and its applications
               in trading, investment and wealth management. Python code
               examples are provided to support the readers' understanding of
               the methodologies and applications. The book also includes more
               than 80 mathematical and programming exercises, with worked
               solutions available to instructors. As a bridge to research in
               this emergent field, the final chapter presents the frontiers of
               machine learning in finance from a researcher's perspective,
               highlighting how many well-known concepts in statistical physics
               are likely to emerge as important methodologies for machine
               learning in finance.",
  publisher = "Springer International Publishing",
  month     =  jul,
  year      =  2020,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Amrhein2019-nq,
  title     = "Inferential statistics as descriptive statistics: There is no
               replication crisis if we don't expect replication",
  author    = "Amrhein, Valentin and Trafimow, David and Greenland, Sander",
  abstract  = "Statistical inference often fails to replicate. One reason is
               that many results may be selected for drawing inference because
               some threshold of a statistic like the P-value was crossed,
               leading to biased reported effect sizes. Nonetheless,
               considerable non-replication is to be expected even without
               selective reporting, and generalizations from single studies are
               rarely if ever warranted. Honestly reported results must vary
               from replication to replication because of varying assumption
               violations and random variation; excessive agreement itself
               would ‚Ä¶",
  journal   = "Am. Stat.",
  publisher = "Informa UK Limited",
  volume    =  73,
  number    = "sup1",
  pages     = "262--270",
  month     =  mar,
  year      =  2019,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Gelman2016-ou,
  title     = "The problems with p-values are not just with p-values",
  author    = "Gelman, Andrew",
  abstract  = "The ASA's statement on p-values says,``Valid scientific
               conclusions based on p-values and related statistics cannot be
               drawn without at least knowing how many and which analyses were
               conducted.'' I agree, but knowledge of how many analyses were
               conducted etc. is not enough. The whole point of the ``garden of
               forking paths''(Gelman and Loken 2014) is that to compute a
               valid p-value you need to know what analyses would have been
               done had the data been different. Even if the researchers only
               did a single analysis of the data at hand ‚Ä¶",
  journal   = "Am. Stat.",
  publisher = "stat.columbia.edu",
  volume    =  70,
  number    =  10,
  year      =  2016
}

@ARTICLE{Dixon2020-cv,
  title         = "Deep Fundamental Factor Models",
  author        = "Dixon, Matthew F and Polson, Nicholas G",
  abstract      = "Deep fundamental factor models are developed to
                   automatically capture non-linearity and interaction effects
                   in factor modeling. Uncertainty quantification provides
                   interpretability with interval estimation, ranking of factor
                   importances and estimation of interaction effects. With no
                   hidden layers we recover a linear factor model and for one
                   or more hidden layers, uncertainty bands for the sensitivity
                   to each input naturally arise from the network weights.
                   Using 3290 assets in the Russell 1000 index over a period of
                   December 1989 to January 2018, we assess a 49 factor model
                   and generate information ratios that are approximately 1.5x
                   greater than the OLS factor model. Furthermore, we compare
                   our deep fundamental factor model with a quadratic LASSO
                   model and demonstrate the superior performance and
                   robustness to outliers. The Python source code and the data
                   used for this study are provided.",
  month         =  mar,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1903.07677"
}

@UNPUBLISHED{Feng2020-uh,
  title    = "Deep Learning in {Characteristics-Sorted} Factor Models",
  author   = "Feng, Guanhao and Polson, Nick and Xu, Jianeng",
  abstract = "To study the characteristics-sorted factor model in asset
              pricing, we develop a bottom-up approach with state-of-the-art
              deep learning optimization. With an economic objective to
              minimize pricing errors, we train a non-reduced-form neural
              network using firm characteristics [inputs], and generate factors
              [intermediate features], to fit security returns [outputs].
              Sorting securities on firm characteristics provides a nonlinear
              activation to create long-short portfolio weights, as a hidden
              layer, from lag characteristics to realized returns. Our model
              offers an alternative perspective for dimension reduction on firm
              characteristics [inputs], rather than factors [intermediate
              features], and allows for both nonlinearity and interactions on
              inputs. Our empirical findings are twofold. We find robust
              statistical and economic evidence in evaluating various
              portfolios and individual stock returns. Finally, we show highly
              significant results in factor investing, improvement in
              dissecting anomalies, as well as the volatility exposures in deep
              characteristics.",
  month    =  mar,
  year     =  2020,
  keywords = "Alpha, Characteristics-Sorted Factor Models, Cross-Sectional
              Return, Deep Learning, Firm Characteristics, Machine Learning,
              Pricing Errors"
}

@MISC{noauthor_undated-kg,
  title = "annurev-economics-080217-053433.pdf"
}

@ARTICLE{Schneider2020-ce,
  title    = "Collider bias in economic history research",
  author   = "Schneider, Eric B",
  abstract = "Economic historians have long been aware of many forms of bias
              that could lead to spurious causal inferences. However, our
              approach to these biases has been muddled at times by dealing
              with each bias separately and by confusion about the sources of
              bias and how to mitigate them. This paper shows how the
              methodology of directed acyclical graphs (DAGs) formulated by
              Pearl (2009) and particularly the concept of collider bias can
              provide economic historians with a unified approach to managing a
              wide range of biases that can distort causal inference. I present
              ten examples of collider bias drawn from economic history
              research, focussing mainly on examples where the authors were
              able to overcome or mitigate the bias. Thus, the paper addresses
              how to diagnose collider bias and also strategies for managing
              it. The paper also shows that quasi-random experimental designs
              are rarely able to overcome collider bias. Although all of these
              biases were understood by economic historians before,
              conceptualising them as collider bias will improve economic
              historians' understanding of the limitations of particular
              sources and help us develop better research designs in the
              future.",
  journal  = "Explor. Econ. Hist.",
  volume   =  78,
  pages    = "101356",
  month    =  oct,
  year     =  2020,
  keywords = "Sample-selection bias; Collider bias; Directed acyclical graphs"
}

@ARTICLE{Khan2007-ls,
  title     = "Robust linear model selection based on least angle regression",
  author    = "Khan, Jafar A and Van Aelst, Stefan and Zamar, Ruben H",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Informa UK Limited",
  volume    =  102,
  number    =  480,
  pages     = "1289--1299",
  month     =  dec,
  year      =  2007
}

@ARTICLE{Shipley2009-xz,
  title    = "Confirmatory path analysis in a generalized multilevel context",
  author   = "Shipley, Bill",
  abstract = "This paper describes how to test, and potentially falsify, a
              multivariate causal hypothesis involving only observed variables
              (i.e., a path analysis) when the data have a hierarchical or
              multilevel structure, when different variables are potentially
              defined at different levels of such a hierarchy, and when
              different variables have different sampling distributions. The
              test is a generalization of Shipley's d-sep test and can be
              conducted using standard statistical programs capable of fitting
              generalized mixed models.",
  journal  = "Ecology",
  volume   =  90,
  number   =  2,
  pages    = "363--368",
  month    =  feb,
  year     =  2009,
  language = "en"
}

@MISC{noauthor_undated-aw,
  title = "Service Delivery and Operational Effectiveness v2.pdf"
}

@ARTICLE{Lettau2020-we,
  title     = "Factors That Fit the Time Series and {Cross-Section} of Stock
               Returns",
  author    = "Lettau, Martin and Pelger, Markus",
  abstract  = "Abstract. We propose a new method for estimating latent asset
               pricing factors that fit the time series and cross-section of
               expected returns. Our estimator gene",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford Academic",
  volume    =  33,
  number    =  5,
  pages     = "2274--2325",
  month     =  may,
  year      =  2020
}

@ARTICLE{Athey2019-lv,
  title         = "Machine Learning methods economists should know about",
  author        = "Athey, Susan and Imbens, Guido",
  abstract      = "We discuss the relevance of the recent Machine Learning (ML)
                   literature for economics and econometrics. First we discuss
                   the differences in goals, methods and settings between the
                   ML literature and the traditional econometrics and
                   statistics literatures. Then we discuss some specific
                   methods from the machine learning literature that we view as
                   important for empirical researchers in economics. These
                   include supervised learning methods for regression and
                   classification, unsupervised learning methods, as well as
                   matrix completion methods. Finally, we highlight newly
                   developed methods at the intersection of ML and
                   econometrics, methods that typically perform better than
                   either off-the-shelf ML or more traditional econometric
                   methods when applied to particular classes of problems,
                   problems that include causal inference for average treatment
                   effects, optimal policy estimation, and estimation of the
                   counterfactual effect of price changes in consumer choice
                   models.",
  month         =  mar,
  year          =  2019,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "econ.EM",
  eprint        = "1903.10075"
}

@ARTICLE{Athey2019-eq,
  title     = "Machine learning methods that economists should know about",
  author    = "Athey, Susan and Imbens, Guido W",
  abstract  = "We discuss the relevance of the recent machine learning (ML)
               literature for economics and econometrics. First we discuss the
               differences in goals, methods, and settings between the ML
               literature and the traditional econometrics and statistics
               literatures. Then we discuss some specific methods from the ML
               literature that we view as important for empirical researchers
               in economics. These include supervised learning methods for
               regression and classification, unsupervised learning methods,
               and matrix completion methods. Finally, we highlight newly
               developed methods at the intersection of ML and econometrics
               that typically perform better than either off-the-shelf ML or
               more traditional econometric methods when applied to particular
               classes of problems, including causal inference for average
               treatment effects, optimal policy estimation, and estimation of
               the counterfactual effect of price changes in consumer choice
               models.",
  journal   = "Annu. Rev. Econom.",
  publisher = "Annual Reviews",
  volume    =  11,
  number    =  1,
  pages     = "685--725",
  month     =  aug,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Liebi2020-rj,
  title    = "The effect of {ETFs} on financial markets: a literature review",
  author   = "Liebi, Luca J",
  abstract = "Exchange-traded funds (ETFs) belong to the fastest growing
              investment products worldwide. Within 15 years, total assets
              invested in ETFs have twenty-folded, reaching over \$3.7 trillion
              at the end of 2018. Increasing demand for passive investments,
              coupled with high liquidity and low transaction costs, are key
              advantages of ETFs compared to their closest substitutes such as
              traditional index funds. Besides the continuous growth of ETFs,
              the Flash Crash in 2010 triggered detailed investigations by
              regulators on how ETFs affect the financial market. This
              literature review provides a broad overview of recent academic
              studies analyzing the effect of ETFs on liquidity, price
              discovery, volatility, and comovement of the underlying
              securities.",
  journal  = "Financial Markets and Portfolio Management",
  volume   =  34,
  number   =  2,
  pages    = "165--178",
  month    =  jun,
  year     =  2020
}

@ARTICLE{Lettau2018-ku,
  title     = "Exchange-traded funds 101 for economists",
  author    = "Lettau, Martin and Madhavan, Ananth",
  abstract  = "Exchange-traded funds (ETFs) represent one of the most important
               financial innovations in decades. An ETF is an investment
               vehicle, with a specific architecture that typically seeks to
               track the performance of a specific index. The first US-listed
               ETF, the SPDR, was launched by State Street in January 1993 and
               seeks to track the S\&P 500 index. It is still today the largest
               ETF by far, with assets of \$178 billion. Following the
               introduction of the SPDR, new ETFs were launched tracking broad
               domestic and international indices, and more specialized sector,
               region, or country indexes. In recent years, ETFs have grown
               substantially in assets, diversity, and market significance,
               including substantial increases in assets in bond ETFs and
               so-called ``smart beta'' funds that track certain investment
               strategies often used by actively traded mutual funds and hedge
               funds. In this paper, we begin by describing the structure and
               organization of exchange-traded funds, contrasting them with
               mutual funds, which are close relatives of exchange-traded
               funds, describing the differences in how ETFs operate and their
               potential advantages in terms of liquidity, lower expenses, tax
               efficiency, and transparency. We then turn to concerns over
               whether the rise in ETFs may raise unexpected risks for
               investors or greater instability in financial markets. While
               concerns over financial fragility are worth serious
               consideration, some of the common concerns are overstated, and
               for others, a number of rules and practices are already in place
               that offer a substantial margin of safety.",
  journal   = "J. Econ. Perspect.",
  publisher = "American Economic Association",
  volume    =  32,
  number    =  1,
  pages     = "135--154",
  month     =  feb,
  year      =  2018,
  language  = "en"
}

@ARTICLE{With_undated-ka,
  title  = "Can {ETFs} contribute to systemic risk?",
  author = "with:, Provided in Cooperation"
}

@ARTICLE{Goyal2002-rm,
  title    = "Board leadership structure and {CEO} turnover",
  author   = "Goyal, Vidhan K and Park, Chul W",
  abstract = "We study whether bestowing chief executive officer (CEO) and
              board chairman duties on one individual affects a boards decision
              to dismiss an ineffective CEO. The results show that the
              sensitivity of CEO turnover to firm performance is significantly
              lower when the CEO and chairman duties are vested in the same
              individual. These results are consistent with the view that the
              lack of independent leadership in firms that combine the CEO and
              Chairman positions makes it difficult for the board to remove
              poorly performing managers.",
  journal  = "Journal of Corporate Finance",
  volume   =  8,
  number   =  1,
  pages    = "49--66",
  month    =  jan,
  year     =  2002,
  keywords = "Corporate governance; Leadership structure; CEO duties; CEO
              turnover; Sensitivity to firm performance"
}

@BOOK{Bebchuk2009-sj,
  title     = "Pay Without Performance: The Unfulfilled Promise of Executive
               Compensation",
  author    = "Bebchuk, Lucian A and Fried, Jesse M",
  abstract  = "The company is under-performing, its share price is trailing,
               and the CEO gets...a multi-million-dollar raise. This story is
               familiar, for good reason: as this book clearly demonstrates,
               structural flaws in corporate governance have produced
               widespread distortions in executive pay. Pay without Performance
               presents a disconcerting portrait of managers' influence over
               their own pay--and of a governance system that must
               fundamentally change if firms are to be managed in the interest
               of shareholders. Lucian Bebchuk and Jesse Fried demonstrate that
               corporate boards have persistently failed to negotiate at arm's
               length with the executives they are meant to oversee. They give
               a richly detailed account of how pay practices--from option
               plans to retirement benefits--have decoupled compensation from
               performance and have camouflaged both the amount and
               performance-insensitivity of pay. Executives' unwonted influence
               over their compensation has hurt shareholders by increasing pay
               levels and, even more importantly, by leading to practices that
               dilute and distort managers' incentives. This book identifies
               basic problems with our current reliance on boards as guardians
               of shareholder interests. And the solution, the authors argue,
               is not merely to make these boards more independent of
               executives as recent reforms attempt to do. Rather, boards
               should also be made more dependent on shareholders by
               eliminating the arrangements that entrench directors and
               insulate them from their shareholders. A powerful critique of
               executive compensation and corporate governance, Pay without
               Performance points the way to restoring corporate integrity and
               improving corporate performance.",
  publisher = "Harvard University Press",
  month     =  jul,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Adams2010-cm,
  title    = "The Role of Boards of Directors in Corporate Governance: A
              Conceptual Framework and Survey",
  author   = "Adams, Renee B and Hermalin, Benjamin E and Weisbach, Michael S",
  abstract = "The Role of Boards of Directors in Corporate Governance: A
              Conceptual Framework and Survey by Renee B. Adams, Benjamin E.
              Hermalin and Michael S. Weisbach. Published in volume 48, issue
              1, pages 58-107 of Journal of Economic Literature, March 2010,
              Abstract: This paper is a survey of the literature...",
  journal  = "J. Econ. Lit.",
  volume   =  48,
  number   =  1,
  pages    = "58--107",
  month    =  mar,
  year     =  2010
}

@ARTICLE{Masulis2007-yg,
  title     = "Corporate governance and acquirer returns",
  author    = "Masulis, Ronald W and Wang, Cong and Xie, Fei",
  abstract  = "ABSTRACT We examine whether corporate governance mechanisms,
               especially the market for corporate control, affect the
               profitability of firm acquisitions. We find that acquirers with
               more antitakeover provisions experience significantly lower
               announcement-period abnormal stock returns. This supports the
               hypothesis that managers at firms protected by more antitakeover
               provisions are less subject to the disciplinary power of the
               market for corporate control and thus are more likely to indulge
               in empire-building acquisitions that destroy shareholder value.
               We also find that acquirers operating in more competitive
               industries or separating the positions of CEO and chairman of
               the board experience higher abnormal announcement returns.",
  journal   = "J. Finance",
  publisher = "Wiley",
  volume    =  62,
  number    =  4,
  pages     = "1851--1889",
  month     =  aug,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Brick2006-dj,
  title    = "{CEO} compensation, director compensation, and firm performance:
              Evidence of cronyism?",
  author   = "Brick, Ivan E and Palmon, Oded and Wald, John K",
  abstract = "We model CEO and director compensation using firm
              characteristics, CEO characteristics, and governance variables.
              After controlling for monitoring proxies, we find a significant
              positive relationship between CEO and director compensation. We
              hypothesize that this relationship could be due to unobserved
              firm complexity (omitted variables), and/or to excess
              compensation of directors and managers. We also find evidence
              that excess compensation (both director and CEO) is associated
              with firm underperformance. We therefore conclude that the
              evidence is consistent with excessive compensation due to mutual
              back scratching or cronyism. The evidence suggests that excessive
              compensation has an effect on firm performance that is
              independent of the poor governance variables discussed by
              previous studies.",
  journal  = "Journal of Corporate Finance",
  volume   =  12,
  number   =  3,
  pages    = "403--423",
  month    =  jun,
  year     =  2006,
  keywords = "Director compensation; CEO compensation; Firm performance;
              Cronyism"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Farrell2003-fl,
  title     = "Impact of firm performance expectations on {CEO} turnover and
               replacement decisions",
  author    = "Farrell, K A and Whidbee, D A",
  abstract  = "Our analysis suggests that boards focus on deviation from
               expected performance, rather than performance alone, in making
               the CEO turnover decision, especially when there is agreement
               (less dispersion) among analysts about the firm's earnings
               forecast or there are a ‚Ä¶",
  journal   = "J. Account. Econ.",
  publisher = "Elsevier",
  year      =  2003
}

@ARTICLE{Faleye2007-rb,
  title    = "Classified boards, firm value, and managerial entrenchment",
  author   = "Faleye, Olubunmi",
  abstract = "This paper shows that classified boards destroy value by
              entrenching management and reducing director effectiveness.
              First, I show that classified boards are associated with a
              significant reduction in firm value and that this holds even
              among complex firms, although such firms are often regarded as
              most likely to benefit from staggered board elections. I then
              examine how classified boards entrench management by focusing on
              CEO turnover, executive compensation, proxy contests, and
              shareholder proposals. My results indicate that classified boards
              significantly insulate management from market discipline, thus
              suggesting that the observed reduction in value is due to
              managerial entrenchment and diminished board accountability.",
  journal  = "J. financ. econ.",
  volume   =  83,
  number   =  2,
  pages    = "501--529",
  month    =  feb,
  year     =  2007,
  keywords = "Classified boards; Managerial entrenchment; Executive
              compensation"
}

@ARTICLE{Blei2016-lg,
  title         = "Variational Inference: A Review for Statisticians",
  author        = "Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D",
  abstract      = "One of the core problems of modern statistics is to
                   approximate difficult-to-compute probability densities. This
                   problem is especially important in Bayesian statistics,
                   which frames all inference about unknown quantities as a
                   calculation involving the posterior density. In this paper,
                   we review variational inference (VI), a method from machine
                   learning that approximates probability densities through
                   optimization. VI has been used in many applications and
                   tends to be faster than classical methods, such as Markov
                   chain Monte Carlo sampling. The idea behind VI is to first
                   posit a family of densities and then to find the member of
                   that family which is close to the target. Closeness is
                   measured by Kullback-Leibler divergence. We review the ideas
                   behind mean-field variational inference, discuss the special
                   case of VI applied to exponential family models, present a
                   full example with a Bayesian mixture of Gaussians, and
                   derive a variant that uses stochastic optimization to scale
                   up to massive data. We discuss modern research in VI and
                   highlight important open problems. VI is powerful, but it is
                   not yet well understood. Our hope in writing this paper is
                   to catalyze statistical research on this class of
                   algorithms.",
  month         =  jan,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "1601.00670"
}

@ARTICLE{Heston1993-uj,
  title     = "A closed-form solution for options with stochastic volatility
               with applications to bond and currency options",
  author    = "Heston, Steven L",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  volume    =  6,
  number    =  2,
  pages     = "327--343",
  year      =  1993
}

@ARTICLE{Wu2018-pr,
  title    = "{High-Performance} Computational Intelligence and Forecasting
              Technologies",
  author   = "Wu, K and Simon, H D",
  abstract = "Author(s): Wu, K; Simon, HD | Abstract: This report provides an
              introduction to the Computational Intelligence and Forecasting
              Technologies (CIFT) project at Lawrence Berkeley National
              Laboratory (LBNL). The main objective of CIFT is to promote the
              use of high-performance computing (HPC) tools and techniques for
              analysis of streaming data. After noticing the data volume being
              given as the explanation for the five-month delay for SEC and
              CFTC to issue their report on the 2010 Flash Crash, LBNL started
              the CIFT project to apply HPC technologies to manage and analyze
              financial data. Making timely decisions with streaming data is a
              requirement for many different applications, such as avoiding
              impending failure in the electric power grid or a liquidity
              crisis in financial markets. In all these cases, the HPC tools
              are well suited in handling the complex data dependencies and
              providing timely solutions. Over the years, CIFT has worked on a
              number of different forms of streaming data, including those from
              vehicle traffic, electric power grid, electricity usage, and so
              on. The following sections explain the key features of HPC
              systems, introduce a few special tools used on these systems, and
              provide examples of streaming data analyses using these HPC
              tools.",
  month    =  feb,
  year     =  2018
}

@ARTICLE{Black1973-zi,
  title     = "The Pricing of Options and Corporate Liabilities",
  author    = "Black, Fischer and Scholes, Myron",
  abstract  = "If options are correctly priced in the market, it should not be
               possible to make sure profits by creating portfolios of long and
               short positions in options and their underlying stocks. Using
               this principle, a theoretical valuation formula for options is
               derived. Since almost all corporate liabilities can be viewed as
               combinations of options, the formula and the analysis that led
               to it are also applicable to corporate liabilities such as
               common stock, corporate bonds, and warrants. In particular, the
               formula can be used to derive the discount that should be
               applied to a corporate bond because of the possibility of
               default.",
  journal   = "J. Polit. Econ.",
  publisher = "The University of Chicago Press",
  volume    =  81,
  number    =  3,
  pages     = "637--654",
  month     =  may,
  year      =  1973
}

@ARTICLE{Merton1973-vy,
  title     = "Theory of Rational Option Pricing",
  author    = "Merton, Robert C",
  abstract  = "[The long history of the theory of option pricing began in 1900
               when the French mathematician Louis Bachelier deduced an option
               pricing formula based on the assumption that stock prices follow
               a Brownian motion with zero drift. Since that time, numerous
               researchers have contributed to the theory. The present paper
               begins by deducing a set of restrictions on option pricing
               formulas from the assumption that investors prefer more to less.
               These restrictions are necessary conditions for a formula to be
               consistent with a rational pricing theory. Attention is given to
               the problems created when dividends are paid on the underlying
               common stock and when the terms of the option contract can be
               changed explicitly by a change in exercise price or implicitly
               by a shift in the investment or capital structure policy of the
               firm. Since the deduced restrictions are not sufficient to
               uniquely determine an option pricing formula, additional
               assumptions are introduced to examine and extend the seminal
               Black-Scholes theory of option pricing. Explicit formulas for
               pricing both call and put options as well as for warrants and
               the new ``down-and-out'' option are derived. The effects of
               dividends and call provisions on the warrant price are examined.
               The possibilities for further extension of the theory to the
               pricing of corporate liabilities are discussed.]",
  journal   = "The Bell Journal of Economics and Management Science",
  publisher = "[Wiley, RAND Corporation]",
  volume    =  4,
  number    =  1,
  pages     = "141--183",
  year      =  1973
}

@ARTICLE{Varghese2019-zv,
  title         = "Cloud Futurology",
  author        = "Varghese, Blesson and Leitner, Philipp and Ray, Suprio and
                   Chard, Kyle and Barker, Adam and Elkhatib, Yehia and Herry,
                   Herry and Hong, Cheol-Ho and Singer, Jeremy and Tso, Fung Po
                   and Yoneki, Eiko and Zhani, Mohamed-Faten",
  abstract      = "The Cloud has become integral to most Internet-based
                   applications and user gadgets. This article provides a brief
                   history of the Cloud and presents a researcher's view of the
                   prospects for innovating at the infrastructure, middleware,
                   and application and delivery levels of the already crowded
                   Cloud computing stack.",
  month         =  feb,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DC",
  eprint        = "1902.03656"
}

@ARTICLE{Orus2019-oa,
  title     = "Quantum computing for finance: Overview and prospects",
  author    = "Or{\'u}s, Rom{\'a}n and Mugel, Samuel and Lizaso, Enrique",
  abstract  = "We discuss how quantum computation can be applied to financial
               problems, providing an overview of current approaches and
               potential prospects. We review quantum optimization algorithms,
               and expose how quantum annealers can be used to optimize
               portfolios, find arbitrage opportunities, and perform credit
               scoring. We also discuss deep-learning in finance, and
               suggestions to improve these methods through quantum machine
               learning. Finally, we consider quantum amplitude estimation, and
               how it can result in a quantum speed-up for Monte Carlo
               sampling. This has direct applications to many current financial
               methods, including pricing of derivatives and risk analysis.
               Perspectives are also discussed.",
  journal   = "Reviews in Physics",
  publisher = "Elsevier",
  volume    =  4,
  pages     = "100028",
  month     =  nov,
  year      =  2019
}

@ARTICLE{Egger2020-mw,
  title     = "Quantum Computing for Finance: {State-of-the-Art} and Future
               Prospects",
  author    = "Egger, D J and Gambella, C and Marecek, J and McFaddin, S and
               Mevissen, M and Raymond, R and Simonetto, A and Woerner, S and
               Yndurain, E",
  abstract  = "This article outlines our point of view regarding the
               applicability, state-of-the-art, and potential of quantum
               computing for problems in finance. We provide an introduction to
               quantum computing as well as a survey on problem classes in
               finance that are computationally challenging classically and for
               which quantum computing algorithms are promising. In the main
               part, we describe in detail quantum algorithms for specific
               applications arising in financial services, such as those
               involving simulation, optimization, and machine learning
               problems. In addition, we include demonstrations of quantum
               algorithms on IBM Quantum back-ends and discuss the potential
               benefits of quantum algorithms for problems in financial
               services. We conclude with a summary of technical challenges and
               future prospects.",
  journal   = "IEEE Transactions on Quantum Engineering",
  publisher = "ieeexplore.ieee.org",
  volume    =  1,
  pages     = "1--24",
  year      =  2020,
  keywords  = "learning (artificial intelligence);mail order;quantum
               computing;machine learning problems;IBM Quantum
               back-ends;finance;problem classes;quantum computing
               algorithms;detail quantum
               algorithms;Qubit;Computers;Banking;Quantum
               mechanics;Insurance;Financial management;machine learning
               algorithms;optimization;quantum computing;simulation"
}

@INCOLLECTION{Etro2015-yq,
  title     = "The Economics of Cloud Computing",
  booktitle = "Cloud Technology: Concepts, Methodologies, Tools, and
               Applications",
  author    = "Etro, Federico",
  abstract  = "This chapter examines the economic impact of the diffusion of a
               new technology as cloud computing. This will allow firms to rent
               computing power and storage from service providers, and to pay
               on demand, with a profound impact on the cost structure of all
               the industries, turning some of the fixed cos...",
  publisher = "IGI Global",
  pages     = "2135--2148",
  year      =  2015,
  language  = "en"
}

@ARTICLE{Rebentrost2018-xc,
  title         = "Quantum computational finance: Monte Carlo pricing of
                   financial derivatives",
  author        = "Rebentrost, Patrick and Gupt, Brajesh and Bromley, Thomas R",
  abstract      = "Financial derivatives are contracts that can have a complex
                   payoff dependent upon underlying benchmark assets. In this
                   work, we present a quantum algorithm for the Monte Carlo
                   pricing of financial derivatives. We show how the relevant
                   probability distributions can be prepared in quantum
                   superposition, the payoff functions can be implemented via
                   quantum circuits, and the price of financial derivatives can
                   be extracted via quantum measurements. We show how the
                   amplitude estimation algorithm can be applied to achieve a
                   quadratic quantum speedup in the number of steps required to
                   obtain an estimate for the price with high confidence. This
                   work provides a starting point for further research at the
                   interface of quantum computing and finance.",
  month         =  apr,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "quant-ph",
  eprint        = "1805.00109"
}

@MISC{Nuzzo2014-qy,
  title        = "Scientific method: Statistical errors",
  author       = "Nuzzo, Regina",
  abstract     = "P values, the 'gold standard' of statistical validity, are
                  not as reliable as many scientists assume.",
  month        =  feb,
  year         =  2014,
  howpublished = "\url{http://www.nature.com/news/scientific-method-statistical-errors-1.14700}",
  note         = "Accessed: 2021-3-6"
}

@ARTICLE{Suzuki2020-et,
  title    = "Causal Diagrams: Pitfalls and Tips",
  author   = "Suzuki, Etsuji and Shinozaki, Tomohiro and Yamamoto, Eiji",
  abstract = "Graphical models are useful tools in causal inference, and causal
              directed acyclic graphs (DAGs) are used extensively to determine
              the variables for which it is sufficient to control for
              confounding to estimate causal effects. We discuss the following
              ten pitfalls and tips that are easily overlooked when using DAGs:
              1) Each node on DAGs corresponds to a random variable and not its
              realized values; 2) The presence or absence of arrows in DAGs
              corresponds to the presence or absence of individual causal
              effect in the population; 3) ``Non-manipulable'' variables and
              their arrows should be drawn with care; 4) It is preferable to
              draw DAGs for the total population, rather than for the exposed
              or unexposed groups; 5) DAGs are primarily useful to examine the
              presence of confounding in distribution in the notion of
              confounding in expectation; 6) Although DAGs provide qualitative
              differences of causal structures, they cannot describe details of
              how to adjust for confounding; 7) DAGs can be used to illustrate
              the consequences of matching and the appropriate handling of
              matched variables in cohort and case-control studies; 8) When
              explicitly accounting for temporal order in DAGs, it is necessary
              to use separate nodes for each timing; 9) In certain cases, DAGs
              with signed edges can be used in drawing conclusions about the
              direction of bias; and 10) DAGs can be (and should be) used to
              describe not only confounding bias but also other forms of bias.
              We also discuss recent developments of graphical models and their
              future directions.",
  journal  = "J. Epidemiol.",
  volume   =  30,
  number   =  4,
  pages    = "153--162",
  month    =  apr,
  year     =  2020,
  keywords = "bias; causal inference; causality; confounding; directed acyclic
              graphs",
  language = "en"
}

@MISC{Malinsky_undated-ns,
  title        = "Causal structure learning from multivariate time series in
                  settings with unmeasured confounding",
  author       = "Malinsky, Daniel",
  howpublished = "\url{http://proceedings.mlr.press/v92/malinsky18a/malinsky18a.pdf}",
  note         = "Accessed: 2021-1-17"
}

@INCOLLECTION{Morgan2014-le,
  title     = "Repeated Observations and the Estimation of Causal Effects",
  booktitle = "Counterfactuals and Causal Inference: Methods and Principles for
               Social Research",
  author    = "Morgan, Stephen L and Winship, Christopher",
  abstract  = "As discussed in previous chapters, the fundamental challenge of
               causal inference is that an individual cannot be simultaneously
               observed in both the treatment and control states. In some
               situations, however, it is possible to observe the same
               individual or unit of observation in the treatment and control
               states at different points in time. If the potential outcomes do
               not evolve in time for reasons other than the treatment, then
               the causal effect of a treatment can be estimated as the
               difference between an individual's observed outcome in the
               control state at time 1 and the same individual's observed
               outcome in the treatment state at time 2. The assumption that
               potential outcomes are stable in time (and thus age for
               individuals) is often heroic. If, however, potential outcomes
               evolve in a predictable way, then it may be possible to use the
               longitudinal structure of the data to predict the counterfactual
               outcomes of each individual.We begin our discussion with the
               interrupted time series (ITS) design, which we introduced
               already with the example of the year of the fire horse in
               Section 2.8.1. The ITS design is the simplest case where the
               goal is to determine the degree to which a treatment shifts the
               underlying trajectory of an outcome.",
  publisher = "Cambridge University Press",
  pages     = "354--416",
  month     =  nov,
  year      =  2014
}

@ARTICLE{Denis1997-vd,
  title    = "Ownership structure and top executive turnover",
  author   = "Denis, David J and Denis, Diane K and Sarin, Atulya",
  abstract = "We report that ownership structure significantly affects the
              likelihood of a change in top executive. Controlling for stock
              price performance, the probability of top executive turnover is
              negatively related to the ownership stake of officers and
              directors and positively related to the presence of an outside
              blockholder. In addition, the likehood of a change in top
              executive is significantly less sensitive to stock price
              performance in firms with higher managerial ownership. Finally,
              we document an unusually high rate of corporate control activity
              in the twelve months preceding top executive turnover. We
              conclude that ownership structure has an important influence on
              internal monitoring efforts and that this influence stems in part
              from the effect of ownership structure on external control
              threats.",
  journal  = "J. financ. econ.",
  volume   =  45,
  number   =  2,
  pages    = "193--221",
  month    =  aug,
  year     =  1997,
  keywords = "Ownership structure; Management turnover; Blockholders; Takeovers"
}

@MISC{Markowitz1952-pd,
  title   = "Portfolio Selection",
  author  = "Markowitz, Harry",
  journal = "The Journal of Finance",
  volume  =  7,
  number  =  1,
  pages   = "77",
  year    =  1952
}

@MISC{Roy1961-ni,
  title   = "Portfolio Selection",
  author  = "Roy, A D and Markowitz, Harry M",
  journal = "Econometrica",
  volume  =  29,
  number  =  1,
  pages   = "99",
  year    =  1961
}

@ARTICLE{Parrino1997-el,
  title    = "{CEO} turnover and outside succession A cross-sectional analysis",
  author   = "Parrino, Robert",
  abstract = "This study examines chief executive officer (CEO) turnover. It
              reports new evidence on factors that affect the likelihoods of
              voluntary and forced turnover, and for both of these turnover
              types, whether the new CEO is from inside the firm, from another
              firm in the industry, or from outside the industry. The evidence
              is consistent with arguments that poor CEOs are easier to
              identify and less costly to replace in industries that consist of
              similar firms than in heterogeneous industries. The likelihoods
              of forced turnover and of an intra-industry appointment increase
              with industry homogeneity.",
  journal  = "J. financ. econ.",
  volume   =  46,
  number   =  2,
  pages    = "165--197",
  month    =  nov,
  year     =  1997,
  keywords = "Management turnover; Corporate governance"
}

@ARTICLE{Weisbach1988-ti,
  title    = "Outside directors and {CEO} turnover",
  author   = "Weisbach, Michael S",
  abstract = "This paper examines the relation between the monitoring of CEOs
              by inside and outside directors and CEO resignations. CEO
              resignations are predicted using stock returns and earnings
              changes as measures of prior performance. There is a stronger
              association between prior performance and the probability of a
              resignation for companies with outsider-dominated boards than for
              companies with insider-dominated boards. This result does not
              appear to be a function of ownership effects, size effects, or
              industry effects. Unexpected stock returns on days when
              resignations are announced are consistent with the view that
              directors increase firm value by removing bad management.",
  journal  = "J. financ. econ.",
  volume   =  20,
  pages    = "431--460",
  month    =  jan,
  year     =  1988
}

@ARTICLE{Jensen1990-hb,
  title     = "Performance Pay and {Top-Management} Incentives",
  author    = "Jensen, Michael C and Murphy, Kevin J",
  abstract  = "Our estimates of the pay-performance relation (including pay,
               options, stockholdings, and dismissal) for chief executive
               officers indicate that CEO wealth changes $3.25 for every $1,000
               change in shareholder wealth. Although the incentives generated
               by stock ownership are large relative to pay and dismissal
               incentives, most CEOs hold trivial fractions of their firm's
               stock, and ownership levels have declined over the past 50
               years. We hypothesize that public and private political forces
               impose constraints that reduce the pay-performance sensitivity.
               Declines in both the pay-performance relation and the level of
               CEO pay since the 1930s are consistent with this hypothesis.",
  journal   = "J. Polit. Econ.",
  publisher = "The University of Chicago Press",
  volume    =  98,
  number    =  2,
  pages     = "225--264",
  month     =  apr,
  year      =  1990
}

@ARTICLE{Shleifer1997-me,
  title     = "A survey of corporate governance",
  author    = "Shleifer, Andrei and Vishny, Robert W",
  abstract  = "ABSTRACT This article surveys research on corporate governance,
               with special attention to the importance of legal protection of
               investors and of ownership concentration in corporate governance
               systems around the world.",
  journal   = "J. Finance",
  publisher = "Wiley",
  volume    =  52,
  number    =  2,
  pages     = "737--783",
  month     =  jun,
  year      =  1997,
  language  = "en"
}

@ARTICLE{Khanna1995-ac,
  title     = "Managers of financially distressed firms: Villains or
               scapegoats?",
  author    = "Khanna, Naveen and Poulsen, Annette B",
  abstract  = "ABSTRACT In this article, we provide evidence concerning the
               extent to which managers are to blame when their firms become
               bankrupt. We study a sample of firms that file for Chapter 11
               and determine the actions taken by the firms' managers during
               the three-year period before the filing. We compare the sample
               with a control sample of firms that performed better. We suggest
               that the comparison provides evidence on the way managers act as
               their firms sink into financial trouble and whether financial
               distress is the result of incompetence or excessively
               self-serving managerial decisions or due to factors outside of
               management's control. We find that managers of the Chapter 11
               firms and the control firms make very similar decisions and
               that, on average, neither set of managers is perceived to be
               taking value-reducing actions. These results do not change when
               we control for managerial turnover or managerial ownership. We
               also find that when managers are replaced in firms that
               eventually file for Chapter 11 protection, the market does not
               respond positively, regardless of whether the new managers are
               from inside or outside the firm. Our findings suggest that when
               managers are blamed for financial distress, they are serving as
               scapegoats.",
  journal   = "J. Finance",
  publisher = "Wiley",
  volume    =  50,
  number    =  3,
  pages     = "919--940",
  month     =  jul,
  year      =  1995,
  language  = "en"
}

@ARTICLE{Bertrand2003-ur,
  title     = "Managing with style: The effect of managers on firm policies",
  author    = "Bertrand, Marianne and Schoar, Antoinette",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press",
  volume    =  118,
  number    =  4,
  pages     = "1169--1208",
  year      =  2003
}

@ARTICLE{Liu2021-xi,
  title    = "Direction-of-change forecasting in commodity futures markets?",
  author   = "Liu, Jiadong and Papailiasb, Fotis and Quinna, Barry",
  abstract = "This paper examines direction-of-change predictability in
              commodity futures markets using a variety of binary probabilistic
              techniques. As well as traditional techniques, we apply Variable
              Length Markov Chain (VLMC) analysis, an innovative technique
              popularised in computational biology when predicting DNA
              sequences (B{\"u}hlmann, Wyner, et al., 1999). To the best of our
              knowledge, this is the first application of VLMC in finance. Our
              results show that both VLMC and technical analysis methods
              provide strong predictability of the direction-of-change of
              commodity returns, with annualised mean returns of approximately
              8\%, substantially higher than the passive long strategy. Our
              results suggest that a short-term learning effect is present in
              commodities market which can be exploited using innovative
              direction-of-change forecasting techniques.",
  journal  = "International Review of Financial Analysis",
  pages    = "101677",
  month    =  feb,
  year     =  2021,
  keywords = "Forecasting commodity futures; Direction-of-change; Dynamic
              probit model; Variable length Markov chain; Return signal
              momentum"
}

@ARTICLE{Behrendt2019-jn,
  title    = "{RTransferEntropy} --- Quantifying information flow between
              different time series using effective transfer entropy",
  author   = "Behrendt, Simon and Dimpfl, Thomas and Peter, Franziska J and
              Zimmermann, David J",
  abstract = "This paper shows how to quantify and test for the information
              flow between two time series with Shannon transfer entropy and
              R{\'e}nyi transfer entropy using the R package RTransferEntropy.
              We discuss the methodology, the bias correction applied to
              calculate effective transfer entropy and outline how to conduct
              statistical inference. Furthermore, we describe the package in
              detail and demonstrate its functionality by means of several
              simulated processes and present an application to financial time
              series.",
  journal  = "SoftwareX",
  volume   =  10,
  pages    = "100265",
  month    =  jul,
  year     =  2019,
  keywords = "Shannon transfer entropy; R{\'e}nyi transfer entropy; Effective
              transfer entropy; Bootstrap inference"
}

@ARTICLE{Bailey2013-el,
  title     = "An {Open-Source} Implementation of the {Critical-Line} Algorithm
               for Portfolio Optimization",
  author    = "Bailey, David H and L{\'o}pez de Prado, Marcos",
  abstract  = "Portfolio optimization is one of the problems most frequently
               encountered by financial practitioners. The main goal of this
               paper is to fill a gap in the literature by providing a
               well-documented, step-by-step open-source implementation of
               Critical Line Algorithm (CLA) in scientific language. The code
               is implemented as a Python class object, which allows it to be
               imported like any other Python module, and integrated seamlessly
               with pre-existing code. We discuss the logic behind CLA
               following the algorithm's decision flow. In addition, we
               developed several utilities that support finding answers to
               recurrent practical problems. We believe this publication will
               offer a better alternative to financial practitioners, many of
               whom are currently relying on generic-purpose optimizers which
               often deliver suboptimal solutions. The source code discussed in
               this paper can be downloaded at the authors' websites (see
               Appendix).",
  journal   = "Algorithms",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  6,
  number    =  1,
  pages     = "169--196",
  month     =  mar,
  year      =  2013,
  language  = "en"
}

@ARTICLE{noauthor_undated-xi,
  title    = "Is There a Replication Crisis in Finance?",
  author   = "Jensen, Theis Ingerslev and Kelly, Bryan T and Pedersen, Lasse
              Heje",
  abstract = "Several papers argue that financial economics faces a replication
              crisis because the majority of studies cannot be replicated or
              are the result of multiple testing of too many factors. We
              develop and estimate a Bayesian model of factor replication,
              which leads to different conclusions. The majority of asset
              pricing factors: (1) can be replicated, (2) can be clustered into
              13 themes, the majority of which are significant parts of the
              tangency portfolio, (3) work out of sample in a new large data
              set covering 93 countries, and (4) have evidence that is
              strengthened (not weakened) by the large number of observed
              factors.",
  month    =  jan,
  year     =  2021,
  keywords = "asset pricing, factors, data mining, replication, multiple
              testing, external validity, empirical Bayes, Bayesian statistics"
}

@ARTICLE{Lopez_de_Prado2016-mm,
  title   = "Building Diversified Portfolios that Outperform {Out-of-Sample}",
  author  = "Lopez de Prado, Marcos",
  journal = "Journal of Portfolio Management",
  volume  =  42,
  number  =  4,
  pages   = "59--69",
  year    =  2016
}

@INCOLLECTION{Steinbach2004-hc,
  title     = "The Challenges of Clustering High Dimensional Data",
  booktitle = "New Directions in Statistical Physics: Econophysics,
               Bioinformatics, and Pattern Recognition",
  author    = "Steinbach, Michael and Ert{\"o}z, Levent and Kumar, Vipin",
  editor    = "Wille, Luc T",
  abstract  = "Cluster analysis divides data into groups (clusters) for the
               purposes of summarization or improved understanding. For
               example, cluster analysis has been used to group related
               documents for browsing, to find genes and proteins that have
               similar functionality, or as a means of data compression. While
               clustering has a long history and a large number of clustering
               techniques have been developed in statistics, pattern
               recognition, data mining, and other fields, significant
               challenges still remain. In this chapter we provide a short
               introduction to cluster analysis, and then focus on the
               challenge of clustering high dimensional data. We present a
               brief overview of several recent techniques, including a more
               detailed description of recent work of our own which uses a
               concept-based clustering approach.",
  publisher = "Springer Berlin Heidelberg",
  pages     = "273--309",
  year      =  2004,
  address   = "Berlin, Heidelberg"
}

@INPROCEEDINGS{Ding2004-qw,
  title     = "K-means clustering via principal component analysis",
  booktitle = "Proceedings of the twenty-first international conference on
               Machine learning",
  author    = "Ding, Chris and He, Xiaofeng",
  abstract  = "Principal component analysis (PCA) is a widely used statistical
               technique for unsupervised dimension reduction. K-means
               clustering is a commonly used data clustering for performing
               unsupervised learning tasks. Here we prove that principal
               components are the continuous solutions to the discrete cluster
               membership indicators for K-means clustering. New lower bounds
               for K-means objective function are derived, which is the total
               variance minus the eigenvalues of the data covariance matrix.
               These results indicate that unsupervised dimension reduction is
               closely related to unsupervised learning. Several implications
               are discussed. On dimension reduction, the result provides new
               insights to the observed effectiveness of PCA-based data
               reductions, beyond the conventional noise-reduction explanation
               that PCA, via singular value decomposition, provides the best
               low-dimensional linear approximation of the data. On learning,
               the result suggests effective techniques for K-means data
               clustering. DNA gene expression and Internet newsgroups are
               analyzed to illustrate our results. Experiments indicate that
               the new bounds are within 0.5-1.5\% of the optimal values.",
  publisher = "Association for Computing Machinery",
  pages     = "29",
  series    = "ICML '04",
  month     =  jul,
  year      =  2004,
  address   = "New York, NY, USA",
  location  = "Banff, Alberta, Canada"
}

@UNPUBLISHED{Lopez_de_Prado2018-rw,
  title    = "Detection of False Investment Strategies Using Unsupervised
              Learning Methods",
  author   = "Lopez de Prado, Marcos and Lewis, Michael J",
  abstract = "Most investment strategies uncovered by practitioners and
              academics are false. This partially explains the high rate of
              failure, especially among quantitative hedge funds (smart beta,
              factor investing, stat-arb, CTAs, etc.) In this paper we examine
              why false positives are so prevalent in finance, why researchers
              fail (in many cases purposely) to detect them, and why firms are
              able to monetize their scheme. Beyond merely pointing to this
              industrywide problem, we offer a practical solution. We hope that
              the machine learning tools presented in this paper will help
              financial academic journals filter out false positives, and bring
              up the retraction rate to reasonable levels. The SEC, FINRA and
              other regulatory agencies worldwide could use these tools to take
              a more active role in curving this rampant financial fraud. A
              presentation based on this paper can be found at
              https://ssrn.com/abstract=3173146.",
  month    =  aug,
  year     =  2018,
  keywords = "Backtest overfitting, selection bias, multiple testing,
              quantitative investments, machine learning, financial fraud"
}

@ARTICLE{Goutte1999-oj,
  title    = "On clustering {fMRI} time series",
  author   = "Goutte, C and Toft, P and Rostrup, E and Nielsen, F and Hansen, L
              K",
  abstract = "Analysis of fMRI time series is often performed by extracting one
              or more parameters for the individual voxels. Methods based,
              e.g., on various statistical tests are then used to yield
              parameters corresponding to probability of activation or
              activation strength. However, these methods do not indicate
              whether sets of voxels are activated in a similar way or in
              different ways. Typically, delays between two activated signals
              are not identified. In this article, we use clustering methods to
              detect similarities in activation between voxels. We employ a
              novel metric that measures the similarity between the activation
              stimulus and the fMRI signal. We present two different clustering
              algorithms and use them to identify regions of similar
              activations in an fMRI experiment involving a visual stimulus.",
  journal  = "Neuroimage",
  volume   =  9,
  number   =  3,
  pages    = "298--310",
  month    =  mar,
  year     =  1999,
  language = "en"
}

@ARTICLE{Tibshirani2001-fx,
  title     = "Estimating the number of clusters in a data set via the gap
               statistic",
  author    = "Tibshirani, Robert and Walther, Guenther and Hastie, Trevor",
  abstract  = "We propose a method (the ?gap statistic?) for estimating the
               number of clusters (groups) in a set of data. The technique uses
               the output of any clustering algorithm (e.g. K-means or
               hierarchical), comparing the change in within-cluster dispersion
               with that expected under an appropriate reference null
               distribution. Some theory is developed for the proposal and a
               simulation study shows that the gap statistic usually
               outperforms other methods that have been proposed in the
               literature.",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Wiley",
  volume    =  63,
  number    =  2,
  pages     = "411--423",
  year      =  2001,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

@ARTICLE{Colquhoun2014-kd,
  title     = "An investigation of the false discovery rate and the
               misinterpretation of p-values",
  author    = "Colquhoun, David",
  abstract  = "If you use p=0.05 to suggest that you have made a discovery, you
               will be wrong at least 30\% of the time. If, as is often the
               case, experiments are underpowered, you will be wrong most of
               the time. This conclusion is demonstrated from several points of
               view. First, tree diagrams which show the close analogy with the
               screening test problem. Similar conclusions are drawn by
               repeated simulations of t-tests. These mimic what is done in
               real life, which makes the results more persuasive. The
               simulation method is used also to evaluate the extent to which
               effect sizes are over-estimated, especially in underpowered
               experiments. A script is supplied to allow the reader to do
               simulations themselves, with numbers appropriate for their own
               work. It is concluded that if you wish to keep your false
               discovery rate below 5\%, you need to use a three-sigma rule, or
               to insist on p$\leq$0.001. And never use the word 'significant'.",
  journal   = "R. Soc. Open Sci.",
  publisher = "The Royal Society",
  volume    =  1,
  number    =  3,
  pages     = "140216",
  month     =  nov,
  year      =  2014,
  keywords  = "false discovery rate; reproducibility; significance tests;
               statistics",
  language  = "en"
}

@ARTICLE{Harvey2017-mq,
  title     = "Presidential address: The scientific outlook in financial
               economics: Scientific outlook in finance",
  author    = "Harvey, Campbell R",
  abstract  = "ABSTRACT Given the competition for top journal space, there is
               an incentive to produce ?significant? results. With the
               combination of unreported tests, lack of adjustment for multiple
               tests, and direct and indirect p-hacking, many of the results
               being published will fail to hold up in the future. In addition,
               there are basic issues with the interpretation of statistical
               significance. Increasing thresholds may be necessary, but still
               may not be sufficient: if the effect being studied is rare, even
               t > 3 will produce a large number of false positives. Here I
               explore the meaning and limitations of a p-value. I offer a
               simple alternative (the minimum Bayes factor). I present
               guidelines for a robust, transparent research culture in
               financial economics. Finally, I offer some thoughts on the
               importance of risk-taking (from the perspective of authors and
               editors) to advance our field. SUMMARY Empirical research in
               financial economics relies too much on p-values, which are
               poorly understood in the first place. Journals want to publish
               papers with positive results and this incentivizes researchers
               to engage in data mining and ?p-hacking.? The outcome will
               likely be an embarrassing number of false positives?effects that
               will not be repeated in the future. The minimum Bayes factor
               (which is a function of the p-value) combined with prior odds
               provides a simple solution that can be reported alongside the
               usual p-value. The Bayesianized p-value answers the question:
               What is the probability that the null is true? The same
               technique can be used to answer: What threshold of t-statistic
               do I need so that there is only a 5\% chance that the null is
               true? The threshold depends on the economic plausibility of the
               hypothesis.",
  journal   = "J. Finance",
  publisher = "Wiley",
  volume    =  72,
  number    =  4,
  pages     = "1399--1440",
  month     =  aug,
  year      =  2017,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

@ARTICLE{Ioannidis2019-ab,
  title     = "What Have We (Not) Learnt from Millions of Scientific Papers
               with {P} Values?",
  author    = "Ioannidis, John P A",
  abstract  = "ABSTRACTP values linked to null hypothesis significance testing
               (NHST) is the most widely (mis)used method of statistical
               inference. Empirical data suggest that across the biomedical
               literature (1990?2015), when abstracts use P values 96\% of them
               have P values of 0.05 or less. The same percentage (96\%)
               applies for full-text articles. Among 100 articles in PubMed, 55
               report P values, while only 4 present confidence intervals for
               all the reported effect sizes, none use Bayesian methods and
               none use false-discovery rate. Over 25 years (1990?2015), use of
               P values in abstracts has doubled for all PubMed, and tripled
               for meta-analyses, while for some types of designs such as
               randomized trials the majority of abstracts report P values.
               There is major selective reporting for P values. Abstracts tend
               to highlight most favorable P values and inferences use even
               further spin to reach exaggerated, unreliable conclusions. The
               availability of large-scale data on P values from many papers
               has allowed the development and applications of methods that try
               to detect and model selection biases, for example, p-hacking,
               that cause patterns of excess significance. Inferences need to
               be cautious as they depend on the assumptions made by these
               models and can be affected by the presence of other biases
               (e.g., confounding in observational studies). While much of the
               unreliability of past and present research is driven by small,
               underpowered studies, NHST with P values may be also
               particularly problematic in the era of overpowered big data.
               NHST and P values are optimal only in a minority of current
               research. Using a more stringent threshold, as in the recently
               proposed shift from P < 0.05 to P < 0.005, is a temporizing
               measure to contain the flood and death-by-significance. NHST and
               P values may be replaced in many fields by other, more
               fit-for-purpose, inferential methods. However, curtailing
               selection biases requires additional measures, beyond changes in
               inferential methods, and in particular reproducible research
               practices.",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  73,
  number    = "sup1",
  pages     = "20--25",
  month     =  mar,
  year      =  2019
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Gelman_undated-te,
  title        = "The garden of forking paths: Why multiple comparisons can be
                  a problem, even when there is no ``fishing expedition'' or
                  ``p-hacking'' and the research hypothesis was posited ahead
                  of time‚àó",
  author       = "Gelman, Andrew and Loken, Eric",
  howpublished = "\url{http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf}",
  note         = "Accessed: 2021-3-7"
}

@ARTICLE{Monschang2020-mz,
  title    = "{Sup-ADF-style} bubble-detection methods under test",
  author   = "Monschang, Verena and Wilfling, Bernd",
  abstract = "In this paper, we analyze the capacity of supremum augmented
              Dickey--Fuller (SADF), generalized SADF (GSADF), and of several
              heteroscedasticity-adjusted sup-ADF-style tests for detecting and
              date-stamping financial bubbles. Our Monte Carlo simulations find
              that the majority of the sup-ADF-style tests exhibit substantial
              size distortions, when the data-generating process is subject to
              leverage effects. Moreover, the sup-ADF-style tests often have
              low empirical power in identifying a (flexible and empirically
              relevant) rational stock-price bubble, recently proposed in the
              literature. In a simulation study, we compare the effectiveness
              of two real-time bubble date-stamping procedures (Procedures 1
              and 2), both based on variants of the backward SADF (BSADF) test.
              While Procedure 1 (predominantly) provides better estimates of
              the bubbles' origination and termination dates than Procedure 2,
              the first procedure frequently stamps non-existing bubbles. In an
              empirical application, we use NASDAQ data covering a time-span of
              45 years and find that the bubble date-stamping outcomes of both
              procedures are sensitive to the data frequency chosen by the
              econometrician.",
  journal  = "Empir. Econ.",
  month    =  mar,
  year     =  2020
}

@ARTICLE{Barredo_Arrieta2020-un,
  title    = "Explainable Artificial Intelligence ({XAI)}: Concepts,
              taxonomies, opportunities and challenges toward responsible {AI}",
  author   = "Barredo Arrieta, Alejandro and D{\'\i}az-Rodr{\'\i}guez, Natalia
              and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and
              Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and
              Molina, Daniel and Benjamins, Richard and Chatila, Raja and
              Herrera, Francisco",
  abstract = "In the last few years, Artificial Intelligence (AI) has achieved
              a notable momentum that, if harnessed appropriately, may deliver
              the best of expectations over many application sectors across the
              field. For this to occur shortly in Machine Learning, the entire
              community stands in front of the barrier of explainability, an
              inherent problem of the latest techniques brought by
              sub-symbolism (e.g. ensembles or Deep Neural Networks) that were
              not present in the last hype of AI (namely, expert systems and
              rule based models). Paradigms underlying this problem fall within
              the so-called eXplainable AI (XAI) field, which is widely
              acknowledged as a crucial feature for the practical deployment of
              AI models. The overview presented in this article examines the
              existing literature and contributions already done in the field
              of XAI, including a prospect toward what is yet to be reached.
              For this purpose we summarize previous efforts made to define
              explainability in Machine Learning, establishing a novel
              definition of explainable Machine Learning that covers such prior
              conceptual propositions with a major focus on the audience for
              which the explainability is sought. Departing from this
              definition, we propose and discuss about a taxonomy of recent
              contributions related to the explainability of different Machine
              Learning models, including those aimed at explaining Deep
              Learning methods for which a second dedicated taxonomy is built
              and examined in detail. This critical literature analysis serves
              as the motivating background for a series of challenges faced by
              XAI, such as the interesting crossroads of data fusion and
              explainability. Our prospects lead toward the concept of
              Responsible Artificial Intelligence, namely, a methodology for
              the large-scale implementation of AI methods in real
              organizations with fairness, model explainability and
              accountability at its core. Our ultimate goal is to provide
              newcomers to the field of XAI with a thorough taxonomy that can
              serve as reference material in order to stimulate future research
              advances, but also to encourage experts and professionals from
              other disciplines to embrace the benefits of AI in their activity
              sectors, without any prior bias for its lack of interpretability.",
  journal  = "Inf. Fusion",
  volume   =  58,
  pages    = "82--115",
  month    =  jun,
  year     =  2020,
  keywords = "Explainable Artificial Intelligence; Machine Learning; Deep
              Learning; Data Fusion; Interpretability; Comprehensibility;
              Transparency; Privacy; Fairness; Accountability; Responsible
              Artificial Intelligence"
}

@ARTICLE{Goodman2017-cj,
  title     = "European Union Regulations on Algorithmic {Decision-Making} and
               a ``Right to Explanation''",
  author    = "Goodman, Bryce and Flaxman, Seth",
  abstract  = "We summarize the potential impact that the European Union's new
               General Data Protection Regulation will have on the routine use
               of machine learning algorithms. Slated to take effect as law
               across the EU in 2018, it will restrict automated individual
               decision-making (that is, algorithms that make decisions based
               on user-level predictors) which ``significantly affect'' users.
               The law will also effectively create a ``right to explanation,''
               whereby a user can ask for an explanation of an algorithmic
               decision that was made about them. We argue that while this law
               will pose large challenges for industry, it highlights
               opportunities for computer scientists to take the lead in
               designing algorithms and evaluation frameworks which avoid
               discrimination and enable explanation.",
  journal   = "AIMag",
  publisher = "ojs.aaai.org",
  volume    =  38,
  number    =  3,
  pages     = "50--57",
  month     =  oct,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Goldstein2015-dj,
  title     = "Peeking Inside the Black Box: Visualizing Statistical Learning
               With Plots of Individual Conditional Expectation",
  author    = "Goldstein, Alex and Kapelner, Adam and Bleich, Justin and
               Pitkin, Emil",
  abstract  = "This article presents individual conditional expectation (ICE)
               plots, a tool for visualizing the model estimated by any
               supervised learning algorithm. Classical partial dependence
               plots (PDPs) help visualize the average partial relationship
               between the predicted response and one or more features. In the
               presence of substantial interaction effects, the partial
               response relationship can be heterogeneous. Thus, an average
               curve, such as the PDP, can obfuscate the complexity of the
               modeled relationship. Accordingly, ICE plots refine the PDP by
               graphing the functional relationship between the predicted
               response and the feature for individual observations.
               Specifically, ICE plots highlight the variation in the fitted
               values across the range of a covariate, suggesting where and to
               what extent heterogeneities might exist. In addition to
               providing a plotting suite for exploratory analysis, we include
               a visual test for additive structure in the data-generating
               model. Through simulated examples and real datasets, we
               demonstrate how ICE plots can shed light on estimated models in
               ways PDPs cannot. Procedures outlined are available in the R
               package ICEbox.",
  journal   = "J. Comput. Graph. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  24,
  number    =  1,
  pages     = "44--65",
  month     =  jan,
  year      =  2015
}

@INPROCEEDINGS{Datta2016-yb,
  title     = "Algorithmic Transparency via Quantitative Input Influence:
               Theory and Experiments with Learning Systems",
  booktitle = "2016 {IEEE} Symposium on Security and Privacy ({SP})",
  author    = "Datta, A and Sen, S and Zick, Y",
  abstract  = "Algorithmic systems that employ machine learning play an
               increasing role in making substantive decisions in modern
               society, ranging from online personalization to insurance and
               credit decisions to predictive policing. But their
               decision-making processes are often opaque-it is difficult to
               explain why a certain decision was made. We develop a formal
               foundation to improve the transparency of such decision-making
               systems. Specifically, we introduce a family of Quantitative
               Input Influence (QII) measures that capture the degree of
               influence of inputs on outputs of systems. These measures
               provide a foundation for the design of transparency reports that
               accompany system decisions (e.g., explaining a specific credit
               decision) and for testing tools useful for internal and external
               oversight (e.g., to detect algorithmic discrimination).
               Distinctively, our causal QII measures carefully account for
               correlated inputs while measuring influence. They support a
               general class of transparency queries and can, in particular,
               explain decisions about individuals (e.g., a loan decision) and
               groups (e.g., disparate impact based on gender). Finally, since
               single inputs may not always have high influence, the QII
               measures also quantify the joint influence of a set of inputs
               (e.g., age and income) on outcomes (e.g. loan decisions) and the
               marginal influence of individual inputs within such a set (e.g.,
               income). Since a single input may be part of multiple
               influential sets, the average marginal influence of the input is
               computed using principled aggregation measures, such as the
               Shapley value, previously applied to measure influence in
               voting. Further, since transparency reports could compromise
               privacy, we explore the transparency-privacy tradeoff and prove
               that a number of useful transparency reports can be made
               differentially private with very little addition of noise. Our
               empirical validation with standard machine learning algorithms
               demonstrates that QII measures are a useful transparency
               mechanism when black box access to the learning system is
               available. In particular, they provide better explanations than
               standard associative measures for a host of scenarios that we
               consider. Further, we show that in the situations we consider,
               QII is efficiently approximable and can be made differentially
               private while preserving accuracy.",
  publisher = "ieeexplore.ieee.org",
  pages     = "598--617",
  month     =  may,
  year      =  2016,
  keywords  = "data privacy;decision making;learning (artificial
               intelligence);algorithmic transparency;quantitative input
               influence;learning systems;algorithmic systems;machine
               learning;decision making systems;online
               personalization;insurance decisions;credit decisions;predictive
               policing;QII measures;transparency reports;system
               decisions;algorithmic discrimination;transparency
               queries;marginal influence;principled aggregation
               measures;Shapley value;transparency-privacy tradeoff;black box
               access;Decision making;Atmospheric measurements;Particle
               measurements;Privacy;Correlation;Machine learning
               algorithms;Algorithm design and
               analysis;transparency;fairness;machine learning"
}

@MISC{Lundberg_undated-tm,
  title       = "shap",
  author      = "Lundberg, Scott",
  abstract    = "A game theoretic approach to explain the output of any machine
                 learning model. - slundberg/shap",
  institution = "Github"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Bailey2014-zu,
  title     = "The deflated Sharpe ratio: correcting for selection bias,
               backtest overfitting, and non-normality",
  author    = "Bailey, David H and De Prado, Marcos Lopez",
  abstract  = "‚Ä¶ Follow IIJ on Twitter. Article. The Deflated Sharpe Ratio:
               Correcting for Selection Bias , Backtest Overfitting, and
               Non-Normality. David H. Bailey and Marcos L{\'o}pez de Prado.
               The Journal of Portfolio Management Special 40th Anniversary
               Issue 2014,.",
  journal   = "The Journal of Portfolio Management",
  publisher = "Institutional Investor Journals Umbrella",
  volume    =  40,
  number    =  5,
  pages     = "94--107",
  year      =  2014
}

@ARTICLE{Aghabarari2021-hd,
  title     = "Is there help indeed, if there is help in need? The case of
               credit unions during the global financial crisis",
  author    = "Aghabarari, L and Guettler, A and Naeem, M and Van Doornik, B",
  abstract  = "Credit unions (CUs) may respond to a financial shock differently
               than other types of banks because of their unique
               membership-based governance structure. We exploit the financial
               crisis of 2008/09 as a negative shock to Brazilian banks and
               analyze the lending behavior of CUs in comparison to non-CUs and
               the subsequent effects on the commercial clients' labor force.
               We find that during the financial crisis, CUs tightened their
               members' access to credit to a lesser extent (insurance effect)
               than did other bank types. Notably, the labor market impact of
               the insurance effect of CUs is positive for very small firms.
               \copyright{} 2021 Western Economic Association International",
  journal   = "Econ. Inq.",
  publisher = "Blackwell Publishing Inc.",
  year      =  2021,
  keywords  = "credit unions; financial crisis; financial intermediaries; labor
               market outcomes; relationship lending"
}

@ARTICLE{Aghabarari2021-qt,
  title     = "Is there help indeed, if there is help in need? The case of
               credit unions during the global financial crisis",
  author    = "Aghabarari, Leila and Guettler, Andre and Naeem, Mahvish and Van
               Doornik, Bernardus",
  abstract  = "Abstract Credit unions (CUs) may respond to a financial shock
               differently than other types of banks because of their unique
               membership-based governance structure. We exploit the financial
               crisis of 2008/09 as a negative shock to Brazilian banks and
               analyze the lending behavior of CUs in comparison to non-CUs and
               the subsequent effects on the commercial clients' labor force.
               We find that during the financial crisis, CUs tightened their
               members' access to credit to a lesser extent (insurance effect)
               than did other bank types. Notably, the labor market impact of
               the insurance effect of CUs is positive for very small firms.",
  journal   = "Econ. Inq.",
  publisher = "Wiley",
  number    = "ecin.12982",
  month     =  mar,
  year      =  2021,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Fernandez2020-az,
  title     = "Random forest explainability using counterfactual sets",
  author    = "Fern{\'a}ndez, Rub{\'e}n R and Mart{\'\i}n de Diego, Isaac and
               Ace{\~n}a, V{\'\i}ctor and Fern{\'a}ndez-Isabel, Alberto and
               Moguerza, Javier M",
  abstract  = "Nowadays, Machine Learning (ML) models are becoming ubiquitous
               in today's society, supporting people with their day-to-day
               decisions. In this context, Explainable ML is a field of
               Artificial Intelligence (AI) that focuses on making predictive
               models and their decisions interpretable by humans, enabling
               people to trust predictive models and to understand the
               underlying processes. A counterfactual is an effective type of
               Explainable ML technique that explains predictions by describing
               the changes needed in a sample to flip the outcome of the
               prediction. In this paper, we introduce counterfactual sets, an
               explanation approach that uses a set of counterfactuals to
               explain a prediction rather than a single counterfactual, by
               defining a sub-region of the feature space where the
               counterfactual holds. A method to extract counterfactual sets
               from a Random Forest (RF), the
               RandomForestOptimalCounterfactualSetExtractor(RF‚àíOCSE), is
               presented. The method is based on a partial fusion of tree
               predictors from a RF into a single Decision Tree (DT) using a
               modification of the CART algorithm, and it obtains a
               counterfactual set that contains the optimal counterfactual. The
               proposal is validated through several experiments against
               existing alternatives on ten well-known datasets by comparing
               the percentage of valid counterfactuals, distance to the factual
               sample, and counterfactual sets quality.",
  journal   = "Inf. Fusion",
  publisher = "Elsevier BV",
  volume    =  63,
  pages     = "196--207",
  month     =  nov,
  year      =  2020,
  keywords  = "Explainable machine learning; Counterfactual sets;
               Counterfactual; Information fusion; Random forest; Decision tree",
  language  = "en"
}

@ARTICLE{Bussmann2020-pi,
  title    = "Explainable {AI} in Fintech Risk Management",
  author   = "Bussmann, Niklas and Giudici, Paolo and Marinelli, Dimitri and
              Papenbrock, Jochen",
  abstract = "The paper proposes an explainable AI model that can be used in
              fintech risk management and, in particular, in measuring the
              risks that arise when credit is borrowed employing peer to peer
              lending platforms. The model employs Shapley values, so that AI
              predictions are interpreted according to the underlying
              explanatory variables. The empirical analysis of 15,000 small and
              medium companies asking for peer to peer lending credit reveals
              that both risky and not risky borrowers can be grouped according
              to a set of similar financial characteristics, which can be
              employed to explain and understand their credit score and,
              therefore, to predict their future behavior.",
  journal  = "Front Artif Intell",
  volume   =  3,
  pages    = "26",
  month    =  apr,
  year     =  2020,
  keywords = "credit risk management; explainable AI; financial technologies;
              logistic regression; peer to peer lending; predictive models",
  language = "en"
}

@ARTICLE{Papenbrock2021-gt,
  title     = "Matrix evolutions: Synthetic correlations and explainable
               machine learning for constructing robust investment portfolios",
  author    = "Papenbrock, Jochen and Schwendner, Peter and Jaeger, Markus and
               Kr{\"u}gel, Stephan",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  pages     = "jfds.2021.1.056",
  month     =  mar,
  year      =  2021,
  language  = "en"
}

@MISC{Cox1985-cq,
  title   = "A Theory of the Term Structure of Interest Rates",
  author  = "Cox, John C and Ingersoll, Jonathan E and Ross, Stephen A",
  journal = "Econometrica",
  volume  =  53,
  number  =  2,
  pages   = "385",
  year    =  1985
}

@ARTICLE{Ghilagaber2020-wq,
  title     = "Bayesian change-point modelling of the effects of
               3-points-for-a-win rule in football",
  author    = "Ghilagaber, Gebrenegus and Munezero, Parfait",
  journal   = "J. Appl. Stat.",
  publisher = "Informa UK Limited",
  volume    =  47,
  number    =  2,
  pages     = "248--264",
  month     =  jan,
  year      =  2020,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

@ARTICLE{Dimopoulos1995-zg,
  title    = "Use of some sensitivity criteria for choosing networks with good
              generalization ability",
  author   = "Dimopoulos, Yannis and Bourret, Paul and Lek, Sovan",
  abstract = "In most applications of the multilayer perceptron (MLP) the main
              objective is to maximize the generalization ability of the
              network. We show that this ability is related to the sensitivity
              of the output of the MLP to small input changes. Several criteria
              have been proposed for the evaluation of the sensitivity. We
              propose a new index and present a way for improving these
              sensitivity criteria. Some numerical experiments allow a first
              comparison of the efficiencies of these criteria.",
  journal  = "Neural Process. Letters",
  volume   =  2,
  number   =  6,
  pages    = "1--4",
  month    =  dec,
  year     =  1995
}

@ARTICLE{Owramipur2013-rq,
  title     = "Football result prediction with Bayesian network in Spanish
               {League-Barcelona} team",
  author    = "Owramipur, Farzin and Eskandarian, Parinaz and Mozneb, Faezeh
               Sadat",
  journal   = "International Journal of Computer Theory and Engineering",
  publisher = "IACSIT Press",
  volume    =  5,
  number    =  5,
  pages     = "812",
  year      =  2013
}

@ARTICLE{Leo2019-re,
  title     = "Machine Learning in Banking Risk Management: A Literature Review",
  author    = "Leo, Martin and Sharma, Suneel and Maddulety, K",
  abstract  = "There is an increasing influence of machine learning in business
               applications, with many solutions already implemented and many
               more being explored. Since the global financial crisis, risk
               management in banks has gained more prominence, and there has
               been a constant focus around how risks are being detected,
               measured, reported and managed. Considerable research in
               academia and industry has focused on the developments in banking
               and risk management and the current and emerging challenges.
               This paper, through a review of the available literature seeks
               to analyse and evaluate machine-learning techniques that have
               been researched in the context of banking risk management, and
               to identify areas or problems in risk management that have been
               inadequately explored and are potential areas for further
               research. The review has shown that the application of machine
               learning in the management of banking risks such as credit risk,
               market risk, operational risk and liquidity risk has been
               explored; however, it doesn't appear commensurate with the
               current industry level of focus on both risk management and
               machine learning. A large number of areas remain in bank risk
               management that could significantly benefit from the study of
               how machine learning can be applied to address specific
               problems.",
  journal   = "Risks",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  7,
  number    =  1,
  pages     = "29",
  month     =  mar,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Lewandowski2009-pj,
  title    = "Generating random correlation matrices based on vines and
              extended onion method",
  author   = "Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry",
  abstract = "We extend and improve two existing methods of generating random
              correlation matrices, the onion method of Ghosh and Henderson [S.
              Ghosh, S.G. Henderson, Behavior of the norta method for
              correlated random vector generation as the dimension increases,
              ACM Transactions on Modeling and Computer Simulation (TOMACS) 13
              (3) (2003) 276--294] and the recently proposed method of Joe [H.
              Joe, Generating random correlation matrices based on partial
              correlations, Journal of Multivariate Analysis 97 (2006)
              2177--2189] based on partial correlations. The latter is based on
              the so-called D-vine. We extend the methodology to any regular
              vine and study the relationship between the multiple correlation
              and partial correlations on a regular vine. We explain the onion
              method in terms of elliptical distributions and extend it to
              allow generating random correlation matrices from the same joint
              distribution as the vine method. The methods are compared in
              terms of time necessary to generate 5000 random correlation
              matrices of given dimensions.",
  journal  = "J. Multivar. Anal.",
  volume   =  100,
  number   =  9,
  pages    = "1989--2001",
  month    =  oct,
  year     =  2009,
  keywords = "Dependence vines; Correlation matrix; Partial correlation; Onion
              method"
}

@ARTICLE{Thadewald2007-uj,
  title     = "{Jarque--Bera} test and its competitors for testing normality --
               A power comparison",
  author    = "Thadewald, Thorsten and B{\"u}ning, Herbert",
  journal   = "J. Appl. Stat.",
  publisher = "Informa UK Limited",
  volume    =  34,
  number    =  1,
  pages     = "87--105",
  month     =  jan,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Brodersen2015-vk,
  title     = "Inferring causal impact using Bayesian structural time-series
               models",
  author    = "Brodersen, Kay H and Gallusser, Fabian and Koehler, Jim and
               Remy, Nicolas and Scott, Steven L",
  abstract  = "An important problem in econometrics and marketing is to infer
               the causal impact that a designed market intervention has
               exerted on an outcome metric over time. This paper proposes to
               infer causal impact on the basis of a diffusion-regression
               state-space model that predicts the counterfactual market
               response in a synthetic control that would have occurred had no
               intervention taken place. In contrast to classical
               difference-in-differences schemes, state-space models make it
               possible to (i) infer the temporal evolution of attributable
               impact, (ii) incorporate empirical priors on the parameters in a
               fully Bayesian treatment, and (iii) flexibly accommodate
               multiple sources of variation, including local trends,
               seasonality and the time-varying influence of contemporaneous
               covariates. Using a Markov chain Monte Carlo algorithm for
               posterior inference, we illustrate the statistical properties of
               our approach on simulated data. We then demonstrate its
               practical utility by estimating the causal effect of an online
               advertising campaign on search-related site visits. We discuss
               the strengths and limitations of state-space models in enabling
               causal attribution in those settings where a randomised
               experiment is unavailable. The CausalImpact R package provides
               an implementation of our approach.",
  journal   = "aoas",
  publisher = "Institute of Mathematical Statistics",
  volume    =  9,
  number    =  1,
  pages     = "247--274",
  month     =  mar,
  year      =  2015,
  keywords  = "advertising; Causal inference; counterfactual; difference in
               differences; econometrics; market research; observational;
               synthetic control;",
  language  = "en"
}

@ARTICLE{noauthor_undated-cj,
  title = "{HierarchicalCausal.pdf}"
}

@BOOK{Pearl2009-uo,
  title     = "Causality",
  author    = "Pearl, Judea",
  abstract  = "Written by one of the preeminent researchers in the field, this
               book provides a comprehensive exposition of modern analysis of
               causation. It shows how causality has grown from a nebulous
               concept into a mathematical theory with significant applications
               in the fields of statistics, artificial intelligence, economics,
               philosophy, cognitive science, and the health and social
               sciences. Judea Pearl presents and unifies the probabilistic,
               manipulative, counterfactual, and structural approaches to
               causation and devises simple mathematical tools for studying the
               relationships between causal connections and statistical
               associations. The book will open the way for including causal
               analysis in the standard curricula of statistics, artificial
               intelligence, business, epidemiology, social sciences, and
               economics. Students in these fields will find natural models,
               simple inferential procedures, and precise mathematical
               definitions of causal concepts that traditional texts have
               evaded or made unduly complicated. The first edition of
               Causality has led to a paradigmatic change in the way that
               causality is treated in statistics, philosophy, computer
               science, social science, and economics. Cited in more than 5,000
               scientific publications, it continues to liberate scientists
               from the traditional molds of statistical thinking. In this
               revised edition, Judea Pearl elucidates thorny issues, answers
               readers' questions, and offers a panoramic view of recent
               advances in this field of research. Causality will be of
               interests to students and professionals in a wide variety of
               fields. Anyone who wishes to elucidate meaningful relationships
               from data, predict effects of actions and policies, assess
               explanations of reported events, or form theories of causal
               understanding and causal speech will find this book stimulating
               and invaluable.",
  publisher = "Cambridge University Press",
  month     =  sep,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Kim2021-rp,
  title    = "Causal graphical views of fixed effects and random effects models",
  author   = "Kim, Yongnam and Steiner, Peter M",
  abstract = "Despite the long-standing discussion on fixed effects (FE) and
              random effects (RE) models, how and under what conditions both
              methods can eliminate unmeasured confounding bias has not yet
              been widely understood in practice. Using a simple
              pretest-posttest design in a linear setting, this paper
              translates the conventional algebraic formalization of FE and RE
              models into causal graphs and provides intuitively accessible
              graphical explanations about their data-generating and
              bias-removing processes. The proposed causal graphs highlight
              that FE and RE models consider different data-generating models.
              RE models presume a data-generating model that is identical to a
              randomized controlled trial, while FE models allow for unobserved
              time-invariant treatment-outcome confounding. Augmenting regular
              causal graphs that describe data-generating processes by adding
              the computational structures of FE and RE estimators, the paper
              visualizes how FE estimators (gain score and deviation score
              estimators) and RE estimators (quasi-deviation score estimators)
              offset unmeasured confounding bias. In contrast to standard
              regression or matching estimators that reduce confounding bias by
              blocking non-causal paths via conditioning, FE and RE estimators
              offset confounding bias by deliberately creating new non-causal
              paths and associations of opposite sign. Though FE and RE
              estimators are similar in their bias-offsetting mechanisms, the
              augmented graphs reveal their subtle differences that can result
              in different biases in observational studies.",
  journal  = "Br. J. Math. Stat. Psychol.",
  volume   =  74,
  number   =  2,
  pages    = "165--183",
  month    =  may,
  year     =  2021,
  keywords = "bias offsetting; causal graph; demeaning; fixed effect; gain
              score; random effect",
  language = "en"
}

@BOOK{Cunningham2021-mj,
  title     = "Causal inference: The mixtape",
  author    = "Cunningham, Scott",
  abstract  = "Ashenfelter, O., 415--423 Assumptions: common support, 209, 221;
               conditional independence assumption (CIA), 176, 208--209,
               221n14, 224--226, 228, 240; continuity, 245, 255--260; defined,
               101; identifying, 182--183, 393; independence, 135--140;
               monotonicity, 350; parallel trend, 413, 422, 425--433; parallel
               trends, 413; stable unit treatment value assumption (SUTVA),
               140--141, 348, 348n13, 478n16",
  publisher = "Yale University Press",
  year      =  2021
}

@ARTICLE{Elwert2014-gk,
  title    = "Endogenous Selection Bias: The Problem of Conditioning on a
              Collider Variable",
  author   = "Elwert, Felix and Winship, Christopher",
  abstract = "Endogenous selection bias is a central problem for causal
              inference. Recognizing the problem, however, can be difficult in
              practice. This article introduces a purely graphical way of
              characterizing endogenous selection bias and of understanding its
              consequences (Hern{\'a}n et al. 2004). We use causal graphs
              (direct acyclic graphs, or DAGs) to highlight that endogenous
              selection bias stems from conditioning (e.g., controlling,
              stratifying, or selecting) on a so-called collider variable,
              i.e., a variable that is itself caused by two other variables,
              one that is (or is associated with) the treatment and another
              that is (or is associated with) the outcome. Endogenous selection
              bias can result from direct conditioning on the outcome variable,
              a post-outcome variable, a post-treatment variable, and even a
              pre-treatment variable. We highlight the difference between
              endogenous selection bias, common-cause confounding, and
              overcontrol bias and discuss numerous examples from social
              stratification, cultural sociology, social network analysis,
              political sociology, social demography, and the sociology of
              education.",
  journal  = "Annu. Rev. Sociol.",
  volume   =  40,
  pages    = "31--53",
  month    =  jul,
  year     =  2014,
  keywords = "causality; confounding; directed acyclic graphs; identification;
              selection",
  language = "en"
}

@ARTICLE{Wright1934-kq,
  title     = "The Method of Path Coefficients",
  author    = "Wright, Sewall",
  abstract  = "The Annals of Mathematical Statistics",
  journal   = "aoms",
  publisher = "Institute of Mathematical Statistics",
  volume    =  5,
  number    =  3,
  pages     = "161--215",
  month     =  sep,
  year      =  1934,
  language  = "en"
}

@ARTICLE{Spiegler2016-kx,
  title     = "Bayesian Networks and Boundedly Rational Expectations",
  author    = "Spiegler, Ran",
  abstract  = "Abstract. I present a framework for analyzing decision making
               under imperfect understanding of correlation structures and
               causal relations. A decision maker (DM",
  journal   = "Q. J. Econ.",
  publisher = "Oxford Academic",
  volume    =  131,
  number    =  3,
  pages     = "1243--1290",
  month     =  aug,
  year      =  2016
}

@TECHREPORT{Cantone2019-ne,
  title       = "Thinking beyond borders: how important are reciprocity
                 arrangements for the use of sectoral capital buffers",
  author      = "Cantone, David and Jahn, Nadya and Rancoita, Elena",
  abstract    = "The European Central Bank (ECB) is the central bank of the 19
                 European Union countries which have adopted the euro. Our main
                 task is to maintain price stability in the euro area and so
                 preserve the purchasing power of the single currency.",
  institution = "European Central Bank",
  year        =  2019
}

@ARTICLE{Young2019-bi,
  title     = "Channeling fisher: Randomization tests and the statistical
               insignificance of seemingly significant experimental results",
  author    = "Young, Alwyn",
  abstract  = "Abstract. I follow R. A. Fisher'sThe Design of Experiments
               (1935), using randomization statistical inference to test the
               null hypothesis of no treatment effects",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press (OUP)",
  volume    =  134,
  number    =  2,
  pages     = "557--598",
  month     =  may,
  year      =  2019,
  copyright = "https://academic.oup.com/journals/pages/open\_access/funder\_policies/chorus/standard\_publication\_model",
  language  = "en"
}

@BOOK{Wolpin2014-yf,
  title     = "The limits of inference without theory",
  author    = "Wolpin, Kenneth",
  editor    = "{MIT press}",
  abstract  = "The role of theory in ex ante policy evaluations and the limits
               that eschewing theory places on inference In this rigorous and
               well-crafted work, Kenneth Wolpin examines the role of theory in
               inferential empirical work in economics and the social sciences
               in general---that is, any research that uses raw data to go
               beyond the mere statement of fact or the tabulation of
               statistics. He considers in particular the limits that eschewing
               the use of theory places on inference.Wolpin finds that the
               absence of theory in inferential work that addresses
               microeconomic issues is pervasive. That theory is unnecessary
               for inference is exemplified by the expression ``let the data
               speak for themselves.'' This approach is often called ``reduced
               form.'' A more nuanced view is based on the use of experiments
               or quasi-experiments to draw inferences. Atheoretical approaches
               stand in contrast to what is known as the structuralist
               approach, which requires that a researcher specify an explicit
               model of economic behavior---that is, a theory. Wolpin offers a
               rigorous examination of both structuralist and nonstructuralist
               approaches. He first considers ex ante policy evaluation,
               highlighting the role of theory in the implementation of
               parametric and nonparametric estimation strategies. He
               illustrates these strategies with two examples, a wage tax and a
               school attendance subsidy, and summarizes the results from
               applications. He then presents a number of examples that
               illustrate the limits of inference without theory: the effect of
               unemployment benefits on unemployment duration; the effect of
               public welfare on women's labor market and demographic outcomes;
               the effect of school attainment on earnings; and a famous field
               experiment in education dealing with class size. Placing each
               example within the context of the broader literature, he
               contrasts them to recent work that relies on theory for
               inference.",
  publisher = "MIT Press",
  year      =  2014
}

@ARTICLE{Imai2019-my,
  title     = "When should we use unit fixed effects regression models for
               causal inference with longitudinal data?",
  author    = "Imai, Kosuke and Kim, In Song",
  abstract  = "Abstract Many researchers use unit fixed effects regression
               models as their default methods for causal inference with
               longitudinal data. We show that the ability of these models to
               adjust for unobserved time-invariant confounders comes at the
               expense of dynamic causal relationships, which are permitted
               under an alternative selection-on-observables approach. Using
               the nonparametric directed acyclic graph, we highlight two key
               causal identification assumptions of unit fixed effects models:
               Past treatments do not directly influence current outcome, and
               past outcomes do not affect current treatment. Furthermore, we
               introduce a new nonparametric matching framework that elucidates
               how various unit fixed effects models implicitly compare treated
               and control observations to draw causal inference. By
               establishing the equivalence between matching and weighted unit
               fixed effects estimators, this framework enables a diverse set
               of identification strategies to adjust for unobservables in the
               absence of dynamic causal relationships between treatment and
               outcome variables. We illustrate the proposed methodology
               through its application to the estimation of GATT membership
               effects on dyadic trade volume.",
  journal   = "Am. J. Pol. Sci.",
  publisher = "Wiley",
  volume    =  63,
  number    =  2,
  pages     = "467--490",
  month     =  apr,
  year      =  2019,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

@ARTICLE{Abadie2017-yz,
  title         = "The Risk of Machine Learning",
  author        = "Abadie, Alberto and Kasy, Maximilian",
  abstract      = "Many applied settings in empirical economics involve
                   simultaneous estimation of a large number of parameters. In
                   particular, applied economists are often interested in
                   estimating the effects of many-valued treatments (like
                   teacher effects or location effects), treatment effects for
                   many groups, and prediction models with many regressors. In
                   these settings, machine learning methods that combine
                   regularized estimation and data-driven choices of
                   regularization parameters are useful to avoid over-fitting.
                   In this article, we analyze the performance of a class of
                   machine learning estimators that includes ridge, lasso and
                   pretest in contexts that require simultaneous estimation of
                   many parameters. Our analysis aims to provide guidance to
                   applied researchers on (i) the choice between regularized
                   estimators in practice and (ii) data-driven selection of
                   regularization parameters. To address (i), we characterize
                   the risk (mean squared error) of regularized estimators and
                   derive their relative performance as a function of simple
                   features of the data generating process. To address (ii), we
                   show that data-driven choices of regularization parameters,
                   based on Stein's unbiased risk estimate or on
                   cross-validation, yield estimators with risk uniformly close
                   to the risk attained under the optimal (unfeasible) choice
                   of regularization parameters. We use data from recent
                   examples in the empirical economics literature to illustrate
                   the practical applicability of our results.",
  month         =  mar,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1703.10935"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Pearl2010-ds,
  title     = "The mathematics of causal relations",
  author    = "Pearl, Judea",
  abstract  = "Almost two decades have passed since Paul Holland published his
               highly cited review paper on the Neyman-Rubin approach to causal
               inference [Holland, 1986]. Our understanding of causal inference
               has since increased severalfold, due primarily to ‚Ä¶",
  journal   = "Causality and Psychopathology: Finding the Determinants of
               Disorders and their Cures (P. Shrout, K. Keyes and K. Ornstein,
               eds. ). Oxford University Press, Corvallis, OR",
  publisher = "cs.ucla.edu",
  pages     = "47--65",
  year      =  2010
}

@ARTICLE{Lee2015-xv,
  title    = "Directional shadow price estimation of {CO2}, {SO2} and {NOx} in
              the United States coal power industry 1990--2010",
  author   = "Lee, Chia-Yen and Zhou, Peng",
  abstract = "Shadow prices, also termed marginal abatement costs, provide
              valuable guidelines to support environmental regulatory policies
              for CO2, SO2 and NOx, the key contributors to climate change.
              This paper complements the existing models and describes a
              directional marginal productivity (DMP) approach to estimate
              directional shadow prices (DSPs) which present substitutability
              among three emissions and are jointly estimated. We apply the
              method to a case study of CO2, SO2 and NOx produced by coal power
              plants operating between 1990 and 2010 in the United States. We
              find that DSP shows 1.1 times the maximal shadow prices estimated
              in the current literature. We conclude that estimating the shadow
              prices of each by-product separately may lead to an
              overestimation of the marginal productivity and an
              underestimation of the shadow prices.",
  journal  = "Energy Econ.",
  volume   =  51,
  pages    = "493--502",
  month    =  sep,
  year     =  2015,
  keywords = "Shadow price; Emissions trading; Directional distance function;
              Marginal abatement cost; Coal power plant"
}

@ARTICLE{Wei2013-bz,
  title    = "An empirical analysis of the {CO2} shadow price in Chinese
              thermal power enterprises",
  author   = "Wei, Chu and L{\"o}schel, Andreas and Liu, Bing",
  abstract = "This paper estimates the shadow price of CO2 and explores its
              determinants for thermal power enterprises in China. Using a
              parametric quadratic directional distance function, we evaluate
              the inefficiency and shadow price of CO2 for 124 power
              enterprises in 2004, applying deterministic and econometric
              methods. A regression analysis is undertaken to examine the
              factors that drive shadow prices. Our results indicate that there
              are large inefficiencies in terms of electricity production and
              CO2 emissions. The shadow price is a negative function of firm
              size, age, and coal share, and is positively correlated with the
              technology level. This correlation between shadow prices and its
              determinants is not sensitive to changing assumptions regarding
              directional vectors, although such changes do alter the
              distribution of shadow prices. The large variation witnessed in
              shadow prices across power enterprises argues in favor of
              market-based regulation such as an emissions trading system as
              opposed to the command-and-control regulation currently used in
              China to minimize overall abatement cost.",
  journal  = "Energy Econ.",
  volume   =  40,
  pages    = "22--31",
  month    =  nov,
  year     =  2013,
  keywords = "CO; Shadow price; Directional distance function; Quadratic
              function; China; Thermal power enterprise"
}

@ARTICLE{Verhaest2009-hr,
  title    = "Objective over-education and worker well-being: A shadow price
              approach",
  author   = "Verhaest, Dieter and Omey, Eddy",
  abstract = "This paper examines, for a sample of Flemish school leavers, the
              relation between objective over-education and job satisfaction by
              applying a shadow price approach. We differentiate between direct
              effects of over-education and indirect effects via other job
              characteristics that are associated with over-education.
              Additional fixed-effects estimates are executed to account for
              individual heterogeneity. The utility consequences of
              over-education are found to be large and cannot be compensated by
              a reasonable wage increase at the start of the first employment.
              These outcomes suggest that, at labour-market entry,
              over-education is largely involuntary, and is likely to induce
              negative productivity costs. The negative consequences of
              over-education are also found to diminish with years of work
              experience.",
  journal  = "J. Econ. Psychol.",
  volume   =  30,
  number   =  3,
  pages    = "469--481",
  month    =  jun,
  year     =  2009,
  keywords = "Over-education; Mismatch; Under-employment; Job satisfaction;
              Well-being; Shadow price"
}

@ARTICLE{Luhmann2020-vw,
  title    = "Why carbon emissions pricing and carbon shadow pricing both make
              sense",
  author   = "Luhmann, Hans-Jochen and Balk, Sabine and Dembowski, Hans",
  abstract = "``Carbon emissions pricing'' means that state institutions charge
              a price for emissions. By contrast, ``carbon shadow pricing''
              means that the long-term impacts of emissions are factored into
              the planning of major projects even if current market prices do
              not reflect those impacts yet. Shadow prices are fictional, but
              have a very real impact. A high shadow price for fuel, for
              example, can make a power plant unviable even though it may look
              attractive at current market prices. The climate economist
              Hans-Jochen Luhmann argues that both approaches to carbon pricing
              make sense. They actually complement one another.",
  journal  = "Development and Cooperation",
  year     =  2020
}

@ARTICLE{Canarella2012-cq,
  title     = "Unit Roots and Structural Change: An Application to {US} House
               Price Indices",
  author    = "Canarella, Giorgio and Miller, Stephen and Pollard, Stephen",
  abstract  = "This paper employs time-series analysis to investigate the price
               dynamics of the house price indices included in the
               S\&P/Case?Shiller Composite10 index and the validity of the
               ?ripple effect?, following the approach outlined by Meen (1999).
               More specifically, the paper first considers the time-series
               properties of the capital gain from the sale of houses. That is,
               it examines whether shocks to the capital gain series produce
               permanent or transitory changes. In general, the findings lack
               uniformity and depend upon the assumptions imposed by the
               testing procedures. Secondly, it considers the time-series
               properties of the ratio of regional house price indices to the
               Composite10 index. That is, it examines whether shocks to these
               house price ratios exhibit trend reversion. The tests of this
               ?ripple effect? also display conflicting evidence.",
  journal   = "Urban Stud.",
  publisher = "SAGE Publications Ltd",
  volume    =  49,
  number    =  4,
  pages     = "757--776",
  month     =  mar,
  year      =  2012
}

@ARTICLE{Clapp2004-pg,
  title     = "A semiparametric method for estimating local house price indices",
  author    = "Clapp, John M",
  abstract  = "Spatial autoregressive hedonic models utilize house prices
               lagged in space and time to produce local house price indices,
               for example, the spatial and temporal autoregressive (STAR)
               model might be used this way. This paper complements these
               models with a semiparametric approach, the Local Regression
               Model (LRM). The greater flexibility of the LRM may allow it to
               identify space?time asymmetries missed by other models. The LRM
               is fitted to 49,511 sales from 1972Q1 to 1991Q2 in Fairfax
               County, Virginia. The local price indices display plausible and
               significant variations over space and time. The LRM price
               indices in selected neighborhoods are shown to differ
               significantly from those in some other neighborhoods. A new
               method for estimating standard errors addresses an overlooked
               problem common to all local price indices: how to evaluate the
               amount of noise in the estimates. Out-of-sample prediction
               errors demonstrate that LRM adds significant information to the
               hedonic model.",
  journal   = "Real Estate Econ.",
  publisher = "Wiley",
  volume    =  32,
  number    =  1,
  pages     = "127--160",
  month     =  mar,
  year      =  2004,
  language  = "en"
}

@ARTICLE{McMillen2014-dk,
  title   = "Local quantile house price indices",
  author  = "McMillen, Daniel",
  journal = "J. Urban Econ.",
  year    =  2014
}

@BOOK{Erwin_Diewert2020-pu,
  title     = "Property Price Index: Theory and Practice",
  author    = "Erwin Diewert, W and Nishimura, Kiyohiko G and Shimizu, Chihiro
               and Watanabe, Tsutomu",
  abstract  = "This book answers the question of how exactly property price
               indexes should be constructed.The formation and collapse of
               property bubbles has had a profound impact on the economic
               administration of many nations. The property price bubble that
               began around the mid-1980s in Japan has been called the 20th
               century's biggest bubble. In its aftermath, the country faced a
               period of long-term economic stagnation dubbed the ``lost
               decade.'' Sweden and the United States have also faced collapses
               of property bubbles in the 20th and early 21st centuries,
               respectively.It has been pointed out that the ``information
               gap'' that existed between policy-making authorities and the
               property (including housing) and financial markets was a
               problem. In 2009, the IMF proposed the creation of a housing
               price index to the G20 in order to fill this information gap,
               and the proposal was adopted. Furthermore, in 2011, it was
               suggested that the next economic crisis would be caused by a
               bubble in commercial property prices, and it was decided to
               create a commercial property index as well.This book provides
               practical examples of how the theory of property price indexes
               can be applied to the issues of property as a non-homogenous
               good and a technological and environmental change.",
  publisher = "Springer Japan",
  month     =  jan,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Milunovich2020-nd,
  title     = "Forecasting Australia's real house price index: A comparison of
               time series and machine learning methods",
  author    = "Milunovich, George",
  abstract  = "Abstract We employ 47 different algorithms to forecast
               Australian log real house prices and growth rates, and compare
               their ability to produce accurate out-of-sample predictions. The
               algorithms, which are specified in both single- and
               multi-equation frameworks, consist of traditional time series
               models, machine learning (ML) procedures, and deep learning
               neural networks. A method is adopted to compute iterated
               multistep forecasts from nonlinear ML specifications. While the
               rankings of forecast accuracy depend on the length of the
               forecast horizon, as well as on the choice of the dependent
               variable (log price or growth rate), a few generalizations can
               be made. For one- and two-quarter-ahead forecasts we find a
               large number of algorithms that outperform the random walk with
               drift benchmark. We also report several such outperformances at
               longer horizons of four and eight quarters, although these are
               not statistically significant at any conventional level. Six of
               the eight top forecasts (4 horizons ? 2 dependent variables) are
               generated by the same algorithm, namely a linear support vector
               regressor (SVR). The other two highest ranked forecasts are
               produced as simple mean forecast combinations. Linear
               autoregressive moving average and vector autoregression models
               produce accurate olne-quarter-ahead predictions, while forecasts
               generated by deep learning nets rank well across medium and long
               forecast horizons.",
  journal   = "J. Forecast.",
  publisher = "Wiley",
  volume    =  39,
  number    =  7,
  pages     = "1098--1118",
  month     =  nov,
  year      =  2020,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

@MISC{Wang2018-li,
  title   = "A new machine learning approach to house price estimation",
  author  = "Wang, Changchunwang and Wu, Hui",
  journal = "New Trends in Mathematical Science",
  volume  =  4,
  number  =  6,
  pages   = "165--171",
  year    =  2018
}

@ARTICLE{Park2015-lw,
  title    = "Using machine learning algorithms for housing price prediction:
              The case of Fairfax County, Virginia housing data",
  author   = "Park, Byeonghwa and Bae, Jae Kwon",
  abstract = "House sales are determined based on the Standard \& Poor's
              Case-Shiller home price indices and the housing price index of
              the Office of Federal Housing Enterprise Oversight (OFHEO). These
              reflect the trends of the US housing market. In addition to these
              housing price indices, the development of a housing price
              prediction model can greatly assist in the prediction of future
              housing prices and the establishment of real estate policies.
              This study uses machine learning algorithms as a research
              methodology to develop a housing price prediction model. To
              improve the accuracy of housing price prediction, this paper
              analyzes the housing data of 5359 townhouses in Fairfax County,
              Virginia, gathered by the Multiple Listing Service (MLS) of the
              Metropolitan Regional Information Systems (MRIS). We develop a
              housing price prediction model based on machine learning
              algorithms such as C4.5, RIPPER, Na{\"\i}ve Bayesian, and
              AdaBoost and compare their classification accuracy performance.
              We then propose an improved housing price prediction model to
              assist a house seller or a real estate agent make better informed
              decisions based on house price valuation. The experiments
              demonstrate that the RIPPER algorithm, based on accuracy,
              consistently outperforms the other models in the performance of
              housing price prediction.",
  journal  = "Expert Syst. Appl.",
  volume   =  42,
  number   =  6,
  pages    = "2928--2934",
  month    =  apr,
  year     =  2015,
  keywords = "Housing price index; Housing price prediction model; Machine
              learning algorithms; C4.5; RIPPER; Na{\"\i}ve Bayesian; AdaBoost"
}

@MISC{Briscoe2004-ut,
  title   = "House price indices",
  author  = "Briscoe, Simon",
  journal = "Significance",
  volume  =  1,
  number  =  1,
  pages   = "18--20",
  year    =  2004
}

@ARTICLE{Gupta2021-uo,
  title    = "Machine Learning Predictions of Housing Market Synchronization
              across {US} States: The Role of Uncertainty",
  author   = "Gupta, Rangan and Marfatia, Hardik A and Pierdzioch, Christian
              and Salisu, Afees A",
  abstract = "We analyze the role of macroeconomic uncertainty in predicting
              synchronization in housing price movements across all the United
              States (US) states plus District of Columbia (DC). We first use a
              Bayesian dynamic factor model to decompose the house price
              movements into a national, four regional (Northeast, South,
              Midwest, and West), and state-specific factors. We then study the
              ability of macroeconomic uncertainty in forecasting the
              comovements in housing prices, by controlling for a wide-array of
              predictors, such as factors derived from a large macroeconomic
              dataset, oil shocks, and financial market-related uncertainties.
              To accommodate for multiple predictors and nonlinearities, we
              take a machine learning approach of random forests. Our results
              provide strong evidence of forecastability of the national house
              price factor based on the information content of macroeconomic
              uncertainties over and above the other predictors. This result
              also carries over, albeit by a varying degree, to the factors
              associated with the four census regions, and the overall house
              price growth of the US economy. Moreover, macroeconomic
              uncertainty is found to have predictive content for (stochastic)
              volatility of the national factor and aggregate US house price.
              Our results have important implications for policymakers and
              investors.",
  journal  = "J. Real Estate Fin. Econ.",
  month    =  jan,
  year     =  2021
}

@ARTICLE{Fan2006-ho,
  title     = "Determinants of House Price: A Decision Tree Approach",
  author    = "Fan, Gang-Zhi and Ong, Seow Eng and Koh, Hian Chye",
  abstract  = "The hedonic-based regression approach has been utilised
               extensively to investigate th relationship between house prices
               and housing characteristics. However, this approach is subject t
               criticisms arising from potential problems relating to
               fundamental model assumptions an estimation such as the
               identification of supply and demand, market disequilibrium, the
               selectio of independent variables, the choice of functional form
               of hedonic equation and marke segmentation. This study
               introduces and utilises an alternative approach-the decision tre
               approach, which is an important statistical pattern recognition
               tool. Using the Singapore resal public housing market as a case
               study, the article demonstrates the usefulness of this techniqu
               in examining the relationship between house prices and housing
               characteristics, identifying th significant determinants of
               housing prices and predicting housing prices. The built tree
               show that homebuyers are more concerned about the basic housing
               characteristics of two- and three room flats or four-room flats
               such as floor area, model type and flat age. However, homebuyer
               of five-room flats pay more attention to floor level in addition
               to the basic housin characteristics. In addition, homebuyers of
               executive apartments are less concerned about basi quantitative
               characteristics and have higher housing consumption expectations
               and pay mor attention to 'quality' and service characteristics
               such as recreational facilities and the livin environment.",
  journal   = "Urban Stud.",
  publisher = "SAGE Publications Ltd",
  volume    =  43,
  number    =  12,
  pages     = "2301--2315",
  month     =  nov,
  year      =  2006
}

@INCOLLECTION{Villhauer2021-sr,
  title     = "Machine Learning and Finance",
  booktitle = "Theories of Change: Change Leadership Tools, Models and
               Applications for Investing in Sustainable Development",
  author    = "Villhauer, Bernhard",
  editor    = "Wendt, Karen",
  abstract  = "The article provides a short overview of recent developments
               driven by the application of Artificial Intelligence (AI) or,
               more specifically, Machine Learning (ML) in the financial
               sector. The focus is on the practical consequences of ML use,
               especially at Pretrade analytics, Portfolio Management or in the
               field of service.",
  publisher = "Springer International Publishing",
  pages     = "383--389",
  year      =  2021,
  address   = "Cham"
}

@MISC{Hull2021-wu,
  title   = "Machine Learning and Economics",
  author  = "Hull, Isaiah",
  journal = "Machine Learning for Economics and Finance in TensorFlow 2",
  pages   = "61--86",
  year    =  2021
}

@MISC{Mathur2019-jh,
  title   = "Overview of Machine Learning in Finance",
  author  = "Mathur, Puneet",
  journal = "Machine Learning Applications Using Python",
  pages   = "259--270",
  year    =  2019
}

@MISC{Bracke_undated-ad,
  title   = "Machine Learning Explainability in Finance: An Application to
             Default Risk Analysis",
  author  = "Bracke, Philippe and Datta, Anupam and Jung, Carsten and Sen,
             Shayak",
  journal = "SSRN Electronic Journal"
}

@ARTICLE{Guegan2018-ak,
  title    = "Regulatory learning: How to supervise machine learning models? An
              application to credit scoring",
  author   = "Gu{\'e}gan, Dominique and Hassani, Bertrand",
  abstract = "The arrival of Big Data strategies is threatening the latest
              trends in financial regulation related to the simplification of
              models and the enhancement of the comparability of approaches
              chosen by financial institutions. Indeed, the intrinsic dynamic
              philosophy of Big Data strategies is almost incompatible with the
              current legal and regulatory framework as illustrated in this
              paper. Besides, as presented in our application to credit
              scoring, the model selection may also evolve dynamically forcing
              both practitioners and regulators to develop libraries of models,
              strategies allowing to switch from one to the other as well as
              supervising approaches allowing financial institutions to
              innovate in a risk mitigated environment. The purpose of this
              paper is therefore to analyse the issues related to the Big Data
              environment and in particular to machine learning models
              highlighting the issues present in the current framework
              confronting the data flows, the model selection process and the
              necessity to generate appropriate outcomes.",
  journal  = "The Journal of Finance and Data Science",
  volume   =  4,
  number   =  3,
  pages    = "157--171",
  month    =  sep,
  year     =  2018,
  keywords = "Data science; Credit scoring; Machine learning; AUC; Regulation"
}

@MISC{Targeted_undated-zl,
  title        = "{Erik!Allen}, !{PhD}!",
  author       = "Targeted, ! Which! Develops!",
  howpublished = "\url{https://cbey.yale.edu/sites/default/files/The%20Application%20of%20Machine%20Learning%20to%20Sustainable%20Finance.pdf}",
  note         = "Accessed: 2021-5-20"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Brown2020-ji,
  title     = "Portfolio performance attribution: A machine learning‚Äêbased
               approach",
  booktitle = "Machine Learning for Asset Management",
  author    = "Brown, Ryan and de Silva, Harindra and Neal, Patrick D",
  abstract  = "Summary Accurate performance attribution of an investment
               portfolio involves separation of systematic factors from the
               active management, so that the investor can distinguish the
               component of the return that is due purely to systematic factors
               from that which is due to the investor skill. The application of
               machine learning-based techniques has the potential to be
               invaluable in separating systematic returns from idiosyncratic
               returns in an investment portfolio. This chapter discusses a
               machine learning-based approach to address the issues associated
               with Brinson-based attribution. It introduces the notation and
               algorithm used in the chapter. This is followed by a description
               of the data and a discussion of the results stemming from the
               application of the process to common factor-based portfolio
               strategies that are currently used by investors. The chapter
               concludes with a summary of the results, as well as a discussion
               of potential applications of this approach to performance
               attribution.",
  publisher = "Wiley",
  pages     = "369--386",
  month     =  jun,
  year      =  2020
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Briere2020-ig,
  title     = "Modeling transaction costs when trades may be crowded: A
               Bayesian network using partially observable orders imbalance",
  booktitle = "Machine Learning for Asset Management",
  author    = "Bri{\`e}re, Marie and Lehalle, Charles‚Äêalbert and Nefedova,
               Tamara and Raboun, Amine",
  abstract  = "Summary This chapter shows the use of a Bayesian network to
               model transaction costs on US equity markets using ANcerno data,
               a large database of asset managers' instructions. The trading
               costs with the traditional measure of implementation shortfall,
               which measures the total amount of slippage a strategy might
               experience from its theoretical returns. The chapter reviews the
               existing literature on transaction cost modeling and Bayesian
               networks and presents the data. It provides empirical evidence
               of the influence of investors' trade size and order imbalance on
               transaction costs. The chapter describes the Bayesian network
               method and its application to transaction cost modeling. Order
               flow imbalance being only partially observable (on a subset of
               trades or with a delay), the chapter shows how to design a
               Bayesian network to infer its distribution and how to use this
               information to estimate transaction costs.",
  publisher = "Wiley",
  pages     = "387--430",
  month     =  jun,
  year      =  2020
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@MISC{Lohre2020-pf,
  title     = "Hierarchical risk parity: Accounting for tail dependencies in
               multi‚Äêasset multi‚Äêfactor allocations",
  booktitle = "Machine Learning for Asset Management",
  author    = "Lohre, Harald and Rother, Carsten and Sch{\"a}fer, Kilian Axel",
  abstract  = "Summary This chapter examines the use and merits of hierarchical
               clustering techniques in the context of multi-asset multi-factor
               investing. In particular, it contrasts these techniques with
               several competing risk-based allocation paradigms, such as 1/N,
               minimum-variance, standard risk parity and diversified risk
               parity. The chapter introduces hierarchical risk parity (HRP)
               strategies based on the Pearson correlation coefficient and also
               introduces hierarchical clustering based on the lower tail
               dependence coefficient. The chapter provides an overview of
               traditional risk-based allocation strategies and outlines a
               framework to measure and manage portfolio diversification. It
               examines the performance of the introduced HRP strategies
               relative to the traditional alternatives. The chapter discusses
               Meucci's approach to managing diversification, which serves to
               construct a diversified risk parity strategy based on economic
               factors.",
  publisher = "Wiley",
  pages     = "329--368",
  month     =  jun,
  year      =  2020
}

@ARTICLE{Menchero2005-bl,
  title     = "Optimized Geometric Attribution",
  author    = "Menchero, Jose",
  abstract  = "To address the problem of geometric performance attribution in a
               recently proposed framework of qualitative characteristics and
               quantitative properties, the approach reported here is to solve
               the problem in a two-step fashion. The first step is to define a
               set of attribution effects in their ?pure? geometric form.
               Because these pure effects will not aggregate in a residual-free
               manner, however, the second step is to perturb the geometric
               attribution effects in such a way that they deviate as little as
               possible, subject to the constraint that there be no residual,
               from their pure values. The resulting optimized attribution
               effects represent the most accurate formulation of geometric
               attribution.",
  journal   = "Financial Analysts Journal",
  publisher = "Routledge",
  volume    =  61,
  number    =  4,
  pages     = "60--69",
  month     =  jul,
  year      =  2005
}

@BOOK{Carroll2006-ve,
  title     = "Measurement Error in Nonlinear Models: A Modern Perspective,
               Second Edition",
  author    = "Carroll, Raymond J and Ruppert, David and Stefanski, Leonard A
               and Crainiceanu, Ciprian M",
  abstract  = "It's been over a decade since the first edition of Measurement
               Error in Nonlinear Models splashed onto the scene, and research
               in the field has certainly not cooled in the interim. In fact,
               quite the opposite has occurred. As a result, Measurement Error
               in Nonlinear Models: A Modern Perspective, Second Edition has
               been revamped and ex",
  publisher = "CRC Press",
  month     =  jun,
  year      =  2006,
  language  = "en"
}

@ARTICLE{Hullman2021-aw,
  title         = "To design interfaces for exploratory data analysis, we need
                   theories of graphical inference",
  author        = "Hullman, Jessica and Gelman, Andrew",
  abstract      = "Research and development in computer science and statistics
                   have produced increasingly sophisticated software interfaces
                   for interactive and exploratory analysis, optimized for easy
                   pattern finding and data exposure. But design philosophies
                   that emphasize exploration over other phases of analysis
                   risk confusing a need for flexibility with a conclusion that
                   exploratory visual analysis is inherently model-free and
                   cannot be formalized. We describe how without a grounding in
                   theories of human statistical inference, research in
                   exploratory visual analysis can lead to contradictory
                   interface objectives and representations of uncertainty that
                   can discourage users from drawing valid inferences. We
                   discuss how the concept of a model check in a Bayesian
                   statistical framework unites exploratory and confirmatory
                   analysis, and how this understanding relates to other
                   proposed theories of graphical inference. Viewing
                   interactive analysis as driven by model checks suggests new
                   directions for software and empirical research around
                   exploratory and visual analysis. For example, systems should
                   enable specifying and explicitly comparing data to null and
                   other reference distributions and better representations of
                   uncertainty. Implications of Bayesian and other theories of
                   graphical inference should be tested against outcomes of
                   interactive analysis by people to drive theory development.",
  month         =  apr,
  year          =  2021,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC",
  eprint        = "2104.02015"
}

@ARTICLE{Yao2021-mc,
  title         = "Bayesian hierarchical stacking",
  author        = "Yao, Yuling and Pir{\v s}, Gregor and Vehtari, Aki and
                   Gelman, Andrew",
  abstract      = "Stacking is a widely used model averaging technique that
                   yields asymptotically optimal prediction among all linear
                   averages. We show that stacking is most effective when the
                   model predictive performance is heterogeneous in inputs, so
                   that we can further improve the stacked mixture with a
                   hierarchical model. With the input-varying yet
                   partially-pooled model weights, hierarchical stacking
                   improves average and conditional predictions. Our Bayesian
                   formulation includes constant-weight (complete-pooling)
                   stacking as a special case. We generalize to incorporate
                   discrete and continuous inputs, other structured priors, and
                   time-series and longitudinal data. We demonstrate on several
                   applied problems.",
  month         =  jan,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "2101.08954"
}

@ARTICLE{Aczel2020-wr,
  title     = "Discussion points for Bayesian inference",
  author    = "Aczel, Balazs and Hoekstra, Rink and Gelman, Andrew and
               Wagenmakers, Eric-Jan and Klugkist, Irene G and Rouder, Jeffrey
               N and Vandekerckhove, Joachim and Lee, Michael D and Morey,
               Richard D and Vanpaemel, Wolf and Dienes, Zoltan and van
               Ravenzwaaij, Don",
  journal   = "Nat. Hum. Behav.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  4,
  number    =  6,
  pages     = "561--563",
  month     =  jun,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Liu2021-dj,
  title         = "Inference from non-random samples using Bayesian machine
                   learning",
  author        = "Liu, Yutao and Gelman, Andrew and Chen, Qixuan",
  abstract      = "We consider inference from non-random samples in data-rich
                   settings where high-dimensional auxiliary information is
                   available both in the sample and the target population, with
                   survey inference being a special case. We propose a
                   regularized prediction approach that predicts the outcomes
                   in the population using a large number of auxiliary
                   variables such that the ignorability assumption is
                   reasonable while the Bayesian framework is straightforward
                   for quantification of uncertainty. Besides the auxiliary
                   variables, inspired by Little \& An (2004), we also extend
                   the approach by estimating the propensity score for a unit
                   to be included in the sample and also including it as a
                   predictor in the machine learning models. We show through
                   simulation studies that the regularized predictions using
                   soft Bayesian additive regression trees yield valid
                   inference for the population means and coverage rates close
                   to the nominal levels. We demonstrate the application of the
                   proposed methods using two different real data applications,
                   one in a survey and one in an epidemiology study.",
  month         =  apr,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME",
  eprint        = "2104.05192"
}

@ARTICLE{Gelman2020-wh,
  title         = "Holes in Bayesian statistics",
  author        = "Gelman, Andrew and Yao, Yuling",
  abstract      = "Every philosophy has holes, and it is the responsibility of
                   proponents of a philosophy to point out these problems. Here
                   are a few holes in Bayesian data analysis: (1) the usual
                   rules of conditional probability fail in the quantum realm,
                   (2) flat or weak priors lead to terrible inferences about
                   things we care about, (3) subjective priors are incoherent,
                   (4) Bayesian decision picks the wrong model, (5) Bayes
                   factors fail in the presence of flat or weak priors, (6) for
                   Cantorian reasons we need to check our models, but this
                   destroys the coherence of Bayesian inference. Some of the
                   problems of Bayesian statistics arise from people trying to
                   do things they shouldn't be trying to do, but other holes
                   are not so easily patched. In particular, it may be a good
                   idea to avoid flat, weak, or conventional priors, but such
                   advice, if followed, would go against the vast majority of
                   Bayesian practice and requires us to confront the
                   fundamental incoherence of Bayesian inference. This does not
                   mean that we think Bayesian inference is a bad idea, but it
                   does mean that there is a tension between Bayesian logic and
                   Bayesian workflow which we believe can only be resolved by
                   considering Bayesian logic as a tool, a way of revealing
                   inevitable misfits and incoherences in our model
                   assumptions, rather than as an end in itself.",
  month         =  feb,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "2002.06467"
}

@ARTICLE{Greengard2020-zv,
  title         = "A Fast Linear Regression via {SVD} and Marginalization",
  author        = "Greengard, Philip and Gelman, Andrew and Vehtari, Aki",
  abstract      = "We describe a numerical scheme for evaluating the posterior
                   moments of Bayesian linear regression models with partial
                   pooling of the coefficients. The principal analytical tool
                   of the evaluation is a change of basis from coefficient
                   space to the space of singular vectors of the matrix of
                   predictors. After this change of basis and an analytical
                   integration, we reduce the problem of finding moments of a
                   density over k + m dimensions, to finding moments of an
                   m-dimensional density, where k is the number of coefficients
                   and k + m is the dimension of the posterior. Moments can
                   then be computed using, for example, MCMC, the trapezoid
                   rule, or adaptive Gaussian quadrature. An evaluation of the
                   SVD of the matrix of predictors is the dominant
                   computational cost and is performed once during the
                   precomputation stage. We demonstrate numerical results of
                   the algorithm. The scheme described in this paper
                   generalizes naturally to multilevel and multi-group
                   hierarchical regression models where normal-normal
                   parameters appear.",
  month         =  nov,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "stat.CO",
  eprint        = "2011.04829"
}

@UNPUBLISHED{Kelly2019-zh,
  title    = "Text Selection",
  author   = "Kelly, Bryan T and Manela, Asaf and Moreira, Alan",
  abstract = "Text data is ultra-high dimensional, which makes machine learning
              techniques indispensable for textual analysis. Text is often
              selected---journalists, speechwriters, and others craft messages
              to target their audiences' limited attention. We develop an
              economically motivated high dimensional selection model that
              improves learning from text (and from sparse counts data more
              generally). Our model is especially useful when the choice to
              include a phrase is more interesting than the choice of how
              frequently to repeat it. It allows for parallel estimation,
              making it computationally scalable. A first application revisits
              the partisanship of US congressional speech. We find that earlier
              spikes in partisanship manifested in increased repetition of
              different phrases, whereas the upward trend starting in the 1990s
              is due to entirely distinct phrase selection. Additional
              applications show how our model can backcast, nowcast, and
              forecast macroeconomic indicators using newspaper text, and that
              it substantially improves out-of-sample fit relative to
              alternative approaches.",
  month    =  nov,
  year     =  2019,
  keywords = "Text analysis, machine learning, selection model, high dimension
              forecast, multinomial regression, hurdle, zero inflation,
              partisanship, intermediary capital"
}

@ARTICLE{Speed1986-mp,
  title   = "Questions, Answers and Statistics",
  author  = "Speed, Terry",
  journal = "ICOTS 2",
  year    =  1986
}

@TECHREPORT{Kargar2020-pe,
  title       = "Corporate Bond Liquidity During the {COVID-19} Crisis",
  author      = "Kargar, Mahyar and Lester, Benjamin and Lindsay, David and
                 Liu, Shuo and Weill, Pierre-Olivier and Z{\'u}{\~n}iga, Diego",
  abstract    = "Founded in 1920, the NBER is a private, non-profit,
                 non-partisan organization dedicated to conducting economic
                 research and to disseminating research findings among
                 academics, public policy makers, and business professionals.",
  number      = "w27355",
  institution = "National Bureau of Economic Research",
  month       =  jun,
  year        =  2020
}

@ARTICLE{He2021-pb,
  title    = "Treasury inconvenience yields during the {COVID-19} crisis",
  author   = "He, Zhiguo and Nagel, Stefan and Song, Zhaogang",
  abstract = "In sharp contrast to most previous crisis episodes, the Treasury
              market experienced severe stress and illiquidity during the
              COVID-19 crisis, raising concerns that the safe-haven status of
              US Treasuries may be eroding. We document large shifts in
              Treasury ownership and temporary accumulation of Treasury and
              reverse repo positions on dealer balance sheets during this
              period. We build a dynamic equilibrium asset pricing model in
              which dealers subject to regulatory balance sheet constraints
              intermediate demand/supply shocks from habitat agents and provide
              repo financing to levered investors. The model predicts that
              Treasury inconvenience yields, measured as the spread between
              Treasuries and overnight-index swap rates (OIS), as well as
              spreads between dealers' reverse repo and repo rates, should be
              highly positive during the COVID-19 crisis, as is confirmed in
              the data. The same model framework, adapted to the institutional
              setting in 2007-2009, can also explain the negative Treasury-OIS
              spread observed during the Great Recession.",
  journal  = "J. financ. econ.",
  month    =  may,
  year     =  2021,
  keywords = "Habitat agents; Primary dealers; Repo; Safe asset; Treasury yield"
}

@UNPUBLISHED{DAmico2020-il,
  title    = "Impacts of the Fed Corporate Credit Facilities through the Lenses
              of {ETFs} and {CDX}",
  author   = "D'Amico, Stefania and Kurakula, Vamsidhar and Lee, Stephen",
  abstract = "In this study, we use the liquid and efficient bond ETF prices
              and CDX spreads to quantify the effects of the announcements of
              the Primary and Secondary Market Corporate Credit Facilities on
              the underlying corporate bonds. We find that those announcements
              triggered: (i) large and positive jumps in the prices of
              directly-eligible ETFs as well as ETFs holding eligible bonds and
              their close substitutes; (ii) a discrete drop in the perceived
              credit risk of eligible bonds especially following the April 9th
              announcement; (iii) a roaring back of investment-grade issuance
              and a pick-up in high-yield issuance. Importantly, across all
              ETFs in our sample, the magnitude of their price response does
              not seem directly related to the size of the reduction in either
              credit risk or liquidity risk, but rather appears to reflect
              mostly the eligibility of the ETF and its underlying bonds at the
              Federal Reserve facilities. This leads us to believe that the
              main factor driving the reaction to the announcements might be
              the elimination of ``disaster risk'' for eligible issuers.",
  month    =  may,
  year     =  2020,
  keywords = "Coronavirus crisis, Credit Easing, Corporate bond market"
}

@ARTICLE{Chen2021-dy,
  title   = "Dealers and the Dealer of Last Resort: Evidence from {MBS} Markets
             in the {COVID-19} Crisis",
  author  = "Chen, Jiakai and Liu, Haoyang and Sarkar, Asani and Song, Zhaogang",
  journal = "Federal Reserve Bank of New York Staff Reports",
  volume  =  933,
  year    =  2021
}

@ARTICLE{Tanwar2020-pb,
  title    = "Machine Learning Adoption in {Blockchain-Based} Smart
              Applications: The Challenges, and a Way Forward",
  author   = "Tanwar, Sudeep and Bhatia, Qasim and Patel, Pruthvi and Kumari,
              Aparna and Singh, Pradeep Kumar and Hong, Wei-Chiang",
  abstract = "In recent years, the emergence of blockchain technology (BT) has
              become a unique, most disruptive, and trending technology. The
              decentralized database in BT emphasizes data security and
              privacy. Also, the consensus mechanism in it makes sure that data
              is secured and legitimate. Still, it raises new security issues
              such as majority attack and double-spending. To handle the
              aforementioned issues, data analytics is required on blockchain
              based secure data. Analytics on these data raises the importance
              of arisen technology Machine Learning (ML). ML involves the
              rational amount of data to make precise decisions. Data
              reliability and its sharing are very crucial in ML to improve the
              accuracy of results. The combination of these two technologies
              (ML and BT) can provide highly precise results. In this paper, we
              present a detailed study on ML adoption for making BT-based smart
              applications more resilient against attacks. There are various
              traditional ML techniques, for instance, Support Vector Machines
              (SVM), clustering, bagging, and Deep Learning (DL) algorithms
              such as Convolutional Neural Network (CNN) and Long short-term
              memory (LSTM) can be used to analyse the attacks on a
              blockchain-based network. Further, we include how both the
              technologies can be applied in several smart applications such as
              Unmanned Aerial Vehicle (UAV), Smart Grid (SG), healthcare, and
              smart cities. Then, future research issues and challenges are
              explored. At last, a case study is presented with a conclusion.",
  journal  = "IEEE Access",
  volume   =  8,
  pages    = "474--488",
  year     =  2020,
  keywords = "Blockchain;Security;Machine
              learning;Taxonomy;Databases;Prediction
              algorithms;Malware;Blockchain;machine learning;smart grid;data
              security and privacy;data analytics;smart applications"
}

@ARTICLE{Puschmann2020-bd,
  title     = "Sustainable Digital Finance: The Role of {FinTech}, {InsurTech}
               \& Blockchain for Shaping the World for the Better",
  author    = "Puschmann, Thomas and Leifer, Larry",
  abstract  = "The digital finance revolution, which is often termed the
               ``FinTech Revolution'', is currently changing the financial
               services industry through innovative solutions, often developed
               by start- up and Big Tech companies. The financial sector is one
               of the key pillars of all economic trans- actions in general and
               sustainability specifically. As is so often the case: money is
               power. Inno- vative examples range from digital supply chain
               finance enabled agricultural and fashion value chains bridging
               Western and African as well as Asian countries, digital currency
               fueled smart meters for schools in Africa, digital investment
               marketplaces for forest tokens to crowdfunding enabled
               entrepreneurship in Asia and South America. Digital innovation
               in financial services is changing the way how financial
               resources can be accessed, distributed, and managed. This paper
               analyzes approaches from the emerging field of FinTech,
               InsurTech and blockchain at the intersection of sustainability.
               By reading through almost one-hundred and fifty papers from
               academia and practice as well as analyzing hundreds of start-ups
               in this field, the major building blocks were derived and a
               future research and innovation agenda was developed. The
               Appendix which is mentioned in this paper is downloadable
               online: www.sustainable-digital- finance.org. The results
               indicate that the topic is of great emerging interest. The areas
               of the focus are energy management, financial services,
               governments/NGOs, and transportation as well as some other
               relevant sectors that hold great potential and in which the
               combination of business and sustainability benefits is more
               obvious than ever enabled through innovative technology. The
               analysis reveals that future research and innovation areas might
               be fruitful in (1) novel ways for an integrated measuring of
               business and sustainability benefits, (2) novel sustainable
               cross-industry ecosystems and business models relying on new
               FinTech, InsurTech and blockchain enabled products and services,
               (3) new forms of organizational designs such as autonomous
               distributed organizations and processes built on new governance
               mechanisms and (4) novel FinTech, InsurTech, and blockchain
               applications for Sustainable Digital Finance Ecosystems.",
  publisher = "University of Zurich and Stanford University",
  year      =  2020,
  address   = "Zurich/Stanford",
  language  = "en"
}

@MISC{Yan2018-kr,
  title   = "{InsurTech} and {FinTech}: Banking and Insurance Enablement",
  author  = "Yan, Tan Choon and Schulte, Paul and Chuen, David Lee Kuo",
  journal = "Handbook of Blockchain, Digital Finance, and Inclusion, Volume 1",
  pages   = "249--281",
  year    =  2018
}

@MISC{Beaumont2019-di,
  title   = "{FinTech}, {InsurTech}, {PropTech}, and {RegTech}",
  author  = "Beaumont, Perry H",
  journal = "Digital Finance",
  pages   = "16--65",
  year    =  2019
}

@MISC{Downing2018-ay,
  title   = "Blockchain Startups - Unlikely Heroes for the Insurance Industry?",
  author  = "Downing, Becky",
  journal = "The InsurTech Book",
  pages   = "247--249",
  year    =  2018
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@BOOK{Barberis2019-na,
  title     = "The {REGTECH} Book: The Financial Technology Handbook for
               Investors, Entrepreneurs and Visionaries in Regulation",
  author    = "Barberis, Janos and Arner, Douglas W and Buckley, Ross P",
  abstract  = "The Regulatory Technology Handbook The transformational
               potential of RegTech has been confirmed in recent years with
               US$1.2 billion invested in start-ups (2017) and an expected
               additional spending of US$100 billion by 2020. Regulatory
               technology will not only provide efficiency gains for compliance
               and reporting functions, it will radically change market
               structure and supervision. This book, the first of its kind, is
               providing a comprehensive and invaluable source of information
               aimed at corporates, regulators, compliance professionals,
               start-ups and policy makers. The REGTECH Book brings into a
               single volume the curated industry expertise delivered by
               subject matter experts. It serves as a single reference point to
               understand the RegTech eco-system and its impact on the
               industry. Readers will learn foundational notions such as: ‚Ä¢ The
               economic impact of digitization and datafication of regulation ‚Ä¢
               How new technologies (Artificial Intelligence, Blockchain) are
               applied to compliance ‚Ä¢ Business use cases of RegTech for
               cost-reduction and new product origination ‚Ä¢ The future
               regulatory landscape affecting financial institutions,
               technology companies and other industries Edited by world-class
               academics and written by compliance professionals, regulators,
               entrepreneurs and business leaders, the RegTech Book represents
               an invaluable resource that paves the way for 21st century
               regulatory innovation.",
  publisher = "John Wiley \& Sons",
  month     =  aug,
  year      =  2019,
  language  = "en"
}

@BOOK{Chiu2021-mi,
  title     = "Routledge Handbook of Financial Technology and Law",
  author    = "Chiu, Iris H-Y and Deipenbrock, Gudula",
  abstract  = "Financial technology is rapidly changing and shaping financial
               services and markets. These changes are considered making the
               future of finance a digital one.This Handbook analyses
               developments in the financial services, products and markets
               that are being reshaped by technologically driven changes with a
               view to their policy, regulatory, supervisory and other legal
               implications. The Handbook aims to illustrate the crucial role
               the law has to play in tackling the revolutionary developments
               in the financial sector by offering a framework of legally
               enforceable principles and values in which such innovations
               might take place without threatening the acquis of financial
               markets law and more generally the rule of law and basic human
               rights. With contributions from international leading experts,
               topics will include: Policy, High-level Principles, Trends and
               Perspectives Fintech and Lending Fintech and Payment Services
               Fintech, Investment and Insurance Services Fintech, Financial
               Inclusion and Sustainable Finance Cryptocurrencies and
               Cryptoassets Markets and Trading Regtech and Suptech This
               Handbook will be of great relevance for practitioners and
               students alike, and a first reference point for academics
               researching in the fields of banking and financial markets law.",
  publisher = "Routledge",
  month     =  apr,
  year      =  2021,
  language  = "en"
}

@MISC{Rudin2019-td,
  title   = "Stop explaining black box machine learning models for high stakes
             decisions and use interpretable models instead",
  author  = "Rudin, Cynthia",
  journal = "Nature Machine Intelligence",
  volume  =  1,
  number  =  5,
  pages   = "206--215",
  year    =  2019
}

@MISC{Lorentzen_undated-jl,
  title   = "Peeking into the Black Box: An Actuarial Case Study for
             Interpretable Machine Learning",
  author  = "Lorentzen, Christian and Mayer, Michael",
  journal = "SSRN Electronic Journal"
}

@ARTICLE{Smith2019-vp,
  title     = "Be Wary of {Black-Box} Trading Algorithms",
  author    = "Smith, Gary",
  journal   = "The Journal of Investing",
  publisher = "Institutional Investor Journals Umbrella",
  volume    =  28,
  number    =  5,
  pages     = "7--15",
  year      =  2019
}

@ARTICLE{Aggarwal2016-xk,
  title     = "Model risk -- daring to open up the black box",
  author    = "Aggarwal, A and Beck, M B and Cann, M and Ford, T and Georgescu,
               D and Morjaria, N and Smith, A and Taylor, Y and Tsanakas, A and
               Witts, L and Ye, I",
  abstract  = "AbstractWith the increasing use of complex quantitative models
               in applications throughout the financial world, model risk has
               become a major concern. Such risk is generated by the potential
               inaccuracy and inappropriate use of models in business
               applications, which can lead to substantial financial losses and
               reputational damage. In this paper, we deal with the management
               and measurement of model risk. First, a model risk framework is
               developed, adapting concepts such as risk appetite, monitoring,
               and mitigation to the particular case of model risk. The
               usefulness of such a framework for preventing losses associated
               with model risk is demonstrated through case studies. Second, we
               investigate the ways in which different ways of using and
               perceiving models within an organisation both lead to different
               model risks. We identify four distinct model cultures and argue
               that in conditions of deep model uncertainty, each of those
               cultures makes a valuable contribution to model risk governance.
               Thus, the space of legitimate challenges to models is expanded,
               such that, in addition to a technical critique, operational and
               commercial concerns are also addressed. Third, we discuss
               through the examples of proxy modelling, longevity risk, and
               investment advice, common methods and challenges for quantifying
               model risk. Difficulties arise in mapping model errors to actual
               financial impact. In the case of irreducible model uncertainty,
               it is necessary to employ a variety of measurement approaches,
               based on statistical inference, fitting multiple models, and
               stress and scenario analysis.",
  journal   = "Br. Actuar. J.",
  publisher = "Cambridge University Press (CUP)",
  volume    =  21,
  number    =  2,
  pages     = "229--296",
  month     =  jul,
  year      =  2016,
  keywords  = "Model; Model Risk; Model Error; Model Uncertainty; Risk Culture",
  language  = "en"
}

@ARTICLE{Smith2020-tb,
  title     = "Practical Applications of Be Wary of {Black-Box} Trading
               Algorithms",
  author    = "Smith, Gary",
  journal   = "Practical Applications",
  publisher = "Institutional Investor Journals Umbrella",
  volume    =  7,
  number    =  4,
  pages     = "1--4",
  year      =  2020
}

@MISC{noauthor_2021-qi,
  title        = "Machine learning in {UK} financial services",
  abstract     = "The Bank of England and Financial Conduct Authority conducted
                  a joint survey in 2019 to better understand the current use
                  of Machine Learning in UK financial services.",
  month        =  may,
  year         =  2021,
  howpublished = "\url{https://www.bankofengland.co.uk/report/2019/machine-learning-in-uk-financial-services}",
  note         = "Accessed: 2021-6-1"
}

@ARTICLE{Koont2020-in,
  title   = "{ETFs} in Liquidity Provision",
  author  = "Koont, Naz and Ma, Yiming and Zeng, Yao",
  journal = "Available at SSRN 3796474",
  year    =  2020
}

@INPROCEEDINGS{Brunnermeier2020-zn,
  title     = "Corporate debt overhang and credit policy",
  booktitle = "{BPEA} conference",
  author    = "Brunnermeier, Markus and Krishnamurthy, Arvind and {Others}",
  year      =  2020
}

@ARTICLE{Bryzgalova2019-jv,
  title     = "Forest through the trees: Building cross-sections of stock
               returns",
  author    = "Bryzgalova, Svetlana and Pelger, Markus and Zhu, Jason",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2019,
  language  = "en"
}

@INCOLLECTION{Chang2020-xa,
  title     = "Application of filtering methods in asset pricing",
  booktitle = "Handbook of Financial Econometrics, Mathematics, Statistics, and
               Machine Learning",
  author    = "Chang, Hao and Wu, Yangru",
  publisher = "WORLD SCIENTIFIC",
  pages     = "2303--2321",
  month     =  sep,
  year      =  2020
}

@INCOLLECTION{Shih2020-du,
  title     = "The evolution of capital asset pricing models: Update and
               extension",
  booktitle = "Handbook of Financial Econometrics, Mathematics, Statistics, and
               Machine Learning",
  author    = "Shih, Yi-Cheng and Chen, Sheng-Syan and Lee, Cheng Few and Chen,
               Po-Jung",
  publisher = "WORLD SCIENTIFIC",
  pages     = "4149--4207",
  month     =  sep,
  year      =  2020
}

@INCOLLECTION{Lee2020-el,
  title     = "Asset pricing with disequilibrium price adjustment: Theory and
               empirical evidence",
  booktitle = "Handbook of Financial Econometrics, Mathematics, Statistics, and
               Machine Learning",
  author    = "Lee, Cheng Few and Tsai, Chiung-Min and Lee, Alice C",
  publisher = "WORLD SCIENTIFIC",
  pages     = "3491--3516",
  month     =  sep,
  year      =  2020
}

@INCOLLECTION{Wang2020-tx,
  title     = "Consumption-based asset pricing with prospect theory and habit
               formation",
  booktitle = "Handbook of Financial Econometrics, Mathematics, Statistics, and
               Machine Learning",
  author    = "Wang, Jr-Yan and Hung, Mao-Wei",
  publisher = "WORLD SCIENTIFIC",
  pages     = "1789--1819",
  month     =  sep,
  year      =  2020
}

@INCOLLECTION{Rahman2020-qo,
  title     = "Application of the multivariate average F-test to examine
               relative performance of asset pricing models with individual
               security returns",
  booktitle = "Handbook of Financial Econometrics, Mathematics, Statistics, and
               Machine Learning",
  author    = "Rahman, Shafiqur and Schneider, Matthew J",
  publisher = "WORLD SCIENTIFIC",
  pages     = "391--430",
  month     =  sep,
  year      =  2020
}

@ARTICLE{Hill2020-te,
  title     = "Bayesian Additive Regression Trees: A Review and Look Forward",
  author    = "Hill, Jennifer and Linero, Antonio and Murray, Jared",
  abstract  = "Bayesian additive regression trees (BART) provides a flexible
               approach to fitting a variety of regression models while
               avoiding strong parametric assumptions. The sum-of-trees model
               is embedded in a Bayesian inferential framework to support
               uncertainty quantification and provide a principled approach to
               regularization through prior specification. This article
               presents the basic approach and discusses further development of
               the original algorithm that supports a variety of data
               structures and assumptions. We describe augmentations of the
               prior specification to accommodate higher dimensional data and
               smoother functions. Recent theoretical developments provide
               justifications for the performance observed in simulations and
               other settings. Use of BART in causal inference provides an
               additional avenue for extensions and applications. We discuss
               software options as well as challenges and future directions.",
  journal   = "Annu. Rev. Stat. Appl.",
  publisher = "Annual Reviews",
  volume    =  7,
  number    =  1,
  pages     = "251--278",
  month     =  mar,
  year      =  2020
}

@ARTICLE{Kapelner2016-my,
  title     = "{BartMachine}: Machine learning with Bayesian additive
               regression trees",
  author    = "Kapelner, Adam and Bleich, Justin",
  journal   = "J. Stat. Softw.",
  publisher = "Foundation for Open Access Statistic",
  volume    =  70,
  number    =  4,
  year      =  2016
}

@BOOK{Nagel2021-ic,
  title    = "Machine learning in asset pricing",
  author   = "Nagel, Stefan",
  abstract = "A groundbreaking, authoritative introduction to how machine
              learning can be applied to asset pricing",
  month    =  nov,
  year     =  2021
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@UNPUBLISHED{Martin2019-pe,
  title    = "New paper: Market Efficiency in the Age of Big Data",
  author   = "Martin, Ian and Nagel, Stefan",
  abstract = "New paper: Market Efficiency in the Age of Big Data with Ian
              Martin. We put Bayesian investors into a high-dimensional
              environment in which they have to learn how to predict cash flows
              with a large‚Ä¶",
  month    =  nov,
  year     =  2019
}

@ARTICLE{Rodeh2008-lo,
  title     = "B-trees, shadowing, and clones",
  author    = "Rodeh, Ohad",
  abstract  = "B-trees are used by many file systems to represent files and
               directories. They provide guaranteed logarithmic time
               key-search, insert, and remove. File systems like WAFL and ZFS
               use shadowing, or copy-on-write, to implement snapshots, crash
               recovery, write-batching, and RAID. Serious difficulties arise
               when trying to use b-trees and shadowing in a single system.This
               article is about a set of b-tree algorithms that respects
               shadowing, achieves good concurrency, and implements cloning
               (writeable snapshots). Our cloning algorithm is efficient and
               allows the creation of a large number of clones.We believe that
               using our b-trees would allow shadowing file systems to better
               scale their on-disk data structures.",
  journal   = "ACM Trans. Storage",
  publisher = "Association for Computing Machinery",
  volume    =  3,
  number    =  4,
  pages     = "1--27",
  month     =  feb,
  year      =  2008,
  address   = "New York, NY, USA",
  keywords  = "Shadowing, copy-on-write, b-trees, concurrency, snapshots"
}

@ARTICLE{Lusardi2014-qu,
  title    = "The Economic Importance of Financial Literacy: Theory and
              Evidence",
  author   = "Lusardi, Annamaria and Mitchell, Olivia S",
  abstract = "This paper undertakes an assessment of a rapidly growing body of
              economic research on financial literacy. We start with an
              overview of theoretical research which casts financial knowledge
              as a form of investment in human capital. Endogenizing financial
              knowledge has important implications for welfare as well as
              policies intended to enhance levels of financial knowledge in the
              larger population. Next, we draw on recent surveys to establish
              how much (or how little) people know and identify the least
              financially savvy population subgroups. This is followed by an
              examination of the impact of financial literacy on economic
              decision-making in the United States and elsewhere. While the
              literature is still young, conclusions may be drawn about the
              effects and consequences of financial illiteracy and what works
              to remedy these gaps. A final section offers thoughts on what
              remains to be learned if researchers are to better inform
              theoretical and empirical models as well as public policy.",
  journal  = "J. Econ. Lit.",
  volume   =  52,
  number   =  1,
  pages    = "5--44",
  month    =  mar,
  year     =  2014,
  language = "en"
}

@ARTICLE{Lusardi2011-zn,
  title    = "{FINANCIAL} {LITERACY} {AROUND} {THE} {WORLD}: {AN} {OVERVIEW}",
  author   = "Lusardi, Annamaria and Mitchell, Olivia S",
  abstract = "In an increasingly risky and globalized marketplace, people must
              be able to make well-informed financial decisions. Yet new
              international research demonstrates that financial illiteracy is
              widespread when financial markets are well developed as in
              Germany, the Netherlands, Sweden, Japan, Italy, New Zealand, and
              the United States, or when they are changing rapidly as in
              Russia. Further, across these countries, we show that the older
              population believes itself well informed, even though it is
              actually less well informed than average. Other common patterns
              are also evident: women are less financially literate than men
              and are aware of this shortfall. More educated people are more
              informed, yet education is far from a perfect proxy for literacy.
              There are also ethnic/racial and regional differences:
              city-dwellers in Russia are better informed than their rural
              counterparts, while in the U.S., African Americans and Hispanics
              are relatively less financially literate than others. Moreover,
              the more financially knowledgeable are also those most likely to
              plan for retirement. In fact, answering one additional financial
              question correctly is associated with a 3-4 percentage point
              higher chance of planning for retirement in countries as diverse
              as Germany, the U.S., Japan, and Sweden; in the Netherlands, it
              boosts planning by 10 percentage points. Finally, using
              instrumental variables, we show that these estimates probably
              underestimate the effects of financial literacy on retirement
              planning. In sum, around the world, financial literacy is
              critical to retirement security.",
  journal  = "J. Pension Econ. Financ.",
  volume   =  10,
  number   =  4,
  pages    = "497--508",
  month    =  oct,
  year     =  2011,
  language = "en"
}

@ARTICLE{Huston2010-cg,
  title     = "Measuring financial literacy",
  author    = "Huston, Sandra J",
  abstract  = "Financial literacy (or financial knowledge) is typically an
               input to model the need for financial education and explain
               variation in financial outcomes. Defining and appropriately
               measuring financial literacy is essential to understand
               educational impact as well as barriers to effective financial
               choice. This article summarizes the broad range of financial
               literacy measures used in research over the last decade. An
               overview of the meaning and measurement of financial literacy is
               presented to highlight current limitations and assist
               researchers in establishing standardized, commonly accepted
               financial literacy instruments.",
  journal   = "J. Consum. Aff.",
  publisher = "Wiley",
  volume    =  44,
  number    =  2,
  pages     = "296--316",
  month     =  jun,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Lusardi2010-la,
  title     = "Financial literacy among the young",
  author    = "Lusardi, Annamaria and Mitchell, Olivia S and Curto, Vilsa",
  abstract  = "We examined financial literacy among the young using the most
               recent wave of the 1997 National Longitudinal Survey of Youth.
               We showed that financial literacy is low; fewer than one-third
               of young adults possess basic knowledge of interest rates,
               inflation and risk diversification. Financial literacy was
               strongly related to sociodemographic characteristics and family
               financial sophistication. Specifically, a college-educated male
               whose parents had stocks and retirement savings was about 45
               percentage points more likely to know about risk diversification
               than a female with less than a high school education whose
               parents were not wealthy.",
  journal   = "J. Consum. Aff.",
  publisher = "Wiley",
  volume    =  44,
  number    =  2,
  pages     = "358--380",
  month     =  jun,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Fernandes2014-wz,
  title     = "Financial Literacy, Financial Education, and Downstream
               Financial Behaviors",
  author    = "Fernandes, Daniel and Lynch, John G and Netemeyer, Richard G",
  abstract  = "Policy makers have embraced financial education as a necessary
               antidote to the increasing complexity of consumers' financial
               decisions over the last generation. We conduct a meta-analysis
               of the relationship of financial literacy and of financial
               education to financial behaviors in 168 papers covering 201
               prior studies. We find that interventions to improve financial
               literacy explain only 0.1\% of the variance in financial
               behaviors studied, with weaker effects in low-income samples.
               Like other education, financial education decays over time; even
               large interventions with many hours of instruction have
               negligible effects on behavior 20 months or more from the time
               of intervention. Correlational studies that measure financial
               literacy find stronger associations with financial behaviors. We
               conduct three empirical studies, and we find that the partial
               effects of financial literacy diminish dramatically when one
               controls for psychological traits that have been omitted in
               prior research or when one uses an instrument for financial
               literacy to control for omitted variables. Financial education
               as studied to date has serious limitations that have been masked
               by the apparently larger effects in correlational studies. We
               envisage a reduced role for financial education that is not
               elaborated or acted upon soon afterward. We suggest a real but
               narrower role for ?just-in-time? financial education tied to
               specific behaviors it intends to help. We conclude with a
               discussion of the characteristics of behaviors that might affect
               the policy maker's mix of financial education, choice
               architecture, and regulation as tools to help consumer financial
               behavior. This paper was accepted by Uri Gneezy, behavioral
               economics.",
  journal   = "Manage. Sci.",
  publisher = "INFORMS",
  volume    =  60,
  number    =  8,
  pages     = "1861--1883",
  month     =  aug,
  year      =  2014
}

@ARTICLE{Kuosmanen2020-gl,
  title    = "How much climate policy has cost for {OECD} countries?",
  author   = "Kuosmanen, Timo and Zhou, Xun and Dai, Sheng",
  abstract = "High economic cost of climate policy has attracted critical
              debate since the Kyoto Protocol. However, reliable empirical
              evidence of the abatement cost of green-house gases across
              countries remains scant. In this study we estimate the average
              yearly green-house gas abatement costs per capita for a panel of
              28 OECD countries in years 1990--2015. The marginal abatement
              costs are estimated using a novel data-driven approach based on
              convex quantile regression. Compared to traditional frontier
              estimation methods, the quantile approach takes into account a
              broader set of abatement options and is more robust to
              inefficiency, noise, and heteroscedasticity in empirical data.
              The comparison of OECD countries shows that the actual abatement
              cost per capita has been very modest, much lower than predicted
              in the late 1990s. This result has profound policy implications,
              calling for more ambitious climate change mitigation strategy in
              the future.",
  journal  = "World Dev.",
  volume   =  125,
  pages    = "104681",
  month    =  jan,
  year     =  2020,
  keywords = "Abatement cost; Climate change; Convex quantile regression;
              Green-house gases; Kyoto Protocol; OECD countries"
}

@UNPUBLISHED{Quinn2021-mv,
  title  = "Lurking in the shadows: The impact of emissions target setting on
            carbon pricing and environmental efficiency",
  author = "Quinn, Barry and Gallagher, Ronan and Kuosmanen, Timo",
  series = "Working paper",
  year   =  2021
}

@UNPUBLISHED{Quinn2021-az,
  title  = "Herding to comply: Systemic risk consequences of capital policy
            actions in Europe",
  author = "Quinn, Barrry and Casu, Barbara and Ayadi, Rym and Ben Naceur, Sami
            and Gallagher, Ronan",
  year   =  2021
}

@ARTICLE{Kendall2021-hm,
  title     = "Herding and contrarianism: A matter of preference?",
  author    = "Kendall, Chad",
  abstract  = "Abstract Herding and contrarian strategies produce informational
               ineffciencies when investors ignore private information, instead
               following or bucking past trends. In a simple market model, I
               show theoretically that investors with prospect theory
               preferences generically follow herding or contrarian strategies,
               but do so because of future returns as opposed to past trends. I
               conduct a laboratory experiment to test the theory and to obtain
               an estimate of the distribution of preferences in the subject
               population. I find that approximately 70\% of subjects have
               preferences that induce herding. Using the preference estimates,
               I quantify informational effciencies and predict trade behavior
               in more general environments.",
  journal   = "Rev. Econ. Stat.",
  publisher = "MIT Press - Journals",
  pages     = "1--45",
  month     =  mar,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Pesaran1995-zb,
  title    = "Estimating long-run relationships from dynamic heterogeneous
              panels",
  author   = "Pesaran, M Hashem and Smith, Ron",
  abstract = "In panel data four procedures are widely used: pooling,
              aggregating, averaging group estimates, and cross-section
              regression. In the static case, if the coefficients differ
              randomly, all four procedures give unbiased estimates of
              coefficient means. In the dynamic case, when the coefficients
              differ across groups, pooling and aggregating give inconsistent
              and potentially highly misleading estimates of the coefficients,
              though the cross-section can provide consistent estimates of the
              long-run effects. The theoretical results on the properties of
              the four procedures are illustrated by UK labour demand functions
              for 38 industries over 30 years.",
  journal  = "J. Econom.",
  volume   =  68,
  number   =  1,
  pages    = "79--113",
  month    =  jul,
  year     =  1995,
  keywords = "Dynamic panels; Parameter heterogeneity; Data fields; Sectoral
              employment equations"
}

@ARTICLE{Pepper2002-ct,
  title    = "Robust inferences from random clustered samples: an application
              using data from the panel study of income dynamics",
  author   = "Pepper, John V",
  abstract = "I examine the implications of clustered samples on inference.
              Important differences are revealed in comparisons between the
              estimated asymptotic variances derived assuming random and
              clustered sampling, even when there are only a few observations
              per cluster.",
  journal  = "Econ. Lett.",
  volume   =  75,
  number   =  3,
  pages    = "341--345",
  month    =  may,
  year     =  2002,
  keywords = "Clustered samples; Design effects; PSID."
}

@ARTICLE{Wooldridge2005-vc,
  title     = "Fixed-effects and related estimators for correlated
               random-coefficient and treatment-effect panel data models",
  author    = "Wooldridge, Jeffrey M",
  journal   = "Rev. Econ. Stat.",
  publisher = "MIT Press - Journals",
  volume    =  87,
  number    =  2,
  pages     = "385--390",
  month     =  may,
  year      =  2005,
  language  = "en"
}

@MISC{Chib_undated-fx,
  title   = "Panel Data Modeling and Inference: A Bayesian Primer",
  author  = "Chib, Siddhartha",
  journal = "Advanced Studies in Theoretical and Applied Econometrics",
  pages   = "479--515"
}

@BOOK{Wooldridge2010-jd,
  title     = "Econometric Analysis of Cross Section and Panel Data, second
               edition",
  author    = "Wooldridge, Jeffrey M",
  abstract  = "The second edition of a comprehensive state-of-the-art graduate
               level text on microeconometric methods, substantially revised
               and updated.The second edition of this acclaimed graduate text
               provides a unified treatment of two methods used in contemporary
               econometric research, cross section and data panel methods. By
               focusing on assumptions that can be given behavioral content,
               the book maintains an appropriate level of rigor while
               emphasizing intuitive thinking. The analysis covers both linear
               and nonlinear models, including models with dynamics and/or
               individual heterogeneity. In addition to general estimation
               frameworks (particular methods of moments and maximum
               likelihood), specific linear and nonlinear methods are covered
               in detail, including probit and logit models and their
               multivariate, Tobit models, models for count data, censored and
               missing data schemes, causal (or treatment) effects, and
               duration analysis.Econometric Analysis of Cross Section and
               Panel Data was the first graduate econometrics text to focus on
               microeconomic data structures, allowing assumptions to be
               separated into population and sampling assumptions. This second
               edition has been substantially updated and revised. Improvements
               include a broader class of models for missing data problems;
               more detailed treatment of cluster problems, an important topic
               for empirical researchers; expanded discussion of ``generalized
               instrumental variables'' (GIV) estimation; new coverage (based
               on the author's own recent research) of inverse probability
               weighting; a more complete framework for estimating treatment
               effects with panel data, and a firmly established link between
               econometric approaches to nonlinear panel data and the
               ``generalized estimating equation'' literature popular in
               statistics and other fields. New attention is given to
               explaining when particular econometric methods can be applied;
               the goal is not only to tell readers what does work, but why
               certain ``obvious'' procedures do not. The numerous included
               exercises, both theoretical and computer-based, allow the reader
               to extend methods covered in the text and discover new insights.",
  publisher = "MIT Press",
  month     =  oct,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Ayadi2020-vt,
  title     = "Bank business model migrations in Europe: Determinants and
               effects",
  author    = "Ayadi, Rym and Bongini, Paola and Casu, Barbara and Cucinelli,
               Doriana",
  abstract  = "Abstract In response to post-crisis regulatory reforms, the
               European banking sector has undergone significant changes that
               have led banks to reconsider their strategies, structures and
               operations. Based on a sample of over 3,000 banks from 32
               European countries during the period 2010?2017, we identify
               banks' business models based on cluster analysis and track their
               evolution. We then apply a logistic regression and find that
               banks with higher risk and lower profitability are more likely
               to change their business model. Employing a propensity score
               matching approach, we investigate the effect of migration on
               bank performance and find that changing the business model
               affects banks positively (i.e. migrating banks increase their
               profitability, stability and cost efficiency). The effect of
               migration differs depending on the target business model. When
               switches are a consequence of being acquired or motivated by
               regulatory compliance, the positive impact remains.",
  journal   = "Br. J. Manag.",
  publisher = "Wiley",
  number    = "1467-8551.12437",
  month     =  nov,
  year      =  2020,
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Butler2021-xe,
  title    = "Expert performance and crowd wisdom: Evidence from English
              Premier League predictions",
  author   = "Butler, David and Butler, Robert and Eakins, John",
  abstract = "This paper analyses the forecasting accuracy of experts
              vis-{\`a}-vis laypeople over three seasons of English Premier
              League matches. We find that former professional football players
              have superior forecasting ability when compared to laypeople. The
              results give partial support to the view that a crowd forecast
              offers the greatest precision. Pundits generate a positive return
              while both the crowd and laypeople generate losses. As the
              prediction of multiple score outcomes represents a
              computationally difficult task, both groups display forecasting
              biases including a preference toward specific score forecasts.
              The results are relevant for those concerned with gambling
              behaviour if the forecasting strategies adopted here generalise
              to match betting markets.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  288,
  number   =  1,
  pages    = "170--182",
  month    =  jan,
  year     =  2021,
  keywords = "OR in sports; Prediction; Experts"
}

@ARTICLE{Beal2019-bi,
  title     = "Artificial intelligence for team sports: a survey",
  author    = "Beal, Ryan and Norman, Timothy J and Ramchurn, Sarvapali D",
  abstract  = "\copyright{} Cambridge University Press, 20192019Cambridge
               University PressThe sports domain presents a number of
               significant computational challenges for artificial intelligence
               (AI) and machine learning (ML). In this paper, we explore the
               techniques that have been applied to the challenges within team
               sports thus far. We focus on a number of different areas, namely
               match outcome prediction, tactical decision making, player
               investments, fantasy sports, and injury prediction. By assessing
               the work in these areas, we explore how AI is used to predict
               match outcomes and to help sports teams improve their strategic
               and tactical decision making. In particular, we describe the
               main directions in which research efforts have been focused to
               date. This highlights not only a number of strengths but also
               weaknesses of the models and techniques that have been employed.
               Finally, we discuss the research questions that exist in order
               to further the use of AI and ML in team sports.",
  journal   = "The Knowledge Engineering Review; Cambridge",
  publisher = "Cambridge University Press",
  volume    =  34,
  year      =  2019,
  address   = "United Kingdom--UK, United Kingdom, Cambridge",
  language  = "en"
}

@ARTICLE{Koopman2019-mz,
  title    = "Forecasting football match results in national league
              competitions using score-driven time series models",
  author   = "Koopman, Siem Jan and Lit, Rutger",
  abstract = "We develop a new dynamic multivariate model for the analysis and
              forecasting of football match results in national league
              competitions. The proposed dynamic model is based on the score of
              the predictive observation mass function for a high-dimensional
              panel of weekly match results. Our main interest is in
              forecasting whether the match result is a win, a loss or a draw
              for each team. The dynamic model for delivering such forecasts
              can be based on three different dependent variables: the pairwise
              count of the number of goals, the difference between the numbers
              of goals, or the category of the match result (win, loss, draw).
              The different dependent variables require different
              distributional assumptions. Furthermore, different dynamic model
              specifications can be considered for generating the forecasts. We
              investigate empirically which dependent variable and which
              dynamic model specification yield the best forecasting results.
              We validate the precision of the resulting forecasts and the
              success of the forecasts in a betting simulation in an extensive
              forecasting study for match results from six large European
              football competitions. Finally, we conclude that the dynamic
              model for pairwise counts delivers the most precise forecasts
              while the dynamic model for the difference between counts is most
              successful for betting, but that both outperform benchmark and
              other competing models.",
  journal  = "Int. J. Forecast.",
  volume   =  35,
  number   =  2,
  pages    = "797--809",
  month    =  apr,
  year     =  2019,
  keywords = "Bivariate Poisson; Ordered probit; Skellam; Probabilistic loss
              function"
}

@MISC{Ardia_undated-su,
  title       = "{AdMit}",
  author      = "Ardia, David",
  abstract    = "Adaptive Mixture of Student-t distributions. Contribute to
                 ArdiaD/AdMit development by creating an account on GitHub.",
  institution = "Github"
}

@ARTICLE{Sims2010-hx,
  title    = "But Economics Is Not an Experimental Science",
  author   = "Sims, Christopher A",
  abstract = "But Economics Is Not an Experimental Science by Christopher A.
              Sims. Published in volume 24, issue 2, pages 59-68 of Journal of
              Economic Perspectives, Spring 2010, Abstract: The fact is,
              economics is not an experimental science and cannot be.
              ``Natural'' experiments and ``quasi'' experiments are not in...",
  journal  = "J. Econ. Perspect.",
  volume   =  24,
  number   =  2,
  pages    = "59--68",
  month    =  jun,
  year     =  2010
}

@ARTICLE{Schmidt-Catran2015-bd,
  title     = "The Random Effects in Multilevel Models: Getting Them Wrong and
               Getting Them Right",
  author    = "Schmidt-Catran, Alexander W and Fairbrother, Malcolm",
  abstract  = "Abstract. Many surveys of respondents from multiple countries or
               subnational regions have now been fielded on multiple occasions.
               Social scientists are regularl",
  journal   = "Eur. Sociol. Rev.",
  publisher = "Oxford Academic",
  volume    =  32,
  number    =  1,
  pages     = "23--38",
  month     =  sep,
  year      =  2015,
  language  = "en"
}

@MISC{Reed2015-mi,
  title   = "On the Practice of Lagging Variables to Avoid Simultaneity",
  author  = "Reed, William Robert",
  journal = "Oxford Bulletin of Economics and Statistics",
  volume  =  77,
  number  =  6,
  pages   = "897--905",
  year    =  2015
}

@ARTICLE{Berger2020-os,
  title    = "Did {TARP} reduce or increase systemic risk? The effects of
              government aid on financial system stability",
  author   = "Berger, Allen N and Roman, Raluca A and Sedunov, John",
  abstract = "Theory suggests that government aid to banks may either reduce or
              increase systemic risk. We are the first to address this issue
              empirically, analyzing the Troubled Assets Relief Program (TARP).
              Analysis suggests that TARP significantly reduced contributions
              to systemic risk, particularly for larger and safer banks, and
              those in better local economies. This occurred primarily through
              a capital cushion channel that reduced market leverage by
              increasing the value of common equity. Results are robust to
              endogeneity and selection bias checks. Findings yield policy
              conclusions about whether to aid banks, the best targets for
              future assistance, and short-term versus long-term effects.",
  journal  = "Journal of Financial Intermediation",
  volume   =  43,
  pages    = "100810",
  month    =  jul,
  year     =  2020,
  keywords = "Government assistance; TARP; Banks; Systemic risk; Financial
              crises"
}

@BOOK{Wooldridge2015-mw,
  title     = "Introductory Econometrics: A Modern Approach",
  author    = "Wooldridge, Jeffrey M",
  abstract  = "Discover how empirical researchers today actually think about
               and apply econometric methods with the practical, professional
               approach in Wooldridge's INTRODUCTORY ECONOMETRICS: A MODERN
               APPROACH, 6E. Unlike traditional books, this unique presentation
               demonstrates how econometrics has moved beyond just a set of
               abstract tools to become genuinely useful for answering
               questions in business, policy evaluation, and forecasting
               environments. INTRODUCTORY ECONOMETRICS is organized around the
               type of data being analyzed with a systematic approach that only
               introduces assumptions as they are needed. This makes the
               material easier to understand and, ultimately, leads to better
               econometric practices. Packed with timely, relevant
               applications, the book introduces the latest emerging
               developments in the field. Gain a full understanding of the
               impact of econometrics in real practice today with the insights
               and applications found only in INTRODUCTORY ECONOMETRICS: A
               MODERN APPROACH, 6E.Important Notice: Media content referenced
               within the product description or the product text may not be
               available in the ebook version.",
  publisher = "Cengage Learning",
  month     =  sep,
  year      =  2015,
  language  = "en"
}

@ARTICLE{Ho2020-jr,
  title     = "Ensuring trustworthy use of artificial intelligence and big data
               analytics in health insurance",
  author    = "Ho, Calvin W L and Ali, Joseph and Caals, Karel",
  abstract  = "Technological advances in big data (large amounts of highly
               varied data from many different sources that may be processed
               rapidly), data sciences and artificial intelligence can improve
               health-system functions and promote personalized care and public
               good. However, these technologies will not replace the
               fundamental components of the health system, such as ethical
               leadership and governance, or avoid the need for a robust
               ethical and regulatory environment. In this paper, we discuss
               what a robust ethical and regulatory environment might look like
               for big data analytics in health insurance, and describe
               examples of safeguards and participatory mechanisms that should
               be established. First, a clear and effective data governance
               framework is critical. Legal standards need to be enacted and
               insurers should be encouraged and given incentives to adopt a
               human-centred approach in the design and use of big data
               analytics and artificial intelligence. Second, a clear and
               accountable process is necessary to explain what information can
               be used and how it can be used. Third, people whose data may be
               used should be empowered through their active involvement in
               determining how their personal data may be managed and governed.
               Fourth, insurers and governance bodies, including regulators and
               policy-makers, need to work together to ensure that the big data
               analytics based on artificial intelligence that are developed
               are transparent and accurate. Unless an enabling ethical
               environment is in place, the use of such analytics will likely
               contribute to the proliferation of unconnected data systems,
               worsen existing inequalities, and erode trustworthiness and
               trust.",
  journal   = "Bull. World Health Organ.",
  publisher = "ncbi.nlm.nih.gov",
  volume    =  98,
  number    =  4,
  pages     = "263--269",
  month     =  apr,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Nassar2019-lg,
  title     = "Blockchain for explainable and trustworthy artificial
               intelligence",
  author    = "Nassar, Mohamed and Salah, Khaled and ur Rehman, Muhammad Habib
               and Svetinovic, Davor",
  abstract  = "Abstract The increasing computational power and proliferation of
               big data are now empowering Artificial Intelligence (AI) to
               achieve massive adoption and applicability in many fields. The
               lack of explanation when it comes to the decisions made by
               today's AI algorithms is a major drawback in critical
               decision-making systems. For example, deep learning does not
               offer control or reasoning over its internal processes or
               outputs. More importantly, current black-box AI implementations
               are subject to bias and adversarial attacks that may poison the
               learning or the inference processes. Explainable AI (XAI) is a
               new trend of AI algorithms that provide explanations of their AI
               decisions. In this paper, we propose a framework for achieving a
               more trustworthy and XAI by leveraging features of blockchain,
               smart contracts, trusted oracles, and decentralized storage. We
               specify a framework for complex AI systems in which the decision
               outcomes are reached based on decentralized consensuses of
               multiple AI and XAI predictors. The paper discusses how our
               proposed framework can be utilized in key application areas with
               practical use cases. This article is categorized under:
               Technologies > Machine Learning Technologies > Computer
               Architectures for Data Mining Fundamental Concepts of Data and
               Knowledge > Key Design Issues in Data Mining",
  journal   = "Wiley Interdiscip. Rev. Data Min. Knowl. Discov.",
  publisher = "Wiley",
  month     =  oct,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Confalonieri2021-fv,
  title     = "A historical perspective of explainable Artificial Intelligence",
  author    = "Confalonieri, Roberto and Coba, Ludovik and Wagner, Benedikt and
               Besold, Tarek R",
  abstract  = "Abstract Explainability in Artificial Intelligence (AI) has been
               revived as a topic of active research by the need of conveying
               safety and trust to users in the ?how? and ?why? of automated
               decision-making in different applications such as autonomous
               driving, medical diagnosis, or banking and finance. While
               explainability in AI has recently received significant
               attention, the origins of this line of work go back several
               decades to when AI systems were mainly developed as
               (knowledge-based) expert systems. Since then, the definition,
               understanding, and implementation of explainability have been
               picked up in several lines of research work, namely, expert
               systems, machine learning, recommender systems, and in
               approaches to neural-symbolic learning and reasoning, mostly
               happening during different periods of AI history. In this
               article, we present a historical perspective of Explainable
               Artificial Intelligence. We discuss how explainability was
               mainly conceived in the past, how it is understood in the
               present and, how it might be understood in the future. We
               conclude the article by proposing criteria for explanations that
               we believe will play a crucial role in the development of
               human-understandable explainable systems. This article is
               categorized under: Fundamental Concepts of Data and Knowledge >
               Explainable AI Technologies > Artificial Intelligence",
  journal   = "Wiley Interdiscip. Rev. Data Min. Knowl. Discov.",
  publisher = "Wiley",
  volume    =  11,
  number    =  1,
  month     =  jan,
  year      =  2021,
  copyright = "http://creativecommons.org/licenses/by/4.0/",
  language  = "en"
}

@ARTICLE{Mahdavi2020-od,
  title     = "It's all about data: How to make good decisions in a world awash
               with information",
  author    = "Mahdavi, Mehrzad and Kazemi, Hossein",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  volume    =  2,
  number    =  2,
  pages     = "8--16",
  month     =  apr,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Oberstone2011-kc,
  title     = "Comparing team performance of the English premier league, serie
               A, and la Liga for the 2008-2009 season",
  author    = "Oberstone, Joel",
  journal   = "J. Quant. Anal. Sports",
  publisher = "Walter de Gruyter GmbH",
  volume    =  7,
  number    =  1,
  month     =  jan,
  year      =  2011
}

@ARTICLE{Constantinou2013-ls,
  title     = "Determining the level of ability of football teams by dynamic
               ratings based on the relative discrepancies in scores between
               adversaries",
  author    = "Constantinou, Anthony Costa and Fenton, Norman Elliott",
  abstract  = "AbstractA rating system provides relative measures of
               superiority between adversaries. We propose a novel and simple
               approach, which we call pi-rating, for dynamically rating
               Association Football teams solely on the basis of the relative
               discrepancies in scores through relevant match instances. The
               pi-rating system is applicable to any other sport where the
               score is considered as a good indicator for prediction purposes,
               as well as determining the relative performances between
               adversaries. In an attempt to examine how well the ratings
               capture a team's performance, we have a) assessed them against
               two recently proposed football ELO rating variants and b) used
               them as the basis of a football betting strategy against
               published market odds. The results show that the pi-ratings
               outperform considerably the widely accepted ELO ratings and,
               perhaps more importantly, demonstrate profitability over a
               period of five English Premier League seasons
               (2007/2008--2011/2012), even allowing for the bookmakers'
               built-in profit margin. This is the first academic study to
               demonstrate profitability against market odds using such a
               relatively simple technique, and the resulting pi-ratings can be
               incorporated as parameters into other more sophisticated models
               in an attempt to further enhance forecasting capability.",
  journal   = "J. Quant. Anal. Sports",
  publisher = "Walter de Gruyter GmbH",
  volume    =  9,
  number    =  1,
  pages     = "37--50",
  month     =  mar,
  year      =  2013
}

@ARTICLE{Owen2011-oj,
  title     = "Dynamic Bayesian forecasting models of football match outcomes
               with estimation of the evolution variance parameter",
  author    = "Owen, Alun",
  abstract  = "Abstract. Statistical models of football (soccer) match outcomes
               have potential applications to areas such as the development of
               team rankings and football bett",
  journal   = "IMA J. Manag. Math.",
  publisher = "Oxford Academic",
  volume    =  22,
  number    =  2,
  pages     = "99--113",
  month     =  jan,
  year      =  2011,
  language  = "en"
}

@ARTICLE{Tunaru2010-oj,
  title     = "Valuations of soccer players from statistical performance data",
  author    = "Tunaru, Radu S and Viney, Howard P",
  journal   = "J. Quant. Anal. Sports",
  publisher = "Walter de Gruyter GmbH",
  volume    =  6,
  number    =  2,
  month     =  jan,
  year      =  2010
}

@ARTICLE{Min2008-fi,
  title    = "A compound framework for sports results prediction: A football
              case study",
  author   = "Min, Byungho and Kim, Jinhyuck and Choe, Chongyoun and Eom,
              Hyeonsang and (Bob) McKay, R I",
  abstract = "We propose a framework for sports prediction using Bayesian
              inference and rule-based reasoning, together with an in-game
              time-series approach. The framework is novel in three ways. The
              framework consists of two major components: a rule-based reasoner
              and a Bayesian network component. The two different approaches
              cooperate in predicting the results of sports matches. It is
              motivated by the observation that sports matches are highly
              stochastic, but at the same time, the strategies of a team can be
              approximated by crisp logic rules. Furthermore, because of the
              rule-based component, our framework can give reasonably good
              predictions even when statistical data is scanty: it can be used
              to predict results of matches between teams which have had few
              previous encounters. Machine learning techniques have great
              difficulty in handling such situations of insufficient data.
              Second, our framework is able to consider many factors, such as
              current scores, morale, fatigue, skills, etc. when it predicts
              the results of sports matches: most previous work considered only
              one factor, usually the score. Third, in contrast to most
              previous work on sports results prediction, we use a
              knowledge-based in-game time-series approach to predict sports
              matches. This approach enables our framework to reflect the
              tides/flows of a sports match, making our predictions certainly
              more realistic, and somewhat more accurate. We have implemented a
              football results predictor called FRES (Football Result Expert
              System) based on this framework, and show that it gives
              reasonable and stable predictions.",
  journal  = "Knowledge-Based Systems",
  volume   =  21,
  number   =  7,
  pages    = "551--562",
  month    =  oct,
  year     =  2008,
  keywords = "Rule-based reasoning; Bayesian inference; Sports prediction;
              Football; In-game time-series approach; Simulation"
}

@ARTICLE{McHale2011-xd,
  title    = "A {Bradley-Terry} type model for forecasting tennis match results",
  author   = "McHale, Ian and Morton, Alex",
  abstract = "The paper introduces a model for forecasting match results for
              the top tier of men's professional tennis, the ATP tour.
              Employing a Bradley-Terry type model, and utilising the data
              available on players' past results and the surface of the
              contest, we predict match winners for the coming week's matches,
              having updated the model parameters to take the previous week's
              results into account. We compare the model to two logit models:
              one using official rankings and another using the official
              ranking points of the two competing players. Our model provides
              superior forecasts according to each of five criteria measuring
              the predictive performance, two of which relate to betting
              returns.",
  journal  = "Int. J. Forecast.",
  volume   =  27,
  number   =  2,
  pages    = "619--630",
  month    =  apr,
  year     =  2011,
  keywords = "Bradley-Terry model; Logit; Ranking evaluation; Sport; Betting"
}

@ARTICLE{Scarf2009-ny,
  title    = "A numerical study of designs for sporting contests",
  author   = "Scarf, Philip and Yusof, Muhammad Mat and Bilbao, Mark",
  abstract = "Operational Research may be used to compare different designs for
              a sporting contest or tournament. This paper considers a
              methodology for this purpose. We propose a number of tournament
              metrics that can be used to measure the success of a sporting
              contest or tournament, and describe how these metrics may be
              evaluated for a particular tournament design. Knowledge of these
              measures can then be used to compare competing designs, such as
              round-robin, pure knockout and hybrids of these designs. We show,
              for example, how the design of the tournament influences the
              outcome uncertainty of the tournament and the number of
              unimportant matches within the tournament. In this way, where new
              designs are proposed, the implications of these designs may be
              explored within a modelling paradigm. In football (soccer), the
              UEFA Champions League has adopted a number of designs over its 50
              year history; the design of the tournament has been modified
              principally in response to the changing demands of national
              league football and television -- the paper uses this particular
              tournament to illustrate the methodology.",
  journal  = "Eur. J. Oper. Res.",
  volume   =  198,
  number   =  1,
  pages    = "190--198",
  month    =  oct,
  year     =  2009,
  keywords = "OR in sport; Sports tournaments; Simulation; Competitive balance"
}

@ARTICLE{Egidi2018-cu,
  title     = "Combining historical data and bookmakers' odds in modelling
               football scores",
  author    = "Egidi, Leonardo and Pauli, Francesco and Torelli, Nicola",
  abstract  = "Modelling football outcomes has gained increasing attention, in
               large part due to the potential for making substantial profits.
               Despite the strong connection existing between football models
               and the bookmakers' betting odds, no authors have used the
               latter for improving the fit and the predictive accuracy of
               these models. We have developed a hierarchical Bayesian Poisson
               model in which the scoring rates of the teams are convex
               combinations of parameters estimated from historical data and
               the additional source of the betting odds. We apply our analysis
               to a nine-year dataset of the most popular European leagues in
               order to predict match outcomes for their tenth seasons. In this
               article, we provide numerical and graphical checks for our
               model.",
  journal   = "Stat. Modelling",
  publisher = "SAGE Publications",
  volume    =  18,
  number    = "5-6",
  pages     = "436--459",
  month     =  dec,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Baio2010-hk,
  title     = "Bayesian hierarchical model for the prediction of football
               results",
  author    = "Baio, Gianluca and Blangiardo, Marta",
  abstract  = "The problem of modelling football data has become increasingly
               popular in the last few years and many different models have
               been proposed with the aim of estimating the characteristics
               that bring a team to lose or win a game, or to predict the score
               of a particular match. We propose a Bayesian hierarchical model
               to fulfil both these aims and test its predictive strength based
               on data about the Italian Serie A 1991?1992 championship. To
               overcome the issue of overshrinkage produced by the Bayesian
               hierarchical model, we specify a more complex mixture model that
               results in a better fit to the observed data. We test its
               performance using an example of the Italian Serie A 2007?2008
               championship.",
  journal   = "J. Appl. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  37,
  number    =  2,
  pages     = "253--264",
  month     =  feb,
  year      =  2010
}

@MISC{Zou2020-bb,
  title   = "A Bayesian {In-Play} Prediction Model for Association Football
             Outcomes",
  author  = "Zou, Qingrong and Song, Kai and Shi, Jian",
  journal = "Applied Sciences",
  volume  =  10,
  number  =  8,
  pages   = "2904",
  year    =  2020
}

@BOOK{Lunn2012-au,
  title     = "The {BUGS} Book: A Practical Introduction to Bayesian Analysis",
  author    = "Lunn, David and Jackson, Chris and Best, Nicky and Thomas,
               Andrew and Spiegelhalter, David",
  abstract  = "Bayesian statistical methods have become widely used for data
               analysis and modelling in recent years, and the BUGS software
               has become the most popular software for Bayesian analysis
               worldwide. Authored by the team that originally developed this
               software, The BUGS Book provides a practical introduction to
               this program and its use. The text presents",
  publisher = "CRC Press",
  month     =  oct,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Mishra2021-ar,
  title     = "Are Indian sectoral indices oil shock prone? An empirical
               evaluation",
  author    = "Mishra, Shekhar and Mishra, Sibanjan",
  abstract  = "We present the impact of oil shocks on a disaggregate Indian
               stock market by using 10 sectoral indices for the period from
               2010-2019. We use the novel Ready's (2018) oil shock
               decomposition mechanism. Using the rolling regression and
               dynamic conditional correlation, we ascertain the time varying
               properties of different oil shocks on the sectoral returns. To
               understand the asymmetric effect, we employ the markov switching
               regression. The results of the study are presented in two sets:
               One, we observe a structural dynamic shift in the conditional
               correlation between the oil shocks and sectoral returns across
               the study period. Also, the markov regime regression confirms
               that demand shocks have positive effect on all sectors in high
               and low volatility regime signalling the huge consumption demand
               in Indian industries. The supply shocks are not significant
               either in low or high volatility regimes indicating collapse of
               economic activities in such case. the demand and supply shocks
               have positive and negative impact on all sectoral indices,
               respectively. The influence of risk shocks is observed to be
               negative and statistically significant on all the sectoral
               equity indices. These results are helpful for investors in
               devising optimal portfolio with rational risk exposure to
               different sectors. They would be able to ascertain the optimal
               market timing for investment and orient their investment
               strategy towards better industry rotation. The policymakers also
               be able to formulate better macroeconomic strategies
               commensurating with the time varying effects of oil price shocks
               on Indian economy.",
  journal   = "Resour. Policy",
  publisher = "Elsevier",
  volume    =  70,
  pages     = "101889",
  month     =  mar,
  year      =  2021,
  keywords  = "Oil shocks; Sectoral indices; Demand; Supply and risk shocks;
               Markov switching regression"
}

@ARTICLE{Ready2018-lq,
  title     = "Oil Prices and the Stock Market",
  author    = "Ready, Robert C",
  abstract  = "Abstract. This paper develops a novel method for classifying oil
               price changes as supply or demand driven using information in
               asset prices. Motivated by a simp",
  journal   = "Rev Financ",
  publisher = "Oxford Academic",
  volume    =  22,
  number    =  1,
  pages     = "155--176",
  month     =  feb,
  year      =  2018
}

@ARTICLE{Cetinkaya-Rundel2018-ue,
  title     = "Infrastructure and tools for teaching computing throughout the
               statistical curriculum",
  author    = "{\c C}etinkaya-Rundel, Mine and Rundel, Colin",
  journal   = "Am. Stat.",
  publisher = "Informa UK Limited",
  volume    =  72,
  number    =  1,
  pages     = "58--65",
  month     =  jan,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Zuo2021-et,
  title     = "Variable selection with second-generation P-values",
  author    = "Zuo, Yi and Stewart, Thomas G and Blume, Jeffrey D",
  journal   = "Am. Stat.",
  publisher = "Informa UK Limited",
  pages     = "1--21",
  month     =  jun,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Blume2019-oe,
  title     = "An introduction to second-generation p-values",
  author    = "Blume, Jeffrey D and Greevy, Robert A and Welty, Valerie F and
               Smith, Jeffrey R and Dupont, William D",
  journal   = "Am. Stat.",
  publisher = "Informa UK Limited",
  volume    =  73,
  number    = "sup1",
  pages     = "157--167",
  month     =  mar,
  year      =  2019,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

@INPROCEEDINGS{noauthor_2016-za,
  title       = "Data analytics: the skills need in {STEM}",
  booktitle   = "Teaching statistics and data science",
  institution = "Royal Statistical Society",
  year        =  2016,
  conference  = "Royal Statistical Society Annual Conference"
}

@ARTICLE{Amrhein2019-fc,
  title    = "Scientists rise up against statistical significance",
  author   = "Amrhein, Valentin and Greenland, Sander and McShane, Blake",
  journal  = "Nature",
  volume   =  567,
  number   =  7748,
  pages    = "305--307",
  month    =  mar,
  year     =  2019,
  keywords = "Research data; Research management",
  language = "en"
}

@ARTICLE{Lommers2021-tc,
  title     = "Confronting machine learning with financial research",
  author    = "Lommers, Kristof and Harzli, Ouns El and Kim, Jack",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  pages     = "jfds.2021.1.068",
  month     =  jun,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Leung2021-zq,
  title     = "The promises and pitfalls of machine learning for predicting
               stock returns",
  author    = "Leung, Edward and Lohre, Harald and Mischlich, David and Shea,
               Yifei and Stroh, Maximilian",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  pages     = "jfds.2021.1.062",
  month     =  apr,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Worthington2002-gf,
  title     = "The Impact of Student Perceptions and Characteristics on
               Teaching Evaluations: A case study in finance education",
  author    = "Worthington, Andrew C",
  abstract  = "This study uses an ordered probit model to examine the impact of
               student characteristics and perceptions of the teaching
               evaluation process on student ratings. The results indicate that
               expected grade, ethnic background, gender and age are a
               significant influence on student ratings. A primary
               student-based influence on teaching evaluation performance would
               appear to be the perceived potential outcome of the evaluation
               in terms of tenure, promotion and salary decisions, and
               improvements in teaching and staff allocation. The impact of
               student perceptions and characteristics is also found to vary
               across the various dimensions of teaching performance with the
               potential bias being highest for evaluation questions relating
               to overall performance, and lowest for questions relating to
               formative assessment and deep learning outcomes.",
  journal   = "Assessment \& Evaluation in Higher Education",
  publisher = "Routledge",
  volume    =  27,
  number    =  1,
  pages     = "49--64",
  month     =  jan,
  year      =  2002
}

@ARTICLE{Jaeger2021-ep,
  title     = "Interpretable machine learning for diversified portfolio
               construction",
  author    = "Jaeger, Markus and Kr{\"u}gel, Stephan and Marinelli, Dimitri
               and Papenbrock, Jochen and Schwendner, Peter",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  pages     = "jfds.2021.1.066",
  month     =  jun,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Madan2020-nc,
  title     = "Machine trading: Theory, advances, and applications",
  author    = "Madan, Dilip B and Sharaiha, Yazid M",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  volume    =  2,
  number    =  3,
  pages     = "8--24",
  month     =  jul,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Cetinkaya-Rundel2021-pq,
  title     = "A Fresh Look at Introductory Data Science",
  author    = "{\c C}etinkaya-Rundel, Mine and Ellison, Victoria",
  abstract  = "ABSTRACTThe proliferation of vast quantities of available
               datasets that are large and complex in nature has challenged
               universities to keep up with the demand for graduates trained in
               both the statistical and the computational set of skills
               required to effectively plan, acquire, manage, analyze, and
               communicate the findings of such data. To keep up with this
               demand, attracting students early on to data science as well as
               providing them a solid foray into the field becomes increasingly
               important. We present a case study of an introductory
               undergraduate course in data science that is designed to address
               these needs. Offered at Duke University, this course has no
               prerequisites and serves a wide audience of aspiring statistics
               and data science majors as well as humanities, social sciences,
               and natural sciences students. We discuss the unique set of
               challenges posed by offering such a course, and in light of
               these challenges, we present a detailed discussion into the
               pedagogical design elements, content, structure, computational
               infrastructure, and the assessment methodology of the course. We
               also offer a repository containing all teaching materials that
               are open-source, along with supplementary materials and the R
               code for reproducing the figures found in the article.",
  journal   = "Journal of Statistics and Data Science Education",
  publisher = "Taylor \& Francis",
  volume    =  29,
  number    = "sup1",
  pages     = "S16--S26",
  month     =  jan,
  year      =  2021
}

@ARTICLE{Beckman2021-pb,
  title     = "Implementing Version Control With Git and {GitHub} as a Learning
               Objective in Statistics and Data Science Courses",
  author    = "Beckman, Matthew D and {\c C}etinkaya-Rundel, Mine and Horton,
               Nicholas J and Rundel, Colin W and Sullivan, Adam J and Tackett,
               Maria",
  abstract  = "AbstractA version control system records changes to a file or
               set of files over time so that changes can be tracked and
               specific versions of a file can be recalled later. As such, it
               is an essential element of a reproducible workflow that deserves
               due consideration among the learning objectives of statistics
               courses. This article describes experiences and implementation
               decisions of four contributing faculty who are teaching
               different courses at a variety of institutions. Each of these
               faculty has set version control as a learning objective and
               successfully integrated one such system (Git) into one or more
               statistics courses. The various approaches described in the
               article span different implementation strategies to suit student
               background, course type, software choices, and assessment
               practices. By presenting a wide range of approaches to teaching
               Git, the article aims to serve as a resource for statistics and
               data science instructors teaching courses at any level within an
               undergraduate or graduate curriculum.",
  journal   = "Journal of Statistics and Data Science Education",
  publisher = "Taylor \& Francis",
  volume    =  29,
  number    = "sup1",
  pages     = "S132--S144",
  month     =  jan,
  year      =  2021
}

@ARTICLE{Boettiger2017-my,
  title         = "An Introduction to Rocker: Docker Containers for {R}",
  author        = "Boettiger, Carl and Eddelbuettel, Dirk",
  abstract      = "We describe the Rocker project, which provides a widely-used
                   suite of Docker images with customized R environments for
                   particular tasks. We discuss how this suite is organized,
                   and how these tools can increase portability, scaling,
                   reproducibility, and convenience of R users and developers.",
  month         =  oct,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SE",
  eprint        = "1710.03675"
}

@ARTICLE{Baumer2019-ol,
  title     = "A Grammar for Reproducible and Painless {Extract-Transform-Load}
               Operations on Medium Data",
  author    = "Baumer, Benjamin S",
  abstract  = "ABSTRACTMany interesting datasets available on the Internet are
               of a medium size?too big to fit into a personal computer?s
               memory, but not so large that they would not fit comfortably on
               its hard disk. In the coming years, datasets of this magnitude
               will inform vital research in a wide array of application
               domains. However, due to a variety of constraints they are
               cumbersome to ingest, wrangle, analyze, and share in a
               reproducible fashion. These obstructions hamper thorough
               peer-review and thus disrupt the forward progress of science. We
               propose a predictable and pipeable framework for R (the
               state-of-the-art statistical computing environment) that
               leverages SQL (the venerable database architecture and query
               language) to make reproducible research on medium data a
               painless reality. Supplementary material for this article is
               available online.",
  journal   = "J. Comput. Graph. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  28,
  number    =  2,
  pages     = "256--264",
  month     =  apr,
  year      =  2019
}

@ARTICLE{Kim2021-fl,
  title     = "{Easy-to-Use} Cloud Computing for Teaching Data Science",
  author    = "Kim, Brian and Henke, Graham",
  abstract  = "ABSTRACTOne of the biggest hurdles of teaching data science and
               programming techniques to beginners is simply getting started
               with the technology. With multiple versions of the same coding
               language available (e.g., Python 2 and Python 3), various
               additional libraries and packages to install, as well as
               integrated development environments to navigate, the first step
               can be the most daunting. We show the advantages of using cloud
               computing to solve this issue and demonstrate one way of
               implementing it to allow beginners to get started with coding
               immediately. Using user-friendly Jupyter notebooks along with
               the interactive capabilities possible through Binder, we provide
               introductory Python and SQL material that students can access
               without downloading anything. This lets students to get started
               with coding right away without getting frustrated figuring out
               what to install. Example introductory modules on using Python
               and SQL for data analysis are provided through GitHub at
               https://github.com/Coleridge-Initiative/ada-intro-python and
               https://github.com/Coleridge-Initiative/ada-intro-sql.",
  journal   = "Journal of Statistics and Data Science Education",
  publisher = "Taylor \& Francis",
  volume    =  29,
  number    = "sup1",
  pages     = "S103--S111",
  month     =  jan,
  year      =  2021
}

@ARTICLE{Schwab-McCoy2021-xf,
  title     = "Data Science in 2020: Computing, Curricula, and Challenges for
               the Next 10 Years",
  author    = "Schwab-McCoy, Aimee and Baker, Catherine M and Gasper, Rebecca E",
  abstract  = "AbstractIn the past 10 years, new data science courses and
               programs have proliferated at the collegiate level. As faculty
               and administrators enter the race to provide data science
               training and attract new students, the road map for teaching
               data science remains elusive. In 2019, 69 college and university
               faculty teaching data science courses and developing data
               science curricula were surveyed to learn about their curricula,
               computing tools, and challenges they face in their classrooms.
               Faculty reported teaching a variety of computing skills in
               introductory data science (albeit fewer computing topics than
               statistics topics), and that one of the biggest challenges they
               face is teaching computing to a diverse audience with varying
               preparation. The ever-evolving nature of data science is a major
               hurdle for faculty teaching data science courses, and a call for
               more data science teaching resources was echoed in many
               responses.",
  journal   = "Journal of Statistics and Data Science Education",
  publisher = "Taylor \& Francis",
  volume    =  29,
  number    = "sup1",
  pages     = "S40--S50",
  month     =  jan,
  year      =  2021
}

@ARTICLE{Fiksel2019-qy,
  title     = "Using {GitHub} Classroom To Teach Statistics",
  author    = "Fiksel, Jacob and Jager, Leah R and Hardin, Johanna S and Taub,
               Margaret A",
  abstract  = "AbstractGit and GitHub are common tools for keeping track of
               multiple versions of data analytic content, which allow for more
               than one person to simultaneously work on a project. GitHub
               Classroom aims to provide a way for students to work on and
               submit their assignments via Git and GitHub, giving teachers an
               opportunity to facilitate the integration of these version
               control tools into their undergraduate statistics courses. In
               the Fall 2017 semester, we implemented GitHub Classroom in two
               educational settings?an introductory computational statistics
               lab and a more advanced computational statistics course. We
               found many educational benefits of implementing GitHub
               Classroom, such as easily providing coding feedback during
               assignments and making students more confident in their ability
               to collaborate and use version control tools for future data
               science work. To encourage and ease the transition into using
               GitHub Classroom, we provide free and publicly available
               resources?both for students to begin using Git/GitHub and for
               teachers to use GitHub Classroom for their own courses.",
  journal   = "J. Stat. Educ.",
  publisher = "Taylor \& Francis",
  volume    =  27,
  number    =  2,
  pages     = "110--119",
  month     =  may,
  year      =  2019
}

@ARTICLE{Bryan2018-eu,
  title     = "Excuse Me, Do You Have a Moment to Talk About Version Control?",
  author    = "Bryan, Jennifer",
  abstract  = "ABSTRACTData analysis, statistical research, and teaching
               statistics have at least one thing in common: these activities
               all produce many files! There are data files, source code,
               figures, tables, prepared reports, and much more. Most of these
               files evolve over the course of a project and often need to be
               shared with others, for reading or edits, as a project unfolds.
               Without explicit and structured management, project organization
               can easily descend into chaos, taking time away from the primary
               work and reducing the quality of the final product. This unhappy
               result can be avoided by repurposing tools and workflows from
               the software development world, namely, distributed version
               control. This article describes the use of the version control
               system Git and the hosting site GitHub for statistical and data
               scientific workflows. Special attention is given to projects
               that use the statistical language R and, optionally, R Markdown
               documents. Supplementary materials include an annotated set of
               links to step-by-step tutorials, real world examples, and other
               useful learning resources. Supplementary materials for this
               article are available online.",
  journal   = "Am. Stat.",
  publisher = "Taylor \& Francis",
  volume    =  72,
  number    =  1,
  pages     = "20--27",
  month     =  jan,
  year      =  2018
}

@ARTICLE{Nordmann2020-yj,
  title    = "Ten simple rules for supporting a temporary online pivot in
              higher education",
  author   = "Nordmann, Emily and Horlin, Chiara and Hutchison, Jacqui and
              Murray, Jo-Anne and Robson, Louise and Seery, Michael K and
              MacKay, Jill R D",
  abstract = "As continued COVID-19 disruption looks likely across the world,
              perhaps until 2021, contingency plans are evolving in case of
              further disruption in the 2020-2021 academic year. This includes
              delivering face-to-face programs fully online for at least part
              of the upcoming academic year for new and continuing cohorts.
              This temporary pivot will necessitate distance teaching and
              learning across almost every conceivable pedagogy, from
              fundamental degrees to professionally accredited ones. Each
              institution, program, and course will have its own myriad of
              individualized needs; however, there is a common question that
              unites us all: how do we provide teaching and assessment to
              students in a manner that is accessible, fair, equitable, and
              provides the best learning whilst acknowledging the temporary
              nature of the pivot? No ``one size fits all'' solution exists,
              and many of the choices that need to be made will be far from
              simple; however, this paper provides a starting point and basic
              principles to facilitate discussions taking place around the
              globe by balancing what we know from the pedagogy of online
              learning with the practicalities imposed by this crisis and any
              future crises.",
  journal  = "PLoS Comput. Biol.",
  volume   =  16,
  number   =  10,
  pages    = "e1008242",
  month    =  oct,
  year     =  2020,
  language = "en"
}

@INCOLLECTION{Reisinger2018-ft,
  title     = "Finite difference methods for medium-and high-dimensional
               derivative pricing {PDEs}",
  booktitle = "{High-Performance} Computing in Finance",
  author    = "Reisinger, Christoph and Wissmann, Rasmus",
  publisher = "Chapman and Hall/CRC",
  pages     = "175--195",
  year      =  2018
}

@PHDTHESIS{Riseth2018-wj,
  title    = "Algorithms for decision making",
  author   = "Riseth, Asbj{\o}rn Nilsen",
  abstract = "We investigate algorithms for different steps in the decision
              making process, focusing on systems where we are uncertain about
              the outcomes but can quantify how probable they are using random
              variables. Any decision one makes in such a situation leads to a
              distribution of outcomes and requires a way to evaluate a
              decision. The standard approach is to marginalise the
              distribution of outcomes into a single number that tries in some
              way to summarise the value of each decision. After selecting a
              marginalisation approach, mathematicians and decision makers
              focus their analysis on the marginalised value but ignore the
              distribution. We argue that we should also be investigating the
              implications of the chosen mathematical approach for the whole
              distribution of outcomes. We illustrate the effect different
              mathematical formulations have on the distribution with one-stage
              and sequential decision problems. We show that different ways to
              marginalise the distributions can result in very similar
              decisions but each way has a different complexity and
              computational cost. It is often computationally intractable to
              approximate optimal decisions to high precision and much research
              goes into developing algorithms that are suboptimal in the
              marginalised sense, but work within the computational budget
              available. If the performance of these algorithms is evaluated
              they are mainly judged based on the marginalised values, however,
              comparing the performance using the full distribution provides
              interesting information: We provide numerical examples from
              dynamic pricing applications where the suboptimal algorithm
              results in higher profit than the optimal algorithm in more than
              half of the realisations, which is paid for with a more
              significant underperformance in the remaining realisations. All
              the problems discussed in this thesis lead to continuous
              optimisation problems. We develop a new algorithm that can be
              used on top of existing optimisation algorithms to reduce the
              cost of approximating solutions. The algorithm is tested on a
              range of optimisation problems and is shown to be competitive
              with existing methods.",
  year     =  2018,
  school   = "University of Oxford"
}

@MISC{noauthor_undated-nk,
  title        = "{RStudio} Workbench",
  abstract     = "Build great data science products",
  howpublished = "\url{https://www.rstudio.com/products/workbench/}",
  note         = "Accessed: 2021-7-19"
}

@ARTICLE{Baumer2014-dt,
  title    = "{R} Markdown: Integrating A Reproducible Analysis Tool into
              Introductory Statistics",
  author   = "Baumer, B and {\c C}etinkaya-Rundel, Mine and Bray, Andrew and
              Loi, Linda and Horton, N",
  abstract = "Nolan and Temple Lang argue that ``the ability to express
              statistical computations is an essential skill.'' A key related
              capacity is the ability to conduct and present data analysis in a
              way that another person can understand and replicate. The
              copy-and-paste workflow that is an artifact of antiquated
              user-interface design makes reproducibility of statistical
              analysis more difficult, especially as data become increasingly
              complex and statistical methods become increasingly
              sophisticated. R Markdown is a new technology that makes creating
              fully-reproducible statistical analysis simple and painless. It
              provides a solution suitable not only for cutting edge research,
              but also for use in an introductory statistics course. We present
              evidence that R Markdown can be used effectively in introductory
              statistics courses, and discuss its role in the rapidly-changing
              world of statistical computation.",
  journal  = "undefined",
  year     =  2014
}

@ARTICLE{Kaplan2007-wc,
  title    = "Computing and Introductory Statistics",
  author   = "Kaplan, Daniel",
  abstract = "Author(s): Kaplan, Daniel | Abstract: Much of the computing that
              students do in introductory statistics courses is based on
              techniques that were developed before computing became
              inexpensive and ubiquitous. Now that computing is readily
              available to all students, instructors can change the way we
              teach statistical concepts. This article describes computational
              ideas that can support teaching George Cobb's Three Rs of
              statistical inference: Randomize, Repeat, Reject.",
  journal  = "Technology Innovations in Statistics Education",
  volume   =  1,
  number   =  1,
  month    =  oct,
  year     =  2007
}

@ARTICLE{Lee2021-ct,
  title    = "Bayesian Hierarchical Modeling: Application Towards Production
              Results in the Eagle Ford Shale of South Texas",
  author   = "Lee, Se Yoon and Mallick, Bani K",
  abstract = "Recently, the petroleum industry has faced the era of data
              explosion, and many oil and gas companies resort to data-driven
              approaches for unconventional field development planning. The
              objective of this paper is to analyze shale oil wells in a shale
              reservoir and develop a statistical model useful for upstream.
              Shale oil wells dataset comprises three aspects of information:
              oil production rate time series data; well completion data; and
              well location data. However, traditional decline curve analysis
              only utilizes the temporal trajectory of the production rates.
              Motivated by this, we propose a Bayesian hierarchical model that
              exploits the full aspects of the shale oil wells data. The
              proposed model provides the following three functionalities:
              first, estimations of a production decline curve at an individual
              well and entire reservoir levels; second, identification of
              significant completion predictors explaining a well productivity;
              and third, spatial predictions for the oil production rate
              trajectory of a new well provided completion predictors. As a
              fully Bayesian approach has been adopted, the functionalities are
              endowed with uncertainty quantification which is a crucial task
              in investigating unconventional reservoirs. The data for this
              study come from 360 shale oil wells completed in the Eagle Ford
              Shale of South Texas.",
  journal  = "Sankhya B",
  month    =  jan,
  year     =  2021
}

@ARTICLE{Cesa2017-px,
  title     = "A brief history of quantitative finance",
  author    = "Cesa, Mauro",
  abstract  = "In this introductory paper to the issue, I will travel through
               the history of how quantitative finance has developed and
               reached its current status, what problems it is called to
               address, and how they differ from those of the pre-crisis world.",
  journal   = "Probability, Uncertainty and Quantitative Risk",
  publisher = "SpringerOpen",
  volume    =  2,
  number    =  1,
  pages     = "1--16",
  month     =  jun,
  year      =  2017,
  language  = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Giacomini2021-bc,
  title     = "Robust Bayesian inference for set‚Äêidentified models",
  author    = "Giacomini, Raffaella and Kitagawa, Toru",
  abstract  = "This paper reconciles the asymptotic disagreement between
               Bayesian and frequentist inference in set‚Äêidentified models by
               adopting a multiple‚Äêprior (robust) Bayesian approach. We propose
               new tools for Bayesian inference in set‚Äêidentified models and
               show that they have a well‚Äêdefined posterior interpretation in
               finite samples and are asymptotically valid from the frequentist
               perspective. The main idea is to construct a prior class that
               removes the source of the disagreement: the need to specify an
               unrevisable prior for the structural parameter given the
               reduced‚Äêform parameter. The corresponding class of posteriors
               can be summarized by reporting the `posterior lower and upper
               probabilities' of a given event and/or the `set of posterior
               means' and the associated `robust credible region'. We show that
               the set of posterior means is a consistent estimator of the true
               identified set and the robust credible region has the correct
               frequentist asymptotic coverage for the true identified set if
               it is convex. Otherwise, the method provides posterior inference
               about the convex hull of the identified set. For
               impulse‚Äêresponse analysis in set‚Äêidentified Structural Vector
               Autoregressions, the new tools can be used to overcome or
               quantify the sensitivity of standard Bayesian inference to the
               choice of an unrevisable prior.",
  journal   = "Econometrica",
  publisher = "The Econometric Society",
  volume    =  89,
  number    =  4,
  pages     = "1519--1556",
  year      =  2021,
  language  = "en"
}
