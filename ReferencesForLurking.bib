@article{stiglitz2017report,
  title={Report of the high-level commission on carbon prices},
  author={Stiglitz, Joseph E and Stern, Nicholas and Duan, Maosheng and Edenhofer, Ottmar and Giraud, Ga{\"e}l and Heal, Geoffrey M and La Rovere, Emilio L{\`e}bre and Morris, Adele and Moyer, Elisabeth and Pangestu, Mari and others},
  year={2017}
}
@article{Hans-Jochen.2020, 
year = {2020}, 
title = {{Why carbon emissions pricing and carbon shadow pricing both make sense}}, 
author = {Hans-Jochen, Luhmann, and Sabine, Balk, and Hans, Dembowski,}, 
journal = {Development and Cooperation}, 
url = {https://www.dandc.eu/en/article/why-carbon-emissions-pricing-and-carbon-shadow-pricing-both-make-sense\#:\textbackslashtextasciitilde:text=Appropriate\%20carbon\%20prices\&text=\%E2\%80\%9CCarbon\%20emissions\%20pricing\%E2\%80\%9D\%20means\%20that,not\%20reflect\%20those\%20impacts\%20yet.}, 
abstract = {{“Carbon emissions pricing” means that state institutions charge a price      for emissions. By contrast, “carbon shadow pricing” means that the      long-term impacts of emissions are factored into the planning of major      projects even if current market prices do not reflect those impacts yet.      Shadow prices are fictional, but have a very real impact. A high shadow      price for fuel, for example, can make a power plant unviable even though      it may look attractive at current market prices. The climate economist      Hans-Jochen Luhmann argues that both approaches to carbon pricing make      sense. They actually complement one another.}}, 
keywords = {}
}
@article{Kuosmanen.2020, 
year = {2020}, 
keywords = {Abatement cost,Climate change,Convex quantile regression,Green-house gases,Kyoto Protocol,OECD countries}, 
title = {{How much climate policy has cost for OECD countries?}}, 
author = {Kuosmanen, Timo and Zhou, Xun and Dai, Sheng}, 
journal = {World Development}, 
issn = {0305-750X}, 
doi = {10.1016/j.worlddev.2019.104681}, 
url = {https://www.sciencedirect.com/science/article/pii/S0305750X19303298?casa\_token=BSZRivYMFkUAAAAA:CgyUdEfsv770aBWeGG\_NKOz-99W1Jb3KSZDq8jB5b2XsWW2Czd-FSG297LraMmz7Q2dwmSjy}, 
abstract = {{ High economic cost of climate policy has attracted critical debate since the Kyoto Protocol. However, reliable empirical evidence of the abatement cost of green-house gases across countries remains scant. In this study we estimate the average yearly green-house gas abatement costs per capita for a panel of 28 OECD countries in years 1990–2015. The marginal abatement costs are estimated using a novel data-driven approach based on convex quantile regression. Compared to traditional frontier estimation methods, the quantile approach takes into account a broader set of abatement options and is more robust to inefficiency, noise, and heteroscedasticity in empirical data. The comparison of OECD countries shows that the actual abatement cost per capita has been very modest, much lower than predicted in the late 1990s. This result has profound policy implications, calling for more ambitious climate change mitigation strategy in the future.}}, 
pages = {104681}, 
volume = {125}, 
month = {1}
}
@article{Kumar.20207q, 
year = {2020}, 
title = {{CO2 mitigation policy for Indian thermal power sector: Potential gains from emission trading}}, 
author = {Kumar, Surender and Managi, Shunsuke and Jain, Rakesh Kumar}, 
journal = {Energy Economics}, 
issn = {01409883}, 
doi = {10.1016/j.eneco.2019.104653}, 
abstract = {{This study shows potential cost savings by adoption of emission trading in India. At the Paris Agreement, India pledged to reduce CO2 emissions intensity by about 30–35\% by 2030 relative to 2005. Applying joint production function of electricity and CO2 emissions, we find that India could have saved about US\$ 5 to 8 billion, if she had constituted an emission trading system, with the provision of banking and borrowing over the study period of 5 years. To our knowledge, this is the first study measuring foregone gains due to absence of a nationwide carbon emission-trading program in coal fired thermal power sector, using an ex-post analysis. © 2019 Elsevier B.V.}}, 
pages = {104653}, 
volume = {86}, 
keywords = {}
}
@article{Kuosmanen.2021, 
year = {2021}, 
title = {{Shadow prices and marginal abatement costs: Convex quantile regression approach}}, 
author = {Kuosmanen, Timo and Zhou, Xun}, 
journal = {European Journal of Operational Research}, 
issn = {0377-2217}, 
doi = {10.1016/j.ejor.2020.07.036}, 
abstract = {{Marginal abatement cost (MAC) is a critically important concept for efficient environmental policy and management. In this paper we argue that most empirical studies using frontier estimation methods such as data envelopment analysis (DEA) over-estimate MACs. The first methodological contribution of this paper is to clarify the conceptual distinction between the shadow price and MAC in order to analyze three sources of upward bias due to the limited set of abatement options, inefficiency, and noisy data. Our second methodological contribution is to develop a novel MAC estimation approach based on convex quantile regression. Compared to the traditional methods, convex quantile regression is more robust to the choice of the direction vector, random noise, and heteroscedasticity. Empirical application to the U.S. electric power plants demonstrates that the upward bias of DEA may be a serious problem in real-world applications.}}, 
pages = {666--675}, 
number = {2}, 
volume = {289}, 
keywords = {}
}
@article{Xian.2022, 
year = {2022}, 
title = {{Capturing the least costly measure of CO2 emission abatement: Evidence from the iron and steel industry in China}}, 
author = {Xian, Yujiao and Yu, Dan and Wang, Ke and Yu, Jian and Huang, Zhimin}, 
journal = {Energy Economics}, 
issn = {0140-9883}, 
doi = {10.1016/j.eneco.2022.105812}, 
abstract = {{Estimating the marginal abatement cost (MAC) of CO2 emission is critical in formulating emission reduction targets and policies. Existing studies rarely emphasized the impact of random noise on MAC estimation, and downscaling the production activity is conventionally applied as the only measure for emission abatement while the possibilities of other measures, such as increase the investment, are often neglected. This paper estimates the least MAC of CO2 for Chinese iron and steel enterprises using a stochastic semi-nonparametric method which considers both inefficiency and random noise. Multiple measures including downscaling the production activity and increasing the inputs investment, are all considered for identifying the least-cost measure for reducing emissions. In addition, the strategies corresponding to adjustment on production and response to environmental regulation of each enterprise are included in the estimation, which makes it possible for identifying the upper and lower bound of MACs. Empirical results indicate that i) the stochastic semi-nonparametric method provides a more consistent estimates with the production process, ii) the average MAC of CO2 emissions in China's iron and steel industry ranges from 2.07 to 2395 yuan/ton, and iii) increasing labor is identified as the least-cost abatement measures for most of the iron and steel enterprises listed in China's top 500 enterprise. Policy implications have been put forward to reduce the carbon abatement cost in China's iron and steel industry.}}, 
pages = {105812}, 
volume = {106}, 
keywords = {}
}

@article{Kousmanen.2021, 
year = {2021}, 
title = {{Shadow prices and marginal abatement costs: Convex quantile regression approach}}, 
author = {Kuosmanen, Timo and Zhou, Xun}, 
journal = {European Journal of Operational Research}, 
issn = {0377-2217}, 
doi = {10.1016/j.ejor.2020.07.036}, 
abstract = {{Marginal abatement cost (MAC) is a critically important concept for efficient environmental policy and management. In this paper we argue that most empirical studies using frontier estimation methods such as data envelopment analysis (DEA) over-estimate MACs. The first methodological contribution of this paper is to clarify the conceptual distinction between the shadow price and MAC in order to analyze three sources of upward bias due to the limited set of abatement options, inefficiency, and noisy data. Our second methodological contribution is to develop a novel MAC estimation approach based on convex quantile regression. Compared to the traditional methods, convex quantile regression is more robust to the choice of the direction vector, random noise, and heteroscedasticity. Empirical application to the U.S. electric power plants demonstrates that the upward bias of DEA may be a serious problem in real-world applications.}}, 
pages = {666--675}, 
number = {2}, 
volume = {289}, 
keywords = {}
}
@article{Timo.2020, 
year = {2020}, 
keywords = {Abatement cost,Climate change,Convex quantile regression,Green-house gases,Kyoto Protocol,OECD countries}, 
title = {{How much climate policy has cost for OECD countries?}}, 
author = {Kuosmanen, Timo and Zhou, Xun and Dai, Sheng}, 
journal = {World Development}, 
issn = {0305-750X}, 
doi = {10.1016/j.worlddev.2019.104681}, 
url = {https://www.sciencedirect.com/science/article/pii/S0305750X19303298?casa\_token=BSZRivYMFkUAAAAA:CgyUdEfsv770aBWeGG\_NKOz-99W1Jb3KSZDq8jB5b2XsWW2Czd-FSG297LraMmz7Q2dwmSjy}, 
abstract = {{ High economic cost of climate policy has attracted critical debate since the Kyoto Protocol. However, reliable empirical evidence of the abatement cost of green-house gases across countries remains scant. In this study we estimate the average yearly green-house gas abatement costs per capita for a panel of 28 OECD countries in years 1990–2015. The marginal abatement costs are estimated using a novel data-driven approach based on convex quantile regression. Compared to traditional frontier estimation methods, the quantile approach takes into account a broader set of abatement options and is more robust to inefficiency, noise, and heteroscedasticity in empirical data. The comparison of OECD countries shows that the actual abatement cost per capita has been very modest, much lower than predicted in the late 1990s. This result has profound policy implications, calling for more ambitious climate change mitigation strategy in the future.}}, 
pages = {104681}, 
volume = {125}, 
month = {1}
}
@article{Xian.2022, 
year = {2022}, 
title = {{Capturing the least costly measure of CO2 emission abatement: Evidence from the iron and steel industry in China}}, 
author = {Xian, Yujiao and Yu, Dan and Wang, Ke and Yu, Jian and Huang, Zhimin}, 
journal = {Energy Economics}, 
issn = {0140-9883}, 
doi = {10.1016/j.eneco.2022.105812}, 
abstract = {{Estimating the marginal abatement cost (MAC) of CO2 emission is critical in formulating emission reduction targets and policies. Existing studies rarely emphasized the impact of random noise on MAC estimation, and downscaling the production activity is conventionally applied as the only measure for emission abatement while the possibilities of other measures, such as increase the investment, are often neglected. This paper estimates the least MAC of CO2 for Chinese iron and steel enterprises using a stochastic semi-nonparametric method which considers both inefficiency and random noise. Multiple measures including downscaling the production activity and increasing the inputs investment, are all considered for identifying the least-cost measure for reducing emissions. In addition, the strategies corresponding to adjustment on production and response to environmental regulation of each enterprise are included in the estimation, which makes it possible for identifying the upper and lower bound of MACs. Empirical results indicate that i) the stochastic semi-nonparametric method provides a more consistent estimates with the production process, ii) the average MAC of CO2 emissions in China's iron and steel industry ranges from 2.07 to 2395 yuan/ton, and iii) increasing labor is identified as the least-cost abatement measures for most of the iron and steel enterprises listed in China's top 500 enterprise. Policy implications have been put forward to reduce the carbon abatement cost in China's iron and steel industry.}}, 
pages = {105812}, 
volume = {106}, 
keywords = {}
}
@article{Dai.2020, 
year = {2020}, 
title = {{Forward-looking assessment of the GHG abatement cost: Application to China}}, 
author = {Dai, Sheng and Zhou, Xun and Kuosmanen, Timo}, 
journal = {Energy Economics}, 
issn = {0140-9883}, 
doi = {10.1016/j.eneco.2020.104758}, 
abstract = {{Evaluation of abatement costs is critical in setting reduction goals and devising climate policy. However, reliable forward-looking assessment of the short-term effects of climate policy remains a major challenge. Using panel data of 30 Chinese provinces during 1997–2015, we first estimate the marginal CO2 abatement costs using a novel data-driven approach, convex quantile regression. Based on the marginal abatement cost estimates and China's plans regarding carbon intensity reduction and economic growth, we present a forward-looking assessment of the abatement costs for Chinese provinces for 2016–2020. Our main finding is that all the Chinese provinces have a negative abatement cost, which means these provinces can benefit from an increase in the absolute level of CO2 emissions despite the constraint on carbon intensity. The magnitudes of economic benefits exhibit a significant regional disparity because some provinces can increase more CO2 emissions than others. However, there is still costly carbon intensity abatement relative to a counterfactual where the provinces meet their economic growth targets but in the absence of the intensity reduction constraints. Policy implications have been proposed to enhance the efficiency and fairness of climate policy in China.}}, 
pages = {104758}, 
volume = {88}, 
keywords = {}
}
@article{lee2014,
title = {A new approach to measuring shadow price: Reconciling engineering and economic perspectives},
journal = {Energy Economics},
volume = {46},
pages = {66-77},
year = {2014},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2014.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S0140988314001765},
author = {Sang-choon Lee and Dong-hyun Oh and Jeong-dong Lee},
keywords = {Shadow price, Undesirable outputs, Carbon abatement, Power plants},
abstract = {Shadow price is one of the most important pieces of information in environmental decision making. Two different approaches—namely, economic and engineering—have been applied to obtain the shadow price of undesirable outputs, while using different methodological backgrounds and perspectives. The current study proposes a new conceptual framework and an economic estimation model to reconcile the shadow price estimates derived via the two approaches. We also suggest a new mapping rule that incorporates the concept of abatement level, which is a basic element in the engineering approach. As a result, the proposed model generates continuously changing estimates—i.e., comprising a shadow price curve—based on the abatement level. We further investigate the determinant factors of shadow price by using second-step regression. The suggested methodology is used to investigate the shadow price of carbon emissions in South Korean electricity generating plants, thus yielding relevant policy implications.}
}
@article{lee2015,
title = {Directional shadow price estimation of CO2, SO2 and NOx in the United States coal power industry 1990–2010},
journal = {Energy Economics},
volume = {51},
pages = {493-502},
year = {2015},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2015.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140988315002339},
author = {Chia-Yen Lee and Peng Zhou},
keywords = {Shadow price, Emissions trading, Directional distance function, Marginal abatement cost, Coal power plant},
abstract = {Shadow prices, also termed marginal abatement costs, provide valuable guidelines to support environmental regulatory policies for CO2, SO2 and NOx, the key contributors to climate change. This paper complements the existing models and describes a directional marginal productivity (DMP) approach to estimate directional shadow prices (DSPs) which present substitutability among three emissions and are jointly estimated. We apply the method to a case study of CO2, SO2 and NOx produced by coal power plants operating between 1990 and 2010 in the United States. We find that DSP shows 1.1 times the maximal shadow prices estimated in the current literature. We conclude that estimating the shadow prices of each by-product separately may lead to an overestimation of the marginal productivity and an underestimation of the shadow prices.}
}
@article{wei2020,
title = {The shadow prices of CO2 and SO2 for Chinese Coal-fired Power Plants: A partial frontier approach},
journal = {Energy Economics},
volume = {85},
pages = {104576},
year = {2020},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2019.104576},
url = {https://www.sciencedirect.com/science/article/pii/S0140988319303718},
author = {Xiao Wei and Ning Zhang},
keywords = {Partial quantile order-α frontier, Full frontier, China coal-fired power plants, Parametric approach},
abstract = {Shadow prices are widely employed in guiding environmental policies. This paper contributes to the literature by developing a novel partial frontier approach for estimating shadow prices. The proposed method is advanced in handling outliers, and provides a precise estimation of the marginal rate of substitution along the production frontier. To cope with the different disposability characteristics of undesirable emissions, SO2 is treated as an environmental input that is strongly disposable. Because most different undesirable outputs are jointly reduced, this study is the first to use directional derivatives instead of partial derivatives and proposes a new separation method that separates individual directional shadow price from the cost of joint reduction of multiple undesirable outputs. We apply the proposed method to a sample of 93 coal-fired power plants covering six years. The estimated average shadow prices of CO2 and SO2 are 69 $/tonne and 2525 $/tonne under the newly developed partial frontiers. When assuming some undesirable output to be strongly disposable, both shadow prices fall (20.32 $/tonne and 1018.23 $/tonne). The present paper suggests the estimated shadow prices will be much different across different levels of disposability. The cost of joint reduction is calculated as 91.05 $/unit given the current production level, which leads to a directional shadow price for the two undesirable outputs of 91.32 $/tonne and 1250.06 $/tonne. The estimated shadow prices for both undesirable outputs dispersed widely under the partial frontiers. Spatial distribution of estimated shadow prices shows that coastal provinces and municipalities have higher CO2 shadow prices but lower SO2 shadow prices, whereas the northeast provinces are the opposite. The CO2 shadow price is comparatively stable over the study period while the SO2 shadow price fluctuates. Policy implications are discussed.}
}
@ARTICLE{Luhmann2020,
  title    = "Why carbon emissions pricing and carbon shadow pricing both make
              sense",
  author   = "Luhmann, Hans-Jochen and Balk, Sabine and Dembowski, Hans",
  abstract = "``Carbon emissions pricing'' means that state institutions charge
              a price for emissions. By contrast, ``carbon shadow pricing''
              means that the long-term impacts of emissions are factored into
              the planning of major projects even if current market prices do
              not reflect those impacts yet. Shadow prices are fictional, but
              have a very real impact. A high shadow price for fuel, for
              example, can make a power plant unviable even though it may look
              attractive at current market prices. The climate economist
              Hans-Jochen Luhmann argues that both approaches to carbon pricing
              make sense. They actually complement one another.",
  journal  = "Development and Cooperation",
  year     =  2020
}

@REPORT{CPLC2017,
  title     = "Report of the High-Level Commission on Carbon Prices",
  author    = "Stiglitz, Joseph E. and Stern, Nicholas",
  abstract  = "",
  journal   = "Carbon Pricing Leadership Coalition",
  publisher = "Routledge",
  year      =  2017,
  url       = "https://static1.squarespace.com/static/54ff9c5ce4b0a53decccfb4c/t/59b7f2409f8dce5316811916/1505227332748/CarbonPricing_FullReport.pdf"
}


@ARTICLE{Gallagher2019,
  title     = "Regulatory own goals: the unintended consequences of economic
               regulation in professional football",
  author    = "Gallagher, Ronan and Quinn, Barry",
  abstract  = "ABSTRACTResearch question: In 2010, the governing body of
               European football, UEFA, approved ?Financial Fair Play?
               regulations. Designed to encourage financial discipline, promote
               stability and foster competitive balance, they focus on a
               financial breakeven constraint. We analyse the impact of such
               constraints on the joint sporting and financial efficiency of
               English football clubs.Research methods: The simultaneous
               production of both sporting and financial outputs are modelled
               using stochastic, non-parametric efficiency analysis. The sample
               is an unbalanced panel representing 60 clubs spanning the
               2003/2004 to 2016/2017 seasons.Results and findings: The
               Financial Fair Play breakeven regulation reduces average club
               efficiency, raises the relative importance of financial goals
               (capturing revenue share) whilst lowering the relative
               importance of sporting goals (capturing point share). The
               efficiency costs of regulation are not borne equally by
               clubs.Implications: Breakeven regulations reduce the joint
               sporting and financial efficiency of regulated clubs, with the
               efficiency loss positively related to the severity of the
               breakeven constraint. The Financial Fair Play regulations
               further entrench the financial and sporting power of elite clubs
               and potentially undermine league competitive intensity by
               shifting the relative focus of clubs away from sporting
               productivity toward financial productivity.",
  journal   = "European Sport Management Quarterly",
  publisher = "Routledge",
  pages     = "1--20",
  month     =  apr,
  year      =  2019
}


@ARTICLE{Ozili2019,
  title     = "Non-performing loans in European systemic and non-systemic banks",
  author    = "Ozili, Peterson K",
  abstract  = "Purpose The distinction between systemic banks (GSIBs) and
               non-systemic banks (non-GSIBs) is driven by policy reasons. This
               study aims to examine the behaviour of non-performing loans in
               European GSIBs and non-GSIBs from 2004 to
               2013.Design/methodology/approach The author uses regression
               methodology to analyse the association between non-performing
               loans (NPLs) and the state of the economy.Findings The author
               finds that more profitable banks witness higher NPLs regardless
               of them being systemic or non-systemic. Secondly, GSIBs have
               fewer NPLs during economic booms and during periods of increased
               lending, while non-GSIBs experience higher NPLs during periods
               of increased lending. The author also observes that European
               non-GSIBs that exceed regulatory capital requirement also
               experience higher NPLs. In the post-crisis period, there is a
               significant and negative relationship between NPLs and the
               economic cycle for GSIBs in the post-financial crisis period and
               a significant and positive relationship between NPLs, loan
               supply and bank profitability for GSIBs in the post-financial
               crisis period; on the other hand, there is a significant and
               negative relationship between NPLs and regulatory capital ratios
               for non-GSIBs in the post-financial crisis period and a
               significant and positive relationship between NPLs and bank
               profitability for non-GSIBs in the post-financial crisis period.
               The findings have implications.Originality/value To the best of
               the author's knowledge, the literature on the determinants of
               NPL has not empirically examined the behaviour of NPLs in
               European GSIBs and non-GSIBs. This paper examines this issue to
               provide insights to help policymakers and academics understand
               the peculiarities of NPLs in Europe.",
  journal   = "Journal of Financial Economic Policy",
  publisher = "Emerald Publishing Limited",
  volume    =  12,
  number    =  3,
  pages     = "409--424",
  month     =  jan,
  year      =  2019,
  keywords  = "credit risk; nonperforming loans; systemic banks; systemic risk;
               impaired loans; asset quality; European banks; Europe; bank
               profitability."
}


@ARTICLE{Burniaux2000,
  title     = "A multi-gas assessment of the Kyoto Protocol",
  author    = "Burniaux, Jean-Marc",
  abstract  = "… Expressed in this way, OECD countries will have to reduce
               their emissions by some 20 to 30 per cent … has been undertaken
               using macroeconomic global models to quantify the economic costs
               of implementing the Protocol by (see Weyant and Hill , 1999 ;
               OECD, 1999 ) …",
  journal   = "OECD Economics Department Working Papers No. 270",
  publisher = "OECD",
  year      =  2000
}



@article{Halkos2014,
	Abstract = {This paper applies the probabilistic approach developed by
               Daraio and Simar (J Prod Anal 24:93--121, 2005, Advanced robust
               and nonparametric methods in efficiency analysis. Springer
               Science, New York, 2007a, J Prod Anal 28:13--32, 2007b) in order
               to develop conditional and unconditional data envelopment
               analysis (DEA) models for the measurement of countries'
               environmental efficiency levels for a sample of 110 countries in
               2007. In order to capture the effect of countries compliance
               with the Kyoto protocol agreement (KPA) policies, we condition
               first the years since a country has signed the KPA until 2007
               and secondly the obliged percentage level of countries' emission
               reductions. Particularly, various DEA models have been applied
               alongside with bootstrap techniques in order to determine the
               effect of KPA on countries' environmental efficiencies. The
               study illustrates how the recent developments in efficiency
               analysis and statistical inference can be applied when
               evaluating environmental performance issues. The results
               indicate a nonlinear relationship between countries' obliged
               percentage levels of emission reductions and their environmental
               efficiency levels. Finally, a similar nonlinear relationship is
               also recorded between the duration which a country has signed
               the KPA and its environmental efficiency levels.},
	Author = {Halkos, George E and Tzeremes, Nickolaos G},
	Journal = {J Prod Anal},
	Language = {en},
	Month = jun,
	Number = 3,
	Pages = {367--382},
	Publisher = {Springer US},
	Title = {Measuring the effect of Kyoto protocol agreement on countries' environmental efficiency in {CO2} emissions: an application of conditional full frontiers},
	Volume = 41,
	Year = 2014}

@article{McKibbin2004,
	Abstract = {In this paper, we update our earlier estimates of the cost of
               the Kyoto Protocol using the G-Cubed model, taking into account
               the new sink allowances from recent negotiations as well as
               allowing for multiple gases and new land clearing estimates. We
               then compare the protocol to an alternative policy outlined in
               McKibbin et al. (Brookings Policy Brief, No. 17. June, The
               Brookings Institution, Washington, 1997; Climate Change Policy
               After Kyoto: A Blueprint for a Realistic Approach, The Brookings
               Institution, Washington, 2002a; J. Econom. Perspect. 16(2)
               (2002b) 107) that does not impose rigid emissions targets. We
               focus particular attention on the sensitivity of compliance
               costs under each policy to unexpected changes in future economic
               conditions. To illustrate the issue, we evaluate the policies
               under two plausible alternative assumptions about a single
               aspect of the future world economy: the rate of productivity
               growth in Russia. We find that moderate growth in Russia would
               raise the cost of the Kyoto Protocol by as much as 50 percent
               but would have little effect on the cost of the alternative
               policy. We conclude that the Kyoto Protocol is inherently
               unstable because unexpected future events could raise compliance
               costs substantially and place enormous pressure on governments
               to abrogate the agreement. The alternative policy would be far
               more stable because it does not subject future governments to
               adverse shocks in compliance costs.},
	Author = {McKibbin, Warwick J and Wilcoxen, Peter J},
	Journal = {Energy Policy},
	Keywords = {Environmental policy; Climate change; General equilibrium model},
	Month = mar,
	Number = 4,
	Pages = {467--479},
	Publisher = {Elsevier},
	Title = {Estimates of the costs of Kyoto: Marrakesh versus the {McKibbin--Wilcoxen} blueprint},
	Volume = 32,
	Year = 2004}

@article{Manne1999,
	Abstract = {This paper has three purposes: 1) to identify the near-term
               costs to the United States of ratifying the Kyoto Protocol ; 2)
               to assess the significance of the Protocol's `` flexibility
               provisions''; and, 3) to evaluate the Kyoto targets in the
               context of the long-term goal of the {\ldots}},
	Author = {Manne, Alan S and Richels, Richard G},
	Journal = {Energy J.},
	Number = {Special Issue-The Cost of the Kyoto Protocol: A Multi-Model Evaluation},
	Publisher = {International Association for Energy Economics},
	Title = {The Kyoto Protocol: a cost-effective strategy for meeting environmental objectives?},
	Volume = 20,
	Year = 1999}

@article{Manne2004,
	Abstract = {Despite the US rejection of the Kyoto Protocol, the meeting of
              the parties to the United Nations Framework Convention on Climate
              Change in July 2001 has increased the likelihood that the
              Protocol will be ratified. This raises a number of issues
              concerning mitigation costs, particularly for the buyers and
              sellers of emission permits. In this paper, we examine how the US
              decision is likely to affect compliance costs for other Annex B
              countries during the first commitment period. We also explore the
              implications for US emissions. Key findings include:
              1.Participating Organisation for Economic Co-operation and
              Development countries may experience a decline in mitigation
              costs, but because of the banking provision contained in the
              Protocol, the decline may not be as great as some would
              suggest;2.If the majority of ``hot air'' is concentrated in a
              small number of countries in Eastern Europe and the former Soviet
              Union, these countries may be able to organize a sellers' cartel
              and extract sizable economic rents; and3.Even in the absence of
              mandatory emission reduction requirements, US emissions in 2010
              may be lower than their business-as-usual baseline because of
              expectations regarding future regulatory requirements.},
	Author = {Manne, Alan and Richels, Richard},
	Journal = {Energy Policy},
	Keywords = {Greenhouse gases; Emissions abatement; Kyoto},
	Month = mar,
	Number = 4,
	Pages = {447--454},
	Title = {{US} rejection of the Kyoto Protocol: the impact on compliance costs and {CO2} emissions},
	Volume = 32,
	Year = 2004}

@article{Nordhaus1999,
	Abstract = {[This paper uses the newly developed RICE-98 model to analyze
               the economics of the Kyoto Protocol. It analyzes versions of the
               Kyoto Protocol that have different approaches to trading
               emissions rights and compares these with efficient approaches.
               The major conclusions are: (a) the net global cost of the Kyoto
               Protocol is \$716 billion in present value, (b) the United
               States bears almost two-thirds of the global cost; and (c) the
               benefit-cost ratio of the Kyoto Protocol is 1/7. Additionally,
               the emissions strategy is highly cost-ineffective, with the
               global temperature reduction achieved at a cost almost 8 times
               the cost of a strategy which is cost-effective in terms of
               ``where'' and ``when'' efficiency. These conclusions assume that
               trading in carbon permits is allowed among the Annex I
               countries.]},
	Author = {Nordhaus, William D and Boyer, Joseph G},
	Journal = {Energy J.},
	Pages = {93--130},
	Publisher = {International Association for Energy Economics},
	Title = {Requiem for Kyoto: An Economic Analysis of the Kyoto Protocol},
	Volume = 20,
	Year = 1999}

@article{Reilly1999,
	Abstract = {The Kyoto Protocol allows reductions in emissions of several
               `greenhouse' gases to be credited against a CO2-equivalent
               emissions limit, calculated using `global warming potential'
               indices for each gas. Using an integrated global-systems model,
               it is shown that a multi-gas control strategy could greatly
               reduce the costs of fulfilling the Kyoto Protocol compared with
               a CO2-only strategy. Extending the Kyoto Protocol to 2100
               without more severe emissions reductions shows little difference
               between the two strategies in climate and ecosystem effects.
               Under a more stringent emissions policy, the use of global
               warming potentials as applied in the Kyoto Protocol leads to
               considerably more mitigation of climate change for multi-gas
               strategies than for the---supposedly equivalent---CO2-only
               control, thus emphasizing the limits of global warming
               potentials as a tool for political decisions.},
	Author = {Reilly, J and Prinn, R and Harnisch, J and Fitzmaurice, J and Jacoby, H and Kicklighter, D and Melillo, J and Stone, P and Sokolov, A and Wang, C},
	Journal = {Nature},
	Language = {en},
	Month = oct,
	Number = 6753,
	Pages = {549--555},
	Publisher = {Nature Publishing Group},
	Title = {Multi-gas assessment of the Kyoto Protocol},
	Volume = 401,
	Year = 1999}

@article{Brechin2003,
	Abstract = {Using a variety of public opinion polls over a number of years
              and from a number of countries this paper revisits the questions
              of crossnational public concern for global warming first examined
              over a decade ago. Although the scientific community today speaks
              out on global climatic change in essentially a unified voice
              concerning its anthropogenic causes and potential devastating
              impacts at the global level, it remains the case that many
              citizens of a number of nations still seem to harbor considerable
              uncertainties about the problem itself. Although it could be
              argued that there has been a slight improvement over the last
              decade in the public's understanding regarding the anthropogenic
              causes of global warming, the people of all the nations studied
              remain largely uniformed about the problem. In a recent
              international study on knowledge about global warming, the
              citizens of Mexico led all fifteen countries surveyed in 2001
              with just twenty‐six percent of the survey respondents correctly
              identifying burning fossil fuels as the primary cause of global
              warming. The citizens of the U.S., among the most educated in the
              world, where somewhere in the middle of the pack, tied with the
              citizens of Brazil at fifteen percent, but slightly lower than
              Cubans. In response to President Bush's withdrawal of the Kyoto
              Protocol in 1991, the U.S. public appears to be far more
              supportive of the action than the citizens of a number of
              European countries where there was considerable outrage about the
              decision.},
	Author = {Brechin, Steven R},
	Journal = {Int. J. Sociol. Soc. Policy},
	Number = 10,
	Pages = {106--134},
	Title = {Comparative public opinion and knowledge on global climatic change and the Kyoto Protocol: the {US} versus the world?},
	Volume = 23,
	Year = 2003}

@article{Buonanno2003,
	Abstract = {We present a model for climate change policy analysis which
               accounts for the possibility that technology evolves
               endogenously and that technical change can be induced by
               environmental policy measures. Both the output production
               technology and the emission--output ratio depend upon a stock of
               knowledge, which accumulates through R\&D activities. Two
               versions of this model are studied, one with endogenous
               technical change but exogenous environmental technical change
               and the other with both endogenous and induced technical change.
               A third version also captures technological spillover effects.
               As an application, the model is simulated allowing for trade of
               pollution permits as specified in the Kyoto Protocol and
               assessing the implications in terms of cost efficiency, economic
               growth and R\&D efforts of the three different specifications of
               technical change.},
	Author = {Buonanno, Paolo and Carraro, Carlo and Galeotti, Marzio},
	Journal = {Res. Energy Econ.},
	Keywords = {Climate policy; Environmental modelling; Integrated assessment; Technical change},
	Month = feb,
	Number = 1,
	Pages = {11--34},
	Publisher = {Elsevier},
	Title = {Endogenous induced technical change and the costs of Kyoto},
	Volume = 25,
	Year = 2003}

@article{Weyant1999,
	Author = {Weyant, J P and Hill, J N},
	Journal = {Energy J.},
	Title = {Kyoto special issue, Introduction and Overview},
	Year = 1999}

@article{Zhang1998,
	Author = {Zhang, Zhongxiang and Folmer, Henk},
	Journal = {Energy Econ.},
	Number = 1,
	Pages = {101--120},
	Publisher = {Elsevier},
	Title = {Economic modelling approaches to cost estimates for the control of carbon dioxide emissions},
	Volume = 20,
	Year = 1998}

@article{Rubin2007,
	Abstract = {CO2 capture and storage (CCS) is receiving considerable attention
              as a potential greenhouse gas (GHG) mitigation option for fossil
              fuel power plants. Cost and performance estimates for CCS are
              critical factors in energy and policy analysis. CCS cost studies
              necessarily employ a host of technical and economic assumptions
              that can dramatically affect results. Thus, particular studies
              often are of limited value to analysts, researchers, and industry
              personnel seeking results for alternative cases. In this paper,
              we use a generalized modeling tool to estimate and compare the
              emissions, efficiency, resource requirements and current costs of
              fossil fuel power plants with CCS on a systematic basis. This
              plant-level analysis explores a broader range of key assumptions
              than found in recent studies we reviewed for three major plant
              types: pulverized coal (PC) plants, natural gas combined cycle
              (NGCC) plants, and integrated gasification combined cycle (IGCC)
              systems using coal. In particular, we examine the effects of
              recent increases in capital costs and natural gas prices, as well
              as effects of differential plant utilization rates, IGCC
              financing and operating assumptions, variations in plant size,
              and differences in fuel quality, including bituminous,
              sub-bituminous and lignite coals. Our results show higher power
              plant and CCS costs than prior studies as a consequence of recent
              escalations in capital and operating costs. The broader range of
              cases also reveals differences not previously reported in the
              relative costs of PC, NGCC and IGCC plants with and without CCS.
              While CCS can significantly reduce power plant emissions of CO2
              (typically by 85--90\%), the impacts of CCS energy requirements
              on plant-level resource requirements and multi-media
              environmental emissions also are found to be significant, with
              increases of approximately 15--30\% for current CCS systems. To
              characterize such impacts, an alternative definition of the
              ``energy penalty'' is proposed in lieu of the prevailing use of
              this term.},
	Author = {Rubin, Edward S and Chen, Chao and Rao, Anand B},
	Journal = {Energy Policy},
	Keywords = {CO capture and storage cost; Comparative power plant economics; Carbon sequestration energy requirements},
	Month = sep,
	Number = 9,
	Pages = {4444--4454},
	Title = {Cost and performance of fossil fuel power plants with {CO2} capture and storage},
	Volume = 35,
	Year = 2007}

@article{Rubin2015,
	Abstract = {The objective of this paper is to assess the current costs of CO2
              capture and storage (CCS) for new fossil fuel power plants and to
              compare those results to the costs reported a decade ago in the
              IPCC Special Report on Carbon Dioxide Capture and Storage
              (SRCCS). Toward that end, we employed a similar methodology based
              on review and analysis of recent cost studies for the major CCS
              options identified in the SRCCS, namely, post-combustion CO2
              capture at supercritical pulverized coal (SCPC) and natural gas
              combined cycle (NGCC) power plants, plus pre-combustion capture
              at coal-based integrated gasification combined cycle (IGCC) power
              plants. We also report current costs for SCPC plants employing
              oxy-combustion for CO2 capture---an option that was still in the
              early stages of development at the time of the SRCCS. To compare
              current CCS cost estimates to those in the SRCCS, we adjust all
              costs to constant 2013 US dollars using cost indices for power
              plant capital costs, fuel costs and other O\&M costs. On this
              basis, we report changes in capital cost, levelized cost of
              electricity, and mitigation costs for each power plant system
              with and without CCS. We also discuss the outlook for future CCS
              costs.},
	Author = {Rubin, Edward S and Davison, John E and Herzog, Howard J},
	Journal = {Int. J. Greenhouse Gas Control},
	Keywords = {Carbon capture and storage; Carbon sequestration; Power plant costs; CCS costs; Economic analysis},
	Month = sep,
	Pages = {378--400},
	Title = {The cost of {CO2} capture and storage},
	Volume = 40,
	Year = 2015}

@article{Viguier2003,
	Abstract = {We estimate reference CO2 emission projections in the European
              Union, and quantify the economic impacts of the Kyoto commitment
              on Member States. We consider the case where each EU member
              individually meets a CO2 emissions target, applying a
              country-wide cap and trade system to meet the target but without
              trade among countries. We use a version of the MIT Emissions
              Prediction and Policy Analysis (EPPA) model, here disaggregated
              to separately include 9 European Community countries and
              commercial and household transportation sectors. We compare our
              results with that of four energy-economic models that have
              provided detailed analyses of European climate change policy. In
              the absence of specific additional climate policy measures, the
              EPPA reference projections of carbon emissions increase by 14\%
              from 1990 levels. The EU-wide target under the Kyoto Protocol to
              the Framework Convention on Climate Change is a reduction in
              emissions to 8\% below 1990 levels. EPPA emissions projections
              are similar to other recent modeling results, but there are
              underlying differences in energy and carbon intensities among the
              projections. If EU countries were to individually meet the EU
              allocation of the Community-wide carbon cap specified in the
              Kyoto Protocol, we find using EPPA that carbon prices vary from
              $91 in the United Kingdom to $385 in Denmark; welfare costs range
              from 0.6\% to 5\%.},
	Author = {Viguier, Laurent L and Babiker, Mustafa H and Reilly, John M},
	Journal = {Energy Policy},
	Keywords = {Kyoto Protocol; European Union; Computable general equilibrium model},
	Month = apr,
	Number = 5,
	Pages = {459--481},
	Title = {The costs of the Kyoto Protocol in the European Union},
	Volume = 31,
	Year = 2003}

@article{Bohringer2003,
	Author = {B{\"o}hringer, Christoph and Vogt, Carsten},
	Journal = {Canadian Journal of Economics/Revue canadienne d'{\'e}conomique},
	Number = 2,
	Pages = {475--496},
	Publisher = {Wiley Online Library},
	Title = {Economic and environmental impacts of the Kyoto Protocol},
	Volume = 36,
	Year = 2003}

@article{Fischer2006,
	Abstract = {[Estimates of marginal abatement costs for reducing carbon
               emissions derived from major economic-energy models vary widely.
               Controlling for policy regimes we use meta-analysis to examine
               the importance of structural modeling choices in explaining
               differences in estimates. The analysis indicates that particular
               assumptions about perfectly foresighted consumers and Armington
               trade elasticities generate lower estimates of marginal
               abatement costs. Other choices are associated with higher cost
               estimates, including perfectly mobile capital, inclusion of a
               backstop technology, and greater disaggregation among regions
               and sectors. Some features, such as greater technological
               detail, seem less significant. Understanding the importance of
               key modeling assumptions, as well as the way the models are used
               to estimate abatement costs, can help guide the development of
               consistent modeling practices for policy evaluation.]},
	Author = {Fischer, Carolyn and Morgenstern, Richard D},
	Journal = {Energy J.},
	Number = 2,
	Pages = {73--86},
	Publisher = {International Association for Energy Economics},
	Title = {Carbon Abatement Costs: Why the Wide Range of Estimates?},
	Volume = 27,
	Year = 2006}

@article{Meyer2003,
	Abstract = {Abstract. An unbiased test for the appropriateness of the simple
               linear regression model is presented. The null hypothesis is
               that the underlying regression fu},
	Author = {Meyer, Mary C},
	Journal = {Biometrika},
	Month = mar,
	Number = 1,
	Pages = {223--232},
	Publisher = {Narnia},
	Title = {A test for linear versus convex regression function using shape‐restricted regression},
	Volume = 90,
	Year = 2003}

@article{Levene1961,
	Author = {Levene, Howard},
	Journal = {Contributions to probability and statistics. Essays in honor of Harold Hotelling},
	Pages = {279--292},
	Publisher = {Stanford University Press},
	Title = {Robust tests for equality of variances},
	Year = 1961}

@article{Brown1974,
	Abstract = {Abstract Alternative formulations of Levene's test statistic for
               equality of variances are found to be robust under nonnormality.
               These statistics use more robust estimators of central location
               in place of the mean. They are compared with the unmodified
               Levene's statistic, a jackknife procedure, and a ?2 test
               suggested by Layard which are all found to be less robust under
               nonnormality.},
	Author = {Brown, Morton B and Forsythe, Alan B},
	Journal = {J. Am. Stat. Assoc.},
	Month = jun,
	Number = 346,
	Pages = {364--367},
	Publisher = {Taylor \& Francis},
	Title = {Robust Tests for the Equality of Variances},
	Volume = 69,
	Year = 1974}

@article{Welch1947,
	Author = {Welch, B L},
	Journal = {Biometrika},
	Keywords = {POPULATION},
	Language = {en},
	Number = {1-2},
	Pages = {28--35},
	Title = {The generalisation of student's problems when several different population variances are involved},
	Volume = 34,
	Year = 1947}

@article{Minsky1986,
	Author = {Minsky, Hyman P},
	Journal = {J. Econ. Issues},
	Number = 2,
	Pages = {345--353},
	Publisher = {Taylor \& Francis},
	Title = {The evolution of financial institutions and the performance of the economy},
	Volume = 20,
	Year = 1986}

@article{Simpson1951,
	Abstract = {SUMMARY The definition of second order interaction in a (2 ? 2 ?
              2) table given by Bartlett is accepted, but it is shown by an
              example that the vanishing of this second order interaction does
              not necessarily justify the mechanical procedure of forming the
              three component 2 ? 2 tables and testing each of these for
              significance by standard methods.*},
	Author = {Simpson, E H},
	Journal = {J. R. Stat. Soc. Series B Stat. Methodol.},
	Month = jul,
	Number = 2,
	Pages = {238--241},
	Title = {The Interpretation of Interaction in Contingency Tables},
	Volume = 13,
	Year = 1951}

@book{Minsky2008,
	Abstract = {``Mr. Minsky long argued markets were crisis prone. His 'moment'
               has arrived.'' -The Wall Street Journal In his seminal work,
               Minsky presents his groundbreaking financial theory of
               investment, one that is startlingly relevant today. He explains
               why the American economy has experienced periods of debilitating
               inflation, rising unemployment, and marked slowdowns-and why the
               economy is now undergoing a credit crisis that he foresaw.
               Stabilizing an Unstable Economy covers: The natural inclination
               of complex, capitalist economies toward instability Booms and
               busts as unavoidable results of high-risk lending practices
               ``Speculative finance'' and its effect on investment and asset
               prices Government's role in bolstering consumption during times
               of high unemployment The need to increase Federal Reserve
               oversight of banks Henry Kaufman, president, Henry Kaufman \&
               Company, Inc., places Minsky's prescient ideas in the context of
               today's financial markets and institutions in a fascinating new
               preface. Two of Minsky's colleagues, Dimitri B. Papadimitriou,
               Ph.D. and president, The Levy Economics Institute of Bard
               College, and L. Randall Wray, Ph.D. and a senior scholar at the
               Institute, also weigh in on Minsky's present relevance in
               today's economic scene in a new introduction. A surge of
               interest in and respect for Hyman Minsky's ideas pervades Wall
               Street, as top economic thinkers and financial writers have
               started using the phrase ``Minsky moment'' to describe America's
               turbulent economy. There has never been a more appropriate time
               to read this classic of economic theory.},
	Author = {Minsky, Hyman},
	Language = {en},
	Month = apr,
	Publisher = {McGraw Hill Professional},
	Title = {Stabilizing an Unstable Economy},
	Year = 2008}

@article{Anderson1952,
	Abstract = {Project Euclid - mathematics and statistics online},
	Author = {Anderson, T W and Darling, D A},
	Journal = {Ann. Math. Stat.},
	Keywords = {0*},
	Language = {en},
	Month = jun,
	Number = 2,
	Pages = {193--212},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Asymptotic Theory of Certain ``Goodness of Fit'' Criteria Based on Stochastic Processes},
	Volume = 23,
	Year = 1952}

@article{Kuosmanen2009,
	Abstract = {Data envelopment analysis (DEA) is known as a nonparametric
               mathematical programming approach to productive efficiency
               analysis. In this paper, we show that DEA can be alternatively
               interpreted as nonparametric least-squares regression subject to
               shape constraints on the frontier and sign constraints on
               residuals. This reinterpretation reveals the classic parametric
               programming model by Aigner and Chu [Aigner, D., S. Chu. 1968.
               On estimating the industry production function. Amer. Econom.
               Rev. 58 826?839] as a constrained special case of DEA. Applying
               these insights, we develop a nonparametric variant of the
               corrected ordinary least-squares (COLS) method. We show that
               this new method, referred to as corrected concave nonparametric
               least squares (C2NLS), is consistent and asymptotically
               unbiased. The linkages established in this paper contribute to
               further integration of the econometric and axiomatic approaches
               to efficiency analysis.},
	Author = {Kuosmanen, Timo and Johnson, Andrew L},
	Journal = {Oper. Res.},
	Month = oct,
	Number = 1,
	Pages = {149--160},
	Publisher = {INFORMS},
	Title = {Data Envelopment Analysis as Nonparametric {Least-Squares} Regression},
	Volume = 58,
	Year = 2009}

@article{Davis2008,
	Abstract = {Despite the extensive literature on prediction of banking crises
              by Early Warning Systems (EWSs), their practical use by policy
              makers is limited, even in the international financial
              institutions. This is a paradox since the changing nature of
              banking risks as more economies liberalise and develop their
              financial systems, as well as ongoing innovation, makes the use
              of EWS for informing policies aimed at preventing crises more
              necessary than ever. In this context, we assess the logit and
              signal extraction EWS for banking crises on a comprehensive
              common dataset. We suggest that logit is the most appropriate
              approach for global EWS and signal extraction for
              country-specific EWS. Furthermore, it is important to consider
              the policy maker's objectives when designing predictive models
              and setting related thresholds since there is a sharp trade-off
              between correctly calling crises and false alarms.},
	Author = {Davis, E Philip and Karim, Dilruba},
	Journal = {Journal of Financial Stability},
	Keywords = {Banking crises; Systemic risk; Early warning systems; Logit estimation; Signal extraction},
	Month = jun,
	Number = 2,
	Pages = {89--120},
	Title = {Comparing early warning systems for banking crises},
	Volume = 4,
	Year = 2008}

@article{Assaf2013,
	Abstract = {This paper analyzes the productivity and efficiency of Turkish
               banks from 2002 to 2010. We obtained estimates of efficiency,
               productivity growth and efficiency growth using a Bayesian
               stochastic frontier approach and focused on accounting for
               Non-Performing Loans (NPLs) for use in our model. Specifically,
               we introduce NPLs as a bad output in an input distance function,
               and estimate a system of non-linear equations subject to
               endogeneity. We confirm that the productivity growth of Turkish
               banks was positive over the period of this study, which was
               mainly due to the improvement in technology, while efficiency
               growth continued to be negative over the same period.
               Methodologically, we also prove that not accounting for NPLs in
               estimating the frontier model might seriously distort the
               efficiency and productivity results. The study also provides
               measures of shadow prices for NPL and discusses the results in
               terms of several interesting trends in Turkish banking. Finally,
               the paper provides efficiency and productivity comparisons
               between domestic and foreign banks.},
	Author = {Assaf, George and Matousek, Roman and Tsionas, Efthymios G},
	Journal = {Journal of Banking \& Finance},
	Keywords = {Turkey; Bank; Non-performing loans; Bayesian distance function;3*;UndesirableOutputs;european banking efficiency;shadow price;Non performing loans;quality of financial intermediation},
	Month = feb,
	Number = 2,
	Pages = {506--517},
	Publisher = {Elsevier},
	Title = {Turkish bank efficiency: Bayesian estimation with undesirable outputs},
	Volume = 37,
	Year = 2013}

@misc{European_Central_Bank2017,
	Author = {{European Central Bank}},
	Booktitle = {European Central Bank - Banking Supervision},
	Howpublished = {\url{https://www.bankingsupervision.europa.eu/press/speeches/date/2017/html/ssm.sp171114_1.en.html}},
	Month = nov,
	Note = {Accessed: 2018-5-31},
	Title = {Banking supervision - What next?},
	Year = 2017}

@article{Altunbas2007,
	Abstract = {This paper analyses the relationship between capital, risk and
               efficiency for a large sample of European banks between 1992 and
               2000. In contrast to the established US evidence we do not find
               a positive relationship between inefficiency and bank
               risk-taking. Inefficient European banks appear to hold more
               capital and take on less risk. Empirical evidence is found
               showing the positive relationship between risk on the level of
               capital (and liquidity), possibly indicating regulators'
               preference for capital as a mean of restricting risk-taking
               activities. We also find evidence that the financial strength of
               the corporate sector has a positive influence in reducing bank
               risk-taking and capital levels. There are no major differences
               in the relationships between capital, risk and efficiency for
               commercial and savings banks although there are for co-operative
               banks. In the case of co-operative banks we do find that capital
               levels are inversely related to risks and we find that
               inefficient banks hold lower levels of capital. Some of these
               relationships also vary depending on whether banks are among the
               most or least efficient operators.},
	Author = {Altunbas, Yener and Carbo, Santiago and Gardener, Edward P M and Molyneux, Philip},
	Journal = {European Financial Management},
	Keywords = {bank capital; risk; efficiency; credit; European banks; E5; E52; G21;european banking efficiency;Mendeley Import (May 05)/Banking/bcp},
	Month = jan,
	Number = 1,
	Pages = {49--70},
	Publisher = {Blackwell Publishing Ltd},
	Title = {Examining the Relationships between Capital, Risk and Efficiency in European Banking},
	Volume = 13,
	Year = 2007}

@article{Spatt2012,
	Author = {Spatt, Chester S},
	Journal = {Harvard Business Law Review Online},
	Pages = {1--9},
	Title = {Complexity of regulation},
	Volume = 3,
	Year = 2012}

@article{Averch1962,
	Author = {Averch, Harvey and Johnson, Leland L},
	Journal = {Am. Econ. Rev.},
	Keywords = {unintended consequences;BAFA2018},
	Number = 5,
	Pages = {1052--1069},
	Publisher = {American Economic Association},
	Title = {Behavior of the Firm Under Regulatory Constraint},
	Volume = 52,
	Year = 1962}

@incollection{Johnson2015,
	Abstract = {This chapter describes the economic insights of the unifying
               framework known as Stochastic semi-Nonparametric Envelopment of
               Data (StoNED), which combines the virtues of the widely used
               neoclassic production models, Data Envelopment Analysis (DEA),
               and Stochastic Frontier Analysis (SFA). Like DEA, StoNED is able
               to estimate an axiomatic production function relaxing the
               functional form specification required in most implementations
               of SFA. However, StoNED is also consistent with the econometric
               models of noise, providing a distinct advantage over standard
               DEA models. Further, StoNED allows for the possibility that
               systematic inefficiency is negligible consistent with
               neoclassical theory, thus providing a unifying framework. StoNED
               is implemented by estimating a conditional mean using convex
               nonparametric least squares (CNLS) followed by using standard
               SFA techniques to estimate the average efficiency and decompose
               the residual. Detailed descriptions of General Algebraic
               Modeling System (GAMS) and matrix laboratory (MATLAB) code will
               aid readers in implementing the StoNED estimator.},
	Author = {Johnson, Andrew L and Kuosmanen, Timo},
	Booktitle = {Benchmarking for Performance Evaluation},
	Language = {en},
	Pages = {117--186},
	Publisher = {Springer, New Delhi},
	Title = {An Introduction to {CNLS} and {StoNED} Methods for Efficiency Analysis: Economic Insights and Computational Aspects},
	Year = 2015}

@article{Merton1936,
	Author = {Merton, Robert K},
	Journal = {Am. Sociol. Rev.},
	Keywords = {unintended consequences;BAFA2018},
	Number = 6,
	Pages = {894--904},
	Publisher = {[American Sociological Association, Sage Publications, Inc.]},
	Title = {The Unanticipated Consequences of Purposive Social Action},
	Volume = 1,
	Year = 1936}

@article{Glass2014a,
	Abstract = {This study examines the relative performance of Japanese
              cooperative banks between 1998 and 2009, explicitly modelling
              non-performing loans as an undesirable output. Three key findings
              emerge. First, the sector is characterized by increasing returns
              to scale which supports the ongoing amalgamation process within
              the sector. Second, although restricted in product offerings,
              markets and their membership base, Japanese cooperatives secured
              both technical progress (a positive shift in the frontier) and a
              decrease in technical inefficiency (distance from the frontier).
              Third, the analysis highlighted that regulatory pressure to
              reduce non-performing loans will have an adverse impact on both
              output and performance. \copyright{} 2012 \copyright{} 2012
              Taylor \& Francis.},
	Author = {Glass, J C and McKillop, D G and Quinn, B and Wilson, J},
	Date-Modified = {2019-12-12 17:23:22 +0000},
	Journal = {European Journal of Finance},
	Keywords = {Japanese cooperative banks; efficiency; regulatory compliance;my papers;UndesirableOutputs;Mendeley Import (May 05)/Productivity/Efficiency;Mendeley Import (May 05)},
	Number = 3,
	Pages = {291--317},
	Title = {Cooperative bank efficiency in Japan: A parametric distance function analysis},
	Volume = 20,
	Year = 2014}

@book{Fare2012,
	Abstract = {Our original reason for writing this book was the desire to
               write down in one place a complete summary of the major results
               in du ality theory pioneered by Ronald W. Shephard in three of
               his books, Cost and Production Functions (1953), Theory of Cost
               and Produc tion Functions (1970), and Indirect Production
               Functions (1974). In this way, newcomers to the field would have
               easy access to these important ideas. In adg,ition, we report a
               few new results of our own. In particular, we show the duality
               relationship between the profit function and the eight
               equivalent representations of technol ogy that were elucidated
               by Shephard. However, in planning the book and discussing it
               with colleagues it became evident that such a book would be more
               useful if it also provided a number of applications of
               Shephard's duality theory to economic problems. Thus, we have
               also attempted to present exam ples of the use of duality theory
               in areas such as efficiency measure ment, index number theory,
               shadow pricing, cost-benefit analysis, and econometric
               estimation. Much of our thinking about duality theory and its
               uses has been influenced by our present and former
               collaborators. They include Charles Blackorby, Shawna Grosskopf,
               Knox Lovell, Robert Russell, and, not surprisingly, Ronald W.
               Shephard. We have also benefit ted over the years from many
               discussions with W. Erwin Diewert.},
	Author = {F{\"a}re, Rolf and Primont, Daniel},
	Keywords = {Mendeley Import (May 05)/Productivity/Efficiency},
	Language = {en},
	Month = sep,
	Publisher = {Springer Netherlands},
	Title = {{Multi-Output} Production and Duality: Theory and Applications},
	Year = 2012}

@article{Matousek2015,
	Abstract = {Abstract This paper investigates the process of banking
               integration in the EU15 countries and the Eurozone by testing
               for convergence in bank efficiency among commercial banks. We
               use a two-step approach: First we estimate efficiency by
               applying an innovative},
	Author = {Matousek, R and Rughoo, A and Sarantis, N and Assaf, A G},
	Journal = {Journal of Banking \&},
	Keywords = {european banking efficiency;UndesirableOutputs},
	Publisher = {Elsevier},
	Title = {Bank performance and convergence during the financial crisis: Evidence from the 'old'European Union and Eurozone},
	Year = 2015}

@article{Fare1990,
	Author = {F{\"a}re, R and Grosskopf, S},
	Journal = {J. Public Econ.},
	Publisher = {North-Holland},
	Title = {A distance function approach to price efficiency},
	Year = 1990}

@article{Malikov2016,
	Abstract = {This paper offers a methodology to address the endogeneity of
               inputs in the directional technology distance function
               (DTDF)-based formulation of banking technology which explicitly
               accommodates the presence of undesirable nonperforming
               loans---an inherent characteristic of the bank's production due
               to its exposure to credit risk. Specifically, we model
               nonperforming loans as an undesirable output in the bank's
               production process. Since the stochastic DTDF describing banking
               technology is likely to suffer from the endogeneity of inputs,
               we propose addressing this problem by considering a system
               consisting of the DTDF and the first-order conditions from the
               bank's cost minimization problem. The first-order conditions
               also allow us to identify the `cost-optimal' directional vector
               for the banking DTDF, thus eliminating the uncertainty
               associated with an ad hoc choice of the direction. We apply our
               cost system approach to the data on large US commercial banks
               for the 2001--2010 period, which we estimate via Bayesian Markov
               chain Monte Carlo methods subject to theoretical regularity
               conditions. We document dramatic distortions in banks'
               efficiency, productivity growth and scale elasticity estimates
               when the endogeneity of inputs is assumed away and/or the DTDF
               is fitted in an arbitrary direction. Copyright \copyright{} 2015
               John Wiley \& Sons, Ltd.},
	Author = {Malikov, Emir and Kumbhakar, Subal C and Tsionas, Mike G},
	Journal = {J. Appl. Econ.},
	Keywords = {UndesirableOutputs;Methodology;quality of financial intermediation},
	Month = nov,
	Number = 7,
	Pages = {1407--1429},
	Publisher = {Wiley Online Library},
	Title = {A Cost System Approach to the Stochastic Directional Technology Distance Function with Undesirable Outputs: The Case of us Banks in 2001--2010},
	Volume = 31,
	Year = 2016}

@article{Glass2010,
	Abstract = {The study investigates how producer-specific environmental
               factors influence the performance of Irish credit unions. The
               empirical analysis uses a two-stage approach. The first stage
               measures efficiency by a data envelopment analysis (DEA)
               estimator, which explicitly incorporates the production of
               undesirable outputs such as bad loans in the modelling, and the
               second stage uses truncated regression to infer how various
               factors influence the (bias-corrected) estimated efficiency. A
               key finding of the analysis is that 68\% of Irish credit unions
               do not incur an extra opportunity cost in meeting regulatory
               guidance on bad debt. \copyright{} 2009 Elsevier B.V. All rights
               reserved.},
	Author = {Glass, J Colin and McKillop, Donal G and Rasaratnam, Syamarlah},
	Journal = {Journal of Banking \& Finance},
	Keywords = {Credit unions; Efficiency; Regulatory compliance;credit union;3*;Mendeley Import (May 05)/Banking/bcp;Mendeley Import (May 05)/Productivity/Efficiency/OC of regulation},
	Month = jan,
	Number = 1,
	Pages = {67--76},
	Publisher = {Elsevier B.V.},
	Title = {Irish credit unions: Investigating performance determinants and the opportunity cost of regulatory compliance},
	Volume = 34,
	Year = 2010}

@article{Kuosmanen2010,
	Abstract = {We discuss the nonparametric approach to profit efficiency
              analysis at the firm and industry levels in the absence of
              complete price information. Two new insights are developed.
              First, we measure profit inefficiency in monetary terms using
              absolute shadow prices. Second, we evaluate all firms using the
              same input--output prices. This allows us to aggregate firm-level
              profit inefficiencies to the overall industry inefficiency.
              Besides the measurement of profit losses, the presented approach
              enables one to recover absolute price information from quantity
              data. We conduct a series of Monte Carlo simulations to study the
              performance of the proposed approach in controlled production
              environments.},
	Author = {Kuosmanen, Timo and Kortelainen, Mika and Sipil{\"a}inen, Timo and Cherchye, Laurens},
	Journal = {Eur. J. Oper. Res.},
	Keywords = {Data envelopment analysis (DEA); Directional distance function; The law of one price;4*},
	Month = apr,
	Number = 2,
	Pages = {584--594},
	Title = {Firm and industry level profit efficiency analysis using absolute and uniform shadow prices},
	Volume = 202,
	Year = 2010}

@article{Lim2012,
	Abstract = {Convex regression is concerned with computing the best fit of a
               convex function to a data set of n observations in which the
               independent variable is (possibly) multidimensional. Such
               regression problems arise in operations research, economics, and
               other disciplines in which imposing a convexity constraint on
               the regression function is natural. This paper studies a
               least-squares estimator that is computable as the solution of a
               quadratic program and establishes that it converges almost
               surely to the ?true? function as n ? $\infty$ under modest
               technical assumptions. In addition to this multidimensional
               consistency result, we identify the behavior of the estimator
               when the model is misspecified (so that the ?true? function is
               nonconvex), and we extend the consistency result to settings in
               which the function must be both convex and nondecreasing (as is
               needed for consumer preference utility functions).},
	Author = {Lim, Eunji and Glynn, Peter W},
	Journal = {Oper. Res.},
	Month = feb,
	Number = 1,
	Pages = {196--208},
	Publisher = {INFORMS},
	Title = {Consistency of Multidimensional Convex Regression},
	Volume = 60,
	Year = 2012}

@article{Kuosmanen2012,
	Abstract = {The field of productive efficiency analysis is currently divided
               between two main paradigms: the deterministic, nonparametric
               Data Envelopment Analysis (DEA) and the parametric Stochastic
               Frontier Analysis (SFA). This paper examines an encompassing
               semiparametric frontier model that combines the DEA-type
               nonparametric frontier, which satisfies monotonicity and
               concavity, with the SFA-style stochastic homoskedastic composite
               error term. To estimate this model, a new two-stage method is
               proposed, referred to as Stochastic Non-smooth Envelopment of
               Data (StoNED). The first stage of the StoNED method applies
               convex nonparametric least squares (CNLS) to estimate the shape
               of the frontier without any assumptions about its functional
               form or smoothness. In the second stage, the conditional
               expectations of inefficiency are estimated based on the CNLS
               residuals, using the method of moments or pseudolikelihood
               techniques. Although in a cross-sectional setting distinguishing
               inefficiency from noise in general requires distributional
               assumptions, we also show how these can be relaxed in our
               approach if panel data are available. Performance of the StoNED
               method is examined using Monte Carlo simulations.},
	Author = {Kuosmanen, Timo and Kortelainen, Mika},
	Journal = {J Prod Anal},
	Keywords = {Mendeley Import (May 05)/Productivity/Efficiency/StoNED;StoNED},
	Language = {en},
	Month = aug,
	Number = 1,
	Pages = {11--28},
	Publisher = {Springer US},
	Title = {Stochastic non-smooth envelopment of data: semi-parametric frontier estimation subject to shape constraints},
	Volume = 38,
	Year = 2012}

@article{Kuosmanen2004,
	Abstract = {This paper explores an intermediate route between the Fisher and
               the Malmquist productivity indexes so as to minimize data
               requirements and assumptions about economic behavior of
               production units and their production technology. Assuming
               quantity data of inputs and outputs and the behavioral
               hypothesis of allocative efficiency, we calculate the exact
               value of the Fisher ideal productivity index using implicit
               shadow prices revealed by the choice of input--output mix. The
               approach is operationalized by means of a nonparametric data
               envelopment analysis (DEA) model. Empirical application to
               Finnish grass silage farms suggests that the Malmquist and the
               Fisher productivity indices yield similar results when averaged
               over firms, but there can be major differences in the results of
               the two approaches at the level of individual firms.},
	Author = {Kuosmanen, Timo and Post, Thierry and Sipil{\"a}inen, Timo},
	Journal = {Journal of Productivity Analysis},
	Language = {en},
	Month = jul,
	Number = {1-2},
	Pages = {95--121},
	Publisher = {Kluwer Academic Publishers},
	Title = {Shadow Price Approach to Total Factor Productivity Measurement: With an Application to Finnish {Grass-Silage} Production},
	Volume = 22,
	Year = 2004}

@book{Conover1999,
	Abstract = {This highly-regarded text serves as a quick reference book which
               offers clear, concise instructions on how and when to use the
               most popular nonparametric procedures. This edition features
               some procedures that have withstood the test of time and are now
               used by many practitioners, such as the Fisher Exact Test for
               two-by-two contingency tables, the Mantel-Haenszel Test for
               combining several contingency tables, the Kaplan-Meier estimates
               of the survival curve, the Jonckheere-Terpstra Test and the Page
               Test for ordered alternatives, and a discussion of the bootstrap
               method.},
	Author = {Conover, W J},
	Edition = {3 edition},
	Language = {en},
	Publisher = {Wiley},
	Title = {Practical nonparametric statistics},
	Year = 1999}

@techreport{Bholat2016,
	Abstract = {Asset quality is an essential part of sound banking. However,
                 asset quality is difficult for banking regulators and
                 investors to assess in the absence of a comm},
	Author = {Bholat, David and Lastra, Rosa M and Markose, Sheri M and Miglionico, Andrea and Sen, Kallol},
	Institution = {Bank of England},
	Keywords = {Non-performing loans, impairment, loan loss provisions, bank capital, data standards, credit risk},
	Month = apr,
	Title = {{Non-Performing} Loans: Regulatory and Accounting Treatments of Assets},
	Year = 2016}

@article{Seijo2011,
	Author = {Seijo, Emilio and Sen, Bodhisattva},
	Journal = {Ann. Stat.},
	Keywords = {Consistency; linear program; semidefinite quadratic program; shape restricted estimation; subdifferentials},
	Language = {en},
	Month = jun,
	Number = 3,
	Pages = {1633--1657},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Nonparametric least squares estimation of a multivariate convex regression function},
	Volume = 39,
	Year = 2011}

@article{Murray1995,
	Abstract = {Empirical estimation of input market power is hindered by
               problems in measuring an input's value of marginal product
               (VMP). By estimating a variable profit function system, however,
               one can infer a factor's VMP through its shadow price. This
               technique is used here to specify a structural equation system,
               which is estimated using time series data for the U.S.
               sawmilling and paper industries, to empirically measure the
               degree of oligopsony power for sawlog and pulpwood inputs
               respectively. Results evaluated at sample means indicate that
               pulpwood markets are more oligopsonistic than sawlog markets,
               though both perform closer to perfect competition than
               monopsony. Time trends for market power differ for each product
               and perfect competition cannot be rejected for sawlogs in later
               years.},
	Author = {Murray, Brian C},
	Journal = {Rev. Econ. Stat.},
	Number = 3,
	Pages = {486--498},
	Publisher = {The MIT Press},
	Title = {Measuring Oligopsony Power with Shadow Prices: {{U.S}}. Markets for Pulpwood and Sawlogs},
	Volume = 77,
	Year = 1995}

@article{Glass2014b,
	Abstract = {This study undertakes a modeling based performance assessment of
              all Irish credit unions between 2002 and 2010, a particularly
              turbulent period in their history. The analysis explicitly
              addresses the current challenges faced by credit unions in that
              the modeling approach used rewards credit unions for reducing
              undesirable outputs (impaired loans and investments) as well as
              for increasing desirable outputs (loans, earning assets and
              members' funds) and decreasing inputs (labour expenditure,
              capital expenditure and fund expenses). The main findings are:
              credit unions are subject to increasing returns to scale;
              technical regression occurred in the years after 2007; there is
              significant scope for an improvement in efficiency through
              expansion of desirable outputs and contraction of undesirable
              outputs and inputs; and that larger credit unions, that are
              better capitalised and pay a higher dividend to members are more
              efficient than their smaller, less capitalised, and lower
              dividend paying counterparts.},
	Author = {Glass, J Colin and McKillop, Donal G and Quinn, Barry},
	Date-Modified = {2019-12-12 17:23:26 +0000},
	Journal = {Financial Accountability \& Management},
	Keywords = {credit unions; efficiency; impaired loans and investments;my papers;credit union;UndesirableOutputs},
	Month = nov,
	Number = 4,
	Pages = {430--453},
	Title = {Modelling the Performance of Irish Credit Unions, 2002 to 2010},
	Volume = 30,
	Year = 2014}

@article{Lutter2013,
	Abstract = {Given that President Obama's Executive Orders on regulation have
               emphasized the importance of retrospective analysis and review
               of existing federal rules, I survey the state of retrospective
               analysis and review of federal regulations. I first ask how much
               is known about the economic merit of past federal regulatory
               decisions, based on retrospective economic analyses of their
               effects. I review reports of the Office of Management and Budget
               and related literature, but unlike those reports I find only
               five rules, issued by the National Highway Traffic Safety
               Administration (NHTSA), for which retrospective analyses provide
               estimates of both costs and reasonably good proxies for benefits
               (e.g., adverse health effects avoided). Other retrospective
               studies of federal rules estimate are incomplete, estimating
               only the compliance costs, or only the benefits, or only costs
               and measures of effectiveness, like emissions reductions, which
               do not closely relate to people's welfare.I also seek to explain
               differences in the practice of retrospective analysis and review
               between NHTSA, which appears to have the best record of
               retrospective analyses among federal agencies, and the
               Environmental Protection Agency (EPA), an important regulatory
               agency. I find that NHTSA regularly conducts such analyses and
               reviews, while EPA rarely does retrospective analysis and
               presents rulemakings that look like business as usual as being
               the result of a retrospective review. I analyze the role of data
               limitations, statutory authority, and institutional incentives
               in influencing the different behaviors of these two agencies. I
               conclude that differences in data availability and in particular
               the lack of relevant control groups, are an important barrier to
               retrospective analysis at EPA. This data deficiency, combined
               with important restrictions on EPA's ability to consider
               information on net benefits or cost-effectiveness in its
               rule-making, are together the biggest hindrance to generating
               better information about the effects of its rules. I conclude
               with recommendations intended to generate more measurement of
               the actual effects of regulations.},
	Author = {Lutter, Randall},
	Journal = {Journal of Benefit-Cost Analysis},
	Keywords = {retrospective requlatory analysis; regulatory review},
	Month = jan,
	Number = 1,
	Pages = {17--38},
	Publisher = {Cambridge University Press},
	Title = {Regulatory policy: what role for retrospective analysis and review?},
	Volume = 4,
	Year = 2013}

@article{Anderson1954,
	Abstract = {Abstract Some (large sample) significance points are tabulated
               for a distribution-free test of goodness of fit which was
               introduced earlier by the authors. The test, which uses the
               actual observations without grouping, is sensitive to
               discrepancies at the tails of the distribution rather than near
               the median. An illustration is given, using a numerical example
               used previously by Birnbaum in illustrating the Kolmogorov test.},
	Author = {Anderson, T W and Darling, D A},
	Journal = {J. Am. Stat. Assoc.},
	Month = dec,
	Number = 268,
	Pages = {765--769},
	Publisher = {Taylor \& Francis},
	Title = {A Test of Goodness of Fit},
	Volume = 49,
	Year = 1954}

@incollection{Galema2016,
	Abstract = {This chapter explores European bank efficiency and performance.
               First, the authors provide an overview of the key estimation
               methods for efficiency and discuss selected applications to the
               European banking sector. Second, they apply stochastic frontier
               analysis to investigate the extent to which the reallocation of
               supervisory powers is associated with efficiency differences
               between European banks. In doing so, the discussion focuses
               particularly on whether direct supervision by the Single
               Supervisory Mechanism (SSM) as opposed to national competent
               authority (NCA) is related to cost and profit efficiency.},
	Author = {Galema, Rients and Koetter, Michael},
	Booktitle = {The Palgrave Handbook of European Banking},
	Editor = {Beck, Thorsten and Casu, Barbara},
	Keywords = {Research Idea;banking union;european banking efficiency},
	Language = {en},
	Pages = {257--292},
	Publisher = {Palgrave Macmillan, London},
	Title = {European Bank Efficiency and Performance: The Effects of Supranational Versus National Bank Supervision},
	Year = 2016}

@article{Carmichael2011,
	Abstract = {No abstract is available for this item.},
	Author = {Carmichael, Fiona and McHale, Ian and Thomas, Dennis},
	Journal = {Bull. Econ. Res.},
	Keywords = {read once;Mendeley Import (May 05)/football;Football Economics},
	Number = 4,
	Pages = {464--479},
	Publisher = {Wiley Blackwell},
	Title = {Maintaining Market Position: Team Performance, Revenue And Wage Expenditure In The English Premier League},
	Volume = 63,
	Year = 2011}

@techreport{Kuosmanen2013,
	Author = {Kuosmanen, Timo},
	Institution = {OECD},
	Title = {Green productivity in agriculture: A critical synthesis},
	Year = 2013}

@article{Stigler1971,
	Abstract = {The potential uses of public resources and powers to improve the
               economic status of economic groups (such as industries and
               occupations) are analyzed to provide a scheme of the demand for
               regulation. The characteristics of the political process which
               allow relatively small groups to obtain such regulation is then
               sketched to provide elements of a theory of supply of
               regulation. A variety of empirical evidence and illustration is
               also presented.},
	Author = {Stigler, George J},
	Journal = {The Bell Journal of Economics and Management Science},
	Keywords = {unintended consequences;BAFA2018},
	Number = 1,
	Pages = {3--21},
	Publisher = {[Wiley, RAND Corporation]},
	Title = {The Theory of Economic Regulation},
	Volume = 2,
	Year = 1971}

@article{Kuosmanen2006,
	Abstract = {The Law of One Price (LoOP) states that all firms face the same
              prices for their inputs and outputs under market equilibrium.
              Taken here as a normative condition for `efficiency prices', this
              law has powerful implications for productive efficiency analysis,
              which have remained unexploited thus far. This paper shows how
              LoOP-based weight restrictions can be incorporated in Data
              Envelopment Analysis (DEA). Utilizing the relation between
              industry-level and firm-level cost efficiency measures, we
              propose to apply a set of input prices that is common for all
              firms and that maximizes the cost efficiency of the industry. Our
              framework allows for firm-specific output weights and for
              variable returns-to-scale, and preserves the linear programming
              structure of the standard DEA. We apply the proposed methodology
              to the evaluation of the research efficiency of economics
              departments of Dutch Universities. This application shows that
              the methodology is computationally tractable for practical
              efficiency analysis, and that it helps in deepening the DEA
              analysis.},
	Author = {Kuosmanen, Timo and Cherchye, Laurens and Sipil{\"a}inen, Timo},
	Journal = {Eur. J. Oper. Res.},
	Keywords = {Data envelopment analysis; Law of one price; Industry-level efficiency; Weight restrictions; Research efficiency;4*},
	Month = may,
	Number = 3,
	Pages = {735--757},
	Title = {The law of one price in data envelopment analysis: Restricting weight flexibility across firms},
	Volume = 170,
	Year = 2006}

@article{Kuosmanen2008,
	Abstract = {Summary We examine a nonparametric least-squares regression
               model that endogenously selects the functional form of the
               regression function from the family of continuous, monotonic
               increasing and globally concave functions that can be
               nondifferentiable. We show that this family of functions can be
               characterized without a loss of generality by a subset of
               continuous, piece-wise linear functions whose intercept and
               slope coefficients are constrained to satisfy the required
               monotonicity and concavity conditions. This representation
               theorem is useful at least in three respects. First, it enables
               us to derive an explicit representation for the regression
               function, which can be used for assessing marginal properties
               and for the purposes of forecasting and ex post economic
               modelling. Second, it enables us to transform the infinite
               dimensional regression problem into a tractable quadratic
               programming (QP) form, which can be solved by standard QP
               algorithms and solver software. Importantly, the QP formulation
               applies to the general multiple regression setting. Third, an
               operational computational procedure enables us to apply
               bootstrap techniques to draw statistical inference.},
	Author = {Kuosmanen, Timo},
	Journal = {Econom. J.},
	Keywords = {Concavity; Convexity; Curve fitting; Linear splines; Local linear approximation; Nonparametric methods; Regression analysis;Mendeley Import (May 05)/Productivity/Efficiency/StoNED;StoNED},
	Month = jul,
	Number = 2,
	Pages = {308--325},
	Publisher = {Blackwell Publishing Ltd},
	Title = {Representation theorem for convex nonparametric least squares},
	Volume = 11,
	Year = 2008}

@article{Demirguc-Kunt2005,
	Abstract = {A rapidly growing empirical literature is studying the causes
               and consequences of bank fragility in contemporary economies.
               The paper reviews the two basic methodologies adopted in
               cross-country empirical studies, the signals approach and the
               multivariate probability model, and their application to study
               the determinants of banking crises. The use of these models to
               provide early warnings for crises is also reviewed, as are
               studies of the economic effects of banking crises and of the
               policies to forestall them. The paper concludes by identifying
               directions for future research.},
	Author = {Demirg{\"u}{\c c}-Kunt, Asli and Detragiache, Enrica},
	Date-Modified = {2019-12-12 19:25:30 +0000},
	Journal = {Natl. Inst. Econ. Rev.},
	Month = apr,
	Number = 1,
	Pages = {68--83},
	Publisher = {SAGE Publications Ltd},
	Title = {{Cross-Country} Empirical Studies of Systemic Bank Distress: A Survey},
	Volume = 192,
	Year = 2005}

@article{Arrow1996,
	Author = {Arrow, K J and Cropper, M L and Eads, G C and Hahn, R W and Lave, L B and Noll, R G and Portney, P R and Russell, M and Schmalensee, R and Smith, V K and Stavins, R N},
	Journal = {Science},
	Language = {en},
	Month = apr,
	Number = 5259,
	Pages = {221--222},
	Title = {Is there a role for benefit-cost analysis in environmental, health, and safety regulation?},
	Volume = 272,
	Year = 1996}

@article{Fare1993,
	Abstract = {Many production activities generate undesirable byproducts in
               conjunction with the desirable outputs they produce. Pittman
               (1983) showed how to adjust productivity calculations, and
               F{\"a}re et al. (1989) showed how to adjust efficiency measures,
               in the presence of undesirable outputs. Here we show how to
               estimate output distance functions as frontiers in order to
               generate shadow values of the undesirable outputs that are
               required to make both types of adjustment. An empirical
               application is provided.},
	Author = {F{\"a}re, Rolf and Grosskopf, Shawna and {C. A. Knox Lovell} and Yaisawarng, Suthathip},
	Journal = {Rev. Econ. Stat.},
	Keywords = {WIP},
	Number = 2,
	Pages = {374--380},
	Publisher = {The MIT Press},
	Title = {Derivation of Shadow Prices for Undesirable Outputs: A Distance Function Approach},
	Volume = 75,
	Year = 1993}

@article{Wang2013b,
	Abstract = {Improving energy efficiency and productivity is one of the most
              cost-effective ways for achieving the sustainable development
              target in China. This paper employs non-radial directional
              distance function approach to empirically investigate energy
              efficiency and energy productivity by including CO2 emissions as
              an undesirable output. Three production scenarios, namely energy
              conservation (EC), energy conservation and emission reduction
              (ECER), and energy conservation, emission reduction and economic
              growth (ECEREG), are specified to assess China's energy
              efficiency and productivity growth during the period of Eleventh
              Five-Year Plan. Our empirical results show that there exist
              substantial differences in China's total-factor energy efficiency
              and productivity under different scenarios. Under the ECEREG
              scenario, the national average total-factor energy efficiency
              score was 0.6306 in 2005-2010, while the national average
              total-factor energy productivity increased by 0.27\% annually
              during the period. The main driving force for energy productivity
              growth in China was energy technological change rather than
              energy efficiency change. ?? 2013 Elsevier B.V.},
	Author = {Wang, H and Zhou, P and Zhou, D Q},
	Date-Modified = {2019-12-12 17:22:58 +0000},
	Journal = {Energy Econ.},
	Keywords = {CO2 emissions; Energy efficiency; Energy productivity; Non-radial directional distance function},
	Month = nov,
	Pages = {795--803},
	Title = {Scenario-based energy efficiency and productivity in China: A non-radial directional distance function analysis},
	Volume = 40,
	Year = 2013}

@article{Wang2013a,
	Abstract = {Considering carbon dioxide (CO2) emitters in different groups
              that may not have the same technology, this paper extends the
              Malmquist emission performance index (MCPI) for measuring changes
              in CO2 emission performance using a nonparametric metafrontier.
              The MCPI can be decomposed into two components: efficiency change
              index and technological change index. Meanwhile, the
              interrelationships between two MCPIs, relative to the
              metafrontier and the group frontier respectively, are also
              examined by the key factor, technology gap ratio (TGR).},
	Author = {Wang, Qunwei and Zhang, Huiming and Zhang, Wei},
	Date-Modified = {2019-12-12 17:22:53 +0000},
	Journal = {Math. Comput. Model.},
	Keywords = {CO2 emission performance; Malmquist index; Metafrontier function; Undesirable output; CO emission performance},
	Month = sep,
	Number = 5,
	Pages = {1068--1073},
	Title = {A Malmquist {CO2} emission performance index based on a metafrontier approach},
	Volume = 58,
	Year = 2013}

@article{Zhou2010,
	Abstract = {This paper introduces a Malmquist CO2 emission performance index
              (MCPI) for measuring changes in total factor carbon emission
              performance over time. The MCPI is derived by solving several
              data envelopment analysis models. Bootstrapping MCPI is proposed
              to perform statistical inferences on the MCPI results. Using the
              index the emission performance of the world's 18 top CO2 emitters
              from 1997 to 2004 is studied. The results obtained show that the
              total factor carbon emission performance of the countries as a
              whole improved by 24\% over the period and this was mainly driven
              by technological progress. The results of a cross-country
              regression analysis to investigate the determinants of the
              resulting MCPI are presented.},
	Author = {Zhou, P and Ang, B W and Han, J Y},
	Journal = {Energy Econ.},
	Keywords = {Bootstrap; C61; Carbon dioxide emissions; D24; Data envelopment analysis; Malmquist index; Q43; Q54; Total factor productivity},
	Month = jan,
	Number = 1,
	Pages = {194--201},
	Title = {Total factor carbon emission performance: A Malmquist index analysis},
	Volume = 32,
	Year = 2010}

@article{Bretholt2013,
	Abstract = {This article tests several nonparametric DEA models for their
              ability to accurately decompose CO2 emissions change using a
              Malmquist styled decomposition framework. This production
              oriented activity analysis involves panel data and two data sets
              from the literature for comparison. A new Latent Variable radial
              input-oriented technology is introduced that is closely
              associated with a Koopmans Efficient Slacks Based Model. The
              Latent Variable technology simultaneously reduces inputs and
              undesirable outputs in a single Multiple Objective Linear
              Program. This production theoretic methodology is adapted to
              preserve both scale efficiency and causality within the
              envelopment framework. Finally, the application studies
              demonstrate the internal consistency of the Latent Variable
              reduction coefficients, which overturns previous results and
              paves the way for further research into undesirable
              externalities.},
	Author = {Bretholt, Abraham and Pan, Jeh-Nan},
	Journal = {Omega},
	Keywords = {DEA; Decomposition; Efficiency; Malmquist Index; Multicriteria; Optimization; Resource management; Scale efficiency; Slacks based model; Undesirable outputs and externalities},
	Month = apr,
	Number = 2,
	Pages = {315--325},
	Title = {Evolving the latent variable model as an environmental {DEA} technology},
	Volume = 41,
	Year = 2013}

@article{Agee2014,
	Abstract = {The federal government now confronts considerable political
              pressure to add CO2 to the existing set of criteria air
              pollutants. As with current criteria pollutants, proposals call
              for control of CO2, assuming that the control of each of the
              three criteria pollutants is separable from the others. However,
              control of CO2, SO2, and NOX emissions is most appropriately
              viewed as joint rather than separable based on engineering
              relationships. Empirically, we also find considerable jointness.
              Using a 10-year panel for 77 U.S. electric utilities, which
              comprise the largest sector in terms of energy-related CO2
              emissions, we estimate a multiple-input, multiple-output
              directional distance function combining good inputs (production
              capital, pollution control capital, labor, and energy) and a bad
              input (sulfur burned) to produce good outputs (residential and
              industrial/commercial electricity production) and bad outputs
              (SO2, NOX, and CO2). We find that while utilities do not directly
              control CO2 emissions, considerable jointness exists across SO2,
              NOX, and CO2 emissions. Failure to account for this jointness
              increases the cost of pollution control, making it less
              acceptable to the public and policymakers. We also compute the
              technical efficiency of our set of utilities and find that
              considerable cost savings can be achieved by adopting the best
              technology for production of electricity and reduction of
              pollutants.},
	Author = {Agee, Mark D and Atkinson, Scott E and Crocker, Thomas D and Williams, Jonathan W},
	Journal = {Res. Energy Econ.},
	Keywords = {U.S. electric power generation; CO2, SO2, NOX emissions; Efficient cap and trade system design; Directional distance function; Technical change; CO; SO; NO emissions},
	Month = jan,
	Number = 1,
	Pages = {64--82},
	Title = {Non-separable pollution control: Implications for a {CO2} emissions cap and trade system},
	Volume = 36,
	Year = 2014}

@article{Sueyoshi2012,
	Abstract = {This study discusses a new use of DEA (Data Envelopment Analysis)
              environmental assessment to measure MRT (Marginal Rate of
              Transformation) and RS (Rate of Substitution) between desirable
              and undesirable outputs. To discuss MRT and RS, this study first
              examines a concept of disposability from the perspective of
              corporate strategies to adapt a regulation change on undesirable
              outputs. The concept of disposability is separated into natural
              and managerial disposability. Then, this study explores the
              computational framework of RTS (Returns to Scale) and DTS
              (Damages to Scale). The type of RTS is measured within the
              natural disposability, while the type of DTS is measured within
              the managerial disposability. Considering the two types of
              disposability, this study discusses MRT and RS between desirable
              and undesirable outputs. As an illustrative example, this study
              applies the proposed approach to evaluate the performance of US
              coal-fired power plants. This study finds that the regulation
              policy on NOx and SO2 has been effective on their emission
              controls under US Clean Air Act (CAA). The regulation on CO2, or
              a major source of the global warming and climate change, is still
              insufficient in the United States. Therefore, this study
              recommends that US federal and local governments should regulate
              the amount of CO2 emission under the CAA.},
	Author = {Sueyoshi, Toshiyuki and Goto, Mika},
	Journal = {Energy Econ.},
	Keywords = {Environmental assessment; Returns to Scale; Damages to Scale; Rate of Substitution},
	Month = jul,
	Number = 4,
	Pages = {905--917},
	Title = {Returns to Scale, Damages to Scale, Marginal Rate of Transformation and Rate of Substitution in {DEA} Environmental Assessment},
	Volume = 34,
	Year = 2012}

@article{Zhou2014,
	Abstract = {Undesirable outputs (or bads) refer to the byproducts accompanied
              with desirable outputs (or goods) in a production process, e.g.
              sulfur dioxide and carbon dioxide in coal-fired power generation.
              The shadow price of undesirable output, which may be interpreted
              as the opportunity cost of abating one additional unit of
              undesirable output in terms of the loss of desirable output,
              could provide valuable reference information for policy analysis
              and making. A prevalent practice is to use the Shephard or
              directional distance function to derive the shadow price, which
              can be further calculated by parametric or nonparametric
              efficiency models. In application, earlier studies have estimated
              shadow prices at plant, sector and even economy levels. This
              study aims to conduct a systematic review of the studies on
              estimating shadow prices of undesirable outputs with efficiency
              models. We first introduce the methodological framework for
              deriving shadow prices as well as the nonparametric/parametric
              efficiency models for calculating their values. A systematic
              summary of over forty earlier studies in this field is then
              conducted, through which the key features of the existing studies
              are summarized and possible future research directions are
              identified.},
	Author = {Zhou, P and Zhou, X and Fan, L W},
	Journal = {Appl. Energy},
	Keywords = {CO2 emissions; Shadow price; Undesirable output; Efficiency; Distance function; CO emissions},
	Month = oct,
	Pages = {799--806},
	Title = {On estimating shadow prices of undesirable outputs with efficiency models: A literature review},
	Volume = 130,
	Year = 2014}

@article{Kuosmanen2017,
	Abstract = {Estimation of joint production technologies involving multiple
              outputs has proved a vexing challenge. Existing methods are
              unsatisfactory as they either assume away stochastic noise or
              restrict to functional forms that have incorrect output
              curvature. The first contribution of this paper is to develop a
              new probabilistic data generating process that is compatible with
              the directional distance function. The directional distance
              function is a very general functional characterization of
              production technology that has proved useful for modeling joint
              production of multiple outputs. The second contribution of this
              paper is to develop a new estimator of the directional distance
              function that builds upon axiomatic properties and does not
              require any functional form assumptions. The proposed estimator
              is a natural extension of stochastic nonparametric envelopment of
              data (StoNED) framework to multiple output setting. We examine
              the practical aspects and usefulness of the proposed approach in
              the context of incentive regulation of the Finnish electricity
              distribution firms.},
	Author = {Kuosmanen, Timo and Johnson, Andrew},
	Journal = {Eur. J. Oper. Res.},
	Keywords = {Data envelopment analysis (DEA); Economies of scope; Efficiency analysis; Frontier estimation; Stochastic nonparametric envelopment of data (StoNED)},
	Month = oct,
	Number = 2,
	Pages = {792--801},
	Title = {Modeling joint production of multiple outputs in {{StoNED}}: Directional distance function approach},
	Volume = 262,
	Year = 2017}

@article{Lee2012,
	Abstract = {China is the world's largest CO2 producer and energy consumer. In
              this paper, we calculate the maximum technically obtainable CO2
              emissions reduction from the efficient use of inputs and estimate
              the shadow prices of CO2 emissions in order to assess the
              potential cost savings deriving from trading emissions among
              industries by measuring the input distance function for 30
              Chinese manufacturing industries. Our empirical results indicate
              that CO2 emissions could be reduced by as much as 680million tons
              in the aggregate. The shadow prices of CO2 vary from a high of
              $18.82 to a low of zero across industries, with an average of
              $3.13 per ton. Additionally, the estimated indirect Morishima
              elasticities of substitution of capital for fossil fuels indicate
              that the substitutabilities of capital for oil, gas, and coal are
              higher than the substitutability for labor.},
	Author = {Lee, Myunghun and Zhang, Ning},
	Journal = {Energy Econ.},
	Keywords = {Technical efficiency; CO shadow price; Morishima substitution elasticity; Chinese manufacturing industry},
	Month = sep,
	Number = 5,
	Pages = {1492--1497},
	Title = {Technical efficiency, shadow price of carbon dioxide emissions, and substitutability for energy in the Chinese manufacturing industries},
	Volume = 34,
	Year = 2012}

@article{OBrien1981,
	Abstract = {Although experimental effects are usually assessed through
              contrasts of group means, there are situations in which
              differences among the groups' variances are also of interest.
              Such analyses are infrequently used in behavioral research,
              possibly because the most common methods are not robust to
              nonnormally distributed data. A procedure is presented that
              produces robust tests of the equality of cell variances by
              performing a regular ANOVA using a transformation of the
              dependent variable. Special contrasts (e.g., simple effects,
              subeffects) are discussed and an example is given. (29 ref)},
	Author = {O'Brien, Ralph G},
	Journal = {Psychol. Bull.},
	Number = 3,
	Pages = {570--574},
	Title = {A simple test for variance effects in experimental designs},
	Volume = 89,
	Year = 1981}

@article{Zaim2000,
	Abstract = {Abstract The role of the environment is an important issue in the
              policy-making and hence, the accurate assessment of the
              environmental conditions is vital. In this paper, an
              environmental efficiency index is developed for the OECD
              countries using non-parametric techniques. The approach adopted
              is based on the assumption that there is just one production
              process behind the production of both goods and pollution
              emissions. The index derived in this work measures the extent of
              the required output sacrifice, due to the transformation of the
              production process, from one where all outputs are strongly
              disposable to the one which is characterized by weak
              disposability of pollutants. Using this index, we first conduct
              cross-section comparisons on the state of each country's
              production process in its treatment of pollution emissions. We
              then trace each country's modification of their production
              processes overtime. The results indicate that if the
              disposability for CO2emissions were strictly restricted as the
              result of an environmental regulation, the total value of output
              loss to the OECD countries as a whole would correspond to 3.7,
              4.8 and 3.5\% of the total OECD GDP for 1980, 1985 and 1990,
              respectively.},
	Author = {Zaim, O and Taskin, F},
	Journal = {J. Environ. Manage.},
	Keywords = {environmental efficiency index, carbon dioxide emissions, non-parametric efficiency measurement.},
	Month = feb,
	Number = 2,
	Pages = {95--107},
	Title = {Environmental efficiency in carbon dioxide emissions in the {{OECD}}: A non-parametric approach},
	Volume = 58,
	Year = 2000}

@article{Gomes2008,
	Abstract = {Data envelopment analysis (DEA) literature has proposed
               alternative models for performance assessment in the presence of
               undesirable outputs, such as pollutant emissions, where
               increased outputs imply reduced performance. However, the case
               where global equilibrium of outputs should be imposed has not
               yet been considered. We propose that the zero sum gains DEA
               (ZSG-DEA) models look especially suitable for treating
               equilibrium models, where the sum of the quantities produced by
               all decision-making units can be set as the upper admissible
               bound. This paper uses ZSG-DEA models to evaluate the carbon
               dioxide emission case study, which can be considered part of the
               Kyoto Protocol statement.},
	Author = {Gomes, E G and Lins, M P E},
	Journal = {J. Oper. Res. Soc.},
	Language = {en},
	Month = may,
	Number = 5,
	Pages = {616--623},
	Publisher = {Palgrave Macmillan UK},
	Title = {Modelling undesirable outputs with zero sum gains data envelopment analysis models},
	Volume = 59,
	Year = 2008}

@article{Zhou2008,
	Abstract = {Data envelopment analysis (DEA) has recently gained popularity in
              energy efficiency analysis. A common feature of the previously
              proposed DEA models for measuring energy efficiency performance
              is that they treat energy consumption as an input within a
              production framework without considering undesirable outputs.
              However, energy use results in the generation of undesirable
              outputs as by-products of producing desirable outputs. Within a
              joint production framework of both desirable and undesirable
              outputs, this paper presents several DEA-type linear programming
              models for measuring economy-wide energy efficiency performance.
              In addition to considering undesirable outputs, our models treat
              different energy sources as different inputs so that changes in
              energy mix could be accounted for in evaluating energy
              efficiency. The proposed models are applied to measure the energy
              efficiency performances of 21 OECD countries and the results
              obtained are presented.},
	Author = {Zhou, P and Ang, B W},
	Journal = {Energy Policy},
	Keywords = {Energy efficiency; Undesirable outputs; Data envelopment analysis},
	Month = aug,
	Number = 8,
	Pages = {2911--2916},
	Title = {Linear programming models for measuring economy-wide energy efficiency performance},
	Volume = 36,
	Year = 2008}

@article{Zhou2012,
	Abstract = {This paper proposes a parametric frontier approach to estimating
              economy-wide energy efficiency performance from a production
              efficiency point of view. It uses the Shephard energy distance
              function to define an energy efficiency index and adopts the
              stochastic frontier analysis technique to estimate the index. A
              case study of measuring the economy-wide energy efficiency
              performance of a sample of OECD countries using the proposed
              approach is presented. It is found that the proposed parametric
              frontier approach has higher discriminating power in energy
              efficiency performance measurement compared to its nonparametric
              frontier counterparts.},
	Author = {Zhou, P and Ang, B W and Zhou, D Q},
	Journal = {Appl. Energy},
	Keywords = {Energy efficiency; Distance function; Stochastic frontier analysis; Data envelopment analysis},
	Month = feb,
	Number = 1,
	Pages = {196--200},
	Title = {Measuring economy-wide energy efficiency performance: A parametric frontier approach},
	Volume = 90,
	Year = 2012}

@book{Ancev2017,
	Abstract = {This book explores novel research perspectives on the
               intersection of environmental/natural resource economics and
               productivity analysis, emphasizing the link between productivity
               and efficiency measurement and environmental impacts. The
               purpose of the book is to present new approaches and methods for
               measuring environmentally adjusted productivity and efficiency,
               and for incorporating natural resources in standard national
               accounting practices. These methods are applicable in many
               contexts, including air and water pollution, climate change,
               green accounting, and environmental regulation},
	Author = {Ancev, Tihomir and Azad, M A Samad and Hern{\'a}ndez-Sancho, Francesc},
	Language = {en},
	Month = jun,
	Publisher = {Edward Elgar Publishing},
	Title = {New Directions in Productivity Measurement and Efficiency Analysis: Counting the Environment and Natural Resources},
	Year = 2017}

@article{Zhou2012,
	Abstract = {This paper presents a non-radial directional distance function
              approach to modeling energy and CO2emission performance in
              electricity generation from the production efficiency point of
              view. We first define and construct the environmental production
              technologies for the countries with and without CHP plants,
              respectively. The non-radial direction distance function approach
              is then proposed and several indexes are developed to measure
              energy and CO2 emission performance of electricity generation.
              The directional distance functions established can be computed by
              solving a series of data envelopment analysis models. We then
              conduct an empirical study using the dataset for over one hundred
              countries. It is found that OECD countries have better carbon
              emission performance and integrated energy-carbon performance
              than non-OECD countries in electricity generation, while the
              difference in energy performance is not significant.},
	Author = {Zhou, P and Ang, B W and Wang, H},
	Journal = {Eur. J. Oper. Res.},
	Keywords = {Data envelopment analysis; Directional distance function; Energy efficiency; CO emission performance; Electricity generation},
	Month = sep,
	Number = 3,
	Pages = {625--635},
	Title = {Energy and {CO2} emission performance in electricity generation: A non-radial directional distance function approach},
	Volume = 221,
	Year = 2012}

@incollection{Stephens1986,
	Author = {Stephens, Michael A},
	Booktitle = {Goodness-of-fit-techniques},
	Pages = {97--194},
	Publisher = {Routledge},
	Title = {Tests based on {EDF} statistics},
	Year = 1986}

@ARTICLE{Kuosmanen2020,
  title    = "How much climate policy has cost for {OECD} countries?",
  author   = "Kuosmanen, Timo and Zhou, Xun and Dai, Sheng",
  abstract = "High economic cost of climate policy has attracted critical
              debate since the Kyoto Protocol. However, reliable empirical
              evidence of the abatement cost of green-house gases across
              countries remains scant. In this study we estimate the average
              yearly green-house gas abatement costs per capita for a panel of
              28 OECD countries in years 1990--2015. The marginal abatement
              costs are estimated using a novel data-driven approach based on
              convex quantile regression. Compared to traditional frontier
              estimation methods, the quantile approach takes into account a
              broader set of abatement options and is more robust to
              inefficiency, noise, and heteroscedasticity in empirical data.
              The comparison of OECD countries shows that the actual abatement
              cost per capita has been very modest, much lower than predicted
              in the late 1990s. This result has profound policy implications,
              calling for more ambitious climate change mitigation strategy in
              the future.",
  journal  = "World Dev.",
  volume   =  125,
  pages    = "104681",
  month    =  jan,
  year     =  2020,
  keywords = "Abatement cost; Climate change; Convex quantile regression;
              Green-house gases; Kyoto Protocol; OECD countries"
}

@article{Kuosmanen2018b,
	Abstract = {Shadow pricing environmental bads is critically important for
              efficient environmental policy and management. However, most
              empirical studies grossly overestimate the marginal abatement
              costs for three reasons. First, assuming downscaling of
              production as the only abatement option ignores abatement through
              increasing the input use. Second, estimating shadow prices on the
              frontier ignores the impact of inefficiency. Third, estimating
              the frontier by deterministic methods ignores the upward bias due
              to noise in empirical data. To address these problems, a novel
              data-driven approach is developed. Instead of projecting
              inefficient units to the frontier, we estimate the shadow prices
              locally based on the actual level of performance using convex
              quantile regression. Compared to the traditional approaches,
              convex quantile regression is more robust to noise, the choice of
              the direction vector, and heteroscedasticity. Application to the
              U.S. electric power plants provides empirical evidence and
              demonstrates the advantages of the proposed approach.},
	Author = {Kuosmanen, Timo and Zhou, Xun},
	Date-Modified = {2019-12-12 17:23:10 +0000},
	Month = nov,
	Title = {Shadow Prices and Marginal Abatement Costs: Convex Quantile Regression Approach},
	Year = 2018}

@article{Cifci2018,
	Abstract = {International climate agreements such as the Kyoto Protocol of
               1997 and, more recently, the Paris Climate Agreement are fragile
               because, at a national level, political constituencies' value
               systems may conflict with the goal of reducing greenhouse gas
               (GHG) emissions to sustainable levels. Proponents cite climate
               change as the most pressing challenge of our time, contending
               that international cooperation will play an essential role in
               addressing this challenge. Political opponents argue that the
               disproportionate requirements on developed nations to shoulder
               the financial burden will inhibit their economic growth. We find
               empirical evidence that both arguments are likely to be correct.
               We use standard regression techniques to analyze a multi-country
               dataset of GHG emissions, GDP per capita growth, and other
               factors. We estimate that after the Kyoto Protocol (KP) entered
               into force `Annex I' countries reduced GHG emissions on average
               by roughly 1 million metric tons of CO2 equivalent (MTCO2e),
               relative to non-Annex I countries. However, our estimates reveal
               that these countries also experienced an average reduction in
               GDP per capita growth rates of around 1--2 percentage points
               relative to non-Annex I countries.},
	Author = {Cifci, Eren and Oliver, Matthew E},
	Journal = {Sustain. Sci. Pract. Policy},
	Language = {en},
	Month = jan,
	Number = 2,
	Pages = {334},
	Publisher = {Multidisciplinary Digital Publishing Institute},
	Title = {Reassessing the Links between {GHG} Emissions, Economic Growth, and the {{UNFCCC}}: A {Difference-in-Differences} Approach},
	Volume = 10,
	Year = 2018}

@article{De_Angelis2019,
	Abstract = {The paper investigates the relationship between economic growth
               and environmental quality in the context of the Kuznets curve,
               which foresees that growth, while initially causing negative
               externalities for the environment, eventually can be seen also
               as the solution to environmental degradation. The novelty of the
               paper is to analyze the role of environmental policies, and in
               particular the use of market-based and non-market instruments to
               challenge the pollution plague and mitigate climate change. The
               results of fixed effects estimates on a sample of 32 countries
               observed for the period 1992--2012 show the existence of an
               inverted U-shaped relationship between per capita gross domestic
               product (GDP) and per-capita CO2 emissions for the quadratic
               specification, as well as of an N-shaped pattern for the cubic
               specification. Most importantly, the stringency indexes, i.e.,
               the proxies used to account for environmental regulation,
               exhibit negative and strongly significant coefficients,
               suggesting that the policies are effective in reducing
               environmental damages associated with economic growth.},
	Author = {de Angelis, Enrico Maria and Di Giacomo, Marina and Vannoni, Davide},
	Journal = {Sustain. Sci. Pract. Policy},
	Language = {en},
	Month = apr,
	Number = 8,
	Pages = {2273},
	Publisher = {Multidisciplinary Digital Publishing Institute},
	Title = {Climate Change and Economic Growth: The Role of Environmental Policy Stringency},
	Volume = 11,
	Year = 2019}

@article{Chateau2018,
	Abstract = {This paper explores the consequences on the labour markets of
               structural changes induced by decarbonisation policies. These
               policies are likely going to have consequences on labour-income
               distribution given i) existing rigidities in the labour markets,
               and ii) their {\ldots}},
	Author = {Chateau, Jean and Bibas, Ruben and Lanzi, Elisa},
	Journal = {OECD Environmental Working Papers No. 137},
	Publisher = {OECD},
	Title = {Impacts of Green Growth Policies on Labour Markets and Wage Income Distribution},
	Year = 2018}

@article{Vale2016,
	Abstract = {Climate change economics is now four decades old. Much of what it
              has achieved as a field of academic enquiry can be linked back to
              issues of integrated assessment modelling. This paper shows that
              the standard approach is going through a major change in scope as
              of the last five years. The conventional focus on determining
              optimal mitigation paths based on modelling the social cost of
              carbon is being enlarged to embrace promising new waves of
              research. These are: (1) the economics of insurance against
              catastrophic risks; (2) the economics of trade and climate; and
              (3) the economics of climate change adaptation. The paper helps
              to bridge the gap between economics and climate policy by showing
              that the analytical toolkit of climate change economics has
              shifted towards more realistic representations of climatic
              policy.},
	Author = {Vale, Petterson Molina},
	Journal = {Ecol. Econ.},
	Keywords = {Climate change; Economics; Carbon; Adaptation; Insurance; Trade},
	Month = jan,
	Pages = {12--19},
	Title = {The changing climate of climate change economics},
	Volume = 121,
	Year = 2016}

@article{Hovi2012,
	Abstract = {According to two-level game theory, negotiators tailor
               agreements at the international level to be ratifiable at the
               domestic level. This did not happen in the Kyoto negotiations,
               however, in the US case. We interviewed 26 German, Norwegian,
               and US participants in and observers of the climate negotiations
               concerning their views on three explanations for why the United
               States did not become a party to Kyoto. Explanation 1 argues
               that Kyoto delegations mistakenly thought the Senate was
               bluffing when adopting Byrd?Hagel. Explanation 2 contends that
               Europeans preferred a more ambitious agreement without US
               participation to a less ambitious agreement with US
               participation. Finally, explanation 3 suggests that in Kyoto the
               Clinton?Gore administration gave up on Senate ratification, and
               essentially pushed for an agreement that would provide them a
               climate-friendly face. While all explanations received some
               support from interviewees, explanation 1 and (particularly)
               explanation 3 received considerably more support than
               explanation 2.},
	Author = {Hovi, Jon and Sprinz, Detlef F and Bang, Guri},
	Journal = {European Journal of International Relations},
	Month = mar,
	Number = 1,
	Pages = {129--150},
	Publisher = {SAGE Publications Ltd},
	Title = {Why the United States did not become a party to the Kyoto Protocol: German, Norwegian, and {US} perspectives},
	Volume = 18,
	Year = 2012}

@article{Wouter_Botzen2012,
	Abstract = {The damage function in the famous climate-economy model DICE has
              received much criticism. Weitzman (2010) has proposed an
              alternative approach that gives more serious attention to climate
              change impacts for larger temperature increases. We calculate
              optimal climate policy with DICE using this approach. Optimal
              emission abatement trajectories turn out to be very sensitive to
              the damage specification. We summarise the difference between the
              associated optimal abatement costs in NPV terms.},
	Author = {Wouter Botzen, W J and van den Bergh, Jeroen C J M},
	Journal = {Econ. Lett.},
	Keywords = {Climate change; DICE model; Stern discounting},
	Month = oct,
	Number = 1,
	Pages = {372--374},
	Title = {How sensitive is Nordhaus to Weitzman? Climate policy in {DICE} with an alternative damage function},
	Volume = 117,
	Year = 2012}

@article{Markandya2015,
	Abstract = {Summary We investigate the trade-offs between economic growth and
              low carbon targets for developing and developed countries in the
              period up to 2035. Policy options are evaluated with an original
              version of the dynamic CGE model GDynE. Abatement costs appear to
              be strongly detrimental to economic growth for developing
              countries. We investigate options for reducing these costs that
              are consistent with the current negotiations. We show that the
              Green Climate Fund financed through a levy on carbon taxation can
              benefit all parties, and large benefits are associated with
              investment of the Green Climate Fund to foster energy efficiency
              in developing countries.},
	Author = {Markandya, A and Antimiani, A and Costantini, V and Martini, C and Palma, A and Tommasino, M C},
	Journal = {World Dev.},
	Keywords = {climate change policies; Green Climate Fund; developing countries; dynamic CGE energy model},
	Month = oct,
	Pages = {93--107},
	Title = {Analyzing Trade-offs in International Climate Policy Options: The Case of the Green Climate Fund},
	Volume = 74,
	Year = 2015}

@article{Maradan2005,
	Author = {Maradan, David and Vassiliev, Anatoli and {Others}},
	Journal = {Revue Suisse d Economie et de Statistique},
	Number = 3,
	Pages = {377},
	Publisher = {Citeseer},
	Title = {Marginal costs of carbon dioxide abatement: empirical evidence from cross-country analysis},
	Volume = 141,
	Year = 2005}

@misc{Du2015,
	Author = {Du, Limin and Hanley, Aoife and Wei, Chu},
	Journal = {Environmental and Resource Economics},
	Number = 2,
	Pages = {191--216},
	Title = {Marginal Abatement Costs of Carbon Dioxide Emissions in China: A Parametric Analysis},
	Volume = 61,
	Year = 2015}

@misc{Wang2011,
	Author = {Wang, Qunwei and Cui, Qinjun and Zhou, Dequn and Wang, Sisi},
	Journal = {Energy Procedia},
	Pages = {2316--2320},
	Title = {Marginal abatement costs of carbon dioxide in China: A nonparametric analysis},
	Volume = 5,
	Year = 2011}

@misc{Chen2011,
	Author = {Chen, Shiyi},
	Journal = {The World Economy},
	Number = 7,
	Pages = {1148--1167},
	Title = {The Abatement of Carbon Dioxide Intensity in China: Factors Decomposition and Policy Implications},
	Volume = 34,
	Year = 2011}

@article{Choi2012,
	Abstract = {This paper uses nonparametric efficiency analysis technique to
              estimate the energy efficiency, potential emission reductions and
              marginal abatement costs of energy-related CO2 emissions in
              China. We employ a non-radial slacks-based data envelopment
              analysis (DEA) model for estimating the potential reductions and
              efficiency of CO2 emissions for China. The dual model of the
              slacks-based DEA model is then used to estimate the marginal
              abatement costs of CO2 emissions. An empirical study based on
              China's panel data (2001--2010) is carried out and some policy
              implications are also discussed.},
	Author = {Choi, Yongrok and Zhang, Ning and Zhou, P},
	Journal = {Appl. Energy},
	Keywords = {CO emissions; Efficiency; Abatement costs; Slacks-based measure (SBM); Data Envelopment Analysis (DEA); China},
	Month = oct,
	Pages = {198--208},
	Title = {Efficiency and abatement costs of energy-related {CO2} emissions in China: A slacks-based efficiency measure},
	Volume = 98,
	Year = 2012}

@misc{Cheng2019,
	Author = {Cheng, Yu and Lv, Kangjuan and Wang, Jian and Xu, Hao},
	Journal = {Energy Efficiency},
	Number = 4,
	Pages = {863--877},
	Title = {Energy efficiency, carbon dioxide emission efficiency, and related abatement costs in regional China: a synthesis of input--output analysis and {DEA}},
	Volume = 12,
	Year = 2019}

@article{Lee2019,
	Abstract = {Emissions trading (or cap and trade) is a market-based approach
              providing economic incentives for achieving reductions in the
              emissions of pollutants. Marginal abatement costs (MAC), also
              termed shadow prices of air pollution emissions, provide valuable
              guidelines to support environmental regulatory policies for CO2,
              SO2 and NOx, the key contributors to climate change, smog, and
              acid rain. This study estimates the marginal abatement cost of
              undesirable outputs with respect to the Nash equilibrium on the
              stochastic semi-nonparametric envelopment of data (StoNED) in an
              imperfectly competitive market. Considering an endogenous price
              function of electricity, the mixed complementarity problem (MiCP)
              is formulated to identify the Nash equilibrium in a production
              possibility set. The proposed model addresses the four issues of
              MAC estimation in the existing literature. Applying the proposed
              method to an empirical study of 33 coal-fired power plants
              operating in China in 2013 shows that StoNED provides a robust
              frontier that is not sensitive to the outlier and the proposed
              interval of MAC estimation validates the shadow prices
              corresponding to the Nash equilibrium in an imperfectly
              competitive market.},
	Author = {Lee, Chia-Yen and Wang, Ke},
	Journal = {Eur. J. Oper. Res.},
	Keywords = {Data envelopment analysis; Marginal abatement costs; Emissions trading; Nash equilibrium; Stochastic semi-nonparametric frontier},
	Month = feb,
	Number = 1,
	Pages = {390--400},
	Title = {Nash marginal abatement cost estimation of air pollutant emissions using the stochastic semi-nonparametric frontier},
	Volume = 273,
	Year = 2019}

@article{Dafermos2018,
	Abstract = {Using a stock-flow-fund ecological macroeconomic model, we
              analyse (i) the effects of climate change on financial stability
              and (ii) the financial and global warming implications of a green
              quantitative easing (QE) programme. Emphasis is placed on the
              impact of climate change damages on the price of financial assets
              and the financial position of firms and banks. The model is
              estimated and calibrated using global data and simulations are
              conducted for the period 2016--2120. Four key results arise.
              First, by destroying the capital of firms and reducing their
              profitability, climate change is likely to gradually deteriorate
              the liquidity of firms, leading to a higher rate of default that
              could harm both the financial and the non-financial corporate
              sector. Second, climate change damages can lead to a portfolio
              reallocation that can cause a gradual decline in the price of
              corporate bonds. Third, climate-induced financial instability
              might adversely affect credit expansion, exacerbating the
              negative impact of climate change on economic activity. Fourth,
              the implementation of a green corporate QE programme can reduce
              climate-induced financial instability and restrict global
              warming. The effectiveness of this programme depends positively
              on the responsiveness of green investment to changes in bond
              yields.},
	Author = {Dafermos, Yannis and Nikolaidi, Maria and Galanis, Giorgos},
	Journal = {Ecol. Econ.},
	Keywords = {Ecological macroeconomics; Stock-flow consistent modelling; Climate change; Financial stability; Green quantitative easing},
	Month = oct,
	Pages = {219--234},
	Title = {Climate Change, Financial Stability and Monetary Policy},
	Volume = 152,
	Year = 2018}

@article{Ciarli2019,
	Abstract = {We discuss how different models assessing climate change
              integrate aspects of structural change that are crucial to
              improve understanding of the relation between changes in the
              environment and in the economy. We identify six related aspects
              of structural change, which have significant impact on climate
              change: sectoral composition, industrial organisation,
              technology, employment, final demand, and institutions. Economic
              models vary substantially with respect to the aspects of
              structural change that they include, and how they model them. We
              review different modelling families and compare these
              differences: integrated assessment models (IAM), computable
              general equilibrium (CGE) models, structural change models (SCM),
              ecological macroeconomics models in the Keynesian tradition (EMK)
              and evolutionary agent based models (EABM). We find that IAM and
              CGE address few of the aspects of structural change identified;
              SCM focus on the sectoral composition; and EKM study final demand
              and employment structure. But all these models are aggregate and
              omit the complexity of the interactions between structural and
              climate change. EABM have explored a larger number of aspects of
              structural change, modelling their emergence from the interaction
              of microeconomic actors, but have not yet exploited their
              potential to study the interactions among interrelated aspects of
              structural and climate change.},
	Author = {Ciarli, Tommaso and Savona, Maria},
	Journal = {Ecol. Econ.},
	Keywords = {Structural change; Climate change; Economic modelling},
	Month = apr,
	Pages = {51--64},
	Title = {Modelling the Evolution of Economic Structure and Climate Change: A Review},
	Volume = 158,
	Year = 2019}

@article{Tsigaris2019,
	Abstract = {An endogenous growth model with a simple climate system is used
              to examine the potential impacts of climate change on the
              capital-to-net income ratio and the net of depreciation share of
              income to capital, a measure of wealth concentration and income
              distribution between capital and labour respectively, over the
              next two centuries. If climate change only directly affects
              production, as usually assumed, the capital-to-net income ratio
              will increase as compared to what it would be in the absence of
              climate change. The capital-to-income ratio will increase even
              further if climate change affects labour productivity. In both
              cases, the increase in the ratio after 2100 is due to the stock
              of capital being depleted at a lower rate than net income is
              falling. However, the capital-to-net income ratio will be lower
              and eventually fall if damage from climate change increases the
              depreciation rate of capital; this decline is marginally reduced
              if climate change impacts both capital and labour productivity.
              In the case where climate change impacts the depreciation of
              capital, the ratio after 2100 is falling because the stock of
              capital is destroyed faster than net-income is falling.
              Furthermore, climate change reduces the net share of income
              accruing to capital in all scenarios with dramatic changes in the
              case of climate change affecting the depreciation of capital.
              Emissions abatement almost completely mitigates these impacts on
              the capital-to-net income ratio and the net share of income to
              capital.},
	Author = {Tsigaris, Panagiotis and Wood, Joel},
	Journal = {Ecol. Econ.},
	Keywords = {Climate change; Inequality; Wealth; Economic growth},
	Month = aug,
	Pages = {74--86},
	Title = {The potential impacts of climate change on capital in the 21st century},
	Volume = 162,
	Year = 2019}

@article{Stern2013,
	Abstract = {The Structure of Economic Modeling of the Potential Impacts of
              Climate Change: Grafting Gross Underestimation of Risk onto
              Already Narrow Science Models by Nicholas Stern. Published in
              volume 51, issue 3, pages 838-59 of Journal of Economic
              Literature, September 2013, Abstract: Scientists describe t...},
	Author = {Stern, Nicholas},
	Journal = {J. Econ. Lit.},
	Month = sep,
	Number = 3,
	Pages = {838--859},
	Title = {The Structure of Economic Modeling of the Potential Impacts of Climate Change: Grafting Gross Underestimation of Risk onto Already Narrow Science Models},
	Volume = 51,
	Year = 2013}

@article{Wang2014,
	Abstract = {This paper proposes a shape-restricted nonparametric quantile
              regression to estimate the $\tau$-frontier, which acts as a
              benchmark for whether a decision making unit achieves top $\tau$
              efficiency. This method adopts a two-step strategy: first,
              identifying fitted values that minimize an asymmetric absolute
              loss under the nondecreasing and concave shape restriction;
              second, constructing a nondecreasing and concave estimator that
              links these fitted values. This method makes no assumption on the
              error distribution and the functional form. Experimental results
              on some artificial data sets clearly demonstrate its superiority
              over the classical linear quantile regression. We also discuss
              how to enforce constraints to avoid quantile crossings between
              multiple estimated frontiers with different values of $\tau$.
              Finally this paper shows that this method can be applied to
              estimate the production function when one has some prior
              knowledge about the error term.},
	Author = {Wang, Yongqiao and Wang, Shouyang and Dang, Chuangyin and Ge, Wenxiu},
	Journal = {Eur. J. Oper. Res.},
	Keywords = {Productivity and competitiveness; Production frontier; Quantile regression; Shape restriction; Concavity; Non-crossing},
	Month = feb,
	Number = 3,
	Pages = {671--678},
	Title = {Nonparametric quantile frontier estimation under shape restriction},
	Volume = 232,
	Year = 2014}

@article{Kuosmanen2018a,
	Abstract = {High economic cost of climate policy has attracted critical
              debate since the Kyoto Protocol. However, reliable empirical
              evidence of the abatement cost of greenhouse gases across
              countries remains scant. In this study we estimate the average
              yearly greenhouse gas abatement costs per capita for a panel of
              28 OECD countries in years 1990-2015. The marginal abatement
              costs are estimated using a novel data-driven approach based on
              convex quantile regression. Compared to traditional frontier
              estimation methods, the quantile approach takes into account a
              broader set of abatement options and is more robust to
              inefficiency, noise, and heteroscedasticity in empirical data.
              The comparison of OECD countries shows that the actual abatement
              cost per capita has been very modest, much lower than predicted
              in the late 1990s. This result has profound policy implications,
              calling for more ambitious climate change mitigation strategy in
              the future.},
	Author = {Kuosmanen, Timo and Zhou, Xun and Dai, Sheng},
	Date-Modified = {2019-12-12 17:23:05 +0000},
	Month = dec,
	Title = {How much climate policy has cost for {OECD} countries?},
	Year = 2018}

@article{Danielsson2016,
	Abstract = {This paper evaluates the model risk of models used for
              forecasting systemic and market risk. Model risk, which is the
              potential for different models to provide inconsistent outcomes,
              is shown to be increasing with market uncertainty. During calm
              periods, the underlying risk forecast models produce similar risk
              readings; hence, model risk is typically negligible. However, the
              disagreement between the various candidate models increases
              significantly during market distress, further frustrating the
              reliability of risk readings. Finally, particular conclusions on
              the underlying reasons for the high model risk and the
              implications for practitioners and policy makers are discussed.},
	Author = {Danielsson, Jon and James, Kevin R and Valenzuela, Marcela and Zer, Ilknur},
	Journal = {Journal of Financial Stability},
	Keywords = {Model risk; Systemic risk; Value-at-Risk; Expected shortfall; Basel III},
	Month = apr,
	Pages = {79--91},
	Title = {Model risk of risk models},
	Volume = 23,
	Year = 2016}

@article{Embrechts2001,
	Author = {Embrechts, Paul and Dan{\'\i}elsson, J{\'o}n and Goodhart, Charles A E and Keating, Con and Muennich, Felix and Renault, Olivier and Shin, Hyun Song},
	Journal = {FMG Special paper 130},
	Publisher = {FMG},
	Title = {An academic response to Basel {II}},
	Volume = 130,
	Year = 2001}

@article{Gray2014,
	Abstract = {Looking inside organizations at the different positions,
              expertise, and autonomy of the actors, the authors use multisite
              ethnographic data on safety practices to develop a typology of
              how the regulator, as the focal actor in the regulatory process,
              is interpreted within organizations. The findings show that
              organizational actors express constructions of the regulator as
              an ally, threat, and obstacle that vary with organizational
              expertise, authority, and continuity of relationship between the
              organizational member and the regulator. The article makes three
              contributions to the current understandings of organizational
              governance and regulatory compliance, thereby extending both
              institutional and ecological accounts of organizations' behavior
              with respect to their environments. First, the authors document
              not only variation across organizations but variable compliance
              within an organization. Second, the variations described do not
              derive from alternative institutional logics, but from variations
              in positions, autonomy, and expertise within each organization.
              From their grounded theory, the authors hypothesize that these
              constructions carry differential normative interpretations of
              regulation and probabilities for compliance, and thus the third
              contribution, the typology, when correlated with organizational
              hierarchy provides the link between microlevel action and
              discourse and organizational performance.},
	Author = {Gray, Garry C and Silbey, Susan S},
	Journal = {AJS},
	Language = {en},
	Month = jul,
	Number = 1,
	Pages = {96--145},
	Title = {Governing inside the organization: interpreting regulation and compliance},
	Volume = 120,
	Year = 2014}

@article{Westphal2001,
	Abstract = {This study examines firms' decoupling of informal practices from
               formally adopted policies through analysis of the implementation
               of stock repurchase programs by large U.S. corporations in the
               late 1980s and early 1990s, when firms were experiencing
               external pressures to adopt policies that demonstrate corporate
               control over managerial behavior. We develop theory to explain
               variation in the responses of firms to such pressures, i.e., why
               some firms acquiesce by actually implementing stock repurchase
               programs, while others decouple formally adopted repurchase
               programs from actual corporate investments, so that the plans
               remain more symbolic than substantive. Results of a longitudinal
               study of stock repurchase programs over a six-year time period
               show that decoupling is more likely to occur when top executives
               have power over boards to avoid institutional pressures for
               change and when social structural or experiential factors
               enhance awareness among powerful actors of the potential for
               organizational decoupling. The study has implications for future
               research on decoupling, organizational learning, and corporate
               governance.},
	Author = {Westphal, James D and Zajac, Edward J},
	Journal = {Adm. Sci. Q.},
	Month = jun,
	Number = 2,
	Pages = {202--228},
	Publisher = {SAGE Publications Inc},
	Title = {Decoupling Policy from Practice: The Case of Stock Repurchase Programs},
	Volume = 46,
	Year = 2001}

@article{Westphal1998,
	Author = {Westphal, James D and Zajac, Edward J},
	Journal = {Adm. Sci. Q.},
	Pages = {127--153},
	Publisher = {JSTOR},
	Title = {The symbolic management of stockholders: Corporate governance reforms and shareholder reactions},
	Year = 1998}

@article{Westphal1995,
	Abstract = {While current debates about CEO compensation have generally been
               dominated by economic and political perspectives on CEO/board
               relations, we argue in this paper that CEO compensation may be
               driven by symbolic as well as substantive considerations. We
               develop an interdisciplinary theoretical framework to (1)
               explain why alternative explanations rooted in agency and human
               resource logics may be used to reduce ambiguity surrounding the
               adoption of new incentive plans for CEOs and (2) identify the
               possible structural (e.g., institutional, demographic, and
               economic), and interest-based (e.g., political) factors
               influencing the use of such explanations. We generate and test
               hypotheses predicting the alternative explanations for new
               long-term incentive plans using data taken from proxy statements
               over a 15-year period. The findings support the notion that
               explanations for CEO compensation reflect both substance and
               symbolism.},
	Author = {Westphal, James D and Zajac, Edward J},
	Journal = {Adm. Sci. Q.},
	Number = 2,
	Pages = {283--308},
	Publisher = {[Sage Publications, Inc., Johnson Graduate School of Management, Cornell University]},
	Title = {Accounting for the Explanations of {CEO} Compensation: Substance and Symbolism},
	Volume = 40,
	Year = 1995}

@article{Westphal1994,
	Abstract = {This study theoretically and empirically addresses the possible
               separation of substance and symbolism in CEO compensation
               contracts by examining political and institutional determinants
               of long-term incentive plan (LTIP) adoption and use among 570 of
               the largest U.S. corporations over two decades. We find that a
               substantial number of firms are likely to adopt but not actually
               use-or only limitedly use-LTIPs, suggesting a potential
               separation of substance and symbol in CEO compensation
               contracts. Analyses suggest that this decoupling of LTIP
               adoption and use is particularly prevalent in firms with
               powerful CEOs and firms with poor prior performance. Further
               analyses show that whereas early adopters are more likely to
               pursue alignment between CEO and shareholder interests
               substantively, later adopters may pursue legitimacy by
               symbolically controlling agency costs. More generally, the study
               highlights how decoupling in organizations can be understood in
               terms of both micro-political and macro-institutional forces.},
	Author = {Westphal, James D and Zajac, Edward J},
	Journal = {Adm. Sci. Q.},
	Month = sep,
	Number = 3,
	Pages = {367},
	Publisher = {[Sage Publications, Inc., Johnson Graduate School of Management, Cornell University]},
	Title = {Substance and Symbolism in {CEOs'} {Long-Term} Incentive Plans},
	Volume = 39,
	Year = 1994}

@article{Perezts2015,
	Abstract = {The effective implementation of regulation in organizations is an
              ongoing concern for both research and practice, in order to avoid
              deviant behavior and its consequences. However, the way
              compliance with regulations is actually enacted or ``performed''
              within organizations instead of merely executed, remains largely
              under-characterized. Evidence from an ethnographic study in the
              compliance unit of a French investment bank allows us to develop
              a detailed practice approach to how regulation is actually
              implemented in firms. We characterize the work accomplished by
              compliance analysts who are in fact, ``curving'' the script of
              regulation within what we conceptualize as a ``comfort zone''.
              Beyond agency, ethics appears as a key element in linking the
              ``letter of the law'', which serves as a referential anchor to
              guide action, with the complex nature of specific situations. We
              analyze the way individuals and compliance teams cope with,
              interpret, struggle and in fine, perform regulation within this
              comfort zone. A particular interest is thus given to the work of
              embedded ethics in this process, as an enabler to partly recouple
              compliance with the regulated activity. We find that blind
              execution is not only impossible, but also devoid of meaning both
              from regulatory, risk management, and business perspectives in
              organizations. We highlight and characterize a hermeneutic
              dimension to this work, essential to effectively perform
              regulation in complex environments, and we suggest some
              directions for further research.},
	Author = {P{\'e}rezts, Mar and Picard, S{\'e}bastien},
	Journal = {J. Bus. Ethics},
	Month = nov,
	Number = 4,
	Pages = {833--852},
	Title = {Compliance or Comfort Zone? The Work of Embedded Ethics in Performing Regulation},
	Volume = 131,
	Year = 2015}

@article{MacLean2010,
	Abstract = {This theory-building analysis spotlights a dynamic that occurs
               between decoupling, legitimacy, and institutionalized
               misconduct. Using data gathered from a case study of widespread
               deceptive sales practices at a large financial services firm, we
               demonstrate the dangers of decoupling an organizational
               compliance program from the core business activities of an
               organization. We illustrate how decoupling created a ?legitimacy
               facade? that enabled the institutionalization of misconduct and
               precipitated a loss of external legitimacy.},
	Author = {MacLean, Tammy L and Behnam, Michael},
	Journal = {AMJ},
	Month = dec,
	Number = 6,
	Pages = {1499--1520},
	Publisher = {Academy of Management},
	Title = {The Dangers of Decoupling: The Relationship Between Compliance Programs, Legitimacy Perceptions, and Institutionalized Misconduct},
	Volume = 53,
	Year = 2010}

@book{Parker2011,
	Abstract = {Explaining Compliance consists of sixteen specially commissioned
               chapters by the world's leading empirical researchers, examining
               whether and how businesses comply with regulation that is
               designed to affect positive behaviour changes. Each chapter
               consists of reflective summaries on business compliance with
               different state or voluntary regulation, and the theoretical
               lessons to be drawn from it. As a whole, the book develops
               understanding and explanations of how, why and in what
               circumstances, firms come to comply with regulation, and when
               they do not. It also uncovers the complexity, ambiguity and
               transformation of regulation as it is interpreted, implemented
               and negotiated by firms, their stakeholders and internal
               constituencies in everyday business life. This unique and
               detailed resource will appeal to academics, graduate students
               and senior undergraduates in law, political science, sociology,
               criminology, economics, and psychology, as well as business and
               interdisciplinary areas such as law and society, and law and
               economics. Anyone researching business regulation, corporate
               social responsibility, regulation and compliance, enforcement
               and compliance, and public administration, will also find this
               book beneficial.},
	Author = {Parker, Christine and Nielsen, Vibeke Lehmann},
	Language = {en},
	Publisher = {Edward Elgar Publishing},
	Title = {Explaining Compliance: Business Responses to Regulation},
	Year = 2011}

@article{Rossignolo2013,
	Abstract = {Basel III represents a crucial step in strengthening the capital
              rules underlying banking operations, aimed at reducing the
              probability and severity of a systemic crisis. Alongside two
              supplementary capital buffers, the Basel Committee of Banking
              Supervision imposed severe pressure on the Value-at-Risk based
              Internal Models Approach in order to increase. This is to
              increase the capital base by adding the stressed Value-at-Risk
              component in an effort to reduce reliance on internal models
              while keeping the Standardized Approach avenue open. However,
              even though those measures might appear theoretically correct,
              evidence gathered for long and short exposures in Portugal,
              Italy, Greece and Spain highlights several defects in Basel III.
              We emphasize that leptokurtic models, primarily those derived
              from Extreme Value Theory, should be enforced in the regulations
              given their superior performance in market crises, and that Basel
              II could have shielded against 2008 mayhem provided that
              heavy-tailed techniques had been employed. \copyright{} 2012
              Elsevier B.V.},
	Author = {Rossignolo, A F and {Fethi} and Shaban, M},
	Journal = {Journal of Banking and Finance},
	Keywords = {Basel III; Extreme value theory; Internal models approach; PIGS; Standardized approach; Value-at-risk},
	Number = 5,
	Pages = {1323--1339},
	Title = {Market crises and Basel capital requirements: Could Basel {III} have been different? Evidence from Portugal, Ireland, Greece and Spain ({PIGS})},
	Volume = 37,
	Year = 2013}

@book{Fare2006,
	Abstract = {The format of this monograph is three essays, which we arrived
               at after spending a year writing over one hundred pages of what
               we even tually realized was a tedious reworking of old material.
               So we started over determined to write something new. At first
               we thought this approach might not work as a coherent mono
               graph, which is why we chose the essay format rather than
               chapters. As it turns out, there is a common thread---namely the
               directional distance function, which also gave us our title. As
               you shall see, the directional distance function includes
               traditional distance functions and efficiency measures as
               special cases providing a unifying framework for existing
               productivity and efficiency measures. It is also flexible enough
               to open up new areas in productivity and efficiency analysis
               such as environmen tal and aggregation issues. That we did not
               see this earlier is humbling; a student at a recent conference
               raised his hand and asked 'Why didn't you start with the
               directional distance function in the first place? In deed. This
               manuscript is intended to make up for our earlier oversights.
               This monograph contains papers coauthored with Wen-Fu Lee and
               Osman Zaim and one paper written by two former students,
               Hiroyuki Fukuyama and Bill Weber. We thank them for their
               contributions. An other former student, Jim Logan (Logi) read
               and critiqued the manu script for which we are grateful.},
	Author = {F{\"a}re, Rolf and Grosskopf, Shawna},
	Language = {en},
	Month = mar,
	Publisher = {Springer Science \& Business Media},
	Title = {New Directions: Efficiency and Productivity},
	Year = 2006}

@article{Danielsson2002,
	Abstract = {This paper considers the properties of risk measures, primarily
              value-at-risk (VaR), from both internal and external (regulatory)
              points of view. It is argued that since market data is endogenous
              to market behavior, statistical analysis made in times of
              stability does not provide much guidance in times of crisis. In
              an extensive survey across data classes and risk models, the
              empirical properties of current risk forecasting models are found
              to be lacking in robustness while being excessively volatile. For
              regulatory use, the VaR measure may give misleading information
              about risk, and in some cases may actually increase both
              idiosyncratic and systemic risk.},
	Author = {Dan{\i}́elsson, J{\'o}n},
	Journal = {Journal of Banking \& Finance},
	Keywords = {Value-at-risk; Capital adequacy; Financial regulations; Risk models},
	Month = jul,
	Number = 7,
	Pages = {1273--1296},
	Title = {The emperor has no clothes: Limits to risk modelling},
	Volume = 26,
	Year = 2002}

@article{Danielsson2001,
	Abstract = {The implications of Value-at-Risk regulations are analyzed in a
              CARA--normal general equilibrium model. Financial institutions
              are heterogeneous in risk prefere},
	Author = {Danielsson, Jon and Zigrand, Jean-Pierre},
	Keywords = {General equilibrium; Value-at-risk; Risk regulation},
	Month = aug,
	Title = {What Happens When You Regulate Risk? Evidence from a Simple Equilibrium Model},
	Year = 2001}

@incollection{Danielsson2012,
	Author = {Danielsson, Jon and Shin, Hyun Song and Zigrand, Jean-Pierre},
	Booktitle = {Quantifying Systemic Risk},
	Month = apr,
	Pages = {73--94},
	Publisher = {University of Chicago Press},
	Title = {Endogenous and Systemic Risk},
	Year = 2012}

@article{Fox1999,
	Abstract = {Using a standard definition of cost efficiency, a multi-product
              firm may be more efficient at producing each product than any
              other firm, yet it may not be the most efficient firm overall.
              The explanation for this seeming paradox leads to important
              implications for the reporting of efficiency scores at different
              levels of aggregation for the public sector versus the private
              sector. Studies which use aggregate data to evaluate public
              sector performance may be seriously flawed.},
	Author = {Fox, Kevin J},
	Journal = {Econ. Lett.},
	Keywords = {Efficiency measurement; Public sector},
	Month = nov,
	Number = 2,
	Pages = {173--176},
	Title = {Efficiency at different levels of aggregation: public vs. private sector firms},
	Volume = 65,
	Year = 1999}

@article{Smith1990,
	Abstract = {Ratio analysis has been a tool of analysts for as long as
              financial statements have been prepared. Yet its limitation to
              considering only one numerator and one denominator severely
              limits its usefulness. This paper extends the traditional ratio
              analysis to permit the incorporation of any number of dimensions
              of performance, using data envelopment analysis. The method
              produces measures of corporate efficiency, together with a wealth
              of supporting information. The strengths and weaknesses of the
              method applied to financial statements are appraised.},
	Author = {Smith, P},
	Journal = {Omega},
	Keywords = {financial statements; data envelopment analysis; performance measurement; ratio analysis},
	Month = jan,
	Number = 2,
	Pages = {131--138},
	Title = {Data envelopment analysis applied to financial statements},
	Volume = 18,
	Year = 1990}

@article{Afriat1972,
	Author = {Afriat, S N},
	Journal = {Int. Econ. Rev.},
	Number = 3,
	Pages = {568--598},
	Publisher = {[Economics Department of the University of Pennsylvania, Wiley, Institute of Social and Economic Research, Osaka University]},
	Title = {Efficiency Estimation of Production Functions},
	Volume = 13,
	Year = 1972}

@article{Simar2007,
	Author = {Simar, L{\'e}opold and Wilson, Paul W},
	Journal = {J. Econom.},
	Keywords = {bootstrap; data envelopment analysis; dea; nonparametric; technical efficiency; two-stage},
	Month = jan,
	Number = 1,
	Pages = {31--64},
	Title = {Estimation and inference in two-stage, semi-parametric models of production processes},
	Volume = 136,
	Year = 2007}

@book{Bogeoft2011,
	Address = {New York},
	Author = {Bogeoft, P and Otto, L},
	Keywords = {Benchmarking with DEA; Econometrics; Operation Research / Decision Theory; SFA; and R},
	Publisher = {Springer},
	Title = {Benchmarking with {DEA}, {SFA}, and {R}},
	Year = 2011}

@article{Sealey1977,
	Author = {Sealey, C W and Lindley, James T},
	Journal = {J. Finance},
	Month = sep,
	Number = 4,
	Pages = {1251--1266},
	Publisher = {Blackwell Publishing Ltd},
	Title = {Inputs, Outputs, and a Theory of Production and Cost at Depository Financial Institutions},
	Volume = 32,
	Year = 1977}

@incollection{Paradi2011,
	Abstract = {The banking industry has been the object of DEA analyses by a
               significant number of researchers and probably is the most
               heavily studied of all business sectors. Various DEA models have
               been applied in performance assessing problems, and the banks'
               complex production processes have further motivated the
               development and improvement of DEA techniques. The main
               application areas for DEA in bank and branch performance
               analysis include the following: efficiency ranking; resource
               allocation, efficiency trends investigation; environmental
               impacts compensation; examining the impacts of new technology,
               ownership, deregulation, corporate, economic, and political
               events, etc.},
	Author = {Paradi, Joseph C and Yang, Zijiang and Zhu, Haiyan},
	Booktitle = {Handbook on Data Envelopment Analysis},
	Chapter = 13,
	Edition = {Second},
	Editor = {Cooper, William W and Seiford, Larry M and Zhu, Joe},
	Language = {en},
	Pages = {315--361},
	Publisher = {Springer, Boston, MA},
	Series = {International Series in Operations Research \& Management Science},
	Title = {Assessing Bank and Bank Branch Performance},
	Year = 2011}
