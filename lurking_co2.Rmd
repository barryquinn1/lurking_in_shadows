---
title: "*Lurking in the shadows:* The impact of emissions target setting on carbon pricing and enviromental efficiency."
author: 
  - Barry Quinn (Queen's Management School)
  - Ronan Gallagher (Edinburgh Business School)
  - Timo Kuosmanen (Aalto Business School)
date: "`r format(Sys.time(),'%B %d, %Y')`"
abstract: "This paper interogates the impact of the Kyoto Protocol Agreement (KPA) target setting regime on carbon *shadow* pricing and enviromental efficiency. We extract shadow price estimates and efficiency scores from a comprehensive dataset of 125 countries over the four years of the KPA using the holisti StoNED framework. We exploit recent work, which uses the *least-cost alternative* procedure to estimate CO~2~ marginal abatement costs from initial model estimation. Results reveal abatement costs are significantly higher for target setting countries, increase over the sample period, and are an order of magnitude greater than the prevailing price discovery proccess of the prevailing emissions trading pricing mechanisms. Typically, environmental inefficiency scores are higher in countries with higher trade to GDP ratios, higher urban density, and explicit C0~2~ emission targets set under the Kyoto Protocol Agreement. Our findings provide insights into the consequences of policies to curb unwanted byproducts in a regulated system and reveal *future price* consequences of the regulation used to mitigate undesirable outputs."

acknowledgements:
  "We would like to acknowledge Professor Donal McKillop and Professor John Turner for the helpful commentary and advice on this paper."

keywords:
  - Marginal abatement costs
  - Enviromental efficiency
  - Stochastic nonparametric envelopment of data (StoNED)
  - Carbon emissions target setting
  - Kyoto Protocol Agreement
fontsize: 10pt
fontfamily: mathpazo
bibliography: ReferencesForLurking.bib
output: 
  pdf_document:
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    extra_dependencies: ["float"]
...

```{r set up, include=FALSE}
## This chunk is for setting nice options for the output. Notice how
## we create both png and pdf files by default, so that we can easily process
## to both HTML and LaTeX/PDF files later.
library(knitr)
opts_chunk$set(fig.path='figures/paper-',
               cache.path='cache/report-',
               dev=c("png","pdf"),
               fig.width=14,
               fig.height=7,
               dpi=300,
               fig.show='hold',
               fig.pos = "H", out.extra = "",
               fig.lp="fig:",
               cache=TRUE,
               par=TRUE,
               echo=FALSE,
               results="hide",
               message=FALSE,
               warning=FALSE)

knit_hooks$set(par=function(before, options, envir){
    if (before && options$fig.show!='none') par(mar=c(4,4,.1,.1),cex.lab=.95,cex.axis=.9,mgp=c(2,.7,0),tcl=-.3)
}, crop=hook_pdfcrop)

library(tidyverse); library(readxl); library(broom); library(kableExtra); library(knitr); library(gridExtra); library(gtable); library(grid); library(stringr); library(moments); library(nortest); library(car); library(Matching)

MyDeleteItems<-ls()
rm(list=MyDeleteItems)
load("dat_anal.RData")
# options(kableExtra.auto_format = FALSE) # for word output this turns off the html output which is the default and makes tables look terrible
```


# Introduction
Target setting stringency in climate policy has attracted considerable debate since the Kyoto Protocol Agreement (hereafter KPA) [for example, see @De_Angelis2019]. The impact of climate change and the production of greenhouse gas (GHG) emissions are existential challenges of the 21~st~ century. The World Health Organisation estimates a health risk associated with climate change of 250,000 additional deaths per year between 2030 and 2050, assuming the status quo of current abatement practices and global economic growth^[https://www.who.int/news-room/fact-sheets/detail/climate-change-and-health]. This paper aims to add to this debate by investigating the impact of KPA target setting on carbon pricing and environmental efficiency. Specifically, we exploit state-of-the-art frontier methods, namely Stochastic Nonparametric Envelopment of Data (StoNeD), to produce consistent environmental efficiency and marginal abatement estimates. We further decompose the target setting effects using a novel statistical procedure introduced in @Gallagher2019. 

We estimate marginal abatement costs using efficient frontier shadow prices^[The terms "opportunity cost" and "shadow price" are used interchangeably in the efficiency literature. @Fare2012 describes shadow prices as 'virtual' or unforeseen costs to a firm's management. A shadow price obtained from an economic model is the implicit value of untradeable outputs such as pollutants or undesirable byproducts [@Lee2012].   Technically, they value the marginal product faced by the decision-maker based on the optimal choice of outputs and inputs, which maximises utility [@Murray1995]. Under the assumption of rationality, shadow prices reveal the underlying opportunity costs hidden from the researcher [@Kuosmanen2006]. Importantly, this opportunity cost (economic price) definition can also take the form of marginal substitution (transformation) rates between inputs (outputs).]. @Luhmann2020 argue that shadow pricing is as crucial as emission pricing (for example, emissions trading schemes) for appropriate carbon price discovery.  They argue shadow can also be called "future pricing". The word "shadow" highlights that, for financiers to assess a project's actual economic value, fuel is priced higher than current levels. The rationale being, even if there is no carbon emissions pricing,  carbon prices are taken into account, factoring in what markets still ignore. Shadow prices have a long history of revealing the cost of reducing undesirable outputs^[Shadow prices assess the costs of producing some byproduct, or more technically, an undesirable output. They are helpful for regulatory and supervision analysis where quality control of byproducts plays an integral part in the sustainable growth of the regulated system. Shadow price usage is dependent on the observability of a market price for the undesirable output. When we observe output prices, then shadow pricing models can identify the appropriate output mix for revenue maximisation. More commonly, shadow price calculations numerate the price of an undesirable output in units of foregone desirable output.] in terms of reducing the production of desirable outputs (See @Zhou2014 for recent literature on shadow price estimation).  Framed in this way, the shadow price of air pollution emissions is equivalent to a marginal abatement cost and provides valuable guidance to emission reduction policies. 


@Lee2019 critique the body of work on marginal abatement costs of CO~2~ at the country level.  They show that empirical studies use frontier efficiency methods to estimate shadow prices (for country and region level analysis see [@Cheng2019; @Du2015; @Choi2012; @Maradan2005]) where CO~2~ pollutants as treated as undesirable outputs. We argue that these studies systematically over-estimate the marginal abatement cost by ignoring actual performance levels, noise, and the ability to abate via increasing input uses. For instance, an increase in capital investments into cleaner technology could be an alternative use of inputs. @Kuosmanen2018b show that marginal abatement costs are routinely overestimated compared to the market prices of CO~2~. They develop a methodology to counteract this systematic bias, which we follow in our analysis. 

Our results provide a consistent economic cost of the Kyoto Protocol Agreement.  Taking advantage of the explicit target-setting regime from 2008 to 2012, we estimate the marginal abatement cost of CO~2~ emissions.  Unlike previous shadow price estimates of CO~2~ emissions calculated ignoring country-level inefficiencies^[Technically,  cost estimates assume the entity under investigation is on the efficient frontier.], our model uses a quantile approach to incorporate additional information on the relative inefficiency of each country. The analysis results in shadow prices measured as the cost of abatement in constant international dollar terms. 

To disentangle abatement cost differences, we appeal a trigonometric procedure proposed in @Gallagher2019. This innovative test reveals that specific target setting countries typical experienced a higher marginal abatement cost than their non-target setting counterparts, the estimated difference being economically and statistically significant.  Our findings suggest an unintended consequence of climate policy target setting. The proposed market mechanism for trading emissions failed to eliminate these shadow price differences in target setting countries. 

In the next section, we will outline the sample design and define the variables used. Section 2 briefly outlines the innovative model used to estimate CO^2^ emission production. Section 3 presents the results and some brief discussion.

# Literature Review

The academic inquiry into the effective control of climate change has a rich 40-year history.  Historically, holistic models seek to understand how human development, societal choices, and the natural world integrate and influence each other. At their simplistic level, they can provide an estimate of the social cost of carbon pollutants. This top-down approach to the economics of climate change has been at the forefront of the discipline [@Vale2016].  Such a global approach may be dating in the face of stalled international coordination to climate change policy.  

In the run-up to the end of the first commitment period of the Kyoto Protocol Agreement (KPA), there were political moves to create second commitment period targets. The Doha amendment in 2012 extended the scope of the protocol targets to cover the period until 2020.  The Doha Amendment was a bridge arrangement up to 2020 until a new global agreement, negotiated in the *Paris Agreement*, comes into force.  There are critical weaknesses to the top-down methodology of the Paris agreement. Critics cite lacking explicit targets, weak legal impunity for emissions targets, and a more explicit international focus as a critical weakness to country policymakers taking direct ownership of their emissions targets.  

As global cooperation seems less attainable in the last decade, attention in the policy debate has shifted towards a bottom-up strategy to mitigation solutions.  @Vale2016 argues that the lack of collective political will, in turn, has seen a shift in the scope of the academic literature. The recent focus on the economics of catastrophic risk insurance, trade and climate, and climate change adaptation represents a shift towards a more realistic investigation of climate policy in an age where the ideal scenario of globally coordinated climate action seems illusory.

## Cost of KPA literature

@Zhang1998 document and critique the myriad of marginal abatement cost models. They consider both bottom-up technology-based models and top-down macroeconomic models. They conclude, a combination of these models best assesses the overall consequences of controlling CO~2~ emissions.  @Nordhaus1999 use a scenario-based approach to analyze the economics of various trading emission schemes (ETS) for Annex I countries for the KPA. They find costs of the ETS's are seven times greater than the benefits, two/thirds of the net global cost of $716 billion, are borne by the US^[@Brechin2003 uses various public opinion polls to revisits the questions of international public concern for global warming. They find, while the perception has been a slight improvement in the public's understanding regarding the anthropogenic causes of global warming, the data reveals the public remains largely uninformed. They note that President Bush's withdrawal of the KPA in 1991 was supported mainly by the US public while citizens of several European countries voiced considerable outrage about the decision.] and conclude that the proposed schemes are highly cost-ineffective^[ Compared to a *so-called" efficient abatement strategy for global temperature reduction, the proposed strategy was eight times more costly.]. 


This early work was suggestive of a broad approach to abatement cost analytics beyond the consideration of CO~2~ pollution.  @Reilly1999 use the Regional Integrated Model of Climate and Economy (RICE) to show that a multi-gas control strategy could significantly reduce the costs of fulfilling the KPA compared with a CO~2~-only strategy.  Extending the KPA to 2100 without more severe emissions reductions shows little difference between the two strategies in climate and ecosystem effects. They argue that the global warming potential of the KPA are limited in terms and argue for a more comprehensive multi-gas approach.  @Burniaux2000 extends previous OECD analysis to emission abatement of methane and nitrous oxide. They conclude that the economic costs of implementing the targets in the KPA are lower than suggested by previous CO~2~-only results. In the longer term, most abatement will likely have to come from CO~2~, and the inclusion of other gases in the analysis may not substantially alter estimates of economic costs.


In the later years of the KPA period, researchers consider a more statistically sophisticated approach for critiquing the KPA.  @Buonanno2003 adapt the RICE integrated assessment model to account for endogenous technical change^ [They explore three formulations; technical change is endogenous and enters the production function via the domestic stock of knowledge; there is an additional effect of domestic stock of knowledge on the emission-output ratio; the output of domestic R&D spills over the other regions' productivity and emission-output ratio.].  and shows that results are significantly impacted when modelling R&D. They find that total costs of compliance with Kyoto;  are higher with induced technical change; are reduced when trading permits are introduced, and technological spillover reduces the incentive for R&D, but overall costs are higher in the presence of spillovers.  @McKibbin2004 update their earlier estimates of the cost of the KPA using the [G-Cubed model](https://unfccc.int/topics/mitigation/workstreams/response-measures/modelling-tools-to-assess-the-impact-of-the-implementation-of-response-measures/response-measures-models-g-cubed), taking into account the new sink allowances from recent negotiations as well as allowing for multiple gases and new land clearing estimates. They perform a sensitivity analysis of compliance costs to unexpected changes in future economic conditions. The paper evaluates the policies under two plausible alternative assumptions about a single aspect of the future world economy: the rate of productivity growth in Russia. They find moderate growth in Russia would raise the cost of the KPA by as much as fifty per cent but would have little effect on the cost of the alternative policy. They conclude that the KPA is inherently unstable because unexpected future events could raise compliance costs substantially and place enormous pressure on governments to abrogate the agreement. The alternative policy would be far more stable because it does not subject future governments to adverse shocks in compliance costs.  @Fischer2006 find that estimates of marginal abatement costs for reducing carbon emissions in the United States by the significant economic-energy models vary by a factor of five, undermining support for mandatory policies to reduce greenhouse gas emissions. Their meta-analysis explains which modelling assumptions are most important for understanding these cost differences and argues for developing more consistent modelling practices for policy analysis. 


More recent studies focus on a bottom-up approach, showing how a country's economic characteristics fluctuate with abatement challenges. @Halkos2014 applies a probabilistic DEA approach to estimate conditional and unconditional environmental efficiency of 110 countries in 2007. They find that a country's environmental efficiency is influenced in a non-linear fashion by both the obliged percentage levels of emission reductions the duration in which a country has signed the KPA.  @Cifci2018 use regression techniques to illustrate the conflicting political strands of the climate change argument. The results show that the KPA agreement reduced Annex I countries GHG emissions by approximately 1 million metric tons of CO^2^ equivalent relative to non-Annex I countries. Contrariwise, these countries experienced an average reduction in GDP per capita growth of 1-2 per cent relative to non-Annex I countries. Both findings illustrate that the international climate change agreements are fragile because, at a national level, political constituencies' value systems may conflict to reduce greenhouse gas (GHG) emissions to sustainable levels. 

# Methodology

The Kyoto Protocol offers a unique empirical framework to assess the effects of explicit target setting in climate change policy.  The first commitment period for the KPA was 2008 to 2012. Countries defined as developing (non-annexe 1) were not subject to targets, although most ratified the ProtocolProtocol.  The US was the only signatory of the ProtocolProtocol that did not ratify.  This decision was likely the combination, a weak green lobby in Washington DC[@Hovi2012], excessive compliance costs[@Manne2004], poor public understanding of climate change [@Brechin2003], and a strong energy lobby during Bush's tenure.^[Andorra, Palestine, South Sudan and the Vatican also do not follow the ProtocolProtocol. Canada ratified but withdrew effective in December 2012.].  In the run-up to the end of the first commitment period, there were political moves to create targets for a second commitment period. Critics argued that the Paris agreement fell well short of the KPA in set explicit targets and punitive penalties.  

For these reasons, we focus on emissions data from 2008 to 2012, the first commitment period.  For this period, it is easier to say definitively who had set targets and who had not.  The lines got blurred post-2012 when a new negotiation phase began.  The ProtocolProtocol set a target for emissions of a basket of greenhouse gases^[carbon dioxide, CO^2^; methane, CH^4^; nitrous oxide, NO^2^; sulphur fluoride, SF^6^; hydrofluorocarbons, HFCs; and perfluorocarbons; PFCs.] to be reached by the signatories in the period 2008-2012.  This paper extends the work of @Halkos2014. To the best of our knowledge, it is the first study to explicitly provide an economic cost for these emissions targets^[@Halkos2014 investigate the overall environmental efficiency impact of the KPA.].

## Data

We specify a two input-two output frontier efficiency model.  Specifically, we define GDP as a desirable output, CO^2^ emissions from fuel combustion as an undesirable output, and labour force numbers and capital stock as inputs.  GDP and labour force numbers are sourced from the World Bank.  The capital stock captures both current and past accumulations of capital investment. Finally, to capture cross-country and time-varying heterogeneity in CO^2^ production, we use several environmental $Z$ variables. Table 1 provides a detailed description of the modelling variables.

```{r variables, results="asis"}
library(WDI)
library(RJSONIO)
VarCodes<-c("NY.GDP.MKTP.PP.KD",
            "SL.TLF.TOTL.IN",
            "NE.TRD.GNFS.ZS",
            'SP.URB.TOTL.IN.ZS')
GDP<-WDIsearch(VarCodes[1],field="indicator",short = F)
Lab<-WDIsearch(VarCodes[2],field="indicator",short = F)
Trade<-WDIsearch(VarCodes[3],field="indicator",short = F)
Pop<-WDIsearch(VarCodes[4],field="indicator",short = F)
info<-as.data.frame(rbind(GDP,Lab,Trade,Pop),row.names = F,stringsAsFactors = F) %>% filter(description!="")

text_tbl<-tibble(Type=c("Undesirable Output","Desirable Output",rep("Input",2),rep("Enviromental Variable",4)),Variable=c("CO2 emissions from fossil fuel (Millions of metric tonnes)",info$name[1:2],"Capital Stock, PPP (constant international $Billions)",info$name[3:4],"Target Setting Indicator","Year Indicators"),Detail=c("
Emissions were calculated using IEA energy databases and the default methods and emission factors given in the 2006 GLs for National Greenhouse Gas Inventories.",info$description[1:2],"Total capital stock is the sum of government capital stock, private capital stock, and public-private partnerships (PPP) capital stock.  When the PPP capital stock is missing we assume zero.",info$description[3:4],"This variable takes a value of 1 for a country which committed to a hard target of emission reduction during the Kyoto Protocol period and zero otherwise.","A proxy for unobserved between group temporal variation"),Source=c("International Energy Agency" ,info$sourceOrganization[1:2],"IMF and World Bank",info$sourceOrganization[3:4],rep("author's own calculation",2)))

kable(text_tbl,booktabs=TRUE,caption = "Description of variables",longtable=T) %>% 
  kable_styling(latex_options = c("repeat_header","hold_position"),font_size=8,full_width=T) %>% 
  column_spec(2, bold = T, border_right = T) %>%
  column_spec(column = c(1:4),width = c("10em","10em","10em","10em")) %>%
  footnote(general="A detailed description of the inputs, outputs and enviromental variables which index the production frontier model.  Capital stock and GDP are monetary variables and enter the model in real terms measured at purchasing power parity or constant international dollar billions.",threeparttable = T)
```


We use the International Energy Association (IEA) database^[http://data.iea.org/payment/products/115-co2-emissions-from-fuel-combustion-2018-edition-coming-soon.aspx] which provides the most extensive global coverage of CO~2~ emission data. This database estimates CO^2^ from fuel emission measured in Metric Tonnes for over 140 countries from 1960 to 2016.  After removing countries with missing observations, we have a balanced sample of 525 observations for 2008-2012.  Table 2 describes the countries in the sample in terms of target-setting.


```{r targetSetters, results="asis"}
ClimatePolicy_anal %>% 
  filter(TargetSet==1) %>%
  dplyr::select(country) %>%
  distinct() %>%
    unlist(use.names = F) %>%
  paste(sep = ",",collapse = " ,")-> v1
ClimatePolicy_anal %>% 
  filter(TargetSet==0) %>%
  dplyr::select(country) %>%
  distinct() %>%
    unlist(use.names = F) %>%
  paste(sep = ",",collapse = " ,")-> v2
  
tibble("Target Setting Country"=v1,
       "Non Target Setting Country"=v2) %>% 
  kable(booktabs=TRUE,caption = "Target Setting Countries") %>% 
  kable_styling(font_size = 8,latex_options = c("hold_position"),full_width=T) %>%
  column_spec(c(1,2), bold = T, border_right = T) %>%
  column_spec(c(1,2),width = c("20em","20em")) 
```

Some summary statistics of the model variables are presented in Table 3. These statistics reveal that significant variation in outputs and inputs highlights considerable cross-sectional heterogeneity.  The variation is not surprising given the mix of countries outlined in table 2.  Furthermore, notice that some countries have a trade which exceeds GDP (more than 100%).  This excess is usually a feature of small countries with high productivity. Due to their small size, instead of being self-sufficient and producing all the products their population needs, they specialize in a few highly profitable industries. These industries may produce more money from exports than the entire domestic economy, which allows them to purchase imports far above what their domestic economy could otherwise support.  For example, in the sample, three countries have a Trade to GDP ratio of over 200%; Luxembourg, Malta and Singapore.    

\newpage

```{r sumstats1, results="asis"}
sumThatFinal<-function(x){
  p5=quantile(x,na.rm = T,0.05)
  Median=median(x,na.rm = T)
  Mean=mean(x,na.rm = T)
  SD=sd(x,na.rm = T)
  p95=quantile(x,na.rm = T,0.95)
  round(c(p5,Median,Mean,SD,p95),2)
}

sumStats<-ClimatePolicy_anal %>% 
  as_tibble() %>%
  dplyr::select(CO2,y1,x2,x1,z1,z2,z3) %>% 
  group_by(z3) %>% 
  summarise(across(.fns = sumThatFinal)) %>% t


sumStats<-sumStats[-1,c(1,6,2,7,3,8,4,9,5,10)]
rownames(sumStats)<-c("C02 emissions (Million Metric Tons)","GDP (Billion PPP $USD)","Labour Force (Millions)","Capital Stock (Billions PPP $USD)","Urban to Total Population (%)","Trade to GDP (%)")

sumStats %>% 
  kable("latex",booktabs=T, longtable=T,caption = "Summary statistics of inputs, outputs and z variables",align = "c",row.names =T,col.names =rep(c("Non Target","Target"),5))  %>% 
  kable_styling(latex_options =c("hold_position","scale_down"), full_width=T) %>% 
  add_header_above(header =c(" "=1,"5th%ile"=2,"Median"=2,"Mean"=2,"StdDev"=2,"95th%ile"=2)) %>%
  column_spec(1,width="10em") %>%
 footnote(general="This table provides central tendency and spread statistics for the model variables for the sample period by target setting groups. Variables are presented on the measurement basis with which they enter the model, for example Capital stock enters the model in constant $Billions.",threeparttable=T)
```

## Model
The primary focus of our analysis is shadow prices estimates for CO~2~ emission from fossil fuel. Previous studies have provided inaccurate measures as a result of several missteps, including: 

- only considering downscaling of production and not increasing in input use.
- measuring estimates on the frontier, ignoring the actual level of performance.
- deterministic estimation, which explicitly ignores the impact of noise in the data.

These factors combine to overestimate shadow prices, group differences in shadow prices grossly, and the impact of emissions reduction targeting. Our study uses convex quantile regression methods to estimate local approximations of shadow prices calibrated using observed inefficiencies.  Specifically, we exploit the benefits of @Kuosmanen2017 directional distance convex regression and @Wang2014 quantile formulation to reveal shadow prices at observed performance levels. Importantly, this approach is robust to the observed heterogeneity, the choice of direction vector and accommodates noise-based uncertainty. The following linear programming problem is solved to estimate the distance function:

\begin{equation}
\begin{split}
& \underset{\alpha,\beta,\gamma,\delta,\epsilon^-,\epsilon^+}{\text{min}}
 (1-\tau) \sum^{T}_{t=1} \sum^{n}_{i=1}\epsilon^-_{it} + \tau \sum^{T}_{t=1}  \sum^{n}_{i=1}\epsilon^+_{it}  \\
&\text{s.t.} \\
&\gamma^{'}_{it}y_{it}=\alpha_{it}+\beta^{'}_{it}x_{it}+\delta^{'}_{it}b_{it} + \omega Z_{it} -\epsilon^-_{it}+\epsilon^+_{it} \; \forall i ,\forall t \\
&\alpha_{it}+\beta^{'}_{it}x_{it}+\delta^{'}_{it}b_{it}-\gamma^{'}_{it}y_{it} \leq \alpha_{hs}+\beta^{'}_{hs}x_{it}+\delta^{'}_{hs}b_{it}-\gamma^{'}_{hs}y_{it} \; \forall i,h ; \forall t,s \\
& \beta^{'}_{it}g^x+\delta^{'}_{it}g^b+\gamma^{'}_{it}g^y=1 \; \forall i,t\\
& \beta_{it} \geq0,\gamma_{it} \geq0,\delta_{it} \geq0 \; \forall i,t \\
& \epsilon^-_{it} \geq0, \epsilon^+_{it} \geq 0 \; \forall i,t
\end{split}
\end{equation}

Equation 1 is a probabilistic distance function, where the two errors terms ($\epsilon^-$ and $\epsilon^+$) allow for deviations from the frontier, and $\tau$ defines the quantile.  We estimate the model using a balanced panel of 105 countries for five years (2008-2012) where the $Z$ vector includes, trade to GDP ratio, the percentage of the population which is urban, a dummy to the indicator if the country is a target setting, and a set of year dummies.  These environmental variable adjust for observed cross-country and through time fluctuation in the production technology. The estimated model results in performance adjusted dual prices $\gamma^{'}_{it},\beta^{'}_{it} ,\delta^{'}_{it}$ which serve as inputs for the marginal abatement calculations. An additional appealing feature of the specification in equation 1 is a separately estimated intercept for each observation; $\alpha_{it}$. These intercept terms are analogous to *random* effects in multi-level modelling, capturing unobserved time series and cross-sectional variation. 

### Marginal abatement

Marginal abatement estimation uses a series of levels to find the local quantile $\tau^{*}$ for each observed data point. For example, a set of ten quantile levels $\tau=(0.05,0.15,0.25,0.35,0.45,0.55,0.65,0.75,0.85,0.95)$^[We use the GAMS software and the CPLEX solver to find an optimal solution to equation 1]. In general, the number of quantiles is not fixed but should depend on sample size and signal to noise ratio. 

@Kuosmanen2018b note that in the traditional approach to shadow pricing using frontier estimation, marginal abatement costs and shadow prices are interchangeable terms. This feature is because previous approaches only use bad output shadow prices measured in forgone *good* output units.  They expand the marginal abatement cost definition to include incremental use of inputs by considering an optimal combination of shadow price definitions:

1. The marginal rate of transformation between *good* and *bad* outputs (MRT).  
2. The marginal product of inputs on outputs (MP).

In our study, we similarly calculate marginal abatement costs as:

1. Find the largest expectile ($\tau^{*}$) for which the residual ($\epsilon^+ + \epsilon^-$) is non-negative.  

  * For most observations, we find the nearest expectile by checking where the residual $\epsilon = (\epsilon^+) - (\epsilon^-)$ changes sign. For those observations, we take the weighted average of the shadow prices of the nearest executives, weighted by $\epsilon$. For some observations, residuals are positive (or negative) for all executives (the best and the worst performers, respectively). For those, we use shadow prices of the highest/lowest expectile.
 
2. Calculate MRT and MP as the weighted average of quantiles for ($\tau^{*}$)r and ($\tau^{*+1}$) weighted by the distance to the frontier of the quantiles(i.e. the absolute value of the residuals).  Specifically these can be thought of as the sub derivatives with respect to the bad outputs from the distance function, where the marginal rate of substitution of output $i$ on bad output $j$ is $MRT_{\tau}(y_{i},b_{j})=-\frac{\delta \vec{D}_{\tau}/\delta b_{j}}{\delta \vec{D}_{\tau}/\delta y_{i}}$. Similarly the MP of input k on bad output j is $MP_{\tau}(x_{k},b_{j})=\frac{\delta \vec{D}_{\tau}/\delta b_{j}}{\delta \vec{D}_{\tau}/\delta x_{k}}$

3. Use the results from step 2, the marginal abatement cost (MAC) for bad output $j$ is define as:
\begin{equation}
MAC(b_{j})=\displaystyle \min_{i,k}\{p_{i}MRT_{\tau}(y_{i},b_{j}), w_{k}MP_{\tau}(x_{k},b_{j})\}
\end{equation}

In equation 2 $p_{i}$ is the price of output $i$ and $w_{k}$ is the price of input $k$.  This flexible definition of the MAC provides multiple opportunities for abatement. Specifically, bad output $j$ can be abated by either reducing *good* outputs (i.e., downscaling the GDP activity) or increasing the input use (for example, investment in the labour force or capital stock).  This approach uses the least-cost alternative.  In the case where the *good* outputs possess a monetary value, the sub derivatives (dual prices) provide monetary shadow prices for bad outputs, and the above equation simplifies to: 

\begin{equation}
MAC(b_{j})=\displaystyle \min_{i,k}\{MRT_{\tau}(y_{i},b_{j}), MP_{\tau}(x_{k},b_{j})\} 
\end{equation}

In the above calculation, it is essential to ensure that the MRT and MP enter the model simultaneously, given the scale of the inputs and outputs entering the model.  In our model, as both capital stock and GDP enter the model in billions of dollars, the MRT and MP are directly comparably in terms of minimum cost.

## Results and discussion

We estimate a stepwise yield curve of probabilistic benchmark technologies. These technologies extract, at the observed performance level, country-year marginal abatement costs of CO~2~ emissions. We use the direction vector $g(x)=\bar{x}, g(y)=0, g(x)=0$ to estimate the directional distance function model for 10 quantiles $\tau=(0.05,0.15,\dots,0.85,0.95)$, which should be sufficient granularity for a sample size of 525.  Finally, we include a noise term that can capture measurement error in the data.

```{r SPs2, results="asis"}
ClimatePolicy_anal %>% 
  group_by(year) %>%
  summarise("MAC Overall"=mean(MAC,na.rm = T),
            "IQR"=IQR(MAC,na.rm = T),
            "MAC Target Setters"=mean(MAC_T,na.rm = T),
            "IQR1"=IQR(MAC_T,na.rm = T),
            "MAC Non Target Setters"=mean(MAC_No_T,na.rm = T),
            "IQR2"=IQR(MAC_No_T,na.rm = T)) %>%
  kable("latex",digits = 2,caption = "Marginal abatement costs in 2011 dollars per CO2 tonne",align = "c",booktabs=T,col.names = c("",rep(c("Mean","Interquartile Range"),3)),longtable=T) %>%
  kable_styling(latex_options =c("hold_position","scale_down"), full_width=T) %>%
  add_header_above(c("","Full Sample"=2,"Target Setting Countries"=2,"Non Target Setting Countries"=2),bold = T,italic = T) %>%
  row_spec(0,bold = T,italic = T)  %>%
  footnote(general="This table presents yearly mean and interquartile estimates of the marginal abatement costs calculated for the full sample and for each group of countries.  The last four columns disaggregates the mean analysis to compare countries which set emission reduction targets against countries which did not.  GDP(y) and capital stock (x1) are deflated to 2011 international dollars, and are considered to have a unit price.  An alternative interpretation is that the the price multipliers (p,w)=1 in the calculations of the marginal rate of transformation of GDP and the marginal product of the capital stock as they represent both quantity and price. The marginal abatement cost is thus calculated as the minimum of the marginal rate of transformation of GDP and the marginal product of capital stock on C0~2~ emissions.  The MAC is measured in USD per metric ton of CO~2~ emission. Labour (x2) is the total labour force in each country (in millions).  The marginal product of labour is the dual without a price multiplier and is measured in millions of labour force per ton of CO2 emission.",threeparttable = T)
```


Table 4 summarises the marginal abatement cost estimates for each year in our sample period. This table presents the mean and interquartile range for the entire sample, targeting setting countries and their non-target setting counterparts.  Marginal abatement costs illustrate the carbon intensity, where countries with larger manufacturing sectors will have relatively higher MAC estimates.  The MAC estimates are similar to those reported in the literature [@Lee2019;@Bohringer2003;@Viguier2003]. , and comparatively similar to the cost of C0~2~ capture and storage of coal plants estimated by @Rubin2015, who estimates a mitigation cost (constant 2013 dollar per metric tonne of CO~2~) for the capture of *46-99 US dollars* and storage of *53-137 US dollars*.  

### Carbon emissions pricing comparison

There is a common theoretical starting point for *carbon emissions* pricing and *carbon shadow* pricing, a sufficiently high emissions price for imposing zero emissions that cause global warming. An appropriate carbon pricing regime should treat these two options as mutually reinforcing.  Carbon emission pricing being where policymakers add a carbon component to the current market price of pollutants, Shadow pricing, which ascertains a *future price* of the actual economic cost of a climate-relevant project.  Both have a real-world impact in that they drive markets towards factoring in long-term impacts. In practice, the pricing schemes diverge due to political inconvenience and inadequate multilateral commitments.

Since the introduction of the KPA, emission pricing schemes are political motivators to state actors, where it is politically inconvenient to increase such tax in line with climate impacts.  While efforts such as the EU emission trading scheme, introduced in 2008 for major industrial facilities, it has been shown to only cover about 40% of the European greenhouse-gas emissions^[https://www.dandc.eu/en/article/why-carbon-emissions-pricing-and-carbon-shadow-pricing-both-make-sense#:~:text=Appropriate%20carbon%20prices&text=%E2%80%9CCarbon%20emissions%20pricing%E2%80%9D%20means%20that,not%20reflect%20those%20impacts%20yet.].  In contrast, shadow pricing essentially bypasses national governments, as it is in commonly used by multilateral development banks.  At present, only projects in emerging and developing countries routinely apply shadow pricing [@Luhmann2020].  The approach essentially adopted here is a social value of carbon (SVC). The @CPLC2017 commission report established an SVC shadow price range necessary to achieve the Paris temperature target as \$40-80/tCO~2~ by 2020, and \$50-100/tCO~2~ by 2030.


```{r ETSprice, fig.cap="Daily ECX EUA Futures Price", eval=FALSE}
# This is the ICE ECX EUA Future price which is a proxy  
library(Quandl)
library(latex2exp)
Quandl.api_key("xgqqrhAtsi5zPtXvnrxp")
USD_EURO<-Quandl("CUR/EUR", start_date="2008-01-01", end_date="2012-12-3")
pr<-Quandl("CHRIS/ICE_C1",start_date="2008-01-01",end_date="2012-12-31")

pr_usd<-left_join(pr %>% dplyr::select(Date,Settle),USD_EURO %>% rename(Date=DATE),by="Date") %>%
  mutate(pr_usd=Settle*RATE)

pr_usd %>%
  ggplot(aes(x=Date,y=pr_usd)) + 
  geom_line(colour="red")



+
  annotate("text",x=as.Date("2010-07-01"),y=31,label=paste0("Hi =",max(pr$Settle)))+
  annotate("text",x=as.Date("2010-07-01"),y=29,label=paste0("Lo =",min(pr$Settle))) +
  ylab(TeX("\text(Price in) /tCO~2)")) +
  xlab("")
```


```{r ETS comparision,fig.cap="ETS pricing mechanism comparison"}
library(readxl)
library(dplyr)
read_excel("CPI_Data_DashboardExtract.xlsx",
           sheet = "Data_Price",
           skip = 2,na="N/A") ->df
df <- df[,colSums(is.na(df))<nrow(df)]
df %>% 
  dplyr::select(-`Jurisdiction Covered`,-`Instrument Type`) %>%
  gather(Year,Price_USD_2011,-`Name of the initiative`) %>%
  mutate(Year=as.numeric(str_replace(Year,"Price_rate_1_","")),
         Price_USD_2011=as.numeric(Price_USD_2011)) %>%
  rename(Initiative="Name of the initiative") %>%
  filter(Year>=2008 & Year<=2014) %>%
  rbind(
  ClimatePolicy_anal %>% 
  group_by(year) %>%
  summarise("Shadow Price KPA target-setters"=mean(MAC_T,na.rm = T),          
            "Shadow Price KPA non- target setters"=mean(MAC_No_T,na.rm = T)) %>% rename(Year=year) %>%
    gather(Initiative,Price_USD_2011,-Year)) %>%
  ggplot(aes(x=as.character(Year),y=Price_USD_2011,fill=Initiative)) + geom_col(position="dodge") + 
  labs(x="", y="US$/tCO2e",fill="Pricing Mechanism",caption = "Source: World Bank Carbon Pricing Dashboard") +
  coord_flip() +
  theme(text = element_text(size=20),
    legend.text = element_text(size=12)) +
  scale_fill_brewer(type = "qual")
```

Figure 1 compares the 2011 nominal Carbon Prices from the seven of the largest emission trading schemes (ETS) to our shadow price mean estimates. In the first three years of the KPA, the shadow price for both groups (target setting and non-target setting) is a multiple of the prices from existing ETS markets. Interestingly, The Saitama ETS^[The Saitama target setting type ETS is a baseline-and-credit system that sets mandatory emission reduction targets for large buildings and factories. It was established in April 2011 as part of the Saitama Prefecture Global Warming Strategy Promotion Ordinance and is linked to the Tokyo CaT.] carbon pricing mechanism introduced in 2011 provides comparable estimates to our analysis. This mispricing is an encouraging sign that our *least-cost alternative* approach may be more in line with market expectations. That said, Saitama ETS only covers a small fraction of global annual greenhouse gases emissions compared to the EU ETS (5% per annum approximately^[https://carbonpricingdashboard.worldbank.org/map_data]) which may be indicative of an illiquid pricing mechanism.
  
For an emissions trading scheme to work efficiently, allocation of abatement across countries would require that the marginal abatement cost is the same in all countries and over time. The results from table 4 suggest this is not the case. The mean MAC is trending up in both groupings and is typically most significant for target setting countries. Furthermore, the various regional ETS pricing mentioned above, which allows firms from different countries to buy and sell CO~2~ emission allowances to achieve an efficient allocation of abatement, are not working to lower the marginal abatement costs of the period. This visual argument suggests a consistent misallocation of CO~2~ abatement across countries and significant frictions in ETS market price discovery.

### Marginal effect of the environmental variable
Following @Gallagher2019, we investigate the marginal effect of the environmental variables in equation 1 to understand how they impact inefficiency.  Specifically, we consider how inefficiency is affected by the proportion of trade to GDP, the percentage of the urban living in a country's population, and whether the country explicitly sets CO~2~ emission targets in the analysis period. 


```{r reg,results="asis"}
ClimatePolicy_anal %>% mutate(logGDP=log(GDP),
                   logx=log((Capital+LABOUR+CO2)/3+1),
                   regY=logGDP-logx,
                   Yr=as.factor(year),
                   Setter=as.factor(TargetSet)) %>%
  dplyr::select(regY,TRADEtoGDP,URBAN,Setter,Yr,country) ->df_anal

lm(regY~TRADEtoGDP+URBAN+Setter+Yr,data=df_anal) %>%
  tidy() %>%
  # mutate(estimate=exp(estimate))
  slice(-1) %>%
  kable("latex",digits = 3,caption = "Marginal effect of enviromental variables",align = "c",booktabs=T,longtable=T) %>%
  footnote(general="This table shows the marginal effects from the z coefficients in equation (1) by exploiting statistical procedure first outlined in Kuosmenan & Johnson (2015).",threeparttable = T) %>%
  kable_styling(full_width = T)
```

The results from table 5 reveal some interesting features of the inefficient patterns at the country level.  Typically, those with higher trade to GDP ratios and higher urban populations tend to be less efficient over the sample. Interestingly, those countries which are setting targets tend to be more inefficient in the sample period. Finally, there is an overall reduction in inefficiency over the period indicated by the year dummies, although this relationship is not significant in the data.  

\newpage

# Concluding Remarks

This study provides substantial evidence to the ongoing debate on target setting implications in climate policy. We use a unifying frontier efficiency approach that reveals some essential and economically meaningful CO~2~ emissions target setting implications.  The study exploits the explicit target setting period of the Kyoto Protocol to reveal unintended consequences in terms of increased inefficiencies and marginal abatement costs.

The results reveal important implications for emissions trading schemes. For an emissions trading scheme to work efficiently, allocation of abatement across countries would require that the marginal abatement cost is the same in all countries and over time. Table 4 shows a substantive difference across the groups, with the mean MAC increasing over time and typically more significant for target setting countries. 

Furthermore, the various regional ETS carbon price discovery mechanisms, which allow firms from different countries to buy and sell CO~2~ emission allowances to achieve an efficient allocation of abatement, are not working to lower the marginal abatement costs of the period. Our chronological ordering analysis suggests a consistent inefficient allocation of CO~2~ abatement across countries and significant frictions in ETS market price discovery.

Finally, marginal effects estimate of the environmental variables suggests that setting the explicit emissions targets result, having higher trade and more urbanisation typically induces more environmental inefficiency.

Taking together, our results add value to the regulatory economic analysis toolbox, by providing a coherent means to investigate statistically meaningful differences in regulating climate change and the price discovery markets for pollutants.

\newpage 

# Appendix

## A.1: Shadow price group difference testing

We illustrate our test using a cost function but argue it can be generalised to any production technology specification. @Fare2012 prove, using duality theory, that production technologies are validly represented by either a cost function, the conventional production function, or a distance function. The cost function is defined as:
\begin{equation}
C(x,y)=\text{min} \{wx:\text{input x can produce output y}\}
(\#eq:CostFn)
\end{equation}
where $x$ is the input vector, $w$ is the vector of input prices, and $y$ is the vector of $M$ outputs.  To estimate the cost function from data, we assume a cost frontier model:
\begin{equation}
X=C(x,y) + \epsilon
(\#eq:CostFront)
\end{equation}
where $X$ is the observed cost and $\epsilon$ is a random disturbance term. The partial derivative of $C$ with respect to output $m$ is referred to as the shadow price of output $m$ (in other words, the marginal cost). The vector of all $M$ shadow prices is called the gradient vector and is denoted by $VC$.  Figure 2 illustrates the output isoquant in the case of two firms, where the gradient vector $VC$ includes two shadow prices illustrated by the dashed lines. The shadow prices define the slope of the tangent line on the output frontier. 

```{r isoquant, fig.cap=" Two output isoquant", fig.height=5,fig.width=8}
library(ggforce)
arrow = arrow(angle=15, type = "closed")
tibble(y1=c(2,3),y2=c(3,2)) %>% # some data to plot
  ggplot(aes(y1,y2)) +
  # plot point to represent firms
  geom_point(lwd=3, shape=21) + 
  # add a curve to represent frontier
  geom_curve(aes(x=0, y=5,yend=0,xend=5),lwd=2,curvature =-0.4) + 
    # label angle A
  annotate("text",label="A",x=1.25,y=0.25) +
  # label firms
  annotate("text",label="Firm 1",x=2.5,y=3) +
  annotate("text",label="Firm 2",x=3.5,y=2) +
  # theme_classic() +
  xlab("y1") +ylab("y2") + 
  # add radial lines
  geom_segment(aes(x=0,y=0,xend=2.7,yend=4.05),linetype="solid",colour="red") + 
  geom_segment(aes(x=0,y=0,xend=4.05,yend=2.7),linetype="solid",colour="blue") +
  # fixes axes to have the same scale
  coord_fixed(xlim = c(0,6),ylim=c(0,6)) + 
  # allows the graph to start at the origin
  scale_x_continuous(expand = c(0, 0)) + scale_y_continuous(expand = c(0, 0)) +
  # Tangents to represent shadow prices
  geom_segment(aes(x=1.5,y=5.0,xend=4,yend=3.3),linetype="dashed",colour="red") + 
  geom_segment(aes(x=5,y=1.5,xend=3,yend=4.5),linetype="dashed",colour="blue") +
  # curve for angle
  geom_curve(aes(x=1,y=0,xend=0.75,yend=0.5),linetype="dashed",curvature =0.1,colour="blue") +
  # Axis arrows
  # geom_segment(aes(x=0,y=0,xend=0,yend=6),lwd=2,arrow = arrow()) + 
  # geom_segment(aes(x=0,y=0,xend=6,yend=0),lwd=2,arrow = arrow()) +
  theme(axis.ticks = element_blank(), 
        axis.text = element_blank(), 
        axis.line = element_line(arrow = arrow,size = 2),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.background = element_blank())
```

\begin{footnotesize} 
The figure represents a two output production model, where the black arc line is the best practice output frontier. Firm one and Firm two are operating below the frontier and are inefficient. These firms can improve their production efficiency (move towards the output frontier) by simultaneously producing more y2 and y1 for a given level of inputs (costs). Efficiency for each firm is the length of the dashed lines from the origin, the radial distance.  The dashed tangents on the output frontier represent the shadow prices for the firms.
\end{footnotesize} 

From figure 2, it is easy to see that the shadow prices depend on both the curvature of the output isoquant and the output mix, which the ratio y2/y1 can measure. Note that tan A = y2/y1, where A is the angle indicated in figure  2. Note further that the shadow prices depend on this angle (the polar coordinates), **not** the distance to the frontier. Proportional scaling of all outputs by some arbitrary constant along the dashed rays from the origin does not affect the shadow prices.  

What if we have empirically observed a change in the shadow prices (via some regulatory or supervisory shock), and our objective is to test whether this change is statistically significant?  If the output isoquant is held constant, the shadow prices can only change due to a change in the output mix y2/y1. Therefore, we can test if there is a significant change in the output mix. Note that the ratio y2/y1 is entirely independent of the estimation of the frontier. Therefore, the test is immune to possible serial correlation in the finite sample estimates of the shadow prices.  Some standard approaches to testing the significance of the changes in the distribution of y2/y1 are reviewed in the next section^[For completeness, it is worth noting that if the output isoquant is linear (outputs are perfect substitutes in production), then the shadow prices do not change even if the output mix changes. We could test if the curvature of the output set is significant (i.e., if there are significant economies of scope) by comparing the linear and convex regression (see @Meyer2003, for details), but this is not our primary objective. Instead, we are interested in the effect of a change in the regulatory and supervisory environment on shadow prices. This effect can only occur through the change in the output mix]. 

Regulation can influence the output allocation, but not the economies of scope or the shape of the production possibility set. @Zhou2014 argues that a genuine objective of a production unit in the presence of the introduction of a regulatory abatement target is to reduce their undesirable output to the target level. If there is an external abatement target, the producer primarily focuses on achieving that target emissions level.  After attaining this target, the economic objective of the producer is to maximise the production of the desired output to maximise profit.  Thus, this external regulatory shock changes the output allocation mix of desirable output to undesirable output but not the shape of the production possibility set.  

As a practical example, consider a regulatory shock that imposes a new supervisory framework on a regulated system.  In the efficiency literature, regulatory externalities impose technological shifts to the best-practice frontier technology (the solid line in figure 2).  If  Hicks neutrality can be assumed, the effect on the frontier is a parallel shift where the shape of the production possibility set remains unchanged.  

## Application of Statistical Test
Suppose we have two series of the output ratio y2/y1, representing two groups of firms observed in the same period or the same sample of firms observed in two different periods. There are several methods for testing whether the two series are significantly different. 

An obvious possibility is to apply a two-sample t-test for testing the equality of means or the F-test for equal variances. This test requires either that sample size is sufficiently large for asymptotic inferences or that the ratio y2/y1 is normally distributed. 

There are also several nonparametric alternatives. The (Wilcoxon) Mann-Whitney U tests whether the medians of two independent distributions are different. Another possibility is the two-sample Kolmogorov-Smirnov test. If there is a pair of series(e.g., the same firms observed in two different periods), then nonparametric rank-order tests such as Spearman's rho and Kendall's tau can be used to test for correlation between two series of y2/y1.

### Testing procedure {#TestSteps}

There are three steps to the testing procedure for the difference in the ratio series y2/y1.  The first two steps are preliminary in that they establish the statistical properties of the series, which informs the choice of group difference test in the three-step.

1. Test the empirical distribution of the series for normality.  Whether the series is normally distributed determines whether a parametric or nonparametric test is needed. @Stephens1986 recommend the use of a normality test introduced by @Anderson1952 @Anderson1954. This procedure is a rank-sum test for goodness of fit based on the empirical distribution and has the advantage of giving more weight to the tails of the distribution.  

2. Test the homogeneity of variance in the two groups. If step 1 establishes normality, a simple F test of the homogeneity of variance can be performed.  In the presence of non-normality, we turn to the @Brown1974 test, which extended the @Levene1961 ANOVA procedure applied to absolute deviations from the corresponding group mean. This Brown-Forsythe test transforms the variances into the absolute values of their deviations from the median. It uses a ratio of this transformed data as test statistics (See @OBrien1981 for full explanation).

3. If the equal group variance and the normality assumptions are not rejected, then perform a Welch t-test for group mean differences [@Welch1947].  The Kolmogorov-Smirnov nonparametric test provides a more robust statistical inference [@Conover1999]. If only the normality assumption is rejected, the Wilcoxon Mann Whitney test is more appropriate. 

## Application of testing

Our statistical shadow price difference test is based on the underlying data for frontier efficiency. Specifically, it is the ratio of the corresponding bad output to either good output or input that is represented in a shadow price estimate. For example the ratio of CO^2^ emissions to GDP could be used to test statistical differences in the shadow price of the good output calculated as $MRT_{\tau}(y_{i},b_{j})=-\frac{\delta \vec{D}_{\tau}/\delta b_{j}}{\delta \vec{D}_{\tau}/\delta y_{i}}$.


```{r Test1, results="asis"}
library(pander)
# LeveneTest only works with factor variables in the formula
ClimatePolicy_anal$Target<-as.factor(ClimatePolicy_anal$TargetSet)
ClimatePolicy_anal <- ClimatePolicy_anal %>% as_tibble
StatTest<-function(df,x,netput,grp,start,end) {
  p<-c(start:end)
  out<-matrix(NA,nrow =length(p),ncol =8)
  for (i in p) {
    dftest<-subset(df,year==i)
    j=i-2007
    # Allows the function inputs to call the netput variable
    dftest$ratio<-eval(substitute(x),dftest)/eval(substitute(netput),dftest)
    TR<-dftest[eval(substitute(grp),dftest)==1,]$ratio
    CO<-dftest[eval(substitute(grp),dftest)==0,]$ratio
    # Allows grp input to be called as a string
    f<-as.formula(paste0("ratio","~",deparse(substitute(grp))))
    
      out[j,1:4]<-c(ad.test(dftest$ratio)$statistic,
                    leveneTest(f,dftest,center=mean)$`F value`[1],
                    kruskal.test(f,dftest)$statistic,
                    ks.boot(TR,CO)$ks$statistic)
      
      out[j,5:8]<-c(ad.test(dftest$ratio)$p.value,
                    leveneTest(f,dftest,center=mean)$`Pr(>F)`[1],
                    kruskal.test(f,dftest)$p.value,
                    ks.boot(TR,CO)$ks$p.value)
  }
  # To produce a more statistical output required the additional of "stariness" to the out matrix using the pander function add.significance.stars
out<-matrix(paste0(as.matrix(round(out[,c(1:4)],2)),as.matrix(add.significance.stars(out[,c(5:8)]))),nrow=nrow(out))
rownames(out)<-p
out
}

StatTest(ClimatePolicy_anal,CO2,LABOUR,Target,2008,2012) %>% 
kable(caption = "Statistical Analysis of Marginal Abatement Cost Differences",align = "c",booktabs=T, col.names=c("Normality Test","Equality of Variance","Rank sum z-test","Equality of distribution D-test")) %>%  kable_styling(latex_options =c("HOLD_position")) %>% add_header_above(header = c("","Preliminary"=2,"Group Difference "=2),italic = T,bold = T) %>%
  footnote(general="This table provides the group difference statistical testing on the ratios of the bad output with either GDP and Capital depending on which corresponding shadow price satisfying MAC equation. Column 1 presents the Anderson Darling test which is the recommended empirical distribution test for normality by Stephens (1986) as it as it gives more weight to the tails of the distribution.  If normality is rejected, we perform heterogeneity of variance tests. The Brown-Forsythe test is presented in column 2 and is robust in the presence of non-normal data. Finally, columns 3 and 4 present the non-parametric group differenc tests.",threeparttable = T)

```

Table 6 shows the results of the testing approach described in test steps applied each year to the ratio of the variables represented by the MAC estimates. The first column presents the test results of the empirical distribution of the ratio and shows that normality is rejected for all years.  This result means we should use a nonparametric group difference test.  Column 2 presents the equality of variance test across the groups of interest, robust to non-normal distribution.  Equality of variance is not rejected for all years. Columns 3 and 4 of Table 6 provide a statistical analysis of the observed mean differences in shadow prices presented in table 4. In column 3, the Wilcoxon Mann Whitney test provides robust inference when we cannot reject the hypothesis of equality of variance in groups assessed in column 2. The  Kolmogorov Smirnov test provides robust inference if the equality of variance hypothesis is rejected. Given the results of column 2, column 3 results suggest a statistically significant difference in the shadow prices of the two cohorts.  This finding provides some meaningful evidence that target setting countries consistently experienced increased abatement costs over the Kyoto protocol period.


\newpage

# References